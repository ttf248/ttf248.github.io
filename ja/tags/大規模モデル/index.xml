<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>大規模モデル on 向叔の手帳</title>
        <link>https://ttf248.life/ja/tags/%E5%A4%A7%E8%A6%8F%E6%A8%A1%E3%83%A2%E3%83%87%E3%83%AB/</link>
        <description>Recent content in 大規模モデル on 向叔の手帳</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>ja</language>
        <lastBuildDate>Wed, 16 Jul 2025 21:19:42 +0800</lastBuildDate><atom:link href="https://ttf248.life/ja/tags/%E5%A4%A7%E8%A6%8F%E6%A8%A1%E3%83%A2%E3%83%87%E3%83%AB/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>最近の大規模言語モデルの利用経験について</title>
        <link>https://ttf248.life/ja/p/recent-usage-experiences-of-large-models/</link>
        <pubDate>Wed, 09 Jul 2025 21:09:01 +0800</pubDate>
        
        <guid>https://ttf248.life/ja/p/recent-usage-experiences-of-large-models/</guid>
        <description>&lt;p&gt;現状では、どの大規模言語モデルも特によくとはなく、各社それぞれに得意な分野や活用シーンがあります。&lt;/p&gt;
&lt;h2 id=&#34;技術ドキュメント&#34;&gt;技術ドキュメント
&lt;/h2&gt;&lt;p&gt;コードの提供、または IT 技術に関する質問：ChatGPT と Gemini&lt;/p&gt;
&lt;h2 id=&#34;コーディング&#34;&gt;コーディング
&lt;/h2&gt;&lt;p&gt;要件を整理し、コードの修正を要求する：Claude&lt;/p&gt;
</description>
        </item>
        <item>
        <title>ブログ翻訳プロジェクトの雑感：歴史対話</title>
        <link>https://ttf248.life/ja/p/blog-translation-project-musings-historical-conversations/</link>
        <pubDate>Mon, 02 Jun 2025 21:16:24 +0800</pubDate>
        
        <guid>https://ttf248.life/ja/p/blog-translation-project-musings-historical-conversations/</guid>
        <description>&lt;p&gt;ブログ翻訳プロジェクトは当初、複雑に設計されていた——まずMarkdown形式を解析し、プレースホルダーでコンテンツを保護し、最後に大規模言語モデルに送信する仕組みだった。これは完全に無駄であり、大規模言語モデル自体がMarkdownの文法を認識する能力を備えており、元のコンテンツを直接処理し、翻訳時にフォーマットを維持することができたからだ。&lt;/p&gt;
&lt;p&gt;私たちの仕事は、コードのデバッグから、大規模言語モデルの&lt;strong&gt;プロンプト&lt;/strong&gt;のデバッグへと変わった。
モデル：&lt;code&gt;google/gemma-3-4b&lt;/code&gt;
ハードウェア：&lt;code&gt;Nvidia 3060 12GB&lt;/code&gt;
そう、思考しないモデルを選んだ。思考するモデルは翻訳タスクを実行する際に効率が低く、4Bパラメータと12Bパラメータの効果を比較したところ、翻訳タスクにおいてはgemma3の4Bパラメータで十分だった。12Bパラメータは翻訳タスクにおいて明確な利点を持っていなかった。
12Bパラメータの速度：&lt;strong&gt;11.32 tok/sec&lt;/strong&gt;、4Bパラメータの速度：&lt;strong&gt;75.21 tok/sec&lt;/strong&gt;。&lt;/p&gt;
&lt;h2 id=&#34;背景説明&#34;&gt;背景説明
&lt;/h2&gt;&lt;p&gt;システムに様々な条件制限を加えても、出力される翻訳結果には依然として問題が発生することがありました。具体的には、フォーマットの保護が不十分であったり、過剰な説明文が含まれていたりしました。役割定義時には、Markdown形式を保護し、翻訳結果のみを出力することを明示していたにも関わらず、最終的な翻訳は不安定でした。&lt;/p&gt;
&lt;p&gt;その時、以前漫画翻訳プロジェクトで大言語モデルを活用した経験が思い出されました。その時の翻訳精度は、私のものより良かったようです。コードやリクエストデータを確認したところ、漫画翻訳プロジェクトでは、毎回リクエストにコンテキスト（文脈）を付与していました。現在の翻訳内容に加え、過去の翻訳内容もまとめて送信していたのです。&lt;/p&gt;
&lt;p&gt;このメリットは何でしょうか？前後の翻訳の一貫性を高めるだけでなく、出力フォーマットの安定性を確保することにもつながったと考えられます。&lt;/p&gt;
&lt;h2 id=&#34;履歴対話の重要性&#34;&gt;履歴対話の重要性
&lt;/h2&gt;&lt;p&gt;AI 大規模モデル（GPT シリーズ、Claude、Gemini など）の普及に伴い、ますます多くの企業や開発者が API を通じてこれらのモデルにアクセスし、インテリジェントな顧客サポート、コンテンツ生成、コードアシスタントなどのアプリケーションを構築しています。しかし、多くの方は API への初期導入時に共通の問題に直面します：&lt;strong&gt;モデル出力が不整合で文脈理解が欠如しており、場合によっては質問の意図を誤解してしまう&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;この現象を引き起こす主要な原因の一つは——&lt;strong&gt;API リクエスト中に履歴対話の内容を含めないこと&lt;/strong&gt;です。&lt;/p&gt;
&lt;h2 id=&#34;履歴対話とは&#34;&gt;履歴対話とは？
&lt;/h2&gt;&lt;p&gt;履歴対話とは、一度の会話セッションにおいて、モデルとユーザー間の過去のやり取りの記録を指します。OpenAI の Chat Completions API（など、多くの大規模言語モデル API）では、開発者がリクエスト内で完全な &lt;code&gt;messages&lt;/code&gt; 配列を作成し、過去の会話をユーザーとアシスタントのメッセージが交互に並んだ形式で渡す必要があります。&lt;/p&gt;
&lt;h3 id=&#34;例文&#34;&gt;例文
&lt;/h3&gt;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{
  &amp;quot;model&amp;quot;: &amp;quot;gpt-4&amp;quot;,
  &amp;quot;messages&amp;quot;: [
    {&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;退職の手紙を書いてください&amp;quot;},
    {&amp;quot;role&amp;quot;: &amp;quot;assistant&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;かしこまりました。退職理由は何を書くようにしますか？&amp;quot;},
    {&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;個人的なキャリアの追求をしたいと考えていると述べる&amp;quot;}
  ]
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;もし最後の文だけを送った場合：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;個人的なキャリアの追求をしたいと考えていると述べる&amp;quot;}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;モデルは退職の手紙だと全く認識できず、文脈が理解されないため、出力品質は著しく低下します。&lt;/p&gt;
&lt;h2 id=&#34;歴史対話がなぜ重要なのか&#34;&gt;歴史対話がなぜ重要なのか？
&lt;/h2&gt;&lt;h3 id=&#34;1-文脈の構築と一貫性の向上&#34;&gt;1. &lt;strong&gt;文脈の構築と一貫性の向上&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;AIモデルは本質的に「コンテキスト駆動型」であり、過去の出来事を記憶することはできません。除非你&lt;strong&gt;明示的に伝える&lt;/strong&gt;のです。対話履歴を渡すことで、モデルはあなたの意図や話題の背景をより良く理解し、期待される出力を生成できます。&lt;/p&gt;
&lt;h3 id=&#34;2-誤解の低減&#34;&gt;2. &lt;strong&gt;誤解の低減&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;もしあなたがモデルに複数のステップで指示を実行させたい場合（例：文章作成、要約、コードデバッグ）、過去の履歴はモデルが徐々に理解を深め、途中で「逸脱」したり、重要な点を失ったりするのを防ぐのに役立ちます。&lt;/p&gt;
&lt;h3 id=&#34;3-実際の人間のような対話行動のシミュレーション&#34;&gt;3. &lt;strong&gt;実際の人間のような対話行動のシミュレーション&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;実用例として、カスタマーサポートシステム、教育アシスタント、健康相談などにおいて、ユーザーの質問は通常、段階的に展開され、一度に明確な表現で表明されることはありません。会話履歴を保持することで、AIが「記憶力のあるアシスタント」のように振る舞うことができます。&lt;/p&gt;
&lt;h2 id=&#34;api-中における会話履歴の正しい追加方法&#34;&gt;API 中における会話履歴の正しい追加方法
&lt;/h2&gt;&lt;p&gt;OpenAI の API を例に、以下の構造に従うことを推奨します。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;messages = [
    {&amp;quot;role&amp;quot;: &amp;quot;system&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;あなたは専門的な法律アシスタントです&amp;quot;},
    {&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;契約書の有効条件とは何ですか？&amp;quot;},
    {&amp;quot;role&amp;quot;: &amp;quot;assistant&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;契約書が有効であるためには、以下の条件を満たす必要があります：……&amp;quot;},
    {&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;口頭での合意は有効ですか？&amp;quot;}
]

response = openai.ChatCompletion.create(
    model=&amp;quot;gpt-4&amp;quot;,
    messages=messages
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;注意点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;system&lt;/code&gt; メッセージを使用してモデルの動作とアイデンティティを設定します。&lt;/li&gt;
&lt;li&gt;最新の数回の重要な会話のみを保持し、毎回すべての履歴を送信する必要はありません（トークン制限を超えないように）。&lt;/li&gt;
&lt;li&gt;長いセッションでは、早期のコンテンツを切り捨てて、コア情報を要約し、トークンの消費を制御します。&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;実践的推奨事項&#34;&gt;実践的推奨事項
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;対話状態管理&lt;/strong&gt;: バックエンドは、各ユーザーのセッション履歴（例: Redis、データベース）を記録するためのキャッシュメカニズムを設計する必要があります。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;長さ制限&lt;/strong&gt;: OpenAI GPT-4 のコンテキスト長は 128k tokens であり、Claude 3 は 200k～1M पर्यंत可能です。適切なトリミングが必要です。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;動的履歴の要約&lt;/strong&gt;: 履歴が長すぎる場合は、モデルを使用して古い会話を要約し、その結果を対話コンテキストに追加します。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;まとめ&#34;&gt;まとめ
&lt;/h2&gt;&lt;p&gt;AI 大規模モデルの能力は強力ですが、開発者に十分なコンテキスト情報を「与える」必要があります。API リクエストに過去の会話を追加することで、モデル出力の品質と一貫性を大幅に向上させるだけでなく、ユーザーエクスペリエンスをより自然で現実的な対話に近づけることができます。AI 顧客サービス、ライティングアシスタント、プログラミングアシスタント、教育アプリケーションなど、どのような分野でも無視できない最適化テクニックです。&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
