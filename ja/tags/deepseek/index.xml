<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Deepseek on 向叔の手帳</title>
        <link>https://blog.ttf248.life/ja/tags/deepseek/</link>
        <description>Recent content in Deepseek on 向叔の手帳</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>ja</language>
        <lastBuildDate>Wed, 28 May 2025 09:47:38 +0800</lastBuildDate><atom:link href="https://blog.ttf248.life/ja/tags/deepseek/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>ローカルにDeepSeek-R1をデプロイ</title>
        <link>https://blog.ttf248.life/ja/p/ollama-local-deployment-deepseek-r1/</link>
        <pubDate>Fri, 07 Feb 2025 22:41:02 +0800</pubDate>
        
        <guid>https://blog.ttf248.life/ja/p/ollama-local-deployment-deepseek-r1/</guid>
        <description>&lt;p&gt;Ollamaは、ユーザーがローカルで大規模言語モデル（LLM）を実行・デプロイできるように設計されたオープンソースのAIツールです。クラウドサービスに依存することなく、GPTのようなモデルをローカルマシン上で利用するための、便利で効率的な方法を提供し、様々なモデルに対応しています。また、パフォーマンスの最適化に重点を置いており、リソースが限られたデバイスでもスムーズに動作するようにしています。&lt;/p&gt;
&lt;p&gt;Ollama を使用すれば、ユーザーはテキストベースのAIアプリケーションを利用でき、データプライバシーや高額なAPI利用料金を心配することなく、ローカルにデプロイされたモデルと対話できます。様々なモデルをコマンドラインインターフェース（CLI）から呼び出し、自然言語処理や質問応答などのタスクを実行できます。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Ollamaは様々なモデルの試用に向いていますが、Windows版をテストしたところ、ハードウェアの性能を十分に発揮できませんでした。原因はWindows版にあるのかもしれません。Linux版の方が良いかもしれません。32bパラメータのモデルをデプロイしても、メモリやGPUの負荷が低いにも関わらず、応答速度が非常に遅いです。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;ハードウェア概要&#34;&gt;ハードウェア概要
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;OS：Win11&lt;/li&gt;
&lt;li&gt;CPU：i7-10700K&lt;/li&gt;
&lt;li&gt;メモリ：40GB&lt;/li&gt;
&lt;li&gt;グラフィックボード：RTX 3060 12GB&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;環境準備&#34;&gt;環境準備
&lt;/h2&gt;&lt;p&gt;新たにシステム環境変数を追加し、今後の利用を容易にします。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;set OLLAMA_MODELS=E:\ollama&lt;/code&gt;&lt;/strong&gt;&lt;br&gt;
この変数は、Ollamaモデルの保存パスを指定します。&lt;strong&gt;INLINE_CODE_0&lt;/strong&gt; はフォルダパスであり、すべてのローカルモデルファイルがそのディレクトリに格納されます。Ollamaは、このパスに基づいてダウンロードまたはデプロイした言語モデルを読み込み使用します。モデルファイルを別の場所に保存する場合は、このパスを変更するだけです。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;set OLLAMA_HOST=127.0.0.1:8000&lt;/code&gt;&lt;/strong&gt;&lt;br&gt;
この環境変数は、Ollama サービスが利用するホストとポートを設定します。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;INLINE_CODE_0&lt;/strong&gt; はローカルアドレス（localhost）を意味し、Ollama サービスはローカルからのリクエストのみを待ち受けます。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;INLINE_CODE_0&lt;/strong&gt; は、指定されたポート番号で、Ollamaサービスが8000ポートでリクエストを待機し処理することを示します。必要に応じてポート番号を変更できますが、そのポートが他のアプリケーションによって使用されていないことを確認してください。&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;set OLLAMA_ORIGINS=*&lt;/code&gt;&lt;/strong&gt;&lt;br&gt;
この環境変数は、Ollama サービスにアクセスできるリクエストのソースを制御します。&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;あらゆるソース（すべてのドメインとIPアドレス）からのアクセスを許可します。これは通常、開発およびデバッグ環境で使用され、本番環境ではセキュリティを高めるために、特定のドメインまたはIPのみがサービスにアクセスできるように制限することが一般的です。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;deepseek-r1-モデルのデプロイ&#34;&gt;DeepSeek-R1 モデルのデプロイ
&lt;/h2&gt;&lt;p&gt;インストールは簡単ですので、詳細は割愛します。&lt;/p&gt;
&lt;p&gt;インストール後の検証：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;C:\Users\core&amp;gt;ollama -v
ollama version is 0.5.11
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;モデルのデプロイについては、公式ウェブサイトのモデルページを参照し、対応するパラメータを持つモデルを選択してください。&lt;/p&gt;
&lt;p&gt;14bパラメータは会話コンテキストを効果的に記憶できるが、より小さいパラメータのバージョンでは記憶できない。32bパラメータ版はローカル環境での動作が重く、さらなるテストは実施していない。&lt;/p&gt;
&lt;h2 id=&#34;参照資料&#34;&gt;参照資料
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.ollama.com/library/deepseek-r1&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://www.ollama.com/library/deepseek-r1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/SPEvYTmTBxhoEkJqm1yPmw&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://mp.weixin.qq.com/s/SPEvYTmTBxhoEkJqm1yPmw&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://blog.csdn.net/x18990027/article/details/145368094&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://blog.csdn.net/x18990027/article/details/145368094&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
        </item>
        <item>
        <title>ディープシーク、春節前に急成長、NVIDIA株が暴落：背景にある機関投資家の動きと大規模言語モデルの推論チェーン</title>
        <link>https://blog.ttf248.life/ja/p/deepseek-chinese-new-year-nvidia-stock-drop-institutional-operations-large-language-model-chain/</link>
        <pubDate>Fri, 07 Feb 2025 20:36:05 +0800</pubDate>
        
        <guid>https://blog.ttf248.life/ja/p/deepseek-chinese-new-year-nvidia-stock-drop-institutional-operations-large-language-model-chain/</guid>
        <description>&lt;p&gt;旧正月前夜、DeepSeekが一度話題の中心となり、数日のうちにSNSで広く注目を集めました。この突如の急上昇は驚くべきものであり、市場に連鎖的な反応を引き起こしました。一方、NVIDIA株は暴落し、多くの投資家がその見通しを疑心暗鬼に陥りました。一部機関は期間中に大規模な空売りを行い、まるで全てが「周到に計画された」状況を示唆しているかのようでした。&lt;/p&gt;
&lt;h3 id=&#34;申し訳ありませんが翻訳する中国語のテキストが提供されていませんテキストを提供してください&#34;&gt;申し訳ありませんが、翻訳する中国語のテキストが提供されていません。テキストを提供してください。
&lt;/h3&gt;&lt;p&gt;DeepSeekは、深層学習モデルの最適化に特化したAIツールであり、特に自然言語処理（NLP）や画像生成分野での応用を重視しています。旧正月の数日前、このプロジェクトは突如として多くの投資家や技術者の注目を集めました。その開発チームの実績と技術成果が、多くの人々から強い関心を引き起こしました。開発者コミュニティはもちろんのこと、ソーシャルメディアプラットフォームにおいても、DeepSeekに関する議論が技術界の話題を席巻しています。&lt;/p&gt;
&lt;p&gt;しかし、DeepSeekの突如たる爆発的な人気は偶然ではない。分析の結果、多くの人が背後に何らかの組織の操作が関与しているのではないかと疑い始めている。特に、その人気に沸騰した後、NVIDIA株価が著しい下落を見せていることから、明らかに何らかの要因がこの変化を促していることが見て取れる。&lt;/p&gt;
&lt;h3 id=&#34;申し訳ありませんが翻訳する中国語のテキストが提供されていませんテキストを提供してください-1&#34;&gt;申し訳ありませんが、翻訳する中国語のテキストが提供されていません。テキストを提供してください。
&lt;/h3&gt;&lt;p&gt;NVIDIAは、世界をリードするGPUメーカーであり、長らく大規模モデルやAI計算の主要なハードウェアサプライヤーとして機能してきました。AI市場の急速な成長とともに、同社の株価は長期的に好調で、多くの投資家の注目を集めていましたが、DeepSeekの急上昇と市場からの高い関心により、NVIDIA株は暴落しました。&lt;/p&gt;
&lt;p&gt;この現象の背景には、機関投資家の空売り戦略が関わっているかもしれない。近年、AI技術の普及に伴い、NVIDIA株価は大きく上昇し、多くの投資家が過剰なバブルのリスクを感じ始めていた。特にDeepSeekのような技術が爆発的に広まった後、一部の機関はNVIDIA株を空売りすることで、大きな利益を得た可能性がある。正確な市場タイミングの把握とDeepSeekの影響力への予測により、彼らは成功裏に利益を上げた。&lt;/p&gt;
&lt;h3 id=&#34;申し訳ありませんが翻訳する中国語のテキストが提供されていませんテキストを提供してください-2&#34;&gt;申し訳ありませんが、翻訳する中国語のテキストが提供されていません。テキストを提供してください。
&lt;/h3&gt;&lt;p&gt;伝統的なAI応用では、多くの専門家や投資家は生成された画像やテキストといったAIモデルの「結果」に注目していましたが、DeepSeekに関連する議論の中で、より多くの人々が、大規模言語モデル（LLM）背後にある推論連鎖こそが、より注力すべき核心的内容であることに気づき始めています。これまで私たちはモデルの出力結果しか見ることができませんでしたが、今やその背後にあるロジック、アルゴリズムを理解し、これらの要素を調整してモデルのパフォーマンスを最適化することが求められています。&lt;/p&gt;
&lt;p&gt;この思考様式の転換は、AI研究と応用に対する深い考察と言えるでしょう。単純なブラックボックス操作から、モデル内部の動作メカニズムを真に理解することへの転換は、多くの技術者や投資家が人工知能の将来的な発展方向を改めて見直すきっかけとなりました。DeepSeek の人気は、まさにこの思考の突破口であり、人々は最終的な出力結果だけでなく、モデル全体の構築と最適化プロセスに関心を向けるようになりました。&lt;/p&gt;
&lt;h3 id=&#34;申し訳ありませんが翻訳する中国語のテキストが提供されていませんテキストを提供してください-3&#34;&gt;申し訳ありませんが、翻訳する中国語のテキストが提供されていません。テキストを提供してください。
&lt;/h3&gt;&lt;p&gt;DeepSeekの急激な人気、NVIDIA株式の大幅下落、そして市場背景にある機関投資家の空売り操作。これらはすべて、巧妙に仕組まれた陰謀なのかもしれません。大規模言語モデルの思考回路を深く理解することで、AI技術の応用は単なる表面的な積み重ねではなく、モデル内部の論理を探求し最適化することであることが分かります。技術の進歩に伴い、今後DeepSeekのような革新的なツールがさらに多く登場し、AI研究と応用の発展を新たな高みに導くことが期待されます。&lt;/p&gt;
&lt;p&gt;この現象は、AI技術の巨大な可能性を私たちに見せてくれるだけでなく、その背景にあるビジネス対立や資本運用のあり方を考えさせるものとなりました。今後の市場動向は、技術と資本との駆け引きの継続的な注目点となるでしょう。&lt;/p&gt;</description>
        </item>
        
    </channel>
</rss>
