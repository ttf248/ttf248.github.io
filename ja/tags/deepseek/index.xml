<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Deepseek on 向叔の手帳</title>
        <link>https://ttf248.life/ja/tags/deepseek/</link>
        <description>Recent content in Deepseek on 向叔の手帳</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>ja</language>
        <lastBuildDate>Sun, 25 May 2025 14:10:37 +0800</lastBuildDate><atom:link href="https://ttf248.life/ja/tags/deepseek/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>ローカルにdeepseek-R1をデプロイ</title>
        <link>https://ttf248.life/ja/p/ollama-local-deployment-deepseek-r1/</link>
        <pubDate>Fri, 07 Feb 2025 22:41:02 +0800</pubDate>
        
        <guid>https://ttf248.life/ja/p/ollama-local-deployment-deepseek-r1/</guid>
        <description>&lt;p&gt;Ollamaは、ユーザーがローカルで大規模言語モデル（LLM）を実行およびデプロイできるようにすることを目的としたオープンソースのAIツールです。開発者がクラウドサービスに依存することなく、GPTのようなモデルをローカルマシン上で利用できる、便利で効率的な方法を提供することを目指しています。Ollamaは複数のモデルをサポートしており、パフォーマンスの最適化に重点を置いており、リソースが限られたデバイスでもこれらのモデルをスムーズに実行できるようにします。&lt;/p&gt;
&lt;p&gt;Ollama を使用すれば、ユーザーはテキストベースの AI アプリケーションを利用でき、データプライバシーを気にすることなく、また高額な API 利用料金を心配することなく、ローカルにデプロイされたモデルと対話することができます。コマンドラインインターフェース（CLI）を通じて様々なモデルを呼び出し、自然言語処理や質問応答などのタスクを実行できます。&lt;/p&gt;
&lt;p&gt;Ollamaは様々なモデルを試すのに適していますが、Windows版をテストしたところ、ハードウェアの性能を十分に発揮できないようです。原因はWindows版にあるのかもしれません。Linux版の方が良いかもしれません。32bパラメータのモデルをデプロイした場合、メモリやGPUの負荷が低いにも関わらず、応答速度が非常に遅いです。&lt;/p&gt;
&lt;h2 id=&#34;ハードウェア概要&#34;&gt;ハードウェア概要
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;オペレーティングシステム：win11&lt;/li&gt;
&lt;li&gt;CPU：i7-10700K&lt;/li&gt;
&lt;li&gt;メモリ：40GB&lt;/li&gt;
&lt;li&gt;グラフィックボード：RTX 3060 12GB&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;環境準備&#34;&gt;環境準備
&lt;/h2&gt;&lt;p&gt;新たにシステム環境変数を追加し、今後の利用を容易にします。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;set OLLAMA_MODELS=E:\ollama&lt;/code&gt;&lt;/strong&gt;&lt;br&gt;
この変数は、Ollamaモデルの保存パスを指定します。&lt;code&gt;E:\ollama&lt;/code&gt; はフォルダパスで、すべてのローカルモデルファイルがそのディレクトリに格納されていることを示しています。Ollamaは、このパスに基づいてダウンロードまたはデプロイした言語モデルをロードして使用します。モデルファイルを別の場所に保存したい場合は、このパスを変更するだけです。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;set OLLAMA_HOST=127.0.0.1:8000&lt;/code&gt;&lt;/strong&gt;&lt;br&gt;
この環境変数は、Ollama サービスのホストとポートを設定します。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;127.0.0.1&lt;/code&gt; はローカルアドレス（localhost）であり、Ollama サービスはローカルからのリクエストのみをリッスンします。&lt;/li&gt;
&lt;li&gt;8000は指定されたポート番号で、Ollamaサービスが8000ポートでリクエストを待機し処理することを示します。必要に応じてポート番号を変更できますが、そのポートが他のアプリケーションによって使用されていないことを確認してください。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;set OLLAMA_ORIGINS=*&lt;/code&gt;&lt;/strong&gt;&lt;br&gt;
この環境変数は、Ollama サービスにアクセスできるリクエストのソースを制御します。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;*&lt;/code&gt; は、Ollama サービスへのアクセスを任意のソース（すべてのドメインと IP アドレス）から許可することを意味します。これは通常、開発およびデバッグ環境で使用されます。本番環境では、セキュリティを高めるために、特定のドメインまたは IP からのみアクセスできるように、より厳格なソースの制御を指定することが一般的です。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;deepseek-r1モデルのデプロイ&#34;&gt;DeepSeek-R1モデルのデプロイ
&lt;/h2&gt;&lt;p&gt;Ollamaのインストールは簡単で、ここでは詳細を説明しません。&lt;/p&gt;
&lt;p&gt;インストール後の検証：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;C:\Users\core&amp;gt;ollama -v
ollama version is 0.5.11
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;モデルのデプロイについては、公式ウェブサイトのモデルページを参照し、対応するパラメータを持つモデルを選択してください。例：&lt;code&gt;ollama run deepseek-r1:14b&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;14bパラメータは会話のコンテキストを効果的に記憶できますが、より小さいパラメータバージョンではそれができません。32bパラメータバージョンについては、ローカルでのデプロイが非常に遅く、さらなるテストは行っていません。&lt;/p&gt;
&lt;h2 id=&#34;参照資料&#34;&gt;参照資料
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.ollama.com/library/deepseek-r1&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://www.ollama.com/library/deepseek-r1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/SPEvYTmTBxhoEkJqm1yPmw&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://mp.weixin.qq.com/s/SPEvYTmTBxhoEkJqm1yPmw&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://blog.csdn.net/x18990027/article/details/145368094&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://blog.csdn.net/x18990027/article/details/145368094&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
        </item>
        <item>
        <title>ディープシーク、旧正節前の急成長、NVIDIA株の大幅下落：背景にある機関投資家の動きと大規模言語モデルの推論チェーン</title>
        <link>https://ttf248.life/ja/p/deepseek-chinese-new-year-nvidia-stock-drop-institutional-operations-large-language-model-chain/</link>
        <pubDate>Fri, 07 Feb 2025 20:36:05 +0800</pubDate>
        
        <guid>https://ttf248.life/ja/p/deepseek-chinese-new-year-nvidia-stock-drop-institutional-operations-large-language-model-chain/</guid>
        <description>&lt;p&gt;旧正年の前夜、DeepSeekは一躍注目を集め、数日の間にソーシャルメディアで広く話題となった。この突如の爆発的な人気は驚くべきものであり、市場に連鎖反応を引き起こした。一方、NVIDIA株は暴落し、多くの投資家がその見通しを疑念視している。一部機関は、この期間中に大規模な空売りを行っており、まるで全てが「精心策划（じょうみけんさ）」された状況を示唆しているかのようだ。&lt;/p&gt;
&lt;h3 id=&#34;ディープシークの急上昇短時間で注目を集める&#34;&gt;ディープシークの急上昇：短時間で注目を集める
&lt;/h3&gt;&lt;p&gt;DeepSeekは、AIをベースとしたツールであり、特に自然言語処理（NLP）や画像生成分野での応用において、深層学習モデルの最適化に焦点を当てています。旧正月の数日前、このプロジェクトは突然、多くの投資家や技術者の注目を集めました。その背後にあるチームの実績と、提示された技術成果により、多くの人々がこのプロジェクトに強い関心を持つようになりました。開発者コミュニティにおいてもソーシャルメディアプラットフォームにおいても、DeepSeekに関する議論は、ほぼテクノロジー業界の話題を占めています。&lt;/p&gt;
&lt;p&gt;しかし、DeepSeekの突如たる爆発的な人気は偶然ではない。分析の結果、多くの人がこの背後に何らかの機関の操作が関与しているのではないかと疑い始めている。特にその人気に火がついた後、NVIDIA株価が著しい下落を見せていることから、明らかに何らかの要因がこの変化を促していることが見て取れる。&lt;/p&gt;
&lt;h3 id=&#34;エヌビディア株急落空売り操作の黒幕&#34;&gt;エヌビディア株急落：空売り操作の黒幕
&lt;/h3&gt;&lt;p&gt;NVIDIAは、世界をリードするグラフィックス処理ユニット（GPU）メーカーであり、長らく大規模モデルやAIコンピューティングの主要なハードウェアサプライヤーです。AI市場の急速な発展に伴い、NVIDIA株は長期的に好調でしたが、投資家の人気を集めていました。しかし、DeepSeekの人気急上昇と市場からのその技術への高い注目により、NVIDIA株は暴落しました。&lt;/p&gt;
&lt;p&gt;この現象の背景には、機関投資家の空売り戦略が関わっているかもしれません。ここ数年、AI技術の普及に伴い、NVIDIAの株価は大きく押し上げられ、多くの投資家が同社の株価に過大評価のリスクを感じていました。特にDeepSeekのような技術が爆発的に広まった後、一部の機関はNVIDIAの株式を空売りすることで、大きな利益を得た可能性があります。正確な市場機会の捉え方とDeepSeekの影響力に関する予測により、これらの機関は成功裏に利益を上げました。&lt;/p&gt;
&lt;h3 id=&#34;大規模言語モデルの思考チェーンの探求結果からプロセスへ&#34;&gt;大規模言語モデルの思考チェーンの探求：「結果」から「プロセス」へ
&lt;/h3&gt;&lt;p&gt;伝統的な人工知能の応用において、多くの専門家や投資家はAIモデルの「結果」——生成された画像やテキストといった直接的な成果物——に多くの中目を向けてきました。しかしながら、DeepSeekに関連する議論の中で、より多くの人々が、大規模言語モデル（LLM）の背後にある思考連鎖こそが、より注目すべき核心的内容であることに気づき始めています。これまで、私たちは単にモデルの出力結果しか見ることができませんでしたが、今や、その背後にある論理、アルゴリズム、そしてこれらの要素を調整してモデルのパフォーマンスを最適化する方法を理解することが求められています。&lt;/p&gt;
&lt;p&gt;この思考様式の転換は、AI研究と応用に対する深い考察と言えるでしょう。単純なブラックボックス操作から、モデル内部の動作メカニズムを真に理解することへの転換は、多くの技術者や投資家が人工知能の将来的な発展方向を改めて見直すきっかけとなりました。DeepSeek の人気は、まさにこの思考回路の画期的な応用であり、人々は最終的な出力結果だけでなく、モデル全体の構築と最適化プロセスに関心を向け始めています。&lt;/p&gt;
&lt;h3 id=&#34;まとめ&#34;&gt;まとめ
&lt;/h3&gt;&lt;p&gt;DeepSeekの突然の爆発的な人気、NVIDIA株式の大幅下落、そして市場背景にある機関投資家の空売り操作、これら全てには巧妙に仕組まれた陰謀があるように思われます。大規模言語モデルの思考チェーンに対する深い理解を通して見ると、AI技術の応用は単なる表面現象の積み重ねではなく、モデル内部の論理を深く掘り下げて最適化することです。技術が進歩するにつれて、今後DeepSeekのような革新的なツールがさらに多く登場し、AI研究と応用の発展をより高いレベルへと押し進めることが予想されます。&lt;/p&gt;
&lt;p&gt;この現象は、AI技術の莫大な可能性を私たちに見せてくれるだけでなく、その背景にあるビジネスの駆け引きや資本運用のことを考えさせるものとなりました。今後の市場動向が、技術と資本の対立をめぐる継続的な注目点となるでしょう。&lt;/p&gt;</description>
        </item>
        
    </channel>
</rss>
