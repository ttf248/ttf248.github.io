<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Deepseek on 向叔の手帳</title>
        <link>https://ttf248.life/ja/tags/deepseek/</link>
        <description>Recent content in Deepseek on 向叔の手帳</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>ja</language>
        <lastBuildDate>Mon, 02 Jun 2025 20:54:02 +0800</lastBuildDate><atom:link href="https://ttf248.life/ja/tags/deepseek/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>ollama ローカル実行 deepseek-R1</title>
        <link>https://ttf248.life/ja/p/ollama-local-deployment-deepseek-r1/</link>
        <pubDate>Fri, 07 Feb 2025 22:41:02 +0800</pubDate>
        
        <guid>https://ttf248.life/ja/p/ollama-local-deployment-deepseek-r1/</guid>
        <description>&lt;p&gt;Ollamaは、大規模言語モデル（LLM）をローカルで実行およびデプロイすることを目的としたオープンソースのAIツールです。クラウドサービスへの依存なしに、開発者がローカルマシン上でGPTのようなモデルを使用するための簡単なかつ効率的な方法を提供することを目指しています。Ollamaは複数のモデルに対応し、パフォーマンスを最適化することで、リソースが限られたデバイスでもこれらのモデルをスムーズに実行できるように設計されています。&lt;/p&gt;
&lt;p&gt;Ollamaを使用すると、ユーザーはテキストベースのAIアプリケーションを利用でき、ローカルでデプロイされたモデルとインタラクトすることができ、データプライバシーやAPIの使用料金に関する懸念なく、自然言語処理や質問応答などのタスクを実行できます。コマンドラインインターフェース（CLI）を通じて異なるモデルを呼び出し、これらのタスクを実行できます。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;ollamaは様々なモデルを試すのに適しており、Windows版のテストではハードウェアの性能を十分に発揮できなかった可能性があります。これはWindows版の問題かもしれません。Linux版の方が良い結果が得られる可能性があります。32bパラメータのモデルをデプロイし、メモリとGPU負荷が低い場合に、応答速度が遅いことが確認されました。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;ハードウェア概要&#34;&gt;ハードウェア概要
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;オペレーティングシステム: Windows 11&lt;/li&gt;
&lt;li&gt;CPU: i7-10700K&lt;/li&gt;
&lt;li&gt;メモリ: 40GB&lt;/li&gt;
&lt;li&gt;グラフィックカード: RTX 3060 12GB&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;環境準備&#34;&gt;環境準備
&lt;/h2&gt;&lt;p&gt;以下のシステム環境変数を設定し、後続の使用を容易にします：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;set OLLAMA_MODELS=E:\ollama&lt;/code&gt;&lt;/strong&gt;
この変数で Ollama モデルの保存場所を指定します。 &lt;code&gt;E:\ollama&lt;/code&gt; はフォルダパスであり、ダウンロードまたはデプロイしたローカルモデルファイルをすべてここに格納します。Ollama はこのパスに基づいてモデルをロードおよび使用します。モデルファイルの保存場所を変更する場合は、このパスを更新してください。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;set OLLAMA_HOST=127.0.0.1:8000&lt;/code&gt;&lt;/strong&gt;
Ollama サービスのホストとポートを設定します。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;127.0.0.1&lt;/code&gt; はローカルアドレス（localhost）であり、Ollama サービスは本機からのリクエストのみを待ち受けます。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;8000&lt;/code&gt; は指定するポート番号であり、Ollama サービスが 8000 ポートでリクエストを受信および処理します。必要に応じてポート番号を変更できますが、他のアプリケーションで使用されていないことを確認してください。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;set OLLAMA_ORIGINS=*&lt;/code&gt;&lt;/strong&gt;
Ollama サービスへのアクセスを許可するオリジン（ソース）を制御します。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;*&lt;/code&gt; はすべてのオリジン（つまり、すべてのドメインと IP アドレス）が Ollama サービスにアクセスできることを意味します。これは通常、開発およびデバッグ環境で使用されます。本番環境では、セキュリティを高めるために、特定のドメインまたは IP アドレスのみを許可するようにより厳格なオリジン制御を設定することが一般的です。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;deepseek-r1-モデルのデプロイ&#34;&gt;DeepSeek-R1 モデルのデプロイ
&lt;/h2&gt;&lt;p&gt;ollama のインストールは、初心者向けで簡単なため、詳細は省略します。
インストール後の検証：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;C:\Users\core&amp;gt;ollama -v
ollama version is 0.5.11
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;モデルのデプロイについては、公式ウェブサイトのモデルページを参照し、対応するパラメータのモデルを選択してください: &lt;code&gt;ollama run deepseek-r1:14b&lt;/code&gt;
14b パラメータは会話コンテキストを効果的に記憶でき、より小さなパラメータバージョンではコンテキストを記憶できません。32b パラメータバージョンは、ローカルでのデプロイ時に非常に遅延するため、詳細なテストは行っていません。&lt;/p&gt;
&lt;h2 id=&#34;参考文献&#34;&gt;参考文献
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.ollama.com/library/deepseek-r1&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://www.ollama.com/library/deepseek-r1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/SPEvYTmTBxhoEkJqm1yPmw&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://mp.weixin.qq.com/s/SPEvYTmTBxhoEkJqm1yPmw&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://blog.csdn.net/x18990027/article/details/145368094&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://blog.csdn.net/x18990027/article/details/145368094&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
        </item>
        <item>
        <title>DeepSeek（ディープシーク）が旧正月前に急激に人気を博し、NVIDIAの株価が暴落した：その裏にある機関投資家の動きと大規模言語モデルの思考連鎖</title>
        <link>https://ttf248.life/ja/p/deepseek-explodes-before-chinese-new-year-nvidia-stock-plummets-behind-the-scenes-and-large-language-model-reasoning/</link>
        <pubDate>Fri, 07 Feb 2025 20:36:05 +0800</pubDate>
        
        <guid>https://ttf248.life/ja/p/deepseek-explodes-before-chinese-new-year-nvidia-stock-plummets-behind-the-scenes-and-large-language-model-reasoning/</guid>
        <description>&lt;p&gt;旧正月前夕，DeepSeek 一度成为热门话题，短短几天内便在社交媒体上引起了广泛关注。这种突然的爆火，不仅让人惊讶，还带动了市场的连锁反应。与此同时，英伟达的股票却迎来了暴跌，许多投资者对其前景产生了疑虑，部分机构在此期间进行了大规模的做空操作，似乎一切都指向了一个“精心策划”的局面。&lt;/p&gt;
&lt;h3 id=&#34;deepseek-の爆発的な人気短期間で急速に注目を集める&#34;&gt;&lt;strong&gt;DeepSeek の爆発的な人気：短期間で急速に注目を集める&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;DeepSeek は、AI を活用したツールであり、特に自然言語処理（NLP）および画像生成分野における深層学習モデルの最適化に焦点を当てています。旧正月前の数日間、このプロジェクトは投資家や技術者から急増するほどの関心を集めました。その背後にあるチームのパフォーマンスと、提示された技術成果が多くの人々を強く惹きつけました。開発者コミュニティやソーシャルメディアプラットフォームにおいて、DeepSeek に関する議論が技術界全体のあらゆる話題を占めています。&lt;/p&gt;
&lt;p&gt;しかし、DeepSeek の突然爆発的な人気は偶然ではありません。分析の結果、その裏には何らかの組織による操作があったのではないかという疑念が広まりました。特に爆発的な人気が出た後、NVIDIA の株価に顕著な下落が見られ、何らかの要因がこの変化を推進していることが明らかになりました。&lt;/p&gt;
&lt;h3 id=&#34;nvidia-株価暴落ショート売り戦略の裏黒幕&#34;&gt;&lt;strong&gt;NVIDIA 株価暴落：ショート売り戦略の裏黒幕&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;NVIDIA（英伟達），全球领先的圖形處理单元（GPU）制造商であり、多くの大規模言語モデルやAI計算における重要なハードウェアプロバイダーとして長年存在してきた。AI市場の急速な発展に伴い、NVIDIA の株価は長期にわたり堅調で、多くの投資家の注目を集めていた。しかし、DeepSeek の爆発的な人気と市場におけるその技術への高い関心により、NVIDIA 株は急落した。&lt;/p&gt;
&lt;p&gt;この現象の裏には、機関投資家によるショート売り戦略が隠されている可能性がある。過去数年間、AI 技術の普及に伴い、NVIDIA の株価は過度に高騰しており、多くの投資家がその株価に過剰な期待を抱いていた。特に DeepSeek などの技術が爆発的に人気を集めた後、一部の機関投資家は NVIDIA の株式をショートすることで、かなりの利益を得た可能性がある。正確な市場タイミングを捉え、DeepSeek の影響力を予測することによって、これらの機関投資家は成功裏に利益を上げることができた。&lt;/p&gt;
&lt;h3 id=&#34;大規模言語モデルの思考連鎖への接触結果からプロセスへ&#34;&gt;&lt;strong&gt;大規模言語モデルの思考連鎖への接触：結果からプロセスへ&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;従来の人工知能アプリケーションにおいて、多くの実務者や投資家は、AI モデルの「結果」—生成された画像、テキストなどの直接的なアウトプットに多く注目してきました。一方、DeepSeekに関連する議論の中で、ますます多くの人が、大規模言語モデルの背後に隠された思考連鎖こそがより注目すべき核心の内容であることに気づき始めています。これまで、私たちはモデルが出力する結果しか見ることができませんでしたが、今では、その背後にある論理、アルゴリズム、そしてこれらの要素を調整することでモデルのパフォーマンスを最適化する方法を理解する必要があります。&lt;/p&gt;
&lt;p&gt;この思考様式の転換は、AI 研究およびアプリケーションに対する深い考察であることを意味します。単純なブラックボックス操作から、モデル内部の動作メカニズムを真正に理解することへと移行することは、多くの技術者や投資家が人工知能の将来的な発展方向を再考するきっかけとなっています。DeepSeek の爆発的な人気は、まさにこの思考連鎖の画期的な応用であり、人々がモデル全体の構築と最適化プロセスに注目し始め、最終的なアウトプットの結果だけに焦点を当てるのではなく、ということです。&lt;/p&gt;
&lt;h3 id=&#34;まとめ&#34;&gt;&lt;strong&gt;まとめ&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;DeepSeekの突然的な爆発、NVIDIA株の大暴落、そして市場の裏で機関投資家がショートしたという事実は、すべてが緻密に設計された罠であるかのようだ。大規模言語モデルの思考連鎖を深く理解することで、AI技術の応用は表面的な現象の積み重ねではなく、モデル内部ロジックの深い掘り起こしと最適化であることを認識できる。技術の進歩とともに、今後もDeepSeekのような革新的なツールが登場し、AI研究および応用がより高度なレベルへと発展していく可能性がある。&lt;/p&gt;
&lt;p&gt;この現象は、AI技術の巨大な潜在能力を私たちに示唆すると同時に、技術背後にあるビジネスの駆け引きと資本の動きについても考えさせ始めるきっかけとなった。今後の市場の動向は、技術と資本の博弈という継続的な焦点となるだろう。&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
