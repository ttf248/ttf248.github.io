<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Ollama on 向叔の手帳</title>
        <link>https://ttf248.life/ja/tags/ollama/</link>
        <description>Recent content in Ollama on 向叔の手帳</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>ja</language>
        <lastBuildDate>Wed, 28 May 2025 09:47:38 +0800</lastBuildDate><atom:link href="https://ttf248.life/ja/tags/ollama/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>ローカルにDeepSeek-R1をデプロイ</title>
        <link>https://ttf248.life/ja/p/ollama-local-deployment-deepseek-r1/</link>
        <pubDate>Fri, 07 Feb 2025 22:41:02 +0800</pubDate>
        
        <guid>https://ttf248.life/ja/p/ollama-local-deployment-deepseek-r1/</guid>
        <description>&lt;p&gt;Ollamaは、ユーザーがローカルで大規模言語モデル（LLM）を実行・デプロイできるように設計されたオープンソースのAIツールです。クラウドサービスに依存することなく、GPTのようなモデルをローカルマシン上で利用するための、便利で効率的な方法を提供し、様々なモデルに対応しています。また、パフォーマンスの最適化に重点を置いており、リソースが限られたデバイスでもスムーズに動作するようにしています。&lt;/p&gt;
&lt;p&gt;Ollama を使用すれば、ユーザーはテキストベースのAIアプリケーションを利用でき、データプライバシーや高額なAPI利用料金を心配することなく、ローカルにデプロイされたモデルと対話できます。様々なモデルをコマンドラインインターフェース（CLI）から呼び出し、自然言語処理や質問応答などのタスクを実行できます。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Ollamaは様々なモデルの試用に向いていますが、Windows版をテストしたところ、ハードウェアの性能を十分に発揮できませんでした。原因はWindows版にあるのかもしれません。Linux版の方が良いかもしれません。32bパラメータのモデルをデプロイしても、メモリやGPUの負荷が低いにも関わらず、応答速度が非常に遅いです。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;ハードウェア概要&#34;&gt;ハードウェア概要
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;OS：Win11&lt;/li&gt;
&lt;li&gt;CPU：i7-10700K&lt;/li&gt;
&lt;li&gt;メモリ：40GB&lt;/li&gt;
&lt;li&gt;グラフィックボード：RTX 3060 12GB&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;環境準備&#34;&gt;環境準備
&lt;/h2&gt;&lt;p&gt;新たにシステム環境変数を追加し、今後の利用を容易にします。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;set OLLAMA_MODELS=E:\ollama&lt;/code&gt;&lt;/strong&gt;&lt;br&gt;
この変数は、Ollamaモデルの保存パスを指定します。&lt;strong&gt;INLINE_CODE_0&lt;/strong&gt; はフォルダパスであり、すべてのローカルモデルファイルがそのディレクトリに格納されます。Ollamaは、このパスに基づいてダウンロードまたはデプロイした言語モデルを読み込み使用します。モデルファイルを別の場所に保存する場合は、このパスを変更するだけです。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;set OLLAMA_HOST=127.0.0.1:8000&lt;/code&gt;&lt;/strong&gt;&lt;br&gt;
この環境変数は、Ollama サービスが利用するホストとポートを設定します。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;INLINE_CODE_0&lt;/strong&gt; はローカルアドレス（localhost）を意味し、Ollama サービスはローカルからのリクエストのみを待ち受けます。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;INLINE_CODE_0&lt;/strong&gt; は、指定されたポート番号で、Ollamaサービスが8000ポートでリクエストを待機し処理することを示します。必要に応じてポート番号を変更できますが、そのポートが他のアプリケーションによって使用されていないことを確認してください。&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;set OLLAMA_ORIGINS=*&lt;/code&gt;&lt;/strong&gt;&lt;br&gt;
この環境変数は、Ollama サービスにアクセスできるリクエストのソースを制御します。&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;あらゆるソース（すべてのドメインとIPアドレス）からのアクセスを許可します。これは通常、開発およびデバッグ環境で使用され、本番環境ではセキュリティを高めるために、特定のドメインまたはIPのみがサービスにアクセスできるように制限することが一般的です。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;deepseek-r1-モデルのデプロイ&#34;&gt;DeepSeek-R1 モデルのデプロイ
&lt;/h2&gt;&lt;p&gt;インストールは簡単ですので、詳細は割愛します。&lt;/p&gt;
&lt;p&gt;インストール後の検証：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;C:\Users\core&amp;gt;ollama -v
ollama version is 0.5.11
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;モデルのデプロイについては、公式ウェブサイトのモデルページを参照し、対応するパラメータを持つモデルを選択してください。&lt;/p&gt;
&lt;p&gt;14bパラメータは会話コンテキストを効果的に記憶できるが、より小さいパラメータのバージョンでは記憶できない。32bパラメータ版はローカル環境での動作が重く、さらなるテストは実施していない。&lt;/p&gt;
&lt;h2 id=&#34;参照資料&#34;&gt;参照資料
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.ollama.com/library/deepseek-r1&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://www.ollama.com/library/deepseek-r1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/SPEvYTmTBxhoEkJqm1yPmw&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://mp.weixin.qq.com/s/SPEvYTmTBxhoEkJqm1yPmw&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://blog.csdn.net/x18990027/article/details/145368094&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://blog.csdn.net/x18990027/article/details/145368094&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
        </item>
        
    </channel>
</rss>
