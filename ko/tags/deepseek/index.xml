<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Deepseek on 향숙의 수첩</title>
        <link>https://blog.ttf248.life/ko/tags/deepseek/</link>
        <description>Recent content in Deepseek on 향숙의 수첩</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>ko</language>
        <lastBuildDate>Wed, 28 May 2025 09:47:38 +0800</lastBuildDate><atom:link href="https://blog.ttf248.life/ko/tags/deepseek/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>올라마에 deepseek-R1 로컬 배포</title>
        <link>https://blog.ttf248.life/ko/p/ollama-local-deployment-deepseek-r1/</link>
        <pubDate>Fri, 07 Feb 2025 22:41:02 +0800</pubDate>
        
        <guid>https://blog.ttf248.life/ko/p/ollama-local-deployment-deepseek-r1/</guid>
        <description>&lt;p&gt;Ollama는 사용자가 로컬에서 대규모 언어 모델(LLM)을 실행하고 배포할 수 있도록 하는 오픈 소스 AI 도구입니다. 개발자가 클라우드 서비스에 의존하지 않고도 GPT와 같은 모델을 로컬 머신에서 편리하고 효율적으로 사용할 수 있도록 지원하며, 다양한 모델을 지원하고 성능 최적화에 중점을 두어 리소스가 제한적인 장치에서도 원활하게 실행될 수 있도록 합니다.&lt;/p&gt;
&lt;p&gt;올라마를 통해 사용자는 텍스트 기반 AI 애플리케이션을 활용하고, 데이터 프라이버시나 높은 API 사용 비용 걱정 없이 로컬에 배포된 모델과 상호 작용할 수 있습니다. 다양한 모델을 명령줄 인터페이스(CLI)를 통해 호출하여 자연어 처리, 질의응답 등의 작업을 수행할 수 있습니다.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;올라마는 다양한 모델을 시험해 보기에 적합하지만, 윈도우 버전은 하드웨어 성능을 충분히 활용하지 못하는 듯합니다. 리눅스 버전이 더 나을 수도 있습니다. 32B 파라미터 모델을 배포했을 때 메모리나 GPU 사용량이 높지 않음에도 응답 속도가 매우 느립니다.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;하드웨어-개요&#34;&gt;하드웨어 개요
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;운영체제: win11&lt;/li&gt;
&lt;li&gt;CPU：i7-10700K&lt;/li&gt;
&lt;li&gt;메모리: 40GB&lt;/li&gt;
&lt;li&gt;그래픽 카드: RTX 3060 12GB&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;환경-준비&#34;&gt;환경 준비
&lt;/h2&gt;&lt;p&gt;새로운 시스템 환경 변수를 추가하여 향후 사용을 용이하게 합니다&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;set OLLAMA_MODELS=E:\ollama&lt;/code&gt;&lt;/strong&gt;&lt;br&gt;
이 변수는 Ollama 모델이 저장될 경로를 지정합니다. __INLINE_CODE_0__은 모든 로컬 모델 파일이 저장되는 폴더 경로입니다. Ollama는 이 경로를 기준으로 다운로드하거나 배포한 언어 모델을 로드하고 사용합니다. 모델 파일을 다른 위치에 저장하려면 이 경로를 변경하면 됩니다.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;set OLLAMA_HOST=127.0.0.1:8000&lt;/code&gt;&lt;/strong&gt;&lt;br&gt;
이 환경 변수는 Ollama 서비스의 호스트와 포트를 설정합니다&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;__INLINE_CODE_0__은 로컬 주소(localhost)이며, Ollama 서비스가 로컬 요청만 수신하도록 합니다&lt;/li&gt;
&lt;li&gt;__INLINE_CODE_0__은 지정된 포트 번호이며, Ollama 서비스가 8000번 포트에서 요청을 대기하고 처리할 것임을 나타냅니다. 필요에 따라 포트 번호를 변경할 수 있지만, 해당 포트가 다른 애플리케이션에 의해 사용되지 않는지 확인해야 합니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;set OLLAMA_ORIGINS=*&lt;/code&gt;&lt;/strong&gt;&lt;br&gt;
이 환경 변수는 Ollama 서비스에 접근할 수 있는 요청의 출처를 제어합니다&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;INLINE_CODE_0&lt;/strong&gt; 모든 소스(모든 도메인 및 IP 주소)에서 Ollama 서비스에 접근할 수 있도록 허용합니다. 이는 일반적으로 개발 및 디버깅 환경에서 사용되며, 프로덕션 환경에서는 보안 강화를 위해 특정 도메인 또는 IP만 접근하도록 제한하는 것이 일반적입니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;딥시크-r1-모델-배포&#34;&gt;딥시크-R1 모델 배포
&lt;/h2&gt;&lt;p&gt;설치는 간단하니 생략합니다&lt;/p&gt;
&lt;p&gt;설치 후 검증:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;C:\Users\core&amp;gt;ollama -v
ollama version is 0.5.11
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;모델 배포는 공식 모델 페이지에서 해당 매개변수의 모델을 선택하세요: &lt;strong&gt;INLINE_CODE_0&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;14B 파라미터는 대화 맥락을 효과적으로 기억할 수 있지만, 더 작은 파라미터 버전은 그렇지 못합니다. 32B 파라미터 버전은 로컬 배포 시 버벅거림이 심해 더 이상 테스트하지 않았습니다.&lt;/p&gt;
&lt;h2 id=&#34;참고-자료&#34;&gt;참고 자료
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.ollama.com/library/deepseek-r1&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://www.ollama.com/library/deepseek-r1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/SPEvYTmTBxhoEkJqm1yPmw&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://mp.weixin.qq.com/s/SPEvYTmTBxhoEkJqm1yPmw&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://blog.csdn.net/x18990027/article/details/145368094&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://blog.csdn.net/x18990027/article/details/145368094&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
        </item>
        <item>
        <title>딥시크, 설날 직전 갑작스러운 인기와 엔비디아 주가 폭락: 배후의 기관 투자 및 거대 모델 사고 체인</title>
        <link>https://blog.ttf248.life/ko/p/deepseek-chinese-new-year-nvidia-stock-drop-institutional-operations-large-language-model-chain/</link>
        <pubDate>Fri, 07 Feb 2025 20:36:05 +0800</pubDate>
        
        <guid>https://blog.ttf248.life/ko/p/deepseek-chinese-new-year-nvidia-stock-drop-institutional-operations-large-language-model-chain/</guid>
        <description>&lt;p&gt;설 명절 직전, DeepSeek가 한 번의 화제로 주목받으며 며칠 만에 소셜 미디어에서 광범위한 관심을 모았습니다. 이러한 갑작스러운 인기에는 놀라움이 더해진 것은 물론 시장 전체에 연쇄적인 영향을 미쳤습니다. 동시에 엔비디아 주가는 폭락했고, 많은 투자자들이 그 전망에 의문을 제기하며 일부 기관은 대규모 공매도 거래를 진행했습니다. 이 모든 상황은 마치 “계획된” 것처럼 보였습니다.&lt;/p&gt;
&lt;h3 id=&#34;deepseek-的爆火短时间内迅速成为焦点&#34;&gt;&lt;strong&gt;DeepSeek 的爆火：短时间内迅速成为焦点&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;DeepSeek는 AI 기반 도구로, 특히 자연어 처리(NLP) 및 이미지 생성 분야에서 딥러닝 모델 최적화에 집중합니다. 설 명절 직전 이 프로젝트는 갑자기 많은 투자자와 기술 전문가들의 관심을 받기 시작했습니다. 팀의 성과와 시연된 기술적 결과물은 많은 사람들에게 강한 관심을 불러일으켰고, 개발자 커뮤니티나 소셜 미디어 플랫폼에서 DeepSeek에 대한 논의가 기술계 전반의 주요 화두를 차지하고 있습니다.&lt;/p&gt;
&lt;p&gt;그러나 DeepSeek의 갑작스러운 인기 폭발은 우연이 아니다. 분석 결과, 많은 사람들이 이 뒤에 특정 기관의 개입이 있을 가능성을 의심하기 시작했다. 특히 인기가 폭발한 이후 엔비디아 주가가 뚜렷하게 하락했는데, 분명 어떤 요인이 이러한 변화를 촉진하고 있는 것으로 보인다.&lt;/p&gt;
&lt;h3 id=&#34;英伟达股票暴跌做空操作的幕后推手&#34;&gt;&lt;strong&gt;英伟达股票暴跌：做空操作的幕后推手&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;엔비디아는 그래픽 처리 장치(GPU) 제조업체로서, 대규모 모델 및 AI 컴퓨팅의 핵심 하드웨어 공급업체였으며, AI 시장의 급성장과 함께 주가가 꾸준히 상승하여 많은 투자자들의 선호 대상이 되었습니다. 하지만 DeepSeek의 인기에 힘입어 시장의 기술적 관심이 높아지면서 엔비디아 주가는 급락했습니다.&lt;/p&gt;
&lt;p&gt;이러한 현상 이면에는 기관 투자자의 공매도 전략이 관련되었을 수도 있습니다. 지난 몇 년간 AI 기술의 보급과 함께 엔비디아 주가는 크게 상승했고, 많은 투자자들은 그 주가가 과도하게 부풀려졌다고 판단했습니다. 특히 DeepSeek와 같은 기술이 폭발적으로 인기를 얻은 이후, 일부 기관들은 엔비디아 주식을 공매도하여 상당한 이익을 얻었을 가능성이 있습니다. 정확한 시장 상황 파악과 DeepSeek의 영향력에 대한 예측을 통해 이러한 기관들은 성공적으로 이익을 창출했습니다.&lt;/p&gt;
&lt;h3 id=&#34;大模型思维链的接触从结果到过程&#34;&gt;&lt;strong&gt;大模型思维链的接触：从“结果”到“过程”&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;전통적인 인공지능 응용 분야에서 많은 실무자와 투자자들은 생성된 이미지나 텍스트와 같은 AI 모델의 “결과”에 더 집중해 왔습니다. 하지만 DeepSeek 관련 논의에서는 점점 더 많은 사람들이 대규모 모델 뒤에 숨겨진 사고 과정이 더욱 주목할 가치가 있는 핵심 내용이라는 것을 깨닫기 시작했습니다. 과거에는 모델 출력 결과만 볼 수 있었지만, 이제는 그 이면에 숨겨진 논리, 알고리즘을 이해하고 이러한 요소를 조정하여 모델 성능을 최적화하는 것이 더 중요합니다.&lt;/p&gt;
&lt;p&gt;이러한 사고방식의 전환은 AI 연구와 활용에 대한 깊이 있는 고찰이며, 단순한 블랙박스 조작에서 모델 내부 작동 메커니즘을 진정으로 이해하는 변화로 이어지면서 많은 기술 전문가와 투자자들이 인공지능의 미래 발전 방향을 재검토하기 시작했습니다. DeepSeek의 인기 폭발은 바로 이러한 사고방식의 획기적인 응용이며, 사람들은 최종 결과물뿐만 아니라 전체 모델 구축 및 최적화 과정에 주목하기 시작했습니다.&lt;/p&gt;
&lt;h3 id=&#34;总结&#34;&gt;&lt;strong&gt;总结&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;딥시크의 갑작스러운 인기, 엔비디아 주가의 폭락, 그리고 시장 뒤편의 기관의 공매도 작전, 이 모든 현상 뒤에는 치밀하게 설계된 음모가 있는 듯합니다. 거대 언어 모델 사고 체인에 대한 깊이 있는 이해를 통해 AI 기술의 적용은 단순한 표면 현상의 조합이 아니라 모델 내부 로직에 대한 심층적인 탐구와 최적화임을 알 수 있습니다. 기술 발전과 함께 앞으로 딥시크와 같은 혁신적인 도구가 더 많이 등장하여 AI 연구 및 응용을 더욱 높은 수준으로 이끌어갈 것으로 예상됩니다.&lt;/p&gt;
&lt;p&gt;이러한 현상은 AI 기술의 엄청난 잠재력을 보여줄 뿐만 아니라, 기술 뒤에 숨겨진 사업 경쟁과 자본 운용에 대해 생각하게 합니다. 앞으로 시장 추세는 기술과 자본 간의 경쟁이 지속적으로 초점이 될 것입니다.&lt;/p&gt;</description>
        </item>
        
    </channel>
</rss>
