<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Deepseek on 향숙의 수첩</title>
        <link>https://ttf248.life/ko/tags/deepseek/</link>
        <description>Recent content in Deepseek on 향숙의 수첩</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>ko</language>
        <lastBuildDate>Sun, 25 May 2025 14:10:37 +0800</lastBuildDate><atom:link href="https://ttf248.life/ko/tags/deepseek/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>올라마에 deepseek-R1 로컬 배포</title>
        <link>https://ttf248.life/ko/p/ollama-local-deployment-deepseek-r1/</link>
        <pubDate>Fri, 07 Feb 2025 22:41:02 +0800</pubDate>
        
        <guid>https://ttf248.life/ko/p/ollama-local-deployment-deepseek-r1/</guid>
        <description>&lt;p&gt;Ollama는 사용자가 로컬에서 대규모 언어 모델(LLM)을 실행하고 배포할 수 있도록 하는 오픈 소스 AI 도구입니다. 이 도구의 목표는 개발자가 클라우드 서비스에 의존하지 않고도 GPT와 같은 모델을 로컬 머신에서 편리하고 효율적으로 사용할 수 있는 방법을 제공하는 것입니다. Ollama는 다양한 모델을 지원하며 성능 최적화에 중점을 두어 리소스가 제한된 장치에서도 이러한 모델을 원활하게 실행할 수 있도록 합니다.&lt;/p&gt;
&lt;p&gt;올라마를 통해 사용자는 텍스트 기반의 AI 애플리케이션을 사용할 수 있으며, 데이터 개인 정보나 높은 API 사용 비용 걱정 없이 로컬에 배포된 모델과 상호 작용할 수 있습니다. 다양한 모델을 명령줄 인터페이스(CLI)를 통해 호출하여 자연어 처리, 질의 응답 등의 작업을 수행할 수 있습니다.&lt;/p&gt;
&lt;p&gt;올라마는 다양한 모델을 시험해 보기에 적합하지만, 윈도우 버전으로 테스트해 보니 하드웨어 성능을 충분히 활용하지 못하는 듯합니다. 아마 윈도우 버전 때문일 수도 있고, 리눅스 버전이 더 나을 수도 있습니다. 32b 파라미터 모델을 배포했을 때 메모리나 그래픽 카드 사용량이 높지 않음에도 불구하고 응답 속도가 매우 느립니다.&lt;/p&gt;
&lt;h2 id=&#34;하드웨어-개요&#34;&gt;하드웨어 개요
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;운영체제: win11&lt;/li&gt;
&lt;li&gt;CPU：i7-10700K&lt;/li&gt;
&lt;li&gt;메모리: 40GB&lt;/li&gt;
&lt;li&gt;그래픽 카드: RTX 3060 12GB&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;환경-준비&#34;&gt;환경 준비
&lt;/h2&gt;&lt;p&gt;새로운 시스템 환경 변수를 추가하여 향후 사용을 용이하게 합니다&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;set OLLAMA_MODELS=E:\ollama&lt;/code&gt;&lt;/strong&gt;&lt;br&gt;
이 변수는 Ollama 모델이 저장될 경로를 지정합니다. &lt;code&gt;E:\ollama&lt;/code&gt;는 모든 로컬 모델 파일이 해당 디렉터리에 저장되어 있음을 나타내는 폴더 경로입니다. Ollama는 이 경로를 기준으로 다운로드하거나 배포한 언어 모델을 로드하고 사용합니다. 모델 파일을 다른 위치에 저장하려면 이 경로만 변경하면 됩니다.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;set OLLAMA_HOST=127.0.0.1:8000&lt;/code&gt;&lt;/strong&gt;&lt;br&gt;
이 환경 변수는 Ollama 서비스의 호스트와 포트를 설정합니다&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;127.0.0.1&lt;/code&gt;은 로컬 주소(localhost)이며, Ollama 서비스는 로컬에서 온 요청만 수신합니다&lt;/li&gt;
&lt;li&gt;8000은 지정된 포트 번호이며, Ollama 서비스가 8000번 포트에서 요청을 기다리고 처리할 것임을 나타냅니다. 필요에 따라 포트 번호를 변경할 수 있지만, 해당 포트가 다른 애플리케이션에서 사용 중인지 확인해야 합니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;set OLLAMA_ORIGINS=*&lt;/code&gt;&lt;/strong&gt;&lt;br&gt;
이 환경 변수는 Ollama 서비스에 접근할 수 있는 요청의 출처를 제어합니다&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;*&lt;/code&gt;는 모든 소스(즉, 모든 도메인 및 IP 주소)에서 Ollama 서비스에 액세스할 수 있도록 허용합니다. 이는 일반적으로 개발 및 디버깅 환경에서 사용되며, 프로덕션 환경에서는 더 엄격한 소스 제어를 지정하여 특정 도메인 또는 IP만 서비스를 액세스하도록 제한하여 보안을 강화하는 것이 일반적입니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;딥시크-r1-모델-배포&#34;&gt;딥시크-R1 모델 배포
&lt;/h2&gt;&lt;p&gt;올라마 설치는 간단하니 자세한 설명은 생략하겠습니다&lt;/p&gt;
&lt;p&gt;설치 후 검증:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;C:\Users\core&amp;gt;ollama -v
ollama version is 0.5.11
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;모델 배포는 공식 모델 페이지를 참조하여 해당 매개변수가 있는 모델을 선택합니다: &lt;code&gt;ollama run deepseek-r1:14b&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;14b 파라미터는 대화 컨텍스트를 효과적으로 기억할 수 있지만, 더 작은 파라미터 버전은 그렇지 못합니다. 32b 파라미터 버전은 로컬에서 실행하면 매우 느려서 더 이상 테스트하지 않았습니다.&lt;/p&gt;
&lt;h2 id=&#34;참고-자료&#34;&gt;참고 자료
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.ollama.com/library/deepseek-r1&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://www.ollama.com/library/deepseek-r1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/SPEvYTmTBxhoEkJqm1yPmw&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://mp.weixin.qq.com/s/SPEvYTmTBxhoEkJqm1yPmw&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://blog.csdn.net/x18990027/article/details/145368094&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://blog.csdn.net/x18990027/article/details/145368094&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
        </item>
        <item>
        <title>딥시크 스프링 페스티벌 전 갑작스러운 인기, 엔비디아 주식 급락: 뒤에 숨은 기관의 움직임과 거대 모델 사고 체인</title>
        <link>https://ttf248.life/ko/p/deepseek-chinese-new-year-nvidia-stock-drop-institutional-operations-large-language-model-chain/</link>
        <pubDate>Fri, 07 Feb 2025 20:36:05 +0800</pubDate>
        
        <guid>https://ttf248.life/ko/p/deepseek-chinese-new-year-nvidia-stock-drop-institutional-operations-large-language-model-chain/</guid>
        <description>&lt;p&gt;설 명절 직전, DeepSeek가 한 번의 화제로 주목받기 시작하며 며칠 만에 소셜 미디어에서 광범위한 관심을 끌었습니다. 이러한 갑작스러운 인기 폭발은 놀라울 뿐만 아니라 시장 전체에 연쇄적인 영향을 미쳤습니다. 동시에 엔비디아 주가는 급락했고, 많은 투자자들이 그 전망에 대해 의문을 품기 시작했으며, 일부 기관들은 이 기간 동안 대규모의 공매도 포지션을 취했습니다. 모든 상황이 마치 “계획된” 것처럼 보였습니다.&lt;/p&gt;
&lt;h3 id=&#34;딥시크의-폭발적인-인기-짧은-시간-내에-빠르게-주목받음&#34;&gt;딥시크의 폭발적인 인기: 짧은 시간 내에 빠르게 주목받음
&lt;/h3&gt;&lt;p&gt;DeepSeek는 AI 기반 도구로, 특히 자연어 처리(NLP) 및 이미지 생성 분야에서 딥 러닝 모델 최적화에 중점을 두고 있습니다. 설 명절 직전 며칠 동안 이 프로젝트는 갑자기 많은 투자자와 기술 전문가들의 관심을 받기 시작했습니다. 그 뒤에 있는 팀의 성과와 시연된 기술적 결과물은 많은 사람들에게 이 프로젝트에 대한 강한 관심을 불러일으켰습니다. 개발자 커뮤니티나 소셜 미디어 플랫폼에서든, DeepSeek에 대한 논의는 거의 기술계의 모든 화제를 장악했습니다.&lt;/p&gt;
&lt;p&gt;하지만 DeepSeek의 갑작스러운 인기 폭발은 우연이 아니다. 분석 결과, 많은 사람들이 이 뒤에 특정 기관의 개입이 있을 가능성을 의심하기 시작했다. 특히 인기가 폭발한 이후 엔비디아 주가가 뚜렷하게 하락했는데, 분명 어떤 요인들이 이러한 변화를 촉진하고 있는 것으로 보인다.&lt;/p&gt;
&lt;h3 id=&#34;엔비디아-주가-폭락-공매도-작전의-배후-세력&#34;&gt;엔비디아 주가 폭락: 공매도 작전의 배후 세력
&lt;/h3&gt;&lt;p&gt;엔비디아는 그래픽 처리 장치(GPU) 제조업체로서, 수많은 거대 모델과 AI 컴퓨팅의 핵심 하드웨어 공급업체였습니다. AI 시장이 빠르게 발전하면서 엔비디아 주식은 오랫동안 강세를 보였고 심지어 많은 투자자들의 선호 대상이 되기도 했습니다. 하지만 DeepSeek의 폭발적인 인기와 시장의 기술에 대한 높은 관심으로 인해 엔비디아 주가는 급락했습니다.&lt;/p&gt;
&lt;p&gt;이 현상 뒤에는 기관 투자자의 공매도 전략이 관련되어 있을 수도 있습니다. 지난 몇 년간 AI 기술의 보급과 함께 엔비디아 주가는 크게 상승했고, 많은 투자자들은 그 주가가 과도하게 부풀려졌다고 생각하기 시작했습니다. 특히 DeepSeek와 같은 기술이 폭발적으로 인기를 얻은 이후, 일부 기관들은 엔비디아 주식을 공매도하여 상당한 이익을 얻었을 가능성이 있습니다. 정확한 시장 시기 포착과 DeepSeek의 영향력에 대한 예측을 통해 이러한 기관들은 성공적으로 이익을 얻었습니다.&lt;/p&gt;
&lt;h3 id=&#34;대규모-모델-사고-체인의-접촉-결과에서-과정으로&#34;&gt;대규모 모델 사고 체인의 접촉: “결과”에서 “과정”으로
&lt;/h3&gt;&lt;p&gt;전통적인 인공지능 응용 분야에서 많은 실무자와 투자자들은 AI 모델의 “결과” — 예를 들어 생성된 이미지, 텍스트와 같은 직접적인 산출물에 더 많은 관심을 기울였습니다. 하지만 DeepSeek 관련 논의에서는 점점 더 많은 사람들이 대형 모델 뒤에 숨겨진 사고 과정이 더욱 주목할 가치가 있는 핵심 내용이라는 것을 깨닫기 시작했습니다. 과거에는 모델 출력 결과만 볼 수 있었지만, 이제는 그 뒤에 숨겨진 논리, 알고리즘과 이러한 요소를 조정하여 모델의 성능을 최적화하는 방법을 이해해야 합니다.&lt;/p&gt;
&lt;p&gt;이러한 사고방식의 전환은 AI 연구와 응용에 대한 심층적인 고찰인 동시에, 단순한 블랙박스 조작에서 모델 내부 작동 메커니즘을 진정으로 이해하는 것으로의 변화는 많은 기술 전문가와 투자자들이 인공지능의 미래 발전 방향을 다시 한번 검토하게 만들었습니다. DeepSeek의 인기 폭발은 바로 이러한 사고방식의 획기적인 응용이며, 사람들은 이제 최종 출력 결과뿐만 아니라 전체 모델 구축 및 최적화 과정에 주목하기 시작했습니다.&lt;/p&gt;
&lt;h3 id=&#34;요약&#34;&gt;요약
&lt;/h3&gt;&lt;p&gt;딥시크의 갑작스러운 인기, 엔비디아 주식의 폭락, 그리고 시장 뒤에 숨은 기관의 공매도 세력, 이 모든 현상 뒤에는 치밀하게 설계된 음모가 있는 듯합니다. 거대 언어 모델 사고 체인에 대한 깊이 있는 이해를 통해 AI 기술의 적용은 단순한 표면적 현상의 조합이 아니라 모델 내부 로직에 대한 심층적인 탐구와 최적화임을 알 수 있습니다. 기술 발전과 함께 앞으로 딥시크와 같은 혁신적인 도구가 더 많이 등장하여 AI 연구 및 응용을 더욱 높은 수준으로 발전시키는 것을 목격할 수도 있을 것입니다.&lt;/p&gt;
&lt;p&gt;이러한 현상은 인공지능 기술의 엄청난 잠재력을 보여주는 동시에, 기술 뒤에 숨겨진 상업적 경쟁과 자본 운용에 대해 생각하게 합니다. 앞으로 시장 추세는 기술과 자본 간의 경쟁이 지속적으로 초점이 될 것입니다.&lt;/p&gt;</description>
        </item>
        
    </channel>
</rss>
