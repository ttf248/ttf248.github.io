<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Ollama on 향숙의 수첩</title>
        <link>https://ttf248.life/ko/tags/ollama/</link>
        <description>Recent content in Ollama on 향숙의 수첩</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>ko</language>
        <lastBuildDate>Wed, 28 May 2025 09:47:38 +0800</lastBuildDate><atom:link href="https://ttf248.life/ko/tags/ollama/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>올라마에 deepseek-R1 로컬 배포</title>
        <link>https://ttf248.life/ko/p/ollama-local-deployment-deepseek-r1/</link>
        <pubDate>Fri, 07 Feb 2025 22:41:02 +0800</pubDate>
        
        <guid>https://ttf248.life/ko/p/ollama-local-deployment-deepseek-r1/</guid>
        <description>&lt;p&gt;Ollama는 사용자가 로컬에서 대규모 언어 모델(LLM)을 실행하고 배포할 수 있도록 하는 오픈 소스 AI 도구입니다. 개발자가 클라우드 서비스에 의존하지 않고도 GPT와 같은 모델을 로컬 머신에서 편리하고 효율적으로 사용할 수 있도록 지원하며, 다양한 모델을 지원하고 성능 최적화에 중점을 두어 리소스가 제한적인 장치에서도 원활하게 실행될 수 있도록 합니다.&lt;/p&gt;
&lt;p&gt;올라마를 통해 사용자는 텍스트 기반 AI 애플리케이션을 활용하고, 데이터 프라이버시나 높은 API 사용 비용 걱정 없이 로컬에 배포된 모델과 상호 작용할 수 있습니다. 다양한 모델을 명령줄 인터페이스(CLI)를 통해 호출하여 자연어 처리, 질의응답 등의 작업을 수행할 수 있습니다.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;올라마는 다양한 모델을 시험해 보기에 적합하지만, 윈도우 버전은 하드웨어 성능을 충분히 활용하지 못하는 듯합니다. 리눅스 버전이 더 나을 수도 있습니다. 32B 파라미터 모델을 배포했을 때 메모리나 GPU 사용량이 높지 않음에도 응답 속도가 매우 느립니다.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;하드웨어-개요&#34;&gt;하드웨어 개요
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;운영체제: win11&lt;/li&gt;
&lt;li&gt;CPU：i7-10700K&lt;/li&gt;
&lt;li&gt;메모리: 40GB&lt;/li&gt;
&lt;li&gt;그래픽 카드: RTX 3060 12GB&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;환경-준비&#34;&gt;환경 준비
&lt;/h2&gt;&lt;p&gt;새로운 시스템 환경 변수를 추가하여 향후 사용을 용이하게 합니다&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;set OLLAMA_MODELS=E:\ollama&lt;/code&gt;&lt;/strong&gt;&lt;br&gt;
이 변수는 Ollama 모델이 저장될 경로를 지정합니다. __INLINE_CODE_0__은 모든 로컬 모델 파일이 저장되는 폴더 경로입니다. Ollama는 이 경로를 기준으로 다운로드하거나 배포한 언어 모델을 로드하고 사용합니다. 모델 파일을 다른 위치에 저장하려면 이 경로를 변경하면 됩니다.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;set OLLAMA_HOST=127.0.0.1:8000&lt;/code&gt;&lt;/strong&gt;&lt;br&gt;
이 환경 변수는 Ollama 서비스의 호스트와 포트를 설정합니다&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;__INLINE_CODE_0__은 로컬 주소(localhost)이며, Ollama 서비스가 로컬 요청만 수신하도록 합니다&lt;/li&gt;
&lt;li&gt;__INLINE_CODE_0__은 지정된 포트 번호이며, Ollama 서비스가 8000번 포트에서 요청을 대기하고 처리할 것임을 나타냅니다. 필요에 따라 포트 번호를 변경할 수 있지만, 해당 포트가 다른 애플리케이션에 의해 사용되지 않는지 확인해야 합니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;set OLLAMA_ORIGINS=*&lt;/code&gt;&lt;/strong&gt;&lt;br&gt;
이 환경 변수는 Ollama 서비스에 접근할 수 있는 요청의 출처를 제어합니다&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;INLINE_CODE_0&lt;/strong&gt; 모든 소스(모든 도메인 및 IP 주소)에서 Ollama 서비스에 접근할 수 있도록 허용합니다. 이는 일반적으로 개발 및 디버깅 환경에서 사용되며, 프로덕션 환경에서는 보안 강화를 위해 특정 도메인 또는 IP만 접근하도록 제한하는 것이 일반적입니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;딥시크-r1-모델-배포&#34;&gt;딥시크-R1 모델 배포
&lt;/h2&gt;&lt;p&gt;설치는 간단하니 생략합니다&lt;/p&gt;
&lt;p&gt;설치 후 검증:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;C:\Users\core&amp;gt;ollama -v
ollama version is 0.5.11
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;모델 배포는 공식 모델 페이지에서 해당 매개변수의 모델을 선택하세요: &lt;strong&gt;INLINE_CODE_0&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;14B 파라미터는 대화 맥락을 효과적으로 기억할 수 있지만, 더 작은 파라미터 버전은 그렇지 못합니다. 32B 파라미터 버전은 로컬 배포 시 버벅거림이 심해 더 이상 테스트하지 않았습니다.&lt;/p&gt;
&lt;h2 id=&#34;참고-자료&#34;&gt;참고 자료
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.ollama.com/library/deepseek-r1&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://www.ollama.com/library/deepseek-r1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/SPEvYTmTBxhoEkJqm1yPmw&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://mp.weixin.qq.com/s/SPEvYTmTBxhoEkJqm1yPmw&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://blog.csdn.net/x18990027/article/details/145368094&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://blog.csdn.net/x18990027/article/details/145368094&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
        </item>
        
    </channel>
</rss>
