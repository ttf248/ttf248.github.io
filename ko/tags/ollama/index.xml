<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Ollama on 향숙의 수첩</title>
        <link>https://ttf248.life/ko/tags/ollama/</link>
        <description>Recent content in Ollama on 향숙의 수첩</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>ko</language>
        <lastBuildDate>Sun, 25 May 2025 14:10:37 +0800</lastBuildDate><atom:link href="https://ttf248.life/ko/tags/ollama/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>올라마에 deepseek-R1 로컬 배포</title>
        <link>https://ttf248.life/ko/p/ollama-local-deployment-deepseek-r1/</link>
        <pubDate>Fri, 07 Feb 2025 22:41:02 +0800</pubDate>
        
        <guid>https://ttf248.life/ko/p/ollama-local-deployment-deepseek-r1/</guid>
        <description>&lt;p&gt;Ollama는 사용자가 로컬에서 대규모 언어 모델(LLM)을 실행하고 배포할 수 있도록 하는 오픈 소스 AI 도구입니다. 이 도구의 목표는 개발자가 클라우드 서비스에 의존하지 않고도 GPT와 같은 모델을 로컬 머신에서 편리하고 효율적으로 사용할 수 있는 방법을 제공하는 것입니다. Ollama는 다양한 모델을 지원하며 성능 최적화에 중점을 두어 리소스가 제한된 장치에서도 이러한 모델을 원활하게 실행할 수 있도록 합니다.&lt;/p&gt;
&lt;p&gt;올라마를 통해 사용자는 텍스트 기반의 AI 애플리케이션을 사용할 수 있으며, 데이터 개인 정보나 높은 API 사용 비용 걱정 없이 로컬에 배포된 모델과 상호 작용할 수 있습니다. 다양한 모델을 명령줄 인터페이스(CLI)를 통해 호출하여 자연어 처리, 질의 응답 등의 작업을 수행할 수 있습니다.&lt;/p&gt;
&lt;p&gt;올라마는 다양한 모델을 시험해 보기에 적합하지만, 윈도우 버전으로 테스트해 보니 하드웨어 성능을 충분히 활용하지 못하는 듯합니다. 아마 윈도우 버전 때문일 수도 있고, 리눅스 버전이 더 나을 수도 있습니다. 32b 파라미터 모델을 배포했을 때 메모리나 그래픽 카드 사용량이 높지 않음에도 불구하고 응답 속도가 매우 느립니다.&lt;/p&gt;
&lt;h2 id=&#34;하드웨어-개요&#34;&gt;하드웨어 개요
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;운영체제: win11&lt;/li&gt;
&lt;li&gt;CPU：i7-10700K&lt;/li&gt;
&lt;li&gt;메모리: 40GB&lt;/li&gt;
&lt;li&gt;그래픽 카드: RTX 3060 12GB&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;환경-준비&#34;&gt;환경 준비
&lt;/h2&gt;&lt;p&gt;새로운 시스템 환경 변수를 추가하여 향후 사용을 용이하게 합니다&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;set OLLAMA_MODELS=E:\ollama&lt;/code&gt;&lt;/strong&gt;&lt;br&gt;
이 변수는 Ollama 모델이 저장될 경로를 지정합니다. &lt;code&gt;E:\ollama&lt;/code&gt;는 모든 로컬 모델 파일이 해당 디렉터리에 저장되어 있음을 나타내는 폴더 경로입니다. Ollama는 이 경로를 기준으로 다운로드하거나 배포한 언어 모델을 로드하고 사용합니다. 모델 파일을 다른 위치에 저장하려면 이 경로만 변경하면 됩니다.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;set OLLAMA_HOST=127.0.0.1:8000&lt;/code&gt;&lt;/strong&gt;&lt;br&gt;
이 환경 변수는 Ollama 서비스의 호스트와 포트를 설정합니다&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;127.0.0.1&lt;/code&gt;은 로컬 주소(localhost)이며, Ollama 서비스는 로컬에서 온 요청만 수신합니다&lt;/li&gt;
&lt;li&gt;8000은 지정된 포트 번호이며, Ollama 서비스가 8000번 포트에서 요청을 기다리고 처리할 것임을 나타냅니다. 필요에 따라 포트 번호를 변경할 수 있지만, 해당 포트가 다른 애플리케이션에서 사용 중인지 확인해야 합니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;set OLLAMA_ORIGINS=*&lt;/code&gt;&lt;/strong&gt;&lt;br&gt;
이 환경 변수는 Ollama 서비스에 접근할 수 있는 요청의 출처를 제어합니다&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;*&lt;/code&gt;는 모든 소스(즉, 모든 도메인 및 IP 주소)에서 Ollama 서비스에 액세스할 수 있도록 허용합니다. 이는 일반적으로 개발 및 디버깅 환경에서 사용되며, 프로덕션 환경에서는 더 엄격한 소스 제어를 지정하여 특정 도메인 또는 IP만 서비스를 액세스하도록 제한하여 보안을 강화하는 것이 일반적입니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;딥시크-r1-모델-배포&#34;&gt;딥시크-R1 모델 배포
&lt;/h2&gt;&lt;p&gt;올라마 설치는 간단하니 자세한 설명은 생략하겠습니다&lt;/p&gt;
&lt;p&gt;설치 후 검증:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;C:\Users\core&amp;gt;ollama -v
ollama version is 0.5.11
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;모델 배포는 공식 모델 페이지를 참조하여 해당 매개변수가 있는 모델을 선택합니다: &lt;code&gt;ollama run deepseek-r1:14b&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;14b 파라미터는 대화 컨텍스트를 효과적으로 기억할 수 있지만, 더 작은 파라미터 버전은 그렇지 못합니다. 32b 파라미터 버전은 로컬에서 실행하면 매우 느려서 더 이상 테스트하지 않았습니다.&lt;/p&gt;
&lt;h2 id=&#34;참고-자료&#34;&gt;참고 자료
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.ollama.com/library/deepseek-r1&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://www.ollama.com/library/deepseek-r1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/SPEvYTmTBxhoEkJqm1yPmw&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://mp.weixin.qq.com/s/SPEvYTmTBxhoEkJqm1yPmw&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://blog.csdn.net/x18990027/article/details/145368094&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://blog.csdn.net/x18990027/article/details/145368094&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
        </item>
        
    </channel>
</rss>
