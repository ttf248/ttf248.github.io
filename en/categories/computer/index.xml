<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Computer on Uncle Xiang&#39;s Notebook</title>
        <link>https://ttf248.life/en/categories/computer/</link>
        <description>Recent content in Computer on Uncle Xiang&#39;s Notebook</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en</language>
        <lastBuildDate>Fri, 13 Jun 2025 23:25:23 +0800</lastBuildDate><atom:link href="https://ttf248.life/en/categories/computer/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>WeChat Backup Tool Local Network Recognition Failed</title>
        <link>https://ttf248.life/en/p/wechat-backup-tool-local-network-recognition-failed/</link>
        <pubDate>Fri, 13 Jun 2025 23:04:42 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/wechat-backup-tool-local-network-recognition-failed/</guid>
        <description>&lt;p&gt;I regularly clear out data on my phone, including photos and WeChat chat logs, backing them up to my computer. Previously, it worked seamlessly, allowing me to easily identify both my mobile phone and desktop PC within the same local network and directly back up the chat records to my computer. However, today everything has been failing.&lt;/p&gt;
&lt;h2 id=&#34;tried-solutions&#34;&gt;Tried Solutions
&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;Computer connected to WIFI, phone connected to WIFI, both devices are within the same local network, but still cannot be recognized.&lt;/li&gt;
&lt;li&gt;Phone turned on hotspot, computer connected to phone hotspot, still cannot be recognized.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;solutions&#34;&gt;Solutions
&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;Desktops use a wired network, mobile phones use a wireless network, and when restoring WeChat backups, it cannot recognize this as a local area network. I have tested it, and the desktop can normally access the mobile phone&amp;rsquo;s IP address.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;solutions-1&#34;&gt;Solutions
&lt;/h2&gt;&lt;p&gt;Initially, I thought it was something from Tencent, so I asked “Junhun,” to see if he had any ideas. The results he provided were not helpful. I casually tossed it over to “Dubao,” and he offered a surprise – prompting me to realize that my local environment might have many virtual networks or multi-NIC environments.&lt;/p&gt;
&lt;p&gt;This was indeed correct; my desktop has several virtual network cards, such as VMware, ZeroTier, Hyper-V, and Docker Desktop. My desktop also has multiple physical network cards, the primary network card connected to the router and a 2.5G network card forming a sub-local network for another machine.&lt;/p&gt;
&lt;p&gt;Therefore, I disabled all virtual network cards and extra physical network cards on my desktop, retaining the primary network card, and reattempted the backup – it was successful!&lt;/p&gt;</description>
        </item>
        <item>
        <title>Memory Layout and Binary Compatibility</title>
        <link>https://ttf248.life/en/p/memory-layout-and-binary-compatibility/</link>
        <pubDate>Tue, 10 Jun 2025 18:53:02 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/memory-layout-and-binary-compatibility/</guid>
        <description>&lt;p&gt;C++ service crashed. The service depends on a static library for compilation.
The static library made modifications, adding members to the header file and re-publishing the binary static library file.
The service relies on the new binary library file, compiles and runs normally, but then crashes. The crash point appears fine, similar to the crashes caused by compiler upgrades – undefined behavior, an untrustworthy stack trace.
Updating the service’s dependent header files allows it to mutate correctly, and running also works normally.
A detailed explanation is needed, involving computer science knowledge, I suspect it&amp;rsquo;s related to memory layout, with examples for a thorough explanation.&lt;/p&gt;
&lt;p&gt;This is a typical crash caused by mismatches between static library header files and binary library files. It’s indeed related to memory layout. Here’s a detailed explanation:&lt;/p&gt;
&lt;h2 id=&#34;problem-analysis&#34;&gt;Problem Analysis
&lt;/h2&gt;&lt;p&gt;Static libraries’ header files define the structure of classes (including member variables and methods), while their binary files contain the implementation code. During compilation, the compiler generates code based on the class definitions in the header files, including memory layout and access patterns. If the header file and the static library&amp;rsquo;s binary file are not compatible, it can lead to undefined behavior at runtime.&lt;/p&gt;
&lt;h2 id=&#34;key-concepts&#34;&gt;Key Concepts
&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Memory Layout:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In C++, the layout of class member variables in memory is determined by the compiler based on the header file definition.&lt;/li&gt;
&lt;li&gt;If a header file adds member variables, the memory layout of the class will change. For example, adding a new member variable may increase the size ( &lt;code&gt;sizeof&lt;/code&gt; ) of the class or change the offset of the member variables.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Binary Compatibility:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Static library binary files are generated based on the header files. If a service is compiled using older header files and links to a new static library binary at runtime, the service&amp;rsquo;s code will access class member variables according to the old memory layout, while the implementation code of the static library operates on the new memory layout. This inconsistency leads to undefined behavior.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Undefined Behavior:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Undefined behavior may manifest as crashes, incorrect stack traces, or even abnormal program execution results. This is because when the program accesses memory, it may access an incorrect address or an uninitialized region.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;examples&#34;&gt;Examples
&lt;/h2&gt;&lt;p&gt;Let&amp;rsquo;s consider a static library whose header file originally defined the following class:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;class MyClass {
public:
    int a;
    void func();
};
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The binary file of the static library is compiled based on this definition.
Later, the header file was modified to:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;class MyClass {
public:
    int a;
    int b;  // Added member variable
    void func();
};
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A new static library binary file is compiled based on the modified header file.
If the service compiles using the old header file, the following problems may occur:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Inconsistent Memory Layout:&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;The service assumes that &lt;code&gt;MyClass&lt;/code&gt;&amp;rsquo;s size is &lt;code&gt;sizeof(int)&lt;/code&gt;, while the static library assumes it’s &lt;code&gt;sizeof(int) * 2&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;When accessing members of the &lt;code&gt;MyClass&lt;/code&gt; object, the service may access an incorrect memory address.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Method Call Issues:&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;If the implementation code in the static library depends on the new member variable (e.g., &lt;code&gt;b&lt;/code&gt;), and the service&amp;rsquo;s code does not correctly initialize &lt;code&gt;b&lt;/code&gt;, a crash may occur at runtime.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;solutions&#34;&gt;Solutions
&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Maintain Consistency Between Header Files and Binary Files:&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;During service compilation, ensure you are using header files that match the binary file of the static library.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Avoid Incompatible Modifications to Binaries:&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;When modifying class definitions, try to avoid directly adding member variables; instead, use inheritance or other methods.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Recompile All Dependent Components:&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;When a static library is modified, recompile all services that depend on it to ensure consistency between the header files and binary files.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;summary&#34;&gt;Summary
&lt;/h2&gt;&lt;p&gt;The root cause of this issue is inconsistency between the header file and binary file, leading to errors in memory layout. The C++ compiler generates code based on the header file during compilation, while runtime behavior depends on the binary file. If these two don&amp;rsquo;t match, it results in undefined behavior. Ensuring consistency between the header file and binary file can prevent similar problems.&lt;/p&gt;</description>
        </item>
        <item>
        <title>10.243.52.68 also belongs to the local area network.</title>
        <link>https://ttf248.life/en/p/102435268-also-belongs-to-the-local-area-network/</link>
        <pubDate>Fri, 06 Jun 2025 18:46:55 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/102435268-also-belongs-to-the-local-area-network/</guid>
        <description>&lt;p&gt;Continuing from the previous discussion, today we’ll be talking about local area network IP addresses. Last time, in order to synchronize code, the server configured a proxy, and the server and the desktop computer in the house were able to connect to the network. Within a local area network, the proxy program was deployed on the desktop, and the server accessed the internet through the proxy. Code synchronization was very slow, so it was abandoned. Half a month later, when verifying the code on the server, the &lt;code&gt;Git&lt;/code&gt; code synchronization failed with a network error. Without much thought, I examined the error message.&lt;/p&gt;
&lt;h2 id=&#34;incident-scene&#34;&gt;Incident Scene
&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;fatal: unable to access &amp;lsquo;&lt;a class=&#34;link&#34; href=&#34;https://cnb.cool/ttf248/learn/cpp.git/%27&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://cnb.cool/ttf248/learn/cpp.git/’&lt;/a&gt;: Failed to connect to &lt;a class=&#34;link&#34; href=&#34;http://10.243.52.68&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;10.243.52.68&lt;/a&gt; port 7897 after 7 ms: Couldn’t connect to server&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;incident-scene-1&#34;&gt;Incident Scene
&lt;/h2&gt;&lt;p&gt;Naturally, they assumed that there was no network isolation between Alibaba Cloud services and the Tencent Cloud Native Development Platform, leading to code synchronization failures and error messages being thrown into the group. Smart people in the group saw the port information and said, &amp;ldquo;Is this a proxy IP? Then immediately someone said, &amp;lsquo;You&amp;rsquo;re using a local network, and the domain name resolution is incorrect,&amp;rsquo; and it was in the midst of memory loss, completely forgetting that they had configured a proxy.&amp;rdquo;
Seeing the word &lt;strong&gt;local network&lt;/strong&gt;, their brains returned to normal, and they remembered configuring the proxy. The error address was the local network address of their home desktop computer.&lt;/p&gt;
&lt;p&gt;Habitual thinking: 192.168.x.x is a local network address.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;In computer networking, a &lt;strong&gt;Local Area Network (LAN) IP address&lt;/strong&gt; refers to a private IP address used within a local network. These addresses are not directly exposed on the public internet and are primarily used for internal device communication. The &lt;code&gt;10.243.52.68&lt;/code&gt; and &lt;code&gt;192.168.x.x&lt;/code&gt; you mentioned both belong to &lt;strong&gt;private IP address ranges&lt;/strong&gt;, but they belong to different address ranges, and their application scenarios and logical planning also differ. Here&amp;rsquo;s a detailed comparison:&lt;/p&gt;
&lt;h2 id=&#34;private-ip-address-classification-and-ranges&#34;&gt;Private IP Address Classification and Ranges
&lt;/h2&gt;&lt;p&gt;According to &lt;a class=&#34;link&#34; href=&#34;https://datatracker.ietf.org/doc/rfc1918/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;RFC 1918&lt;/a&gt;, private IP addresses are divided into three ranges, each suitable for different sizes of local area networks:&lt;/p&gt;
&lt;p&gt;| &lt;strong&gt;10.0.0.0/8&lt;/strong&gt; | 255.0.0.0 | Approximately 16 million | Large enterprises, campus networks |&lt;/p&gt;
&lt;h2 id=&#34;private-ip-address-classification-and-ranges-1&#34;&gt;Private IP Address Classification and Ranges
&lt;/h2&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Address Range&lt;/th&gt;
&lt;th&gt;Subnet Mask&lt;/th&gt;
&lt;th&gt;Number of Available IPs&lt;/th&gt;
&lt;th&gt;Application Scenario&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;172.16.0.0/12&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;255.240.0.0&lt;/td&gt;
&lt;td&gt;Approximately 1 Million&lt;/td&gt;
&lt;td&gt;Medium-sized Enterprise Networks&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;private-ip-address-classification-and-ranges-2&#34;&gt;Private IP Address Classification and Ranges
&lt;/h2&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Address Range&lt;/th&gt;
&lt;th&gt;Subnet Mask&lt;/th&gt;
&lt;th&gt;Number of Available IPs&lt;/th&gt;
&lt;th&gt;Application Scenario&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;192.168.0.0/16&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;255.255.0.0&lt;/td&gt;
&lt;td&gt;Approximately 65,000&lt;/td&gt;
&lt;td&gt;Small Local Networks (Home, Office)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;ip-address-resolution-in-your-query&#34;&gt;&lt;strong&gt;IP Address Resolution in Your Query:&lt;/strong&gt;
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;10.243.52.68&lt;/code&gt;&lt;/strong&gt;
Belongs to the &lt;strong&gt;&lt;code&gt;10.0.0.0/8&lt;/code&gt;&lt;/strong&gt; range, a typical address for large private networks, often used in enterprise local area networks or wide area networks (such as internal networks across multiple branches).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;192.168.x.x&lt;/code&gt;&lt;/strong&gt;
Belongs to the &lt;strong&gt;&lt;code&gt;192.168.0.0/16&lt;/code&gt;&lt;/strong&gt; range, the most common address for small private networks, widely used in home routers and small offices etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;key-differences&#34;&gt;&lt;strong&gt;Key Differences&lt;/strong&gt;
&lt;/h2&gt;&lt;h3 id=&#34;address-space-size&#34;&gt;&lt;strong&gt;Address Space Size&lt;/strong&gt;
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;10.0.0.0/8&lt;/code&gt;&lt;/strong&gt;:
The address range of the subnet is &lt;code&gt;10.0.0.0 ~ 10.255.255.255&lt;/code&gt;, containing &lt;strong&gt;16,777,216 available IP addresses&lt;/strong&gt;, suitable for large networks (such as enterprises, schools, and data centers) that require a large number of IP addresses.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;192.168.0.0/16&lt;/code&gt;&lt;/strong&gt;:
The address range is &lt;code&gt;192.168.0.0 ~ 192.168.255.255&lt;/code&gt;, containing only &lt;strong&gt;65,536 available IP addresses&lt;/strong&gt;, suitable for small networks with a low number of devices (such as home networks typically having fewer than fifty devices).&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;subnetting-flexibility&#34;&gt;Subnetting Flexibility
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;10.0.0.0/8&lt;/code&gt;&lt;/strong&gt;: Due to the large address space, it can be further divided into multiple subnets (such as &lt;code&gt;10.1.0.0/16&lt;/code&gt;, &lt;code&gt;10.2.0.0/16&lt;/code&gt;, etc.) using a &lt;strong&gt;subnet mask&lt;/strong&gt; for layered network management and traffic isolation in larger networks.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;192.168.0.0/16&lt;/code&gt;&lt;/strong&gt;: Typically uses the default subnet mask &lt;code&gt;255.255.0.0&lt;/code&gt;; subnetting requirements are less frequent, suitable for simple flat network structures.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;common-use-cases&#34;&gt;&lt;strong&gt;Common Use Cases&lt;/strong&gt;
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;10.xxx.xxx.xxx&lt;/code&gt;&lt;/strong&gt;:
&lt;ul&gt;
&lt;li&gt;Enterprise Internal Networks: such as multiple branches of multinational companies connected via VPN, each branch can be assigned a separate subnet (e.g., &lt;code&gt;10.1.1.0/24&lt;/code&gt;, &lt;code&gt;10.1.2.0/24&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;Cloud Service Provider Internal Networks: private cloud environments like AWS and Alibaba Cloud often use the &lt;code&gt;10.&lt;/code&gt; address range.&lt;/li&gt;
&lt;li&gt;Industrial Control Networks: some industrial equipment defaults to using the &lt;code&gt;10.&lt;/code&gt; address range.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;192.168.xxx.xxx&lt;/code&gt;&lt;/strong&gt;:
&lt;ul&gt;
&lt;li&gt;Home/Small Office: default addresses assigned by routers (e.g., &lt;code&gt;192.168.1.1&lt;/code&gt; or &lt;code&gt;192.168.0.1&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;Small Local Area Networks for Sharing: such as WiFi networks built through a router, the number of devices is typically less than 200.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;network-isolation&#34;&gt;&lt;strong&gt;Network Isolation&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;Both are private addresses and &lt;strong&gt;cannot be directly accessed from the public internet&lt;/strong&gt;. They require a connection to the public network via &lt;strong&gt;NAT (Network Address Translation)&lt;/strong&gt; or &lt;strong&gt;VPN&lt;/strong&gt;. However, in practical usage:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;code&gt;10.&lt;/code&gt; segment is commonly used for scenarios requiring more complex network architectures (such as subnets and VLAN segmentation).&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;192.168.&lt;/code&gt; segment is preferred due to its smaller address space, making it suitable for plug-and-play simple networks.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;why-are-you-seeing-addresses-in-the-10-segment&#34;&gt;&lt;strong&gt;Why are you seeing addresses in the &lt;code&gt;10.&lt;/code&gt; segment?&lt;/strong&gt;
&lt;/h2&gt;&lt;p&gt;If you encounter addresses like &lt;code&gt;10.243.52.68&lt;/code&gt; on your local network, possible reasons include:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Enterprise Network Environment:&lt;/strong&gt;
Your company or organization may use the &lt;code&gt;10.&lt;/code&gt; segment as an internal IP address to differentiate between departments or devices (such as servers and office computers).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Virtual Machine or Container Networks:&lt;/strong&gt;
Cloud platforms (like VMware, Docker) often default to using the &lt;code&gt;10.&lt;/code&gt; segment for assigning IP addresses to virtual machines.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Device Default Configuration:&lt;/strong&gt;
Some network devices (such as industrial switches and firewalls) may be pre-configured with the &lt;code&gt;10.&lt;/code&gt; segment by the manufacturer.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-how-to-choose&#34;&gt;&lt;strong&gt;Summary: How to Choose?&lt;/strong&gt;
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Small Networks (Home/Office):&lt;/strong&gt; Prioritize &lt;code&gt;192.168.x.x&lt;/code&gt;, simple configuration and sufficient for use.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Medium to Large Networks (Enterprise/Campus):&lt;/strong&gt; Use &lt;code&gt;10.x.x.x&lt;/code&gt; or &lt;code&gt;172.16.x.x&lt;/code&gt; for subnetting and managing a large number of devices.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Both are fundamentally local area network addresses, the difference lies in address space and applicable scenarios, with no technical superiority – it depends on the networking planning needs.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Git Single Configuration Proxy</title>
        <link>https://ttf248.life/en/p/git-single-configuration-proxy/</link>
        <pubDate>Fri, 06 Jun 2025 18:29:46 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/git-single-configuration-proxy/</guid>
        <description>&lt;p&gt;Accessing GitHub domestically is slow, and you can speed it up by configuring a proxy. There’s also another way: find a domestic hosting platform like CodeOcean or Coding. Configure the corresponding build pipeline to sync your code to GitHub.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;ve been using ‘coding’ for many years – its interface is simple, and they recently released an announcement that the free version can no longer be used, requiring migration to Tencent’s new platform, &lt;code&gt;cnb&lt;/code&gt;.  Let me also complain a bit about Alibaba’s hosting platform; the entire interface design feels very dated/stuffy. &lt;strong&gt;It&amp;rsquo;s like eating a bowl of congee (班味 - ban wei)&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://cnb.cool/ttf248&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://cnb.cool/ttf248&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;repository-migration&#34;&gt;Repository Migration
&lt;/h2&gt;&lt;p&gt;The &lt;code&gt;cnb&lt;/code&gt; website provides migration tools that can batch migrate code from &lt;code&gt;github&lt;/code&gt; to &lt;code&gt;cnb&lt;/code&gt;.
&lt;a class=&#34;link&#34; href=&#34;https://docs.cnb.cool/zh/guide/migration-tools.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://docs.cnb.cool/zh/guide/migration-tools.html&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;git-proxy-configuration&#34;&gt;Git Proxy Configuration
&lt;/h2&gt;&lt;p&gt;To avoid slow synchronization due to the lack of an accelerator, tools will first synchronize code to your local machine and then upload it to the remote repository.&lt;/p&gt;
&lt;p&gt;Git can be configured for HTTP proxies independently using the following commands, without affecting system-wide settings:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Set HTTP proxy
git config --global http.proxy http://proxy.example.com:8080

# Set HTTPS proxy
git config --global https.proxy http://proxy.example.com:8080

# Optional: Configure proxy for a specific domain
git config --global http.https://github.com.proxy http://proxy.example.com:8080
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To remove the proxy configuration, use the following commands:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git config --global --unset http.proxy
git config --global --unset https.proxy
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To view your current proxy configuration:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git config --global --get http.proxy
git config --global --get https.proxy
&lt;/code&gt;&lt;/pre&gt;
</description>
        </item>
        <item>
        <title>Prometheus Monitoring System Histogram and Summary</title>
        <link>https://ttf248.life/en/p/prometheus-monitoring-system-histogram-and-summary/</link>
        <pubDate>Wed, 04 Jun 2025 19:00:28 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/prometheus-monitoring-system-histogram-and-summary/</guid>
        <description>&lt;p&gt;Business systems designed monitoring metrics of type Summary, calculating the average duration: &lt;code&gt;request_duration_milliseconds_sum / request_duration_milliseconds_count&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Reviewing the data, a particular interface was found to have very high average duration, and when examining the time series chart, the average duration suddenly increased – effectively, one request took a long time, which pulled up the overall average.  The goal was to identify exactly when this request occurred, but due to the low number of requests within the period, the data retrieved remained empty.&lt;/p&gt;
&lt;h2 id=&#34;qa&#34;&gt;Q&amp;amp;A
&lt;/h2&gt;&lt;h3 id=&#34;-why-does-_sum-and-_count-have-data&#34;&gt;✅ Why does &lt;code&gt;_sum&lt;/code&gt; and &lt;code&gt;_count&lt;/code&gt; have data?
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;code&gt;_sum&lt;/code&gt; and &lt;code&gt;_count&lt;/code&gt; are the &lt;strong&gt;core metrics&lt;/strong&gt; of the Summary type, and Prometheus always collects and records these values;&lt;/li&gt;
&lt;li&gt;They are &lt;strong&gt;cumulative counters&lt;/strong&gt;, suitable for use with &lt;code&gt;rate()&lt;/code&gt; or &lt;code&gt;increase()&lt;/code&gt;;&lt;/li&gt;
&lt;li&gt;Regardless of request latency changes, as long as there are requests, &lt;code&gt;_sum&lt;/code&gt; and &lt;code&gt;_count&lt;/code&gt; will always have data.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;-why-quantile099-might-not-display-in-a-time-series-chart&#34;&gt;❌ Why &lt;code&gt;{quantile=&amp;quot;0.99&amp;quot;}&lt;/code&gt; might &lt;strong&gt;not display&lt;/strong&gt; in a Time Series Chart
&lt;/h3&gt;&lt;p&gt;Even if Summary is configured with &lt;code&gt;quantile=&amp;quot;0.99&amp;quot;&lt;/code&gt;, this time series may not exist or be missing:
Metrics are definitely configured, and the data hasn&amp;rsquo;t expired. 📉 The request volume is too small, preventing quantile calculation, due to the sliding window mechanism; after this period of time, it will no longer be included in the statistical range.
Quantiles (such as p99) are calculated through sampling statistics:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If the request count over a certain period is too low (e.g., 1~2 requests), the calculation of p99 is &lt;strong&gt;unstable or lacks representativeness&lt;/strong&gt;;&lt;/li&gt;
&lt;li&gt;Prometheus &lt;strong&gt;client SDK will choose not to expose this quantile time series&lt;/strong&gt; to avoid misleading;&lt;/li&gt;
&lt;li&gt;Therefore, you&amp;rsquo;ll see &lt;code&gt;_sum&lt;/code&gt; and &lt;code&gt;_count&lt;/code&gt; accumulating normally, but &lt;code&gt;quantile=&amp;quot;0.99&amp;quot;&lt;/code&gt; has no data.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;histogram-and-summary-differences&#34;&gt;Histogram and Summary Differences
&lt;/h2&gt;&lt;h3 id=&#34;histogram&#34;&gt;&lt;strong&gt;Histogram&lt;/strong&gt;
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;How it Works:&lt;/strong&gt;&lt;br&gt;
A histogram will bucket data, recording the number of samples falling into each bucket.
For example, if buckets are defined as &lt;code&gt;[10ms, 50ms, 100ms, 500ms, 1s]&lt;/code&gt;, each request latency would be assigned to the corresponding bucket.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Advantages:&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Can aggregate data from multiple instances (e.g., multiple service node request latencies) in Prometheus.&lt;/li&gt;
&lt;li&gt;Suitable for calculating percentiles (such as P50, P95, P99) and observing latency distributions.&lt;/li&gt;
&lt;li&gt;Provides flexible querying capabilities, supporting dynamic percentile calculation through PromQL.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Disadvantages:&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Requires predefining the bucket range; an inappropriate choice can lead to uneven data distribution (e.g., all requests falling into one bucket).&lt;/li&gt;
&lt;li&gt;The more buckets you have, the greater the storage and computational overhead.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Suitable Scenarios:&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Aggregating data from multiple instances.&lt;/li&gt;
&lt;li&gt;Dynamically adjusting percentiles or analyzing latency distributions.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;summary&#34;&gt;&lt;strong&gt;Summary&lt;/strong&gt;
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;How it Works:&lt;/strong&gt;
The Summary component calculates percentiles (such as P50, P95, P99) directly on the client and reports the results to Prometheus.
It also records the total number and sum of samples for calculating averages.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Advantages:&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Does not require predefined buckets, providing percentile results directly.&lt;/li&gt;
&lt;li&gt;Suitable for precise percentile calculations in single instances.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Disadvantages:&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Percentile calculation is performed on the client side, preventing aggregation of data from multiple instances in Prometheus.&lt;/li&gt;
&lt;li&gt;Adjusting percentiles (e.g., changing from P95 to P99) requires modifying the code and redeploying.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Use Cases:&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Single instance monitoring where precise percentile accuracy is a high priority.&lt;/li&gt;
&lt;li&gt;When aggregation of data from multiple instances is not required.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;key-difference-comparison&#34;&gt;&lt;strong&gt;Key Difference Comparison&lt;/strong&gt;
&lt;/h3&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Feature&lt;/th&gt;
&lt;th&gt;Histogram&lt;/th&gt;
&lt;th&gt;Summary&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Quantile Calculation&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Calculated dynamically within Prometheus&lt;/td&gt;
&lt;td&gt;Calculated directly on the client side&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;key-differences-comparison&#34;&gt;&lt;strong&gt;Key Differences Comparison&lt;/strong&gt;
&lt;/h3&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Feature&lt;/th&gt;
&lt;th&gt;Histogram&lt;/th&gt;
&lt;th&gt;Summary&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Multi-Instance Aggregation&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Supported&lt;/td&gt;
&lt;td&gt;Not Supported&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;key-differences-comparison-1&#34;&gt;&lt;strong&gt;Key Differences Comparison&lt;/strong&gt;
&lt;/h3&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Feature&lt;/th&gt;
&lt;th&gt;Histogram&lt;/th&gt;
&lt;th&gt;Summary&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Bin Definition&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Requires pre-defined&lt;/td&gt;
&lt;td&gt;Does not require&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;key-differences-comparison-2&#34;&gt;&lt;strong&gt;Key Differences Comparison&lt;/strong&gt;
&lt;/h3&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Feature&lt;/th&gt;
&lt;th&gt;Histogram&lt;/th&gt;
&lt;th&gt;Summary&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Storage Overhead&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Depends on the number of buckets&lt;/td&gt;
&lt;td&gt;Fixed overhead&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;key-differences-comparison-3&#34;&gt;&lt;strong&gt;Key Differences Comparison&lt;/strong&gt;
&lt;/h3&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Feature&lt;/th&gt;
&lt;th&gt;Histogram&lt;/th&gt;
&lt;th&gt;Summary&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Flexibility&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;High (dynamically adjustable bins)&lt;/td&gt;
&lt;td&gt;Low (requires code modification to adjust bins)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;summary-1&#34;&gt;Summary
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;If you need to aggregate data from multiple instances or require flexible quantile adjustments, choose &lt;strong&gt;Histogram&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;If you only need the precise quantiles for a single instance and the quantiles are fixed, choose &lt;strong&gt;Summary&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;In your scenario, given that the service is distributed, it’s recommended to prioritize using &lt;strong&gt;Histogram&lt;/strong&gt; so that all instance data can be aggregated in Prometheus and dynamically calculate quantiles and distributions of latency.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;sliding-window-concept-and-its-relationship-with-histograms-and-summaries&#34;&gt;Sliding Window Concept and Its Relationship with Histograms and Summaries
&lt;/h2&gt;&lt;h3 id=&#34;sliding-window-concept&#34;&gt;&lt;strong&gt;Sliding Window Concept&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;A sliding window is a time-windowing mechanism used to analyze changes in data over a period. It dynamically reflects the system&amp;rsquo;s real-time state by continuously moving a temporal range. The key characteristics of a sliding window are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Fixed Time Range:&lt;/strong&gt; The length of the window is fixed, such as the last 1 minute or 5 minutes.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Real-Time Updates:&lt;/strong&gt; As time passes, the window slides, old data is removed from the window, and new data is added to the window.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Common Uses:&lt;/strong&gt; Used for calculating real-time metrics (such as request rates, averages, percentiles, etc.).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In Prometheus, sliding windows are typically implemented using query functions (like &lt;code&gt;rate()&lt;/code&gt;, &lt;code&gt;avg_over_time()&lt;/code&gt;).&lt;/p&gt;
&lt;h3 id=&#34;sliding-window-and-histogram-relationship&#34;&gt;&lt;strong&gt;Sliding Window and Histogram Relationship&lt;/strong&gt;
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Histogram Data Structure:&lt;/strong&gt;
A histogram will bin sample data and record the count for each bucket. Prometheus periodically scrapes these counts.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sliding Window Implementation:&lt;/strong&gt;
In Prometheus, sliding windows can be applied to histogram data using query statements. For example:
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;rate(http_request_duration_seconds_bucket[5m])&lt;/code&gt;: Calculates the request rate within each bucket over the past 5 minutes.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))&lt;/code&gt;: Calculates the 95th percentile of the request duration over the past 5 minutes.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Advantages:&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Sliding windows can dynamically reflect the recent request latency distribution.&lt;/li&gt;
&lt;li&gt;The binning mechanism of histograms combined with sliding windows allows for efficient calculation of percentiles and distributions.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;sliding-window-and-summary-relationship&#34;&gt;&lt;strong&gt;Sliding Window and Summary Relationship&lt;/strong&gt;
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Summary Data Structure:&lt;/strong&gt;
Summary calculates percentiles directly on the client side and reports them to Prometheus. It also records the total sample count and sum.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sliding Window Implementation:&lt;/strong&gt;
In Prometheus, sliding windows can be applied to Summary data using query statements. For example:
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;rate(http_request_duration_seconds_sum[5m]) / rate(http_request_duration_seconds_count[5m])&lt;/code&gt;: Calculates the average request duration over the past 5 minutes.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Limitations:&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Summary percentiles are calculated on the client side and cannot be recalculated in Prometheus, therefore support for sliding windows with percentiles is limited.&lt;/li&gt;
&lt;li&gt;Sliding windows cannot directly operate on Summary percentiles when aggregating data from multiple instances.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;sliding-window-applicability&#34;&gt;&lt;strong&gt;Sliding Window Applicability&lt;/strong&gt;
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Real-time Monitoring:&lt;/strong&gt; Sliding windows are suitable for monitoring system real-time status, such as request rates over the last minute and latency distributions.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Anomaly Detection:&lt;/strong&gt;  By using a sliding window, it’s possible to quickly identify short-term anomalies (e.g., sudden increase in request latency).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dynamic Analysis:&lt;/strong&gt; Sliding windows can dynamically reflect changes in system trends rather than static global statistics.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;summary-2&#34;&gt;Summary
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Histogram&lt;/strong&gt; combined with a sliding window can dynamically calculate percentiles (such as P95, P99) and request latency distributions, suitable for monitoring distributed systems.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Summary&lt;/strong&gt; combined with a sliding window can calculate simple metrics such as averages, but lacks flexibility regarding percentiles and does not support multi-instance aggregation.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In your scenario, due to the need to monitor extreme request latencies (such as P99) and the average latency of most requests, it is recommended to use &lt;strong&gt;Histogram&lt;/strong&gt; and combine it with a sliding window query to dynamically analyze system performance.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Blog Translation Project Musings: Cultural Transmission, AI Programming</title>
        <link>https://ttf248.life/en/p/blog-translation-project-musings-cultural-transmission-ai-programming/</link>
        <pubDate>Mon, 02 Jun 2025 21:41:00 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/blog-translation-project-musings-cultural-transmission-ai-programming/</guid>
        <description>&lt;p&gt;Cultural Propagation: Its ideological influence, subtle and pervasive.
AI Programming: Not performing software design, resulting in a lot of rework.&lt;/p&gt;
&lt;h2 id=&#34;cultural-propagation&#34;&gt;Cultural Propagation
&lt;/h2&gt;&lt;p&gt;Initially, the project only supported English, Japanese, and Korean. Thinking it was just AI translation, we wondered if supporting more languages wouldn’t be a good idea. So, French, Russian, and Hindi were added. At this point, no problems were detected; when the program executed translations, formatting issues arose due to historical code problems, requiring re-translation of archived articles.&lt;/p&gt;
&lt;p&gt;Statistical timing reminders indicated that it would take nearly 20 hours to complete all translations, given that it was a large local model with 12b parameters. Considering reducing translation time, we deleted French, Russian, and Hindi. This is when things started to feel wrong – why did I instinctively choose Korean and Japanese among the initially supported languages?&lt;/p&gt;
&lt;p&gt;According to global population distribution, these two languages had relatively small audiences, especially Korean, which had approximately 80 million users worldwide. Japanese was slightly more prevalent, with around 1.2 billion people. In contrast, French, Russian, and Hindi had user populations exceeding 100 million.&lt;/p&gt;
&lt;p&gt;At this point, we realized that the popularity of Korean and Japanese wasn’t due to the sheer number of language speakers, but rather the influence of cultural propagation. Korean and Japanese cultures have a wide-reaching impact globally, particularly in Asia. K-pop, anime, and television dramas attracted a large fanbase, leading these fans to naturally develop an interest in the corresponding languages.&lt;/p&gt;
&lt;p&gt;Looking back at our growth history, I frequently watched Japanese anime and manga as a child, and later watched many Korean films and TV series. This led me to instinctively choose these familiar languages when setting up the project’s initial language settings.&lt;/p&gt;
&lt;h3 id=&#34;software-design-and-ai-programming&#34;&gt;Software Design and AI Programming
&lt;/h3&gt;&lt;p&gt;The initial translation assistant was initially just a simple tool, but after experiencing Claude4’s coding capabilities, it gradually expanded its functionality, adding modules for article translation and tag translation. As the features increased, so did the code complexity. Although AI refactored the codebase to appear more organized, in expanding new functionalities or fixing bugs, AI-generated code often suffered from repetition issues.&lt;/p&gt;
&lt;p&gt;AI lacks a holistic understanding of overall structure and design principles when generating code. It typically modifies and extends existing code rather than effectively reusing existing modules, leading to redundant code. Manual cleanup of duplicate code is then required, which inadvertently increases development costs.&lt;/p&gt;
&lt;p&gt;Furthermore, while AI-generated code is syntactically correct, it often suffers from problems in logic and design. For example, a slight adjustment to the prompt in another project resulted in completely different webpage structures lacking consistency. This reflects a lack of proper initial design, with new features added arbitrarily and haphazardly, leading to a chaotic codebase.&lt;/p&gt;
&lt;p&gt;This also reminds us that core software engineering experience remains indispensable. A rational design not only reduces rework but also enhances code maintainability and extensibility. While AI is a powerful tool, it cannot replace human deep understanding and planning of systems.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Blog Translation Project Musings: Historical Conversations</title>
        <link>https://ttf248.life/en/p/blog-translation-project-musings-historical-conversations/</link>
        <pubDate>Mon, 02 Jun 2025 21:16:24 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/blog-translation-project-musings-historical-conversations/</guid>
        <description>&lt;p&gt;The initial design of the blog translation project was overly complex – first parsing Markdown format, then using placeholders to protect the content, and finally sending it to a large model for translation. This was entirely unnecessary; large models inherently possess the ability to recognize Markdown syntax and can directly process the original content while maintaining formatting during translation.&lt;/p&gt;
&lt;p&gt;Our work shifted from debugging code to debugging the &lt;strong&gt;prompting&lt;/strong&gt; of the model.
Model: &lt;code&gt;google/gemma-3-4b&lt;/code&gt;
Hardware: &lt;code&gt;Nvidia 3060 12GB&lt;/code&gt;
Indeed, we chose a non-thinking model – thinking models were inefficient when executing translation tasks. We compared the performance of 4b and 12b parameters, and for translation purposes, gemma3’s 4b parameter was sufficient; there was no significant advantage in terms of 12b parameters.
12b parameter speed: &lt;strong&gt;11.32 tok/sec&lt;/strong&gt; , 4b parameter speed: &lt;strong&gt;75.21 tok/sec&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;background-introduction&#34;&gt;Background Introduction
&lt;/h2&gt;&lt;p&gt;Despite adding various constraints within the &lt;strong&gt;system&lt;/strong&gt;, the output translation results still presented some issues, such as: lack of formatting protection, inclusion of extraneous explanatory content. When defining roles, it was already stated to protect Markdown format and only output translation results; ultimately, the translation remained unstable.&lt;/p&gt;
&lt;p&gt;At this point, I remembered encountering a comic translation project previously, which also leveraged the capabilities of large models. Its translation effect seemed better than mine. Upon reviewing the code and comparing the request data, the comic translation project would include a set of context with each request, in addition to the current translation content, it would also include previous translation content.&lt;/p&gt;
&lt;p&gt;What were the benefits? Not only did this improve the coherence between preceding and following translations, but it also ensured the stability of the output format.&lt;/p&gt;
&lt;h2 id=&#34;the-importance-of-historical-conversations&#34;&gt;The Importance of Historical Conversations
&lt;/h2&gt;&lt;p&gt;As large AI models (such as the GPT series, Claude, Gemini, etc.) become more prevalent, an increasing number of businesses and developers are accessing these models via APIs to build intelligent customer service, content generation, code assistant, and other applications. However, many people encounter a common issue during initial access: &lt;strong&gt;model outputs are disjointed, lack contextual understanding, and even answer the wrong questions.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;A key reason for this phenomenon is – &lt;strong&gt;not including historical conversation content in API requests.&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;what-is-a-history-dialogue&#34;&gt;What is a History Dialogue?
&lt;/h2&gt;&lt;p&gt;A history dialogue refers to the exchange records between a model and a user within a single conversation session. In most large language model APIs (such as OpenAI’s Chat Completions API), developers need to construct the complete &lt;code&gt;messages&lt;/code&gt; array themselves, passing the historical dialogue in turn as &lt;code&gt;user&lt;/code&gt; and &lt;code&gt;assistant&lt;/code&gt; message format.&lt;/p&gt;
&lt;h3 id=&#34;example&#34;&gt;Example
&lt;/h3&gt;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{
  &amp;quot;model&amp;quot;: &amp;quot;gpt-4&amp;quot;,
  &amp;quot;messages&amp;quot;: [
    {&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;Write me a resignation letter&amp;quot;},
    {&amp;quot;role&amp;quot;: &amp;quot;assistant&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;Okay, what do you want to write about as the reason for your resignation?&amp;quot;},
    {&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;I want to pursue personal career development&amp;quot;}
  ]
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you only send the last sentence:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;I want to pursue personal career development&amp;quot;}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The model won&amp;rsquo;t know you are writing a resignation letter, and its output quality will be very poor because it doesn’t understand the context.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;why-historical-dialogue-is-so-important&#34;&gt;Why Historical Dialogue is So Important?
&lt;/h2&gt;&lt;h3 id=&#34;1-build-context-enhance-coherence&#34;&gt;1. &lt;strong&gt;Build Context, Enhance Coherence&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;AI models are inherently “context-driven.” They cannot remember anything that has happened “previously,” unless you explicitly tell it. By passing in the dialogue history, the model can better understand your intent and topic context, resulting in outputs more aligned with expectations.&lt;/p&gt;
&lt;h3 id=&#34;2-reduce-misunderstanding-rate&#34;&gt;2. &lt;strong&gt;Reduce Misunderstanding Rate&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;If you want the model to complete a multi-turn instruction, such as writing, summarizing, or debugging code, historical context allows the model to gradually accumulate understanding and avoid going off-topic or losing focus midway through.&lt;/p&gt;
&lt;h3 id=&#34;3-simulating-realistic-human-dialogue-behavior&#34;&gt;3. &lt;strong&gt;Simulating Realistic Human Dialogue Behavior&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;In practical applications such as customer service systems, educational assistants, and health consultations, user questions often unfold gradually rather than being expressed clearly in a single instance. Preserving dialogue history allows the AI to behave more like a “memoryful assistant.”&lt;/p&gt;
&lt;h2 id=&#34;how-to-correctly-add-historical-conversations-in-an-api&#34;&gt;How to Correctly Add Historical Conversations in an API?
&lt;/h2&gt;&lt;p&gt;Using OpenAI&amp;rsquo;s API as an example, we recommend following the structure below:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;messages = [
    {&amp;quot;role&amp;quot;: &amp;quot;system&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;You are a professional legal assistant&amp;quot;},
    {&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;What are the essential conditions for a contract?&amp;quot;},
    {&amp;quot;role&amp;quot;: &amp;quot;assistant&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;Contract validity requires fulfilling several conditions: ...&amp;quot;},
    {&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;Does an oral agreement count?&amp;quot;}
]

response = openai.ChatCompletion.create(
    model=&amp;quot;gpt-4&amp;quot;,
    messages=messages
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Use the &lt;code&gt;system&lt;/code&gt; message to set the model&amp;rsquo;s behavior and identity.&lt;/li&gt;
&lt;li&gt;Only retain recent key conversations, not necessarily the entire history (to avoid exceeding token limits).&lt;/li&gt;
&lt;li&gt;In long sessions, truncate early content and maintain core information summaries to control token consumption.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;practical-recommendations&#34;&gt;Practical Recommendations
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Dialogue State Management&lt;/strong&gt;: The backend needs to design caching mechanisms to record each user’s conversation history (e.g., Redis, database).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Limit Length&lt;/strong&gt;: OpenAI GPT-4 has a context length of 128k tokens, Claude 3 can reach 200k~1M, and requires reasonable truncation.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dynamic Summarization of History&lt;/strong&gt;: When the historical content is too long, use a model to first summarize the old conversations before adding them to the dialogue context.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;summary&#34;&gt;Summary
&lt;/h2&gt;&lt;p&gt;AI large model capabilities are powerful, but developers need to “feed” it sufficient contextual information. &lt;strong&gt;By adding historical conversations within API requests, not only can the quality and coherence of the model’s output be significantly improved, but users can also experience a more natural and realistic conversation.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Regardless of whether you&amp;rsquo;re building AI customer service, writing assistants, coding helpers, or educational applications, this is an optimization technique that cannot be ignored.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Find all functions not referenced in the Go project.</title>
        <link>https://ttf248.life/en/p/find-all-functions-not-referenced-in-the-go-project/</link>
        <pubDate>Mon, 02 Jun 2025 21:10:47 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/find-all-functions-not-referenced-in-the-go-project/</guid>
        <description>&lt;p&gt;In a Go language project, using &lt;code&gt;staticcheck&lt;/code&gt; to find unused functions is an efficient static analysis method.&lt;/p&gt;
&lt;h3 id=&#34;1-install-staticcheck&#34;&gt;&lt;strong&gt;1. Install staticcheck&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;Ensure that Go (version 1.16+) is installed, and then execute the following command to install &lt;code&gt;staticcheck&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;go install honnef.co/go/tools/cmd/staticcheck@latest
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;2-basic-usage-finding-unused-functions&#34;&gt;&lt;strong&gt;2. Basic Usage: Finding Unused Functions&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;Run the following command in the project root directory:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;staticcheck ./...
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Key Check Rules:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;U1000&lt;/strong&gt;: Detects unused functions, methods, variables, or types.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;U1001&lt;/strong&gt;: Detects unused parameters.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;3-filter-specific-check-rules&#34;&gt;&lt;strong&gt;3. Filter Specific Check Rules&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;If you only want to check unused functions, you can specify the rules:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;staticcheck -checks=U1000 ./...
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;4-output-format&#34;&gt;&lt;strong&gt;4. Output Format&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;Default output format is &lt;code&gt;{path}:{line}:{column}: {message}&lt;/code&gt;, for example:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;main.go:10:2: func UnusedFunction is unused (U1000)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;5-configuration-file-optional&#34;&gt;&lt;strong&gt;5. Configuration File (Optional)&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;Create a &lt;code&gt;.staticcheck.conf&lt;/code&gt; file in the project root directory to customize inspection rules:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{
  &amp;quot;checks&amp;quot;: [&amp;quot;U1000&amp;quot;, &amp;quot;-ST1000&amp;quot;]  // Enable U1000, disable ST1000 (string formatting rule)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;6-integration-with-vs-code&#34;&gt;&lt;strong&gt;6. Integration with VS Code&lt;/strong&gt;
&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;Install the &lt;a class=&#34;link&#34; href=&#34;https://marketplace.visualstudio.com/items?itemName=golang.go&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Go Extension&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Add to &lt;code&gt;settings.json&lt;/code&gt;:&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;7-ignore-specific-code&#34;&gt;&lt;strong&gt;7. Ignore Specific Code&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;Adding a comment &lt;code&gt;//lint:ignore U1000 reason&lt;/code&gt; above the function to ignore the check:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;//lint:ignore U1000 Used by generated code
func UnusedButNeeded() {}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;frequently-asked-questions&#34;&gt;&lt;strong&gt;Frequently Asked Questions&lt;/strong&gt;
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Q: How do I handle unused functions in test files?&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;A: &lt;code&gt;staticcheck&lt;/code&gt; defaults to checking test files. To exclude them, use the &lt;code&gt;-tests=false&lt;/code&gt; flag.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Q: How do I integrate it into CI/CD?&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;A: Add to GitHub Actions:&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;example-output&#34;&gt;&lt;strong&gt;Example Output&lt;/strong&gt;
&lt;/h3&gt;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ staticcheck -checks=U1000 ./...
internal/utils/helper.go:15:2: func privateHelper is unused (U1000)
cmd/server/main.go:23:2: func initConfig is unused (U1000)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using the &lt;code&gt;staticcheck&lt;/code&gt; rule &lt;code&gt;U1000&lt;/code&gt;, you can quickly identify and clean up unused functions, improving code quality.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Git Submodule Merge History Lost</title>
        <link>https://ttf248.life/en/p/git-submodule-merge-history-lost/</link>
        <pubDate>Mon, 02 Jun 2025 16:44:58 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/git-submodule-merge-history-lost/</guid>
        <description>&lt;p&gt;There’s a Git repository locally where submodules were in temporary branches during the initial pull. I made commits on that temporary branch, and then switched back to the &lt;code&gt;main&lt;/code&gt; branch. However, those committed changes seem to be lost and cannot be found within the &lt;code&gt;main&lt;/code&gt; branch. I also can&amp;rsquo;t find any record of that temporary branch.&lt;/p&gt;
&lt;h2 id=&#34;solutions&#34;&gt;Solutions
&lt;/h2&gt;&lt;p&gt;After committing to a temporary branch within the Git submodule and switching back to the &lt;code&gt;main&lt;/code&gt; branch, these commits are no longer found. This situation can be resolved by following the steps below:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Verify Commit History&lt;/strong&gt;: Navigate to the submodule directory and use &lt;code&gt;reflog&lt;/code&gt; to find the lost commits.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Create a New Branch to Save Commits&lt;/strong&gt;: Create a new branch based on the lost commit.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Merge or Cherry-Pick Commits to Main&lt;/strong&gt;: Integrate the code into the main branch.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Here are the specific steps:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Enter the submodule directory
cd path/to/your/submodule

# View reflog records all HEAD changes (including commits not associated with branches)
git reflog

PS F:\dev\notebook\scripts\hugo-content-suite&amp;gt; git reflog
de05175 (HEAD -&amp;gt; main, origin/main, origin/HEAD) HEAD@{0}: checkout: moving from c8d070651310e90d283cb64d98da088c5fe05e73 to main
c8d0706 HEAD@{1}: commit: feat: Add Markdown tag symbol usage documentation, provide detailed syntax examples and effect display
48250f5 HEAD@{2}: commit: feat: Remove article translation preview function, simplify the translation process
b8280b6 HEAD@{3}: commit: feat: Add absolute path acquisition function, support converting relative paths to absolute paths
92c354b HEAD@{4}: commit: fix: Fix article scanning logic, ensure scanning using absolute paths
de05175 (HEAD -&amp;gt; main, origin/main, origin/HEAD) HEAD@{5}: checkout: moving from main to de05175d4ec0828e3ae95d726b09dfff18f67a23
de05175 (HEAD -&amp;gt; main, origin/main, origin/HEAD) HEAD@{6}: clone: from https://cnb.cool/ttf248/hugo-content-suite.git

# Create a new branch based on the lost commit (e.g., using 456def commit)
git checkout -b saved-work 456def

# Switch back to the main branch
git checkout main

# Merge the saved work into main (or use cherry-pick to select specific commits)
git merge saved-work
# Or
git cherry-pick 456def

# Return to the parent project directory and commit submodule updates
cd ..
git add path/to/your/submodule
git commit -m &amp;quot;Update submodule to include new changes&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;key-operational-instructions&#34;&gt;Key Operational Instructions
&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;git reflog&lt;/strong&gt;: Displays the entire history of HEAD, including commits not associated with any branch.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;git checkout -b&lt;/strong&gt;: Creates a new branch from any commit and saves your work.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;git merge/cherry-pick&lt;/strong&gt;: Integrates saved commits into the target branch.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If the reflog doesn&amp;rsquo;t contain records, you may need to try using &lt;code&gt;git fsck --lost-found&lt;/code&gt; to find orphaned commits, but this is a rare occurrence.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>GitHub Pages Easter Egg: Deploying Multiple Pages</title>
        <link>https://ttf248.life/en/p/github-pages-easter-egg-deploy-multiple-sites/</link>
        <pubDate>Wed, 28 May 2025 02:55:54 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/github-pages-easter-egg-deploy-multiple-sites/</guid>
        <description>&lt;p&gt;Recently, my biological clock has been a bit off, still tinkering with GitHub Pages deployments around 2 AM.&lt;/p&gt;
&lt;p&gt;I went to eat after work, and I just wanted to sleep when I finished, then came back around 8:30 PM, feeling drowsy, thinking I’d take a nap, and ended up falling asleep immediately. When I woke up, it was already past 2 AM.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Categories that haven&amp;rsquo;t even launched yet: AI Study Group&lt;/p&gt;
&lt;/blockquote&gt;
&lt;!-- more --&gt;
&lt;h2 id=&#34;facepalm&#34;&gt;Facepalm
&lt;/h2&gt;&lt;p&gt;Yesterday we said there wouldn&amp;rsquo;t be much frontend development, but today it’s not frontend – it’s the experience of UI/UX.&lt;/p&gt;
&lt;h2 id=&#34;project&#34;&gt;Project
&lt;/h2&gt;&lt;p&gt;Please join our old friend, &lt;a class=&#34;link&#34; href=&#34;https://github.com/ttf248/ai-coding-demo&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/ttf248/ai-coding-demo&lt;/a&gt;.
That’s the original self-selected stock project – we&amp;rsquo;re restructuring the overall project structure, and all subsequent AI programming content will be housed within this project.&lt;/p&gt;
&lt;h2 id=&#34;deploying-multiple-pages&#34;&gt;Deploying Multiple Pages
&lt;/h2&gt;&lt;p&gt;The project is hosted domestically at &lt;a class=&#34;link&#34; href=&#34;https://cnb.cool/ttf248/ai-coding-demo&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://cnb.cool/ttf248/ai-coding-demo&lt;/a&gt;. Due to well-known reasons, pages cannot be published within China, so we need to publish them on GitHub outside of it.&lt;/p&gt;
&lt;p&gt;The blog is published on the external GitHub. I haven&amp;rsquo;t tried this before, and also, the current project I’m working on isn’t a traditional blog site; it simply contains a lot of documentation layered with several static HTML design mockups.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ttf248.life/p/github-pages-easter-egg-deploy-multiple-sites/20250528030230.png&#34;
	width=&#34;798&#34;
	height=&#34;530&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;pages&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;150&#34;
		data-flex-basis=&#34;361px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;That’s right – this page is where I discovered that deploying multiple pages using pages won&amp;rsquo;t affect the blog’s publication, but instead adds a new path under the blog’s domain name.&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://ttf248.life/ai-coding-demo/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://ttf248.life/ai-coding-demo/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I was absolutely thrilled when I saw this!&lt;/p&gt;
&lt;h2 id=&#34;ai-study-group&#34;&gt;AI Study Group
&lt;/h2&gt;&lt;p&gt;Yesterday, I created a new category and thought about using AI to learn many computer courses, such as algorithms and LeetCode practice problems.
Each learning record is published on the blog to form a knowledge base. A new category was created: AI Study Group.
Now it seems that different courses require creating separate projects, and all learning notes are written in the project&amp;rsquo;s Readme.md file.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Old problems, the flamboyant beauty of blossoming flowers. (This captures the essence and poetic nature of the original.)

Alternatively, a more literal translation could be: “Old ailments, beautiful eyes like blooming flowers.” However, the first option is generally preferred for its aesthetic quality.</title>
        <link>https://ttf248.life/en/p/old-ailment-stunning-flowers/</link>
        <pubDate>Mon, 26 May 2025 23:54:12 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/old-ailment-stunning-flowers/</guid>
        <description>&lt;p&gt;For many years, I’ve focused on backend development, and recently started to explore &lt;code&gt;AI&lt;/code&gt; programming while dipping my toes into some frontend-related content. However, during this period of tinkering, I gradually realized I was falling back into an old habit – being dazzled by shiny new things. I constantly try to use &lt;code&gt;AI&lt;/code&gt; to create a frontend interface, but in reality, these attempts haven’t provided much practical benefit for my current work and are actually wasting my energy.&lt;/p&gt;
&lt;h2 id=&#34;ai-use-cases&#34;&gt;AI Use Cases
&lt;/h2&gt;&lt;p&gt;In small projects, AI tools can truly shine, particularly when writing independent functions with low coupling to the system and simple business logic. These tasks typically have clear inputs and outputs, and rely less on context – making them well-suited for the current capabilities of AI-assisted programming.&lt;/p&gt;
&lt;p&gt;However, when facing complex system architectures or deep business logic, AI’s limitations begin to emerge. It may generate code that appears reasonable but is actually detached from the project&amp;rsquo;s real needs, or even introduce potential issues that are difficult to debug. In these scenarios, AI is best suited as an assistive tool rather than a fully autonomous code generator. We need to conduct rigorous review and testing of the generated code to ensure it meets actual requirements.&lt;/p&gt;
&lt;h2 id=&#34;errors-and-the-cost-of-learning&#34;&gt;Errors and the Cost of Learning
&lt;/h2&gt;&lt;p&gt;While attempting to generate frontend code using AI, I encountered numerous challenges. As frontend development isn&amp;rsquo;t a domain I’m familiar with, troubleshooting often proved time-consuming and frustrating. Even after adjusting prompts to have the AI rewrite the code, it was difficult to avoid the appearance of some low-level errors. This iterative process not only wasted time but also highlighted that my current focus should be on backend business logic rather than groping around in unfamiliar territory.&lt;/p&gt;
&lt;p&gt;Looking back at the project completed over the weekend, I’m even more convinced that focusing on backend development and user interaction logic, implementing functionality through a console, is the most efficient approach currently. Perhaps systematically learning frontend knowledge would be a better strategy when I have more time and energy.&lt;/p&gt;
&lt;h2 id=&#34;frontend-learning-plan&#34;&gt;Frontend Learning Plan
&lt;/h2&gt;&lt;p&gt;The frontend technology stack is complex and diverse, so it’s unrealistic to quickly master it. I plan to first choose a framework, such as Vue.js or React.js, and deeply learn its core concepts and usage methods. Only after becoming familiar with the fundamentals will I attempt to use AI to generate frontend code, which can effectively avoid errors and wasted time caused by unfamiliarity.&lt;/p&gt;
&lt;p&gt;In short, the focus for this stage should be on backend development, steadily building my core skills. When the timing is right, I’ll then explore the combination of frontend and AI – potentially yielding greater rewards.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Claude4 released, attempting to develop: Hugo tags, hyperlink translation assistant</title>
        <link>https://ttf248.life/en/p/claude-4-release-hugo-tags-hyperlink-translation-assistant/</link>
        <pubDate>Sat, 24 May 2025 03:05:31 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/claude-4-release-hugo-tags-hyperlink-translation-assistant/</guid>
        <description>&lt;p&gt;This site is developed using Hugo, but I’ve always used Chinese titles, which results in less friendly generated article links. In simpler terms, when shared, they don&amp;rsquo;t look as good because the Chinese characters are escaped into formats like %E4%BD%A0%E5%A5%BD within the links. While you can solve this by setting a slug, it’s tedious to do manually every time.&lt;/p&gt;
&lt;p&gt;Therefore, I decided to try using Claude4 to develop a translation assistant that automatically converts Chinese titles to English slugs and adds hyperlinks within the articles. This would eliminate the need for manual setup.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Claude4 is amazing – its contextual understanding has significantly improved, as has its efficiency in handling complex tasks.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;!-- more --&gt;
&lt;h2 id=&#34;project-address&#34;&gt;Project Address
&lt;/h2&gt;&lt;p&gt;Domestic Project Address: &lt;a class=&#34;link&#34; href=&#34;https://cnb.cool/ttf248/hugo-content-suite&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://cnb.cool/ttf248/hugo-content-suite&lt;/a&gt;
International Project Address: &lt;a class=&#34;link&#34; href=&#34;https://github.com/ttf248/hugo-content-suite&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/ttf248/hugo-content-suite&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;code-implementation&#34;&gt;Code Implementation
&lt;/h2&gt;&lt;p&gt;Let&amp;rsquo;s first discuss the implementation approach: We need to scan all articles, extract tag information and article titles, and then call on our local large model (such as gemma-3-12b-it) for translation.&lt;/p&gt;
&lt;p&gt;In actual development, Claude4 showcased several significant advantages compared to previous generation large models. Due to the diverse functional requirements, Claude4 automatically designed an interactive menu, comprehensively considering various usage scenarios. For example, in tag processing, Claude4 not only supports tag statistics and analysis but also includes classification statistics and can even detect &lt;strong&gt;unlabeled articles&lt;/strong&gt;. Furthermore, it provides &lt;strong&gt;preview&lt;/strong&gt; and tag page generation functionalities.&lt;/p&gt;
&lt;p&gt;Whether it&amp;rsquo;s integrating with local large models, adding translation caches, or performing large-scale code refactoring, Claude4 completes everything in one go, with almost no issues. Despite the relatively small project size, it includes many minor features. In previous development processes, large models often forgot earlier content, but Claude4 performed exceptionally well this time, &lt;strong&gt;virtually without any context forgetting issues.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In short, intelligence has increased, and we plan to switch to Claude4 for more development work as our primary coding model.&lt;/p&gt;
&lt;h2 id=&#34;translation-cache&#34;&gt;Translation Cache
&lt;/h2&gt;&lt;p&gt;This approach, besides reducing the number of calls to large models, performs quite efficiently when running a 12b model locally – it doesn’t waste much time. However, if you need to call the large model every time, it will still be somewhat slow. Furthermore, to fix the connections within articles, if a full update operation is executed and the article title is very long, there&amp;rsquo;s occasionally a situation where the two translated results differ, causing the link to change – which is quite awkward.&lt;/p&gt;
&lt;h2 id=&#34;feature-optimization&#34;&gt;Feature Optimization
&lt;/h2&gt;&lt;p&gt;The entire project was handed over to &lt;code&gt;Claude4&lt;/code&gt; to analyze the space for optimization and obtain the following suggestions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Externalize Configuration - Improve maintainability and flexibility&lt;/li&gt;
&lt;li&gt;Structured Logging - Facilitate troubleshooting and monitoring&lt;/li&gt;
&lt;li&gt;Performance Monitoring - Understand system running status&lt;/li&gt;
&lt;li&gt;User Experience - Visual feedback such as progress bars&lt;/li&gt;
&lt;li&gt;Error Handling - A more robust exception handling mechanism&lt;/li&gt;
&lt;li&gt;Code Organization - Clearer module division&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The code was reviewed, and there were no major issues, such as the configuration files. The original code&amp;rsquo;s configurations were converted to default settings, and if a corresponding configuration file wasn’t found during reading, a default one would be automatically generated to prevent user errors.
Requirement: When translating text into English, dynamically calculate the current translation efficiency, estimate the remaining time, and output relevant information to the console: Now it has obtained the character count of the article, the character count for each line being translated, the translation time, and fits the calculation of the translation time for every 100 characters. Simultaneously, it calculates the estimated remaining translation time.&lt;/p&gt;
&lt;p&gt;The code was completed, but the effect wasn&amp;rsquo;t very satisfactory, so I asked AI to provide a new design solution:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Provide multiple efficiency calculation methods: Real-time efficiency, average efficiency, sliding window efficiency&lt;/li&gt;
&lt;li&gt;Improve display method: Progress bars, segmented statistics, dynamic refresh&lt;/li&gt;
&lt;li&gt;Add more useful metrics: API call counts, success rates, etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;After the code was completed, a new surprise was discovered – translation efficiency statistics flooded in in real time, but without endless scrolling.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;Translating text to English (total 7163 characters)...
Detected 53 lines to translate
  [1/53] Stage1/6 [░░░░░░░░░░░░░░░░░░░░░░░░░] 1.9% Translating 354 characters...
    ✅ Completed (3.1s) | API Call #1
    ✅ Completed (1.5s) | API Call #2
    ✅ Completed (0.9s) | API Call #3
    ✅ Completed (0.2s) | API Call #4
    ✅ Completed (1.0s) | API Call #5
    ✅ Completed (1.0s) | API Call #6
    ✅ Completed (0.2s) | API Call #7
    📊 Progress: 13.2% (7/53) | 12.9% (925/7163) 114.6 characters/second 📊
    ⚡ Efficiency: Real-time 76.4 | Average 117.9 | Recent 109.0 | Stage 113.6 characters/second 📊
    🎯 Success Rate: 100.0% (7/7) | Remaining: 46 lines 7 seconds] 9.4% Translating 110 characters...
    ⏱️ Estimated remaining: 55s | Predicted completion time: 00:10:19 8s] 11.3% Translating 114 characters...
    💾 Processing speed: 3211.3 lines/minute | Total time: 8s] 13.2% Translating 16 characters...
  [8/53] Stage1/6 [███░░░░░░░░░░░░░░░░░░░░░░] 15.1% Translating 166 characters...
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Previously, the control program wasn&amp;rsquo;t written with many features, so I was curious about how it was implemented and looked at the code:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-golang&#34;&gt;// Clear screen and redisplay (dynamic refresh effect)
if translationCount &amp;gt; 1 {
    fmt.Print(&amp;quot;\033[6A\033[K&amp;quot;) // Move up 6 lines and clear
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;performance-statistics-menu&#34;&gt;Performance Statistics Menu
&lt;/h3&gt;&lt;p&gt;The newly added &lt;strong&gt;Performance Statistics Menu&lt;/strong&gt;, which I myself designed, isn&amp;rsquo;t as well-designed as this one.&lt;/p&gt;
&lt;p&gt;📊 Performance Statistics:
🔄 Translation Count: 360
⚡ Cache Hit Rate: 1.4% (5/365)
⏱️ Average Translation Time: 315.927234ms
📁 File Operations: 73
❌ Error Count: 0&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Data Mining&lt;/li&gt;
&lt;li&gt;Deep Learning&lt;/li&gt;
&lt;li&gt;Neural Network&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;progress-bar-display&#34;&gt;Progress Bar Display
&lt;/h3&gt;&lt;p&gt;New &lt;strong&gt;Progress Bar Display&lt;/strong&gt;, detailed progress, elapsed time, estimated remaining time.
Please select function (0-13): 10
🔍 Collecting translation target&amp;hellip;
📄 Loaded cache file, containing 0 translation records
📊 Translation Cache Statistics:
🏷️ Total tags: 229
📝 Total articles: 131
✅ Cached: 0
🔄 To be translated: 360
Confirm to generate full translation cache? (y/n): y
🚀 Generating full translation cache&amp;hellip;
📄 Loaded cache file, containing 0 translation records
🔍 Checking translations in cache&amp;hellip;
🔄 Need to translate 360 new tags
[░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 5/360 (1.4%) - Time taken: 3s - Estimated remaining: 3m8s
💾 Saved cache file, containing 5 translation records
[█░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 10/360 (2.8%) - Time taken: 6s - Estimated remaining: 3m28s
💾 Saved cache file, containing 10 translation records
[██░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 15/360 (4.2%) - Time taken: 9s - Estimated remaining: 3m30s
💾 Saved cache file, containing 15 translation records
[██░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 20/360 (5.6%) - Time taken: 13s - Estimated remaining: 3m36s
💾 Saved cache file, containing 20 translation records
[███░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 25/360 (6.9%) - Time taken: 16s - Estimated remaining: 3m33s
💾 Saved cache file, containing 25 translation records
[████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 30/360 (8.3%) - Time taken: 19s - Estimated remaining: 3m30s
💾 Saved cache file, containing 30 translation records
[████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 35/360 (9.7%) - Time taken: 22s - Estimated remaining: 3m25s
💾 Saved cache file, containing 35 translation records&lt;/p&gt;
&lt;hr&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;=== Hugo Blog Management Tool ===

🚀 Core Features
  1. One-click full blog processing (complete blog workflow)

📝 Content Management
  2. Generate tag pages
  3. Generate article slugs
  4. Translate articles into multi-language versions

💾 Cache Management
  5. View cache status
  6. Generate full translation cache
  7. Clear translation cache

  0. Exit the program
&lt;/code&gt;&lt;/pre&gt;
</description>
        </item>
        <item>
        <title>Merge the Pull Request into the Repository of the Fork</title>
        <link>https://ttf248.life/en/p/merge-pullrequest-to-forked-repo/</link>
        <pubDate>Wed, 07 May 2025 18:44:03 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/merge-pullrequest-to-forked-repo/</guid>
        <description>&lt;p&gt;&lt;code&gt;github-readme-stats&lt;/code&gt; is a GitHub profile statistics generator that allows users to display various statistics and charts within their GitHub profiles. It offers multiple customization options to tailor it to user needs.&lt;/p&gt;
&lt;p&gt;I manage my repository habits by grouping them by project; GitHub doesn&amp;rsquo;t support repository grouping, so I have to achieve this by splitting repositories across different organizations. The latest branch of &lt;code&gt;github-readme-stats&lt;/code&gt; cannot support statistics for repositories spanning different organizations; I forked a branch and merged the corresponding code.&lt;/p&gt;
&lt;h2 id=&#34;final-result&#34;&gt;Final Result
&lt;/h2&gt;&lt;p&gt;&lt;img src=&#34;https://github-readme-stats-chi-one-17.vercel.app/api?username=ttf248&amp;amp;hide_title=true&amp;amp;show_icons=true&amp;amp;hide=contribs&amp;amp;line_height=24&amp;amp;include_all_commits=true&amp;amp;count_private=true&amp;amp;bg_color=0000&amp;amp;text_color=8A919F&amp;amp;locale=cn&amp;amp;role=OWNER,COLLABORATOR,ORGANIZATION_MEMBER&amp;amp;timstamp=1746608356&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;GitHub Stats&#34;
	
	
&gt;
&lt;img src=&#34;https://github-readme-stats-chi-one-17.vercel.app/api/top-langs/?username=ttf248&amp;amp;hide_title=true&amp;amp;hide=html,javascript,css&amp;amp;layout=compact&amp;amp;bg_color=0000&amp;amp;text_color=8A919F&amp;amp;locale=cn&amp;amp;role=OWNER,COLLABORATOR,ORGANIZATION_MEMBER&amp;amp;timstamp=1746608356&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;Top Languages&#34;
	
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;pull-request-original-address&#34;&gt;Pull Request Original Address
&lt;/h2&gt;&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/anuraghazra/github-readme-stats/pull/2459&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Adds the ability to include data from organization repositories&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;merging-a-pull-request-into-your-forked-repository&#34;&gt;Merging a Pull Request into Your Forked Repository
&lt;/h2&gt;&lt;p&gt;To merge a &lt;strong&gt;Pull Request (PR)&lt;/strong&gt; into your &lt;strong&gt;forked repository&lt;/strong&gt;, there are several ways to do this, depending on whether you want to:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Merge the PR from the &lt;strong&gt;upstream repository&lt;/strong&gt; into your fork, or&lt;/li&gt;
&lt;li&gt;Merge a PR from another person’s fork into your fork, or&lt;/li&gt;
&lt;li&gt;Merge a PR that was opened on your fork (e.g., someone forked and created a PR for you)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I&amp;rsquo;ll first give you a common scenario: &lt;strong&gt;You have forked a repository and want to merge a PR from the upstream into your fork&lt;/strong&gt;.  Here’s the workflow below 👇&lt;/p&gt;
&lt;h3 id=&#34;-method-one-command-line-most-universal&#34;&gt;✅ Method One: Command Line (Most Universal)
&lt;/h3&gt;&lt;h4 id=&#34;step-1-clone-your-own-fork&#34;&gt;Step 1: Clone your own fork
&lt;/h4&gt;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/your-username/repository-name.git
cd repository-name
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;step-2-add-upstream-original-repository-address&#34;&gt;Step 2: Add Upstream (Original Repository Address)
&lt;/h4&gt;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git remote add upstream https://github.com/original-author-username/repository-name.git
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;step-3-fetch-the-upstream-pr-branch&#34;&gt;Step 3: Fetch the Upstream PR Branch
&lt;/h4&gt;&lt;p&gt;Find the PR number you want to merge, for example PR #123.
You can then checkout the code for that PR like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git fetch upstream pull/123/head:pr-123
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;step-4-switch-and-merge-branches&#34;&gt;Step 4: Switch and Merge Branches
&lt;/h4&gt;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git checkout main    # Or your own target branch
git merge pr-123
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If everything goes well, you can push to your forked GitHub repository:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git push origin main
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;-method-two-github-web-interface-simple-but-limited&#34;&gt;✅ Method Two: GitHub Web Interface (Simple but Limited)
&lt;/h3&gt;&lt;p&gt;If you see a PR on GitHub’s web interface that is against the upstream repository, you can:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Navigate to the PR page.&lt;/li&gt;
&lt;li&gt;Click “&lt;strong&gt;Commits&lt;/strong&gt;” or “&lt;strong&gt;Files changed&lt;/strong&gt;” in the top-right corner to see which branch this PR is based on.&lt;/li&gt;
&lt;li&gt;On your fork page, create a new branch and then manually cherry-pick the PR’s commit (requires Git knowledge).&lt;/li&gt;
&lt;li&gt;Or, click “&lt;strong&gt;Open in GitHub Desktop&lt;/strong&gt;” and use the GUI tool to merge.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;-if-someone-has-created-a-fork-and-submitted-a-pr-to-it-simply-navigate-to-the-pr-page-click-the-merge-pull-request-button-and-youre-done&#34;&gt;🚀 If someone has created a fork and submitted a PR to it, simply navigate to the PR page, click the &amp;ldquo;&lt;strong&gt;Merge pull request&lt;/strong&gt;&amp;rdquo; button, and you’re done.
&lt;/h3&gt;&lt;hr&gt;
&lt;p&gt;Would you like me to walk you through it step-by-step based on your specific scenario (e.g., PR link, whether you&amp;rsquo;re using a web interface or the command line)? Or, please provide the link and I can analyze the simplest approach for you.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Commit messages in Git’s history</title>
        <link>https://ttf248.life/en/p/git-modify-commit-message/</link>
        <pubDate>Wed, 07 May 2025 18:38:31 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/git-modify-commit-message/</guid>
        <description>&lt;ul&gt;
&lt;li&gt;Script for batch modifying Git history commit author information, rewriting Git history records using &lt;code&gt;git filter-branch&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You provided the script to modify the author information in the Git repository history in bulk. The overall approach is correct, but using an array (e.g., &lt;code&gt;OLD_EMAILS=(&amp;quot;...&amp;quot;)&lt;/code&gt;) within the &lt;code&gt;git filter-branch&lt;/code&gt;’s &lt;code&gt;--env-filter&lt;/code&gt; might cause compatibility issues because some shell environments (such as &lt;code&gt;/bin/sh&lt;/code&gt;) do not support array syntax.
To improve compatibility, it&amp;rsquo;s recommended to replace the array with space-separated strings and use a &lt;code&gt;for&lt;/code&gt; loop to iterate through each old email address. Here is an example of the modified script:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#!/bin/sh

git filter-branch --env-filter &#39;
OLD_EMAILS=&amp;quot;TianlongXiang51@gmail.com nick@qq.com tianlongxiang51@gmail.com&amp;quot;
CORRECT_NAME=&amp;quot;tianlong.xiang&amp;quot;
CORRECT_EMAIL=&amp;quot;tianlong.xiang@foxmail.com&amp;quot;

for OLD_EMAIL in $OLD_EMAILS
do
    if [ &amp;quot;$GIT_COMMITTER_EMAIL&amp;quot; = &amp;quot;$OLD_EMAIL&amp;quot; ]
    then
        export GIT_AUTHOR_NAME=&amp;quot;$CORRECT_NAME&amp;quot;
        export GIT_AUTHOR_EMAIL=&amp;quot;$CORRECT_EMAIL&amp;quot;
    fi
    if [ &amp;quot;$GIT_AUTHOR_EMAIL&amp;quot; = &amp;quot;$OLD_EMAIL&amp;quot; ]
    then
        export GIT_COMMITTER_NAME=&amp;quot;$CORRECT_NAME&amp;quot;
        export GIT_COMMITTER_EMAIL=&amp;quot;$CORRECT_EMAIL&amp;quot;
    fi
done
&#39; --tag-name-filter cat -- --branches --tags
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Notes:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It is recommended to back up your repository before executing this script to prevent any unexpected issues.&lt;/li&gt;
&lt;li&gt;This operation rewrites Git history, modifying the author information of commits, which may cause changes in commit hash values.&lt;/li&gt;
&lt;li&gt;If you have already pushed the changes to a remote repository, you need to use a forced push:
Please use forced pushes cautiously, especially in collaborative projects, to avoid affecting others.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;Count all unique authors&amp;rsquo; email addresses in the repository&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git log --format=&#39;%an &amp;lt;%ae&amp;gt;&#39; | sort -u
&lt;/code&gt;&lt;/pre&gt;</description>
        </item>
        <item>
        <title>No coding, design and develop a self-selected stock module.</title>
        <link>https://ttf248.life/en/p/design-develop-custom-stock-module-no-code/</link>
        <pubDate>Thu, 27 Feb 2025 23:20:39 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/design-develop-custom-stock-module-no-code/</guid>
        <description>&lt;ul&gt;
&lt;li&gt;Last month, we experimented with cursor, but due to the limitations of the free quota, we didn&amp;rsquo;t develop overly complex features; we just did some basic testing. We discovered then that Byte also released similar products, both using the same large models – Claude-3.5 – at their core.
Byte’s product is called Trae, initially launched in the Mac version and finally released its Windows version in February of this year. Big companies are good because you can freely “white嫖” (literally translates to &amp;ldquo;free-eat&amp;rdquo;), without having to pay, with unlimited use of Claude-3.5 – this model performs quite well.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Ultimately, we got stuck on the development of candlestick charts. As I don’t understand React at all, I had to give up. To continue developing, I would need to supplement my knowledge of front-end basics, breaking down the task into smaller, more manageable pieces instead of directly giving me a large task: developing candlestick charts.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;issues-found&#34;&gt;Issues Found
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;Due to the lack of training data caused by using foreign AI models and Vue3 + Element-Plus, React was chosen as the frontend framework.&lt;/li&gt;
&lt;li&gt;Occasional syntax errors may exist and require manual fixes.&lt;/li&gt;
&lt;li&gt;Solutions for some complex problems require manual guidance.&lt;/li&gt;
&lt;li&gt;Code structure optimization requires manual instruction.&lt;/li&gt;
&lt;li&gt;The most time-consuming part was packaging the frontend code into a container, due to my zero foundation in &lt;code&gt;.env.production&lt;/code&gt; and &lt;code&gt;tsconfig.json&lt;/code&gt;, I had no concept of these either; I only straightened out the corresponding logic by asking for help from bean sprouts halfway through. There are significant differences between the dev mode and build mode of frontend development, and the checks performed on the code. Backend database and service container scripts were completed in a total of five minutes.
&lt;strong&gt;Currently, AI mainly improves the efficiency of development; having a foundation is best, not that AI will solve all problems for you.&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;repository-address&#34;&gt;Repository Address
&lt;/h2&gt;&lt;p&gt;As the title indicates, this time we’re going to try a chat-based approach with AI instead of writing code, aiming to design and develop a self-trading module. Let&amp;rsquo;s see what results we can achieve.&lt;/p&gt;
&lt;p&gt;Repository address: &lt;a class=&#34;link&#34; href=&#34;https://github.com/ttf248/trae-demo&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/ttf248/trae-demo&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;For detailed usage instructions, please refer to the README.md file in the repository.&lt;/p&gt;
&lt;p&gt;The repository contains numerous submission records, most of which are dialogues between me and Trae, along with my testing of Trae’s functionalities, and notes on whether manual intervention was required to implement each feature.&lt;/p&gt;
&lt;h2 id=&#34;prompt&#34;&gt;Prompt
&lt;/h2&gt;&lt;p&gt;The project is to develop functionality from scratch, based on the following project prototype diagram:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;Based on the project prototype diagram, develop features: stock selection (watchlist), which needs to support adding, deleting, modifying, and querying contracts. The watchlist interface should display basic market data. Support multiple different market switches.

Frontend: react
Backend: golang gin gorm
Database: PostgreSQL

The backend needs to support cross-origin requests, while also considering data validation and error handling. If the backend service is unavailable, the frontend should display an alert message.

The backend needs to display request and response logs; the frontend also prints communication logs for troubleshooting purposes.
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;ui-and-interaction-optimization&#34;&gt;UI and Interaction Optimization
&lt;/h2&gt;&lt;p&gt;The design of the front-end interface relies entirely on Grok. We initially created a prototype within Trae, but it lacked aesthetics. Because the model used has strong coding capabilities but weaker other abilities, we need to use Grok to optimize the front-end UI.&lt;/p&gt;
&lt;p&gt;By taking screenshots of the current interface and uploading them to Grok, we can receive numerous optimization suggestions at once. We then manually evaluate these suggestions and copy them into Trae to execute and observe the results of the optimizations.&lt;/p&gt;
&lt;h3 id=&#34;technology-stack&#34;&gt;Technology Stack
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Frontend: React + TypeScript&lt;/li&gt;
&lt;li&gt;Backend: Golang + Gin + GORM&lt;/li&gt;
&lt;li&gt;Database: PostgreSQL 17&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;system-architecture&#34;&gt;System Architecture
&lt;/h2&gt;&lt;h2 id=&#34;backend-architecture&#34;&gt;Backend Architecture
&lt;/h2&gt;&lt;p&gt;The backend utilizes the Gin framework (Go) to implement RESTful APIs, with the following key modules:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Database Module&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Utilizes GORM as an ORM framework&lt;/li&gt;
&lt;li&gt;Supports database connection configuration via environment variables&lt;/li&gt;
&lt;li&gt;Automatically performs database schema migrations&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Routing Module&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;RESTful API design&lt;/li&gt;
&lt;li&gt;A unified error handling mechanism&lt;/li&gt;
&lt;li&gt;Built-in request logging&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Cross-Origin Handling&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Supports cross-origin requests from local development environments&lt;/li&gt;
&lt;li&gt;Configurable CORS policies&lt;/li&gt;
&lt;li&gt;Supports cookie-based cross-origin access&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;frontend-architecture&#34;&gt;Frontend Architecture
&lt;/h2&gt;&lt;p&gt;The frontend was built using React + TypeScript, implementing the following features:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Stock list display&lt;/li&gt;
&lt;li&gt;Watchlist management&lt;/li&gt;
&lt;li&gt;Real-time quote data display&lt;/li&gt;
&lt;li&gt;Error handling mechanism&lt;/li&gt;
&lt;/ul&gt;</description>
        </item>
        <item>
        <title>Protobuf Zero Value Pitfalls: When Default Values Become an Invisible Killer of Business Logic</title>
        <link>https://ttf248.life/en/p/protobuf-zero-value-trap/</link>
        <pubDate>Thu, 20 Feb 2025 15:26:51 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/protobuf-zero-value-trap/</guid>
        <description>&lt;p&gt;The US stock market has three trading sessions: pre-market, live market, and post-market. The logic for pushing data – whether it’s full data or numerical increments – is optimized to conserve bandwidth (sending as little data as possible). Initially, only the full dataset is sent in the first transmission; subsequent transmissions are incremental updates of all fields.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Why not use the optimal solution? This involves multiple project teams, some of which have been live for many years. As we’re a new integration, we can only strive for compatibility.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;!-- more --&gt;
&lt;h2 id=&#34;a-series-of-issues&#34;&gt;A Series of Issues
&lt;/h2&gt;&lt;p&gt;Just from the summary, it might seem like there aren&amp;rsquo;t any problems, but once the system architecture is brought in, a series of issues arise. Immediately after resolving the previous issue, a new one emerged – this problem was caused by the prior one.&lt;/p&gt;
&lt;h3 id=&#34;unable-to-identify-trading-intervals&#34;&gt;Unable to Identify Trading Intervals
&lt;/h3&gt;&lt;p&gt;The market phase is defined in &lt;code&gt;protobuf&lt;/code&gt; as 0, but due to incremental push data delivery, the business side cannot effectively identify whether this &amp;lsquo;0&amp;rsquo; represents the default value or a genuine business value.&lt;/p&gt;
&lt;p&gt;In simpler terms: Each time a &amp;lsquo;0&amp;rsquo; is received, it’s impossible to determine if it’s the new market phase setting or the &lt;code&gt;protobuf&lt;/code&gt; default value.&lt;/p&gt;
&lt;h3 id=&#34;introducing-optional&#34;&gt;Introducing Optional
&lt;/h3&gt;&lt;p&gt;Since protobuf release 3.15, proto3 supports using the optional keyword (just as in proto2) to provide presence information for a scalar field.&lt;/p&gt;
&lt;p&gt;The group’s communication protocol is based on &lt;code&gt;protobuf&lt;/code&gt;, but due to historical reasons, the version selected was older and did not support the &lt;code&gt;optional&lt;/code&gt; keyword. As you know, because we introduced &lt;code&gt;protobuf&lt;/code&gt; from the ground up, publishing the project as a static library, this resulted in needing to upgrade the entire build chain, which was a very high cost.&lt;/p&gt;
&lt;h3 id=&#34;gcc-version-issues&#34;&gt;GCC Version Issues
&lt;/h3&gt;&lt;p&gt;After painstakingly devising a solution, we planned to release two different underlying versions to control the propagation of dependencies for the new &lt;code&gt;protobuf&lt;/code&gt; version as much as possible. However, during compilation, we discovered that the &lt;code&gt;gcc&lt;/code&gt; version was too low and did not support the new features of &lt;code&gt;protobuf&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The commonly used server types in our team are CentOS7 and CentOS8. The default &lt;code&gt;gcc&lt;/code&gt; version on CentOS7 is 4.8, while the default &lt;code&gt;gcc&lt;/code&gt; version on CentOS8 is 8.3. Since the new features of &lt;code&gt;protobuf&lt;/code&gt; require a &lt;code&gt;gcc&lt;/code&gt; version of 7.4 or higher, CentOS7 could not support it.&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://gcc.gnu.org/bugzilla/show_bug.cgi?id=82461&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Bug 82461 - [7 Regression] Temporary required for brace-initializing (non-literal-type) member variable&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Ultimately, after a lot of troubleshooting, we moved the deployments of related services and the compilation server to CentOS8, resolving this issue.&lt;/p&gt;
&lt;h2 id=&#34;reasonable-enumeration&#34;&gt;Reasonable Enumeration
&lt;/h2&gt;&lt;p&gt;Reviewing the entire problem, there’s a simpler and more efficient solution: adjust the enumeration definition to start numbering from 1 instead of 0. This effectively distinguishes between default values and business values, avoiding all the aforementioned issues.&lt;/p&gt;
&lt;h3 id=&#34;why-starting-from-1-is-more-reasonable&#34;&gt;Why Starting from 1 is More Reasonable?
&lt;/h3&gt;&lt;p&gt;In &lt;code&gt;protobuf&lt;/code&gt;, enum types have a default value fixed to 0. If we define meaningful business values as 0 (e.g., &amp;ldquo;Market Open&amp;rdquo;), in incremental pushes, the business side cannot determine whether the received 0 is a business value or an unset default value. However, if the enum starts from 1, 0 can be reserved for a meaningless default value or “Unknown” state, solving the problem directly.&lt;/p&gt;
&lt;p&gt;Recommended Practice:
When designing &lt;code&gt;protobuf&lt;/code&gt; enums, always define 0 as a meaningless default value (e.g., &lt;code&gt;UNKNOWN&lt;/code&gt; or &lt;code&gt;RESERVED&lt;/code&gt;).
Assign actual business values starting from 1 to ensure they are distinguished from the default value of 0.&lt;/p&gt;
&lt;p&gt;With this small adjustment, we not only resolved the issue of identifying market hours but also provided a valuable lesson for future protocol design.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Background Service TCP Communication Anomaly Troubleshooting</title>
        <link>https://ttf248.life/en/p/backend-service-tcp-communication-troubleshooting/</link>
        <pubDate>Fri, 14 Feb 2025 22:54:13 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/backend-service-tcp-communication-troubleshooting/</guid>
        <description>&lt;p&gt;Business Model: The backend service establishes a connection with the group’s market data gateway using TCP. Each time a connection is established, it must first send an authorization request and then continuously send heartbeat packages to maintain the connection status.&lt;/p&gt;
&lt;p&gt;However, one day, an alert message was received indicating that the service had disconnected. After carefully examining the logs, it was discovered that the backend service was continuously sending heartbeat packages, but the other party did not respond at all, yet the connection remained open.&lt;/p&gt;
&lt;h2 id=&#34;field-summary&#34;&gt;Field Summary
&lt;/h2&gt;&lt;p&gt;I was originally working in the office, pushing project progress, when an alarm message suddenly popped up in the company group. At first glance, I thought it was just a recurring issue – likely due to network timeouts causing heartbeat failures, leading to service disconnection. However, after careful log examination, the actual situation turned out to be different. The backend had sent an authorization login message, but hadn’t received a response; meanwhile, heartbeat packets continued to send persistently, yet the other party never replied with any heartbeat data. After in-depth analysis of the logs, several key issues were exposed:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;No Response to Authorization Message:&lt;/strong&gt; This was likely due to the other system being in the process of restarting, preventing the authorization message from being processed promptly.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sending Heartbeat Data Without Successful Authorization:&lt;/strong&gt; Upon investigation, it was found that this was a logical flaw in the program’s logic. The heartbeat sending function&amp;rsquo;s judgment logic had a defect; it only checked the connection status but missed verifying the authorization status.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Service Did Not Disconnect:&lt;/strong&gt; If the service could have disconnected, it would have triggered a reconnection mechanism and re-sent the authorization message.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Currently, there’s one remaining critical issue that needs to be resolved – why didn&amp;rsquo;t the service disconnect? Solving this problem requires more in-depth and detailed troubleshooting work.&lt;/p&gt;
&lt;h2 id=&#34;analyzing-network-packets&#34;&gt;Analyzing Network Packets
&lt;/h2&gt;&lt;p&gt;&lt;code&gt;tcpdump&lt;/code&gt; is a very powerful network packet capture tool that can be used to capture network packets. By analyzing network packets, we can gain a more intuitive understanding of the details of network communication. Here, we can use &lt;code&gt;tcpdump&lt;/code&gt; to capture network packets for further analysis.
&lt;img src=&#34;https://ttf248.life/p/backend-service-tcp-communication-troubleshooting/20250220151952.png&#34;
	width=&#34;1126&#34;
	height=&#34;202&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;tcpdump&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;557&#34;
		data-flex-basis=&#34;1337px&#34;
	
&gt;
Analyzing the data in the diagram, I see that the heartbeat is constantly being sent normally, and the other server did not respond with any data, but it sent an &lt;code&gt;ACK&lt;/code&gt;, which prevents the connection from disconnecting proactively.&lt;/p&gt;
&lt;h2 id=&#34;common-flag-bit-explanations&#34;&gt;Common Flag Bit Explanations
&lt;/h2&gt;&lt;p&gt;In the TCP protocol, &lt;code&gt;PSH&lt;/code&gt; (Push) and &lt;code&gt;ACK&lt;/code&gt; (Acknowledgment) are two important flag bits used to control data transmission and traffic confirmation, respectively. Their functions are as follows:&lt;/p&gt;
&lt;h3 id=&#34;1-psh-push-flag&#34;&gt;&lt;strong&gt;1. PSH (Push Flag)&lt;/strong&gt;
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Function:&lt;/strong&gt; The &lt;code&gt;PSH&lt;/code&gt; flag’s purpose is to &lt;strong&gt;request that the receiver immediately push data from its buffer to the upper-layer application&lt;/strong&gt; (rather than waiting for the buffer to fill). This means that once a data segment with the &lt;code&gt;PSH&lt;/code&gt; flag is received, the receiver will process and transmit it as quickly as possible to the application, rather than storing it in an operating system buffer.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Typical Scenarios:&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;HTTP/HTTPS Requests:&lt;/strong&gt; Clients setting the &lt;code&gt;PSH&lt;/code&gt; when sending requests (e.g., &lt;code&gt;GET /index.html&lt;/code&gt;) to ensure immediate response from the server.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SSH Protocol:&lt;/strong&gt; Each keystroke triggers a &lt;code&gt;PSH&lt;/code&gt;, ensuring real-time transmission of input characters.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Real-Time Communication:&lt;/strong&gt; Low-latency scenarios like video streaming or online games may utilize &lt;code&gt;PSH&lt;/code&gt; to reduce latency.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Note:&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;PSH&lt;/code&gt; is not mandatory; the receiver can choose to ignore this flag (but still process the data normally).&lt;/li&gt;
&lt;li&gt;The sender may not set &lt;code&gt;PSH&lt;/code&gt;, in which case the receiver will determine when to push data based on its own buffering strategy.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;2-ack-acknowledgment-flag&#34;&gt;&lt;strong&gt;2. ACK (Acknowledgment Flag)&lt;/strong&gt;
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Function:&lt;/strong&gt; The ACK flag indicates that &lt;strong&gt;the previous data segment has been correctly received&lt;/strong&gt;. Each ACK contains an acknowledgment number (Acknowledgment Number), which represents the next byte sequence expected to be received. It is the core mechanism of TCP reliable transmission.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Working Principle:&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;When the sender sends a data segment, it carries the expected ACK value from the receiver (e.g., &lt;code&gt;ACK = Sequence Number + Data Length&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;Upon receiving the data, the receiver generates an ACK message to confirm the received byte sequence number.&lt;/li&gt;
&lt;li&gt;The sender only retransmits unacknowledged data after receiving the corresponding ACK.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Example:&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;If the sender sends a data segment with sequence numbers &lt;code&gt;100~199&lt;/code&gt;, then the expected ACK from the receiver should be &lt;code&gt;200&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;If the receiver has not received some of the data in &lt;code&gt;100~199&lt;/code&gt;, it will inform the sender to retransmit via &lt;code&gt;ACK=150&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;3-combination-of-psh-and-ack&#34;&gt;&lt;strong&gt;3. Combination of PSH and ACK&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;In the TCP header, &lt;code&gt;PSH&lt;/code&gt; and &lt;code&gt;ACK&lt;/code&gt; can appear simultaneously, commonly seen in the following scenarios:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;HTTP Request Response&lt;/strong&gt;:
When a client sends a &lt;code&gt;POST&lt;/code&gt; request (including data), it sets both &lt;code&gt;PSH&lt;/code&gt; and &lt;code&gt;ACK&lt;/code&gt; (to acknowledge previous responses).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Command Transfer after SSH Handshake&lt;/strong&gt;:
After the client enters a command, it sends a data segment with &lt;code&gt;PSH&lt;/code&gt; and &lt;code&gt;ACK&lt;/code&gt; to ensure that the command is immediately transmitted and processed by the server.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;4-other-flagged-associations&#34;&gt;&lt;strong&gt;4. Other Flagged Associations&lt;/strong&gt;
&lt;/h3&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Flag&lt;/th&gt;
&lt;th&gt;Name&lt;/th&gt;
&lt;th&gt;Brief Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;SYN&lt;/td&gt;
&lt;td&gt;Synchronize&lt;/td&gt;
&lt;td&gt;Initiate connection (three-way handshake)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;4-other-flagged-associations-1&#34;&gt;&lt;strong&gt;4. Other Flagged Associations&lt;/strong&gt;
&lt;/h3&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Flag&lt;/th&gt;
&lt;th&gt;Name&lt;/th&gt;
&lt;th&gt;Brief Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;FIN&lt;/td&gt;
&lt;td&gt;End&lt;/td&gt;
&lt;td&gt;Graceful connection closure&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;4-other-flagged-associations-2&#34;&gt;&lt;strong&gt;4. Other Flagged Associations&lt;/strong&gt;
&lt;/h3&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Flag&lt;/th&gt;
&lt;th&gt;Name&lt;/th&gt;
&lt;th&gt;Brief Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;RST&lt;/td&gt;
&lt;td&gt;Reset&lt;/td&gt;
&lt;td&gt;Forcefully terminates the connection (exceptional circumstances)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;4-other-flagged-associations-3&#34;&gt;&lt;strong&gt;4. Other Flagged Associations&lt;/strong&gt;
&lt;/h3&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Flag&lt;/th&gt;
&lt;th&gt;Name&lt;/th&gt;
&lt;th&gt;Brief Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;URG&lt;/td&gt;
&lt;td&gt;Urgent&lt;/td&gt;
&lt;td&gt;Marks an urgent pointer (rarely used)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;4-other-flagged-associations-4&#34;&gt;&lt;strong&gt;4. Other Flagged Associations&lt;/strong&gt;
&lt;/h3&gt;&lt;h3 id=&#34;summary&#34;&gt;Summary
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;PSH&lt;/strong&gt; focuses on &lt;strong&gt;data arriving at the application layer as quickly as possible&lt;/strong&gt;, reducing latency.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ACK&lt;/strong&gt; focuses on &lt;strong&gt;reliable data transmission&lt;/strong&gt;, avoiding packet loss or out-of-order delivery.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The two work together to balance TCP protocol efficiency and reliability.&lt;/p&gt;</description>
        </item>
        <item>
        <title>ollama local deployment of deepseek-R1</title>
        <link>https://ttf248.life/en/p/ollama-local-deployment-deepseek-r1/</link>
        <pubDate>Fri, 07 Feb 2025 22:41:02 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/ollama-local-deployment-deepseek-r1/</guid>
        <description>&lt;p&gt;Ollama is an open-source AI tool designed to enable users to run and deploy large language models (LLMs) locally. Its goal is to provide a convenient and efficient way for developers to use models like GPT on their local machines without relying on cloud services. Ollama supports multiple models and focuses on optimizing performance, allowing even resource-constrained devices to smoothly run these models.&lt;/p&gt;
&lt;p&gt;Through Ollama, users can utilize text-based AI applications and interact with locally deployed models without worrying about data privacy or high API usage fees. You can invoke different models via a command-line interface (CLI) for tasks such as natural language processing and question answering.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Ollama is suitable for experimenting with various models; after testing the Windows version, it couldn&amp;rsquo;t fully leverage the hardware’s performance, possibly due to the Windows version. When deploying 32b parameter models, with low memory and GPU load, the response speed is slow.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;hardware-overview&#34;&gt;Hardware Overview
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;Operating System: Windows 11&lt;/li&gt;
&lt;li&gt;CPU: i7-10700K&lt;/li&gt;
&lt;li&gt;Memory: 40GB&lt;/li&gt;
&lt;li&gt;Graphics Card: RTX 3060 12GB&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;environment-setup&#34;&gt;Environment Setup
&lt;/h2&gt;&lt;p&gt;Add the system environment variable to facilitate subsequent use:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;set OLLAMA_MODELS=E:\ollama&lt;/code&gt;&lt;/strong&gt;
This variable specifies the location where Ollama models are stored. &lt;code&gt;E:\ollama&lt;/code&gt; is a folder path indicating that all local model files will be stored in this directory. Ollama will load and use the language models you download or deploy based on this path. You can store model files in other locations by simply changing this path.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;set OLLAMA_HOST=127.0.0.1:8000&lt;/code&gt;&lt;/strong&gt;
This environment variable sets the host and port for the Ollama service.
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;127.0.0.1&lt;/code&gt; is the localhost address, meaning the Ollama service will only listen for requests from the local machine.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;8000&lt;/code&gt; is the specified port number, indicating that the Ollama service will wait for and process requests on port 8000. You can change the port number if needed, but make sure it&amp;rsquo;s not already in use by another application.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;set OLLAMA_ORIGINS=*&lt;/code&gt;&lt;/strong&gt;
This environment variable controls which origins are allowed to access the Ollama service.
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;*&lt;/code&gt; indicates that all origins (i.e., all domains and IP addresses) can access the Ollama service. This is typically used in development and debugging environments, and in production environments, it&amp;rsquo;s usually necessary to specify stricter origin control, limiting only specific domains or IPs to access your service for enhanced security.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;deepseek-r1-model-deployment&#34;&gt;DeepSeek-R1 Model Deployment
&lt;/h2&gt;&lt;p&gt;ollama installation is straightforward, so we won&amp;rsquo;t detail it here.&lt;/p&gt;
&lt;p&gt;Post-installation verification:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;C:\Users\core&amp;gt;ollama -v
ollama version is 0.5.11
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To deploy the model, refer to the official model page and select the appropriate parameter model: &lt;code&gt;ollama run deepseek-r1:14b&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;The 14b parameter version effectively remembers conversation context; smaller parameter versions cannot retain context. The 32b parameter version is very sluggish when deployed locally and hasn&amp;rsquo;t been further tested.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.ollama.com/library/deepseek-r1&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://www.ollama.com/library/deepseek-r1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/SPEvYTmTBxhoEkJqm1yPmw&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://mp.weixin.qq.com/s/SPEvYTmTBxhoEkJqm1yPmw&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://blog.csdn.net/x18990027/article/details/145368094&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://blog.csdn.net/x18990027/article/details/145368094&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
        </item>
        <item>
        <title>PowerShell 7 and Persistent Settings Command-Line Prediction View</title>
        <link>https://ttf248.life/en/p/powershell-7-persisting-settings-commandline-prediction-view/</link>
        <pubDate>Fri, 07 Feb 2025 22:19:12 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/powershell-7-persisting-settings-commandline-prediction-view/</guid>
        <description>&lt;p&gt;“I’d gotten used to using zsh on Linux, and when I was writing a blog post the other day, I suddenly realized that PowerShell 7 also supports persistent command-line prediction views, so I tried it out. It turned out to be pretty useful after all.”&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“I don&amp;rsquo;t know what I did to enable this feature, but it just appeared—that’s all.”&lt;/p&gt;
&lt;/blockquote&gt;
e&gt;
&lt;h2 id=&#34;powershell-7-a-powerful-tool-across-platforms&#34;&gt;PowerShell 7: A Powerful Tool Across Platforms
&lt;/h2&gt;&lt;h3 id=&#34;cross-platform-features&#34;&gt;Cross-Platform Features
&lt;/h3&gt;&lt;p&gt;PowerShell 7 breaks down platform limitations, allowing you to perform enterprise-level server management on Windows systems, system administration in Linux environments, or daily development tasks on macOS – all with a unified PowerShell 7 tool. This significantly increases productivity and reduces the learning curve and operational complexity associated with platform differences.&lt;/p&gt;
&lt;h3 id=&#34;powerful-features&#34;&gt;Powerful Features
&lt;/h3&gt;&lt;p&gt;It possesses powerful scripting capabilities, supporting object-oriented programming, functions, modules, and other advanced programming features. Through PowerShell 7, users can easily operate the file system, create, delete, copy, move, and perform other operations on files and folders; it can access and modify the registry to deeply adjust system configurations; it can manage processes and services to effectively monitor and control the system&amp;rsquo;s running status. Furthermore, PowerShell 7 can interact with various Windows and non-Windows technologies, such as user and permission management in Active Directory and resource allocation and management on the Azure cloud platform.&lt;/p&gt;
&lt;h3 id=&#34;open-source-ecosystem&#34;&gt;Open Source Ecosystem
&lt;/h3&gt;&lt;p&gt;PowerShell 7 is open source, a feature that allows developers and enthusiasts worldwide to actively participate in its development and improvement. A large number of open-source modules and tools are constantly emerging, enriching the functionality and application scenarios of PowerShell 7. Users can find suitable modules within the open-source community to extend the capabilities of PowerShell 7 or contribute their own code to drive the overall development of the community.&lt;/p&gt;
&lt;h3 id=&#34;compatibility-and-stability&#34;&gt;Compatibility and Stability
&lt;/h3&gt;&lt;p&gt;PowerShell 7 maintains compatibility with older versions of PowerShell while introducing many new features and improvements. These enhancements not only improve performance but also increase stability, allowing users to complete various tasks more smoothly and reducing disruptions caused by software failures.&lt;/p&gt;
&lt;h2 id=&#34;enable-command-line-prediction-view&#34;&gt;Enable Command-Line Prediction View
&lt;/h2&gt;&lt;p&gt;Within the many useful features of PowerShell 7, the &lt;code&gt;Set-PSReadLineOption -PredictionViewStyle ListView&lt;/code&gt; command is a practical tool that enhances the user&amp;rsquo;s command-line input experience.&lt;/p&gt;
&lt;p&gt;While the command itself isn’t necessary to achieve auto-completion, it only provides in-line completion; once enabled, it allows for prediction view, displaying all possible completion options in a list format. Users can then select the desired option using the up and down arrow keys, thereby improving the accuracy and efficiency of command input.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ttf248.life/p/powershell-7-persisting-settings-commandline-prediction-view/20250207222546.png&#34;
	width=&#34;814&#34;
	height=&#34;205&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;powershell7&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;397&#34;
		data-flex-basis=&#34;952px&#34;
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;methods-to-make-commands-persistent&#34;&gt;Methods to Make Commands Persistent
&lt;/h2&gt;&lt;p&gt;To ensure that the &lt;code&gt;Set-PSReadLineOption -PredictionViewStyle ListView&lt;/code&gt; command takes effect every time PowerShell starts, we can add it to PowerShell&amp;rsquo;s profile. A PowerShell profile is a special script that automatically executes its commands when PowerShell launches.&lt;/p&gt;
&lt;h3 id=&#34;determine-configuration-file-path&#34;&gt;Determine Configuration File Path
&lt;/h3&gt;&lt;p&gt;In PowerShell, we can use the &lt;code&gt;$PROFILE&lt;/code&gt; variable to view the path of the configuration file. If the file does not exist under that path, users can manually create one.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;echo $PROFILE
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;open-configuration-file&#34;&gt;Open Configuration File
&lt;/h3&gt;&lt;p&gt;Use a text editor, such as the powerful Notepad++ or the lightweight Visual Studio Code, to open the file corresponding to the configuration file path obtained through the &lt;code&gt;$PROFILE&lt;/code&gt; variable.&lt;/p&gt;
&lt;h3 id=&#34;add-command&#34;&gt;Add Command
&lt;/h3&gt;&lt;p&gt;In the opened configuration file, add the command &lt;code&gt;Set-PSReadLineOption -PredictionViewStyle ListView&lt;/code&gt;. Ensure that the command is written accurately to guarantee that the configuration file takes effect correctly when executed.&lt;/p&gt;
&lt;h3 id=&#34;save-configuration&#34;&gt;Save Configuration
&lt;/h3&gt;&lt;p&gt;After adding the command, save the configuration file and close the text editor. At this point, the configuration file contains the commands we want to execute every time PowerShell starts.&lt;/p&gt;
&lt;h3 id=&#34;verification-settings&#34;&gt;Verification Settings
&lt;/h3&gt;&lt;p&gt;Close the current PowerShell window and restart PowerShell. In the newly launched PowerShell, when entering commands, the command-line input prediction view style should already be displayed in list view according to our settings, indicating that our settings have been successfully applied.&lt;/p&gt;
&lt;p&gt;Through these steps, we not only gained a deeper understanding of the powerful features and characteristics of PowerShell 7 but also learned how to use the command-line input prediction view style to enhance the user experience, and how to make these settings persistent. We hope this knowledge can help you operate PowerShell 7 more confidently and efficiently complete various system management and automation tasks.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/PowerShell/PowerShell/releases&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/PowerShell/PowerShell/releases&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.v2ex.com/t/911909&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://www.v2ex.com/t/911909&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
        </item>
        <item>
        <title>Visual Studio loading a mismatched PDB file</title>
        <link>https://ttf248.life/en/p/visual-studio-load-unmatched-pdb/</link>
        <pubDate>Thu, 23 Jan 2025 20:04:33 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/visual-studio-load-unmatched-pdb/</guid>
        <description>&lt;p&gt;When debugging programs under Windows using Visual Studio, if the PDB file does not match the executable file, Visual Studio will display &amp;ldquo;Unable to load symbol file.&amp;rdquo; The program crashes and generates a crash dump. If it&amp;rsquo;s an mismatched PDB file, Visual Studio cannot smoothly enter the crash site.&lt;/p&gt;
&lt;h2 id=&#34;what-is-a-pdb-file&#34;&gt;What is a PDB File?
&lt;/h2&gt;&lt;p&gt;A PDB file is a debugging information file created by Microsoft, used for debugging programs. It contains information such as the symbol table, source code filenames, line numbers, and other debugging data. A PDB file can be generated during program compilation to aid in debugging.&lt;/p&gt;
&lt;h2 id=&#34;windbg-debugging&#34;&gt;WinDbg Debugging
&lt;/h2&gt;&lt;p&gt;WinDbg is a debugging tool from Microsoft that can be used to debug Windows programs. WinDbg can load mismatched PDB files, but this requires manual loading. The &lt;code&gt;.reload /f /i&lt;/code&gt; command forces the loading of mismatched PDB files.&lt;/p&gt;
&lt;p&gt;However, WinDbg is less convenient to use than Visual Studio, so we want Visual Studio to also be able to load mismatched PDB files.&lt;/p&gt;
&lt;h2 id=&#34;visual-studio-cannot-load-matching-pdb-files&#34;&gt;Visual Studio Cannot Load Matching PDB Files
&lt;/h2&gt;&lt;p&gt;Source code is now generally managed through Git, allowing you to find the corresponding version of the code and recompile it to generate matching PDB files. Why can’t they be loaded? The main reason is that metadata doesn&amp;rsquo;t match.&lt;/p&gt;
&lt;p&gt;There’s a small tool that can modify metadata, generating a new PDB file based on the EXE file information so that Visual Studio can load it.&lt;/p&gt;
&lt;p&gt;ChkMatch download address: &lt;a class=&#34;link&#34; href=&#34;https://www.debuginfo.com/tools/chkmatch.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://www.debuginfo.com/tools/chkmatch.html&lt;/a&gt;
Site cache address: &lt;a class=&#34;link&#34; href=&#34;chkmatch.zip&#34; &gt;chkmatch.zip&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;The ChkMatch utility can be used to check whether an executable and debug information file match. It can also be used to enforce matching between an executable and a debug information file if they are compatible.

For more information about debug information matching and related issues, see this article.

Supported debug information formats: DBG, PDB 2.0, PDB 7.0.

chkmatch [-c ExeFile DebugInfoFile ] |
         [-m ExeFile DebugInfoFile]
-c
Check matching between the executable and the debug information file.
-m
Make the executable and the debug information file match.
ExeFile
The name of the executable file.
DebugInfoFile
The name of the debug information file.
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;using-chkmatch&#34;&gt;Using chkmatch
&lt;/h2&gt;&lt;p&gt;First, perform the check operation to analyze the cause of mismatches and prompt signature mismatch.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;C:\Users\tianlong.xiang\Downloads\chkmatch&amp;gt;ChkMatch.exe -c &amp;quot;D:\Program Files\Rolan\trade\UAT_YinStrade\YinTrade.Main.exe&amp;quot; E:\YinTech\ykcz_securities_trading_client\Sec_Trade\YinTrade.Main\bin\Release\YinTrade.Main.pdb
ChkMatch - version 1.0
Copyright (C) 2004 Oleg Starodumov
http://www.debuginfo.com/


Executable: D:\Program Files\Rolan\trade\UAT_YinStrade\YinTrade.Main.exe
Debug info file: E:\YinTech\ykcz_securities_trading_client\Sec_Trade\YinTrade.Main\bin\Release\YinTrade.Main.pdb

Executable:
TimeDateStamp: c26d9be3
Debug info: 2 ( CodeView )
TimeStamp: f86b0a4f  Characteristics: 0  MajorVer: 0  MinorVer: 0
Size: 122  RVA: 001cdc44  FileOffset: 001cbe44
CodeView format: RSDS
Signature: {428c9b95-39a3-4a8d-a8e5-7be453684757}  Age: 1
PdbFile: D:\stock_UAT\ykcz_securities_trading_client\Sec_Trade\YinTrade.Main\obj\Release\YinTrade.Main.pdb
Debug info: 16 ( Unknown )
TimeStamp: 00000000  Characteristics: 0  MajorVer: 0  MinorVer: 0
Size: 0  RVA: 00000000  FileOffset: 00000000

Debug information file:
Format: PDB 7.00
Signature: {06fae08e-c0a2-4f3d-9c7c-dfc684445dd1}  Age: 1

Result: Unmatched (reason: Signature mismatch)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then perform the modify operation to match the pdb file with the exe file.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;C:\Users\tianlong.xiang\Downloads\chkmatch&amp;gt;ChkMatch.exe -m &amp;quot;D:\Program Files\Rolan\trade\UAT_YinStrade\YinTrade.Main.exe&amp;quot; E:\YinTech\ykcz_securities_trading_client\Sec_Trade\YinTrade.Main\bin\Release\YinTrade.Main.pdb
ChkMatch - version 1.0
Copyright (C) 2004 Oleg Starodumov
http://www.debuginfo.com/


Executable: D:\Program Files\Rolan\trade\UAT_YinStrade\YinTrade.Main.exe
Debug info file: E:\YinTech\ykcz_securities_trading_client\Sec_Trade\YinTrade.Main\bin\Release\YinTrade.Main.pdb

Executable:
TimeDateStamp: c26d9be3
Debug info: 2 ( CodeView )
TimeStamp: f86b0a4f  Characteristics: 0  MajorVer: 0  MinorVer: 0
Size: 122  RVA: 001cdc44  FileOffset: 001cbe44
CodeView format: RSDS
Signature: {428c9b95-39a3-4a8d-a8e5-7be453684757}  Age: 1
PdbFile: D:\stock_UAT\ykcz_securities_trading_client\Sec_Trade\YinTrade.Main\obj\Release\YinTrade.Main.pdb
Debug info: 16 ( Unknown )
TimeStamp: 00000000  Characteristics: 0  MajorVer: 0  MinorVer: 0
Size: 0  RVA: 00000000  FileOffset: 00000000

Debug information file:

## References
- [forcing-to-load-unmatched-symbols-in-visual-studio-2015-debugger](https://stackoverflow.com/questions/38147487/forcing-to-load-unmatched-symbols-in-visual-studio-2015-debugger)&lt;/code&gt;&lt;/pre&gt;
</description>
        </item>
        <item>
        <title>Cursor AI Programming IDE Trial</title>
        <link>https://ttf248.life/en/p/cursor-ai-programming-ide-trial/</link>
        <pubDate>Thu, 23 Jan 2025 19:30:13 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/cursor-ai-programming-ide-trial/</guid>
        <description>&lt;p&gt;It seems like another year has passed, and the biggest change at work is a significant increase in AI participation. Previously, switching between different development languages required developers to be familiar with various language-specific API interfaces. Now, these basic code snippets can all be generated by AI, which is a huge blessing for developers.&lt;/p&gt;
&lt;h2 id=&#34;chatgpt&#34;&gt;ChatGPT
&lt;/h2&gt;&lt;p&gt;As early as 2023, I’ve written two simple introductory articles about it. Now it&amp;rsquo;s been 25 years – how to put this… I haven’t felt a significant improvement. It still needs to develop its own cognition, be able to reasonably break down tasks, and, of course, most importantly, identify whether AI-generated code contains bugs.&lt;/p&gt;
&lt;h2 id=&#34;github-copilot&#34;&gt;Github Copilot
&lt;/h2&gt;&lt;p&gt;It was a long time ago, but I saw some information saying that Singapore deployed the server and it’s available for use in China. No longer need to maintain a VPN connection for extended periods. Of course, when logging in, you still need to connect to a VPN, but this VPN only needs to be used during login, and then you can turn it off.&lt;/p&gt;
&lt;p&gt;In daily use, Github Copilot is also heavily relied upon. This plugin can be directly used in VS Code and Visual Studio without switching between the two applications. Compared to ChatGPT, Github Copilot provides better support for projects, is more user-friendly in interaction, and allows you to feed partial local files to it – &lt;strong&gt;“training” the AI&lt;/strong&gt; – so that the generated code is more aligned with your project.&lt;/p&gt;
&lt;h2 id=&#34;cursor-ai&#34;&gt;Cursor AI
&lt;/h2&gt;&lt;p&gt;Recently I’ve seen a new AI programming IDE, Cursor AI. This IDE is based on Github Copilot, but this IDE is more intelligent and can help you create files directly.
I tried it briefly and found it to be pretty good, but its understanding of existing projects isn&amp;rsquo;t quite there yet. When working with large local project files, or for major refactoring, optimization, and adjustments, developers still need to &lt;strong&gt;break down tasks&lt;/strong&gt;.
Here’s an example: Switching to Cursor’s engineering mode, inputting the following content: “Create a personal resume webpage, supporting multiple different styles switching, and remember to fill in some personal information for data display.”
After several back-and-forths (&lt;strong&gt;pulling&lt;/strong&gt;), you can get the following webpage. Of course, this webpage is relatively simple, but it’s still pretty good for beginners.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Currently, registered users can enjoy a free trial of 150 advanced APIs, while paying users are limited to 5000 advanced APIs.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;cursor-ai-1&#34;&gt;Cursor AI
&lt;/h2&gt;&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://ttf248.life/cursor/index.html&#34; &gt;Resume&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>C&#43;&#43; Bitwise Operations Fundamentals: Bitwise Extraction and Flag Setting</title>
        <link>https://ttf248.life/en/p/cpp-bitwise-operations-flags/</link>
        <pubDate>Fri, 17 Jan 2025 02:23:56 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/cpp-bitwise-operations-flags/</guid>
        <description>&lt;p&gt;In actual C++ development, bitwise operations are a common technique, especially when dealing with system states, flags, or control bits. Bitwise operations can provide very efficient solutions. This article will illustrate how to use bitwise operations to retrieve and set specific flags through an example.&lt;/p&gt;
&lt;h3 id=&#34;bitwise-operations-fundamentals&#34;&gt;Bitwise Operations Fundamentals
&lt;/h3&gt;&lt;p&gt;In computers, data is stored in binary bits (0 and 1). Bitwise operations are operations performed on these binary bits. C++ provides several commonly used bitwise operators:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Bitwise AND (&amp;amp;)&lt;/strong&gt;: Used to check if a particular bit is set to 1.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Bitwise OR (|)&lt;/strong&gt;: Used to set a particular bit to 1.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Bitwise XOR (^)&lt;/strong&gt;: Used to flip a particular bit.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Bitwise NOT (~)&lt;/strong&gt;: Inverts all the bits.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Left Shift (&amp;laquo;)&lt;/strong&gt;: Shifts all bits to the left by a specified number of positions.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Right Shift (&amp;raquo;)&lt;/strong&gt;: Shifts all bits to the right by a specified number of positions.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In this example, we need to perform a series of bitwise operations on an &lt;code&gt;unsigned short&lt;/code&gt; variable &lt;code&gt;wInfo&lt;/code&gt; to represent different states using various flags.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart LR
    A[Original Value: 00010000] --&amp;gt; B[Left Shift: 00010000 &amp;lt;&amp;lt; 1]
    B --&amp;gt; C[Result: 00100000]
    C --&amp;gt; D[Right Shift: 00100000 &amp;gt;&amp;gt; 1]
    D --&amp;gt; E[Result: 00010000]

    subgraph Left Shift Operation
        direction LR
        A --&amp;gt; B --&amp;gt; C
    end

    subgraph Right Shift Operation
        direction LR
        C --&amp;gt; D --&amp;gt; E
    end
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;requirement-analysis&#34;&gt;Requirement Analysis
&lt;/h3&gt;&lt;p&gt;Based on the description, we have a 16-bit flag to represent different states. These states are represented by various binary bits, with each binary bit corresponding to a specific meaning. For example:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;bit0&lt;/strong&gt;: Failure status&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;bit1&lt;/strong&gt;: Compression status&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;bit2&lt;/strong&gt;: Incremental status&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;bit3&lt;/strong&gt;: Presence of subsequent packets&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;bit5&lt;/strong&gt;: Normal request or cancellation&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;using-bitwise-operations&#34;&gt;Using Bitwise Operations
&lt;/h3&gt;&lt;p&gt;We will use bitwise operations to set and retrieve these flags. Specifically:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Bitwise AND:&lt;/strong&gt; Retrieve the value of a particular bit (0 or 1).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Bitwise OR:&lt;/strong&gt; Set a particular bit to 1.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Bitwise XOR:&lt;/strong&gt; Set a particular bit to 0.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We first define an &lt;code&gt;unsigned short&lt;/code&gt; type variable &lt;code&gt;wInfo&lt;/code&gt; to store these flags. Then, we use bitwise operations to check and set the corresponding flags.&lt;/p&gt;
&lt;h3 id=&#34;c-example-code&#34;&gt;C++ Example Code
&lt;/h3&gt;&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;#include &amp;lt;iostream&amp;gt;
#include &amp;lt;bitset&amp;gt;

// Define flag constants
const unsigned short BIT_0_FAIL = 1 &amp;lt;&amp;lt; 0;    // bit0 failed?
const unsigned short BIT_1_COMPRESSED = 1 &amp;lt;&amp;lt; 1; // bit1 compressed?
const unsigned short BIT_2_INCREMENT = 1 &amp;lt;&amp;lt; 2;  // bit2 incremented?
const unsigned short BIT_3_HAS_MORE = 1 &amp;lt;&amp;lt; 3;   // bit3 has more packets?
const unsigned short BIT_5_CANCEL = 1 &amp;lt;&amp;lt; 5;     // bit5 normal request(0) or cancel(1)

// Check if a bit is set
bool isBitSet(unsigned short wInfo, unsigned short bitMask) {
    return (wInfo &amp;amp; bitMask) != 0;
}

// Set a bit to 1
void setBit(unsigned short&amp;amp; wInfo, unsigned short bitMask) {
    wInfo |= bitMask;
}

// Clear a bit (set it to 0)
void clearBit(unsigned short&amp;amp; wInfo, unsigned short bitMask) {
    wInfo &amp;amp;= ~bitMask;
}

int main() {
    // Assume wInfo&#39;s initial value is 0
    unsigned short wInfo = 0;

    // Set bit0 (failure flag)
    setBit(wInfo, BIT_0_FAIL);
    
    // Set bit1 (compressed flag)
    setBit(wInfo, BIT_1_COMPRESSED);
    
    // Print wInfo&#39;s binary value
    std::cout &amp;lt;&amp;lt; &amp;quot;wInfo (in binary): &amp;quot; &amp;lt;&amp;lt; std::bitset&amp;lt;16&amp;gt;(wInfo) &amp;lt;&amp;lt; std::endl;

    // Check each flag
    std::cout &amp;lt;&amp;lt; &amp;quot;bit0 (failed?): &amp;quot; &amp;lt;&amp;lt; (isBitSet(wInfo, BIT_0_FAIL) ? &amp;quot;yes&amp;quot; : &amp;quot;no&amp;quot;) &amp;lt;&amp;lt; std::endl;
    std::cout &amp;lt;&amp;lt; &amp;quot;bit1 (compressed?): &amp;quot; &amp;lt;&amp;lt; (isBitSet(wInfo, BIT_1_COMPRESSED) ? &amp;quot;yes&amp;quot; : &amp;quot;no&amp;quot;) &amp;lt;&amp;lt; std::endl;
    std::cout &amp;lt;&amp;lt; &amp;quot;bit2 (incremented?): &amp;quot; &amp;lt;&amp;lt; (isBitSet(wInfo, BIT_2_INCREMENT) ? &amp;quot;yes&amp;quot; : &amp;quot;no&amp;quot;) &amp;lt;&amp;lt; std::endl;
    std::cout &amp;lt;&amp;lt; &amp;quot;bit3 (has more packets?): &amp;quot; &amp;lt;&amp;lt; (isBitSet(wInfo, BIT_3_HAS_MORE) ? &amp;quot;yes&amp;quot; : &amp;quot;no&amp;quot;) &amp;lt;&amp;lt; std::endl;
    std::cout &amp;lt;&amp;lt; &amp;quot;bit5 (canceled?): &amp;quot; &amp;lt;&amp;lt; (isBitSet(wInfo, BIT_5_CANCEL) ? &amp;quot;yes&amp;quot; : &amp;quot;no&amp;quot;) &amp;lt;&amp;lt; std::endl;

    // Clear bit1 (compressed flag)
    clearBit(wInfo, BIT_1_COMPRESSED);
    
    // Print the updated wInfo
    std::cout &amp;lt;&amp;lt; &amp;quot;Updated wInfo (in binary): &amp;quot; &amp;lt;&amp;lt; std::bitset&amp;lt;16&amp;gt;(wInfo) &amp;lt;&amp;lt; std::endl;

    return 0;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Run the code, recommended for old friends: &lt;a class=&#34;link&#34; href=&#34;https://wandbox.org/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://wandbox.org/&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;wInfo (in binary): 0000000000000001
bit0 (failed?): yes
bit1 (compressed?): no
bit2 (incremented?): no
bit3 (has more packets?): no
bit5 (canceled?): no
Updated wInfo (in binary): 0000000000000000
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;code-explanation&#34;&gt;Code Explanation
&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Flag Definition&lt;/strong&gt;: Use shift operations (&lt;code&gt;1 &amp;lt;&amp;lt; n&lt;/code&gt;) to define each flag bit. For example, &lt;code&gt;1 &amp;lt;&amp;lt; 0&lt;/code&gt; corresponds to &lt;code&gt;bit0&lt;/code&gt;, &lt;code&gt;1 &amp;lt;&amp;lt; 1&lt;/code&gt; corresponds to &lt;code&gt;bit1&lt;/code&gt;, and so on. This way, we allocate a unique binary position for each flag.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Check a Bit&lt;/strong&gt;: The &lt;code&gt;isBitSet&lt;/code&gt; function uses the bitwise AND operation (&lt;code&gt;wInfo &amp;amp; bitMask&lt;/code&gt;) to check if a specific flag is set to 1. If the bit is 1, the function returns &lt;code&gt;true&lt;/code&gt;, otherwise it returns &lt;code&gt;false&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Set a Bit&lt;/strong&gt;: The &lt;code&gt;setBit&lt;/code&gt; function uses the bitwise OR operation (&lt;code&gt;wInfo |= bitMask&lt;/code&gt;) to set a specific flag bit to 1.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Clear a Bit&lt;/strong&gt;: The &lt;code&gt;clearBit&lt;/code&gt; function uses the bitwise AND operation (&lt;code&gt;wInfo &amp;amp;= ~bitMask&lt;/code&gt;) to clear a specific flag bit to 0.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;summary&#34;&gt;Summary
&lt;/h3&gt;&lt;p&gt;Through bitwise operations, we can efficiently handle multiple state flags. This technique is particularly useful in practical development. For example, in embedded development, network protocols, and system status management scenarios, bit flags are often used to represent multiple binary states, saving space and improving efficiency.&lt;/p&gt;
&lt;p&gt;We hope this blog post helps you understand how to use bitwise operations in C++ to perform bitwise selection and setting, and mastering these skills is very helpful for writing efficient and maintainable code!&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Upgrading from a desktop to a 2.5G network card, accelerating local area network connectivity.</title>
        <link>https://ttf248.life/en/p/desktop-upgrade-to-2-5g-network-card-accelerates-local-area-network-interconnection/</link>
        <pubDate>Fri, 10 Jan 2025 00:37:52 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/desktop-upgrade-to-2-5g-network-card-accelerates-local-area-network-interconnection/</guid>
        <description>&lt;p&gt;Desktop hardware three-in-one, in the previous text we mentioned PCIe adapter for solid state drives, where did the old SSDs go? Of course there was no waste, were any of them broken, disassembled and installed on the newly purchased ‘MechMaker Mini-3765H’ (bought a year ago).&lt;/p&gt;
&lt;p&gt;The new machine has powerful hardware specifications: 2.5G dual network interface, PCIe4.0, WiFi6.&lt;/p&gt;
&lt;p&gt;Recently moved house and my room doesn&amp;rsquo;t have a dedicated router for networking, all the machines are connected via wireless network; the ASUS motherboard desktop wireless card performance wasn’t great, or perhaps it was the router’s wireless access, which resulted in slow upload speeds between local networks, leading to poor network speeds between the machines. I purchased a 2.5G NIC and installed it on the desktop.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Thus, all the slots on the motherboard are now full: graphics card, wireless card, 2.5G NIC, PCIe adapter for solid state drives.&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;network-instructions&#34;&gt;Network Instructions
&lt;/h2&gt;&lt;p&gt;Both machines connect to the internet using their original wireless network cards, but are directly connected via Ethernet cables between the two, with both ends equipped with 2.5G network cards.  The specifics of how to physically connect the cables aren&amp;rsquo;t detailed here – numerous tutorials are available online; just remember to disable your firewall. You can select either machine as the gateway.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;graph TD;
    A[Machine 1&amp;lt;br&amp;gt;IP: 192.168.4.1&amp;lt;br&amp;gt;Subnet Mask: 255.255.255.0&amp;lt;br&amp;gt;Default Gateway: - &amp;lt;br&amp;gt;Obtain DNS Automatically] --&amp;gt;|Ethernet Connection (2.5G)| B[Machine 2&amp;lt;br&amp;gt;IP: 192.168.4.2&amp;lt;br&amp;gt;Subnet Mask: 255.255.255.0&amp;lt;br&amp;gt;Default Gateway: 192.168.4.1&amp;lt;br&amp;gt;Obtain DNS Automatically];
    A --&amp;gt;|Wireless Card| Internet;
    B --&amp;gt;|Wireless Card| Internet;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;two-subnet-speed-testing&#34;&gt;Two Subnet Speed Testing
&lt;/h2&gt;&lt;h3 id=&#34;router-lan&#34;&gt;Router LAN
&lt;/h3&gt;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;C:\Users\core\Desktop\iperf-3.1.3-win32&amp;gt;iperf3.exe -c 192.168.3.237
Connecting to host 192.168.3.237, port 5201
[  4] local 192.168.3.122 port 1656 connected to 192.168.3.237 port 5201
[ ID] Interval           Transfer     Bandwidth
[  4]   0.00-1.00   sec  9.17 MBytes  76.7 Mbits/sec
[  4]   1.00-2.00   sec  9.91 MBytes  83.2 Mbits/sec
[  4]   2.00-3.00   sec  8.74 MBytes  73.3 Mbits/sec
[  4]   3.00-4.00   sec  10.2 MBytes  85.2 Mbits/sec
[  4]   4.00-5.00   sec  9.23 MBytes  77.1 Mbits/sec
[  4]   5.00-6.00   sec  8.80 MBytes  73.9 Mbits/sec
[  4]   6.00-7.01   sec  8.00 MBytes  66.8 Mbits/sec
[  4]   7.01-8.00   sec  7.69 MBytes  64.9 Mbits/sec
[  4]   8.00-9.01   sec  9.72 MBytes  81.1 Mbits/sec
[  4]   9.01-10.01  sec  7.63 MBytes  63.6 Mbits/sec
- - - - - - - - - - - - - - - - - - - - - - - - -
[ ID] Interval           Transfer     Bandwidth
[  4]   0.00-10.01  sec  89.0 MBytes  74.6 Mbits/sec                  sender
[  4]   0.00-10.01  sec  89.0 MBytes  74.6 Mbits/sec                  receiver

iperf Done.
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;direct-lan-connection&#34;&gt;Direct LAN Connection
&lt;/h3&gt;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;C:\Users\core\Desktop\iperf-3.1.3-win32&amp;gt;iperf3.exe -c 192.168.4.1
Connecting to host 192.168.4.1, port 5201
[  4] local 192.168.4.2 port 1524 connected to 192.168.4.1 port 5201
[ ID] Interval           Transfer     Bandwidth
[  4]   0.00-1.01   sec   178 MBytes  1.48 Gbits/sec
[  4]   1.01-2.00   sec   204 MBytes  1.72 Gbits/sec
[  4]   2.00-3.00   sec   214 MBytes  1.80 Gbits/sec
[  4]   3.00-4.00   sec   229 MBytes  1.92 Gbits/sec
[  4]   4.00-5.00   sec   202 MBytes  1.69 Gbits/sec
[  4]   5.00-6.00   sec   213 MBytes  1.79 Gbits/sec
[  4]   6.00-7.00   sec   230 MBytes  1.93 Gbits/sec
[  4]   7.00-8.00   sec   192 MBytes  1.61 Gbits/sec
[  4]   8.00-9.00   sec   220 MBytes  1.84 Gbits/sec
[  4]   9.00-10.00  sec   230 MBytes  1.93 Gbits/sec
- - - - - - - - - - - - - - - - - - - - - - - - -
[ ID] Interval           Transfer     Bandwidth
[  4]   0.00-10.00  sec  2.06 GBytes  1.77 Gbits/sec                  sender
[  4]   0.00-10.00  sec  2.06 GBytes  1.77 Gbits/sec                  receiver

iperf Done.
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;references&#34;&gt;References
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://wutongsuimeng.github.io/post/%E7%BB%99hugo%E6%B7%BB%E5%8A%A0mermaid%E6%94%AF%E6%8C%81/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Adding Mermaid Support to Hugo&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>ASUS Z490 motherboard has too many disks, resulting in intermittent disk unrecognition.</title>
        <link>https://ttf248.life/en/p/asus-z490-motherboard-disk-recognition-issues/</link>
        <pubDate>Fri, 10 Jan 2025 00:08:51 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/asus-z490-motherboard-disk-recognition-issues/</guid>
        <description>&lt;p&gt;Continuing from the previous text, an issue suddenly arose where the wireless network card was unable to be recognized. Before rebuilding the partitions, I had also researched other solutions online, such as: removing the motherboard battery and disconnecting power for fifteen minutes; upgrading to the latest version of the BOIS driver, but all attempts were unsuccessful.&lt;/p&gt;
&lt;p&gt;Thinking there were still tasks to complete, I switched to a limited network by pulling a web across from the living room into the room. However, this is when the problem returned – wired networking was also unable to be recognized. I then resorted to the ultimate solution of reinstalling the system, which resulted in partition loss during the boot process. If the issue had consistently occurred, I wouldn’t have spent so long troubleshooting it; the ASUS disk conflict is intermittent and triggered by unstable system restarts.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Last week, I purchased a new 2TB solid-state drive from长江存储 (Jiangang Chengsu – Yangtze Storage) with an M.2 interface, and the machine never rebooted until yesterday when I shut it down once.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&#34;https://ttf248.life/p/asus-z490-motherboard-disk-recognition-issues/20250110002801.png&#34;
	width=&#34;553&#34;
	height=&#34;322&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;Family photo of a disk&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;171&#34;
		data-flex-basis=&#34;412px&#34;
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;reinstalling-the-system&#34;&gt;Reinstalling the System
&lt;/h2&gt;&lt;p&gt;It’s been almost two years since I last reinstalled the system, and my C drive is completely full. Windows keeps throwing old issues, and various software likes to store things on C: Drive. So, I decided to reinstall the system. After reinstalling the system, the network card issue was resolved, and I restored my daily development environment the next day. While preparing to create a system backup, a new problem arose: after restarting the system, the boot partition disappeared.&lt;/p&gt;
&lt;p&gt;Following the steps in the previous article, I rebuilt the boot partition; however, it wasn’t stable, and the partition would frequently fail to load upon restart. I started to wonder if the chassis was being tampered with when I realized that the hard drive cable was loose, but after several checks, there were no issues.&lt;/p&gt;
&lt;h2 id=&#34;memory-retrieval&#34;&gt;Memory Retrieval
&lt;/h2&gt;&lt;p&gt;Many years ago, this machine had once been equipped with a solid-state drive; it was like buying a new PCIE converter (plugged into the graphics card slot) instead of directly installing the hard drive onto the motherboard. This time, it’s installed directly on the motherboard, which may be due to an issue with the motherboard.
&lt;img src=&#34;https://ttf248.life/p/asus-z490-motherboard-disk-recognition-issues/20250110002148.png&#34;
	width=&#34;325&#34;
	height=&#34;545&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;Solid State Converter&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;59&#34;
		data-flex-basis=&#34;143px&#34;
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;motherboard-manual&#34;&gt;Motherboard Manual
&lt;/h2&gt;&lt;p&gt;The motherboard manual has issues, with the labeled SATA port positions differing from the actual positions. Due to a large number of disks, all ports are populated with hard drives, with older SSDs utilizing SATA ports. According to the manual, there is a conflict between the ports. However, after testing, this conflict was found to be unstable and would trigger, causing the corresponding disk to fail to load – specifically, as it’s the system disk, the bootloader is also located on that disk, resulting in boot loader failure during system startup.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ttf248.life/p/asus-z490-motherboard-disk-recognition-issues/20250110002401.png&#34;
	width=&#34;807&#34;
	height=&#34;847&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;Z490&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;95&#34;
		data-flex-basis=&#34;228px&#34;
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;solutions&#34;&gt;Solutions
&lt;/h2&gt;&lt;p&gt;Reinstall the solid state drive onto a PCIe adapter, at this point, the SATA ports on the motherboard will no longer conflict and the system starts normally.&lt;/p&gt;</description>
        </item>
        <item>
        <title>Windows Built-in Disk Cleanup Tool: Storage</title>
        <link>https://ttf248.life/en/p/windows-disk-cleanup-storage/</link>
        <pubDate>Mon, 06 Jan 2025 19:29:45 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/windows-disk-cleanup-storage/</guid>
        <description>&lt;p&gt;I’m not sure from what version it started, but in &lt;code&gt;Windows 11&lt;/code&gt;, the Disk Cleanup tool has been significantly improved and become much smarter.&lt;/p&gt;
&lt;p&gt;The key is that it&amp;rsquo;s an official tool, so it won’t accidentally delete files, won’t have ads, won’t have pop-ups, won’t have background processes, or any unnecessary elements.&lt;/p&gt;
&lt;p&gt;You can access the Disk Cleanup tool in &lt;code&gt;Windows 11&lt;/code&gt; by going to &lt;code&gt;Settings &amp;gt; System &amp;gt; Storage &amp;gt; Temporary Files&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ttf248.life/p/windows-disk-cleanup-storage/20250106194453.png&#34;
	width=&#34;1041&#34;
	height=&#34;775&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;Storage Interface&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;134&#34;
		data-flex-basis=&#34;322px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;For ordinary users, simply selecting “Clean up” is sufficient; the system will provide suggestions based on your usage.&lt;/p&gt;
&lt;p&gt;As a developer, I have many temporary files on my disk, so I choose “Temporary Files,” which contains items like &lt;code&gt;Visual Studio&lt;/code&gt; temporary files and &lt;code&gt;Windows Update&lt;/code&gt; temporary files, etc.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ttf248.life/p/windows-disk-cleanup-storage/Snipaste_2024-12-19_13-39-51.png&#34;
	width=&#34;465&#34;
	height=&#34;980&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;Temporary Files&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;47&#34;
		data-flex-basis=&#34;113px&#34;
	
&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Docker Domestic Mirror Proxy Failed</title>
        <link>https://ttf248.life/en/p/docker-domestic-image-proxy-failure/</link>
        <pubDate>Sat, 04 Jan 2025 18:29:25 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/docker-domestic-image-proxy-failure/</guid>
        <description>&lt;p&gt;Deploying Docker on domestic servers, after deployment, if the company doesn’t provide a registry center, the first thing developers need to do is configure a domestic registry mirror address. It&amp;rsquo;s lucky that today there was a server configured with a registry mirror address, but when pulling images, it kept failing to pull.&lt;/p&gt;
&lt;p&gt;Error response from daemon: “https://registry-1.docker.io/v2/”: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)&lt;/p&gt;
&lt;!-- 20250106 – Two days after, all servers were restored, and this didn’t even make the news! All domestic registry mirrors went down --&gt;
&lt;h2 id=&#34;troubleshooting-and-repair-attempts&#34;&gt;Troubleshooting and Repair Attempts
&lt;/h2&gt;&lt;p&gt;Initially, we attempted to switch to alternative mirror acceleration addresses, hoping to resolve the issue. However, as expected, the problem persisted.&lt;/p&gt;
&lt;p&gt;Subsequently, we began modifying the local DNS configuration in an attempt to find a breakthrough at the network resolution level. Unfortunately, after debugging, the fault remained.&lt;/p&gt;
&lt;p&gt;At this point, the stability of the local network was heavily questioned, so we decisively switched to a mobile hotspot, attempting to bypass potential local network faults. However, the result was discouraging – there were no signs of improvement.&lt;/p&gt;
&lt;h2 id=&#34;problem-propagation&#34;&gt;Problem Propagation
&lt;/h2&gt;&lt;p&gt;We currently have &lt;strong&gt;a few servers deployed domestically&lt;/strong&gt; with Docker environments, and all of them failed to successfully pull the image. We initially hoped to find an alternative solution, but we found that they all consistently failed with identical error messages, indicating that the issue isn&amp;rsquo;t isolated to a single device.&lt;/p&gt;
&lt;p&gt;Further investigation revealed that the image proxy seemingly malfunctioned instantaneously. In this critical moment, we quickly switched to a machine outside of the country, and thankfully, image pulls were restored at this location. This suggests that the problem is likely related to the domestic network links or associated configurations.&lt;/p&gt;
&lt;h2 id=&#34;strategy-adjustment-circumventing-the-issue&#34;&gt;Strategy Adjustment: Circumventing the Issue
&lt;/h2&gt;&lt;p&gt;Given that direct image pulling methods within China have been heavily restricted, while foreign mirrors remain accessible, to expedite project progress, we’ve decided to employ a circumvention tactic. Initially, we switched to foreign servers to successfully pull the required images, subsequently pushing them to domestic mirror repositories to establish a “data bridge.”&lt;/p&gt;
&lt;p&gt;At the same time, we synchronized modifications to the Dockerfile files, replacing image addresses with those adapted for the Chinese environment and then rebuilt the images, ultimately achieving successful deployment.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>CentOS 8 Stream EOL</title>
        <link>https://ttf248.life/en/p/centos-8-stream-eol/</link>
        <pubDate>Sat, 16 Nov 2024 23:24:50 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/centos-8-stream-eol/</guid>
        <description>&lt;p&gt;&lt;code&gt;CentOS Stream&lt;/code&gt; is the upstream open-source development platform prior to Red Hat’s &lt;code&gt;Linux&lt;/code&gt; distribution.&lt;/p&gt;
&lt;p&gt;I first noticed the open-source operating system lifecycle &lt;a class=&#34;link&#34; href=&#34;https://ttf248.life/en/p/redhat-centos-lifecycle/&#34; &gt;redhat and centos life cycle&lt;/a&gt; had ended, and I was wondering what was going on. Besides security issues, dnf wasn’t working, and I recently encountered failures when installing tools – checking the repository sources revealed that &lt;code&gt;CentOS 8 Stream&lt;/code&gt; had reached its end of life.&lt;/p&gt;
&lt;!-- more --&gt;
&lt;h2 id=&#34;centos-stream-introduction&#34;&gt;CentOS Stream Introduction
&lt;/h2&gt;&lt;h3 id=&#34;positioning-and-roles&#34;&gt;Positioning and Roles
&lt;/h3&gt;&lt;p&gt;CentOS Stream sits between Fedora Linux (upstream development) and RHEL (Red Hat Enterprise Linux, downstream development), acting as a bridge.&lt;/p&gt;
&lt;p&gt;It can be considered a version for experiencing the latest Red Hat-based Linux features, suitable for those who want to try out new things.&lt;/p&gt;
&lt;h3 id=&#34;origins-and-background&#34;&gt;Origins and Background
&lt;/h3&gt;&lt;p&gt;Over time, Red Hat began to seek more effective ways to develop its enterprise-grade Linux platform, leading to the launch of CentOS Stream.&lt;/p&gt;
&lt;p&gt;CentOS 8 ended maintenance at the end of 2021, and CentOS Stream continued to be updated as its successor, becoming the future direction of the CentOS project.&lt;/p&gt;
&lt;h3 id=&#34;features-and-advantages&#34;&gt;Features and Advantages
&lt;/h3&gt;&lt;p&gt;CentOS Stream is a rolling release Linux distribution that provides faster updates. It offers greater transparency and more opportunities for community, partner, and customer participation, allowing users to contribute to Red Hat Enterprise Linux (RHEL) more quickly and directly.
The content of CentOS Stream is software that Red Hat intends to include in the next stable version of RHEL, therefore it provides a stable ABI/API for developers and testers within the community.&lt;/p&gt;
&lt;h3 id=&#34;use-cases-and-target-users&#34;&gt;Use Cases and Target Users
&lt;/h3&gt;&lt;p&gt;CentOS Stream is suitable for those CentOS users who want to continue receiving the latest Linux feature updates, as well as developers and partners who wish to participate in Red Hat Enterprise Linux development.&lt;/p&gt;
&lt;p&gt;It also aims to help community members, Red Hat partners, and others take full advantage of innovative open-source software in a more stable and predictable Linux ecosystem.&lt;/p&gt;
&lt;h2 id=&#34;end-of-lifehttpsendoflifedatecentos-stream&#34;&gt;&lt;a class=&#34;link&#34; href=&#34;https://endoflife.date/centos-stream&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;End of Life&lt;/a&gt;
&lt;/h2&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Release&lt;/th&gt;
&lt;th&gt;Released&lt;/th&gt;
&lt;th&gt;Active Support&lt;/th&gt;
&lt;th&gt;Security Support&lt;/th&gt;
&lt;th&gt;Latest&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;td&gt;3 years ago (15 Sep 2021)&lt;/td&gt;
&lt;td&gt;Ends in 2 years and 6 months (31 May 2027)&lt;/td&gt;
&lt;td&gt;Ends in 2 years and 6 months (31 May 2027)&lt;/td&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;end-of-lifehttpsendoflifedatecentos-stream-1&#34;&gt;&lt;a class=&#34;link&#34; href=&#34;https://endoflife.date/centos-stream&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;End of Life&lt;/a&gt;
&lt;/h2&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Release&lt;/th&gt;
&lt;th&gt;Released&lt;/th&gt;
&lt;th&gt;Active Support&lt;/th&gt;
&lt;th&gt;Security Support&lt;/th&gt;
&lt;th&gt;Latest&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;5 years ago (24 Sep 2019)&lt;/td&gt;
&lt;td&gt;Ended 5 months and 3 weeks ago (31 May 2024)&lt;/td&gt;
&lt;td&gt;Ended 5 months and 3 weeks ago (31 May 2024)&lt;/td&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;solutions&#34;&gt;Solutions
&lt;/h2&gt;&lt;p&gt;Rather than bothering with upgrades, we opted for the long-term support version of &lt;code&gt;Ubuntu 24.04&lt;/code&gt;.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Hugo Module Customizing Theme: Explanation of Approach</title>
        <link>https://ttf248.life/en/p/hugo-module-customizing-themes-ideas/</link>
        <pubDate>Fri, 15 Nov 2024 22:01:17 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/hugo-module-customizing-themes-ideas/</guid>
        <description>&lt;p&gt;Browsing through the historical commit records, the site has undergone numerous theme switches. Each theme switch involved some custom modifications, and this is where I’m documenting the approach to customizing themes. My &lt;code&gt;Github&lt;/code&gt; repository briefly maintained the even theme, but due to my obsessive-compulsive tendencies, I resisted upgrading the &lt;code&gt;hugo&lt;/code&gt; compiler to the latest version, which resulted in incompatibility with the even theme, so I switched back to the &lt;code&gt;stack&lt;/code&gt; theme.&lt;/p&gt;
&lt;h2 id=&#34;hugos-modularity&#34;&gt;Hugo&amp;rsquo;s Modularity
&lt;/h2&gt;&lt;p&gt;When we talk about modularity, many people think of Nginx modules and IDEA plugins, among others. Typically, I can upload some modules to satisfy my differentiated needs. The reason everyone likes this kind of module is that it’s sufficiently flexible – you don’t have to put in too much effort to meet your own requirements. Because often, even though the overall situation is similar, there are always some details that differ. This also illustrates the complexity of software, not just technically but also from a business perspective. Most of the time, we face business complexity. This is precisely where the saying “it’s like crossing a mountain range” best applies. Today, not only the internet industry and finance, but even traditional manufacturing industries are using information systems to help businesses with their production and management. Even a leave system can have differences between companies in the same industry.&lt;/p&gt;
&lt;p&gt;Unlike the modules you might be familiar with, Hugo&amp;rsquo;s modules are different – they don’t focus on meeting differentiated needs based on functionality. Instead, they rely primarily on directory structure to identify identical structures.&lt;/p&gt;
&lt;p&gt;Resource link: &lt;a class=&#34;link&#34; href=&#34;https://medium.com/@sunwei.xyz/07-hugo%E6%9E%B6%E6%9E%84-hugo%E7%9A%84%E6%A8%A1%E5%9D%97-8ef5a520a822&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;07. Hugo Architecture — Hugo Modules&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-toml&#34;&gt;[[imports]]
path = &amp;quot;github.com/CaiJimmy/hugo-theme-stack/v3&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;git submodule&lt;/code&gt; approach can still be used, and this article doesn’t recommend it. If you introduce a theme, maintaining it will be more complicated – you&amp;rsquo;ll need to manage the theme as a separate Git repository.&lt;/p&gt;
&lt;h2 id=&#34;theme-modification-logichttpsstackjimmycaicomguidemodify-theme&#34;&gt;&lt;a class=&#34;link&#34; href=&#34;https://stack.jimmycai.com/guide/modify-theme&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Theme Modification Logic&lt;/a&gt;
&lt;/h2&gt;&lt;p&gt;Once you have a solid understanding of the foundational concepts of modularization, customizing themes becomes much simpler. Hugo themes are currently assembled from multiple different modules. To modify one module, simply locate its corresponding template file and make the changes directly.&lt;/p&gt;
&lt;p&gt;As extracted from the official &lt;code&gt;stack&lt;/code&gt; documentation:&lt;/p&gt;
&lt;p&gt;Using this method, there will be no files under the &lt;code&gt;themes&lt;/code&gt; directory.  To modify a theme, you must copy the file you want to modify into the same directory under the &lt;code&gt;layouts&lt;/code&gt; directory.&lt;/p&gt;
&lt;p&gt;For example, to modify the &lt;code&gt;themes/hugo-theme-stack/layouts/partials/head/custom.html&lt;/code&gt; file, you must copy it to &lt;code&gt;layouts/partials/head/custom.html&lt;/code&gt; and modify it there (copying the code from the theme&amp;rsquo;s repository). The same applies to the &lt;code&gt;assets&lt;/code&gt; and &lt;code&gt;static&lt;/code&gt; directories.&lt;/p&gt;
&lt;h2 id=&#34;how-to-find-template-files&#34;&gt;How to Find Template Files
&lt;/h2&gt;&lt;h3 id=&#34;conventional-approach&#34;&gt;Conventional Approach
&lt;/h3&gt;&lt;p&gt;Review the source files of the topic, understand its design rationale, identify the corresponding template file, and modify it.&lt;/p&gt;
&lt;h3 id=&#34;brute-force-approach&#34;&gt;Brute Force Approach
&lt;/h3&gt;&lt;p&gt;As I’m not very familiar with frontend code, I sometimes resort to a brute force approach, such as opening the corresponding page directly in the browser, finding the areas I want to modify, and using “Inspect Element” to pinpoint the &lt;code&gt;css name&lt;/code&gt;, then searching the source code for the relevant file, copying it into the site directory, and making changes.&lt;/p&gt;
&lt;h2 id=&#34;tips&#34;&gt;Tips
&lt;/h2&gt;&lt;p&gt;The official setup provides a default file for customizing styles. To modify specific areas, we can split them into multiple files and import them using &lt;code&gt;custom.scss&lt;/code&gt;. This approach allows for better management of style files.
&lt;img src=&#34;https://ttf248.life/p/hugo-module-customizing-themes-ideas/image.png&#34;
	width=&#34;141&#34;
	height=&#34;375&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;alt text&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;37&#34;
		data-flex-basis=&#34;90px&#34;
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;consolidated-modifications-6h&#34;&gt;Consolidated Modifications (6h)
&lt;/h2&gt;&lt;p&gt;It’s now the first year of &lt;code&gt;AI&lt;/code&gt; coding, and detailed content will not be pasted here for brevity; instead, we&amp;rsquo;ll simply list some of the modifications made to this site, such as adjusting the copy button styles, reconfiguring the code block styles, and &lt;code&gt;ChatGPT&lt;/code&gt; was easily handled.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Overall: Global text style, retaining the display style previously used by merging &lt;code&gt;even&lt;/code&gt; with &lt;code&gt;info cn&lt;/code&gt;, which is friendly for Chinese&lt;/li&gt;
&lt;li&gt;Homepage: Added mouse interaction animation to the right navigation&lt;/li&gt;
&lt;li&gt;Homepage: New article summaries added (took quite a while to implement using a clever workaround)&lt;/li&gt;
&lt;li&gt;Scroll Bar: Improved the styling of the scroll bar&lt;/li&gt;
&lt;li&gt;Code Blocks: Introduced the &lt;code&gt;highlight.js&lt;/code&gt; code highlighting plugin, beautifying the code block styles&lt;/li&gt;
&lt;li&gt;Article Details: Some content is from reprints, with new author information display and original link display&lt;/li&gt;
&lt;li&gt;Archive Pages: Removed the color mask from the category images at the top to display the original image&lt;/li&gt;
&lt;li&gt;Archive Pages: Added a statistical display panel for categorization by year&lt;/li&gt;
&lt;li&gt;Archive Pages: Two-column layout&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The &lt;code&gt;stack&lt;/code&gt; theme has a high component reuse rate, which also led to the lengthy time taken to add new article summaries to the homepage.  After modifying the corresponding components, changes occurred in the article details page as well, resulting in redundant display of content. The &lt;code&gt;golang template&lt;/code&gt; syntax wasn&amp;rsquo;t very familiar, so it took up quite a bit of time.  Component parameter passing was never resolved, and ultimately, through a clever workaround, a &lt;code&gt;JavaScript&lt;/code&gt; script was independently introduced to the homepage to implement the summary preview by using custom special variables.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Sometimes, high component reuse can also be a problem, leading to unintended consequences when modifying one place affecting others. Therefore, when modifying themes, you must pay attention not to disrupt existing logic.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;comments&#34;&gt;Comments
&lt;/h3&gt;&lt;p&gt;This guy&amp;rsquo;s modifications are more refined: &lt;a class=&#34;link&#34; href=&#34;https://blog.reincarnatey.net/2024/0719-better-waline/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://blog.reincarnatey.net/2024/0719-better-waline/&lt;/a&gt;
&lt;del&gt;This site simply enabled the &lt;code&gt;Waline&lt;/code&gt; comment system, as the &lt;code&gt;stack&lt;/code&gt; theme defaults to supporting &lt;code&gt;Waline&lt;/code&gt;. Just configure it in the &lt;code&gt;config.toml&lt;/code&gt; file.&lt;/del&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Recommend contacting via email on the homepage, this site does not open the comments section&lt;/p&gt;
&lt;/blockquote&gt;
</description>
        </item>
        <item>
        <title>Linux backend services handling large volumes of string data – performance is slow.</title>
        <link>https://ttf248.life/en/p/linux-backend-slow-string-processing/</link>
        <pubDate>Wed, 13 Nov 2024 19:42:59 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/linux-backend-slow-string-processing/</guid>
        <description>&lt;p&gt;In the history of C++ development projects, we utilized a custom protocol for communication, which employed a two-dimensional array pattern. When processing large volumes of data, the protocol required iterating through the arrays and performing serialization operations to generate logs. Due to its low efficiency, this resulted in noticeable lag or stuttering within the system under heavy load, as reported by the business departments.&lt;/p&gt;
&lt;h2 id=&#34;problem-identification&#34;&gt;Problem Identification
&lt;/h2&gt;&lt;p&gt;When troubleshooting the issue, we first performed a performance analysis of the system and discovered that CPU utilization increased significantly when processing large amounts of data, and system response times became longer. By analyzing the system logs, we identified numerous serialization operations, which were inefficient when handling two-dimensional arrays, leading to a decline in system performance.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;pstack&lt;/code&gt; tool captured thread information for the service, pinpointing that the log threads spent most of their time processing string concatenation.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;This is today’s focus: different accumulation methods have significant efficiency differences. Historically, the code used the &lt;code&gt;+&lt;/code&gt; operator, which frequently creates temporary objects and is very inefficient. You know it&amp;rsquo;s bad, but you don&amp;rsquo;t know &lt;em&gt;how&lt;/em&gt; bad it is.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;demo-verification&#34;&gt;Demo Verification
&lt;/h2&gt;&lt;p&gt;Based on the project code, we extracted the business logic and wrote a simple demo to verify the efficiency issues of string concatenation. We compiled and ran it in &lt;code&gt;Release&lt;/code&gt; mode using the &lt;code&gt;vs2022&lt;/code&gt; compiler under &lt;code&gt;windows&lt;/code&gt; and the &lt;code&gt;gcc8.5&lt;/code&gt; compiler under &lt;code&gt;linux&lt;/code&gt;, comparing the efficiencies.&lt;/p&gt;
&lt;h3 id=&#34;key-point-explanation&#34;&gt;Key Point Explanation
&lt;/h3&gt;&lt;p&gt;The project utilized Method Four, and before obtaining test data, readers were encouraged to consider which method was most efficient and which was least efficient. I was quite surprised by the results.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Method 1 (&lt;code&gt;+=&lt;/code&gt; Concatenation)&lt;/strong&gt;: Directly concatenates each field using &lt;code&gt;+=&lt;/code&gt; into a string.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Method 2 (&lt;code&gt;std::ostringstream&lt;/code&gt; Concatenation)&lt;/strong&gt;: Uses a stream (&lt;code&gt;std::ostringstream&lt;/code&gt;) to concatenate fields, which is more efficient, especially when dealing with large amounts of data.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Method 3 (Pre-allocated Memory &lt;code&gt;+=&lt;/code&gt; Concatenation)&lt;/strong&gt;: Pre-allocates enough memory for the string using &lt;code&gt;reserve&lt;/code&gt;, reducing the overhead of memory reallocation and improving performance.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Method 4 (&lt;code&gt;bodys = bodys + body + &amp;quot;\n&amp;quot;&lt;/code&gt;)&lt;/strong&gt;: Creates a new temporary string object each time it concatenates, leading to decreased performance, particularly when dealing with large-scale concatenation due to repeated memory allocation and copying.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Referring to the results, we can see that the project inadvertently selected the least efficient method.&lt;/p&gt;
&lt;p&gt;Furthermore, let&amp;rsquo;s analyze the optimization efficiency of different platforms and compilers. Microsoft’s &lt;code&gt;visual studio&lt;/code&gt; consistently performs excellently in terms of string optimization, while the &lt;code&gt;gcc&lt;/code&gt; compiler has somewhat lower optimization efficiency in this area.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;When running the code on different machines, direct comparison between the two datasets is meaningless; instead, we can compare the differences between the various concatenation methods.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;key-points-explanation&#34;&gt;Key Points Explanation
&lt;/h3&gt;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;Windows platform under VS2022 compiler

----------------------------------------
Data Generation Time: 0.054 seconds.
----------------------------------------

----------------------------------------
Data Merging Performance:
----------------------------------------
+ Data merging (+=) took: 0.053 seconds.
+ ostringstream Data merging took: 0.054 seconds.
+ Pre-reserved Data merging took: 0.045 seconds.
+ Data merging (bodys = bodys + body + &amp;quot;\n&amp;quot;) took: 16.108 seconds.

----------------------------------------
Data Merging Complete.
----------------------------------------

Program finished.
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;Linux platform under gcc8.5 compiler
----------------------------------------
Data Generation Time: 0.108 seconds.
----------------------------------------

----------------------------------------
Data Merging Performance:
----------------------------------------
+ Data merging (+=) took: 0.100 seconds.
+ ostringstream Data merging took: 0.083 seconds.
+ Pre-reserved Data merging took: 0.057 seconds.
+ Data merging (bodys = bodys + body + &amp;quot;\n&amp;quot;) took: 29.298 seconds.

----------------------------------------
Data Merging Complete.
----------------------------------------

Program finished.
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;#include &amp;lt;iostream&amp;gt;
#include &amp;lt;string&amp;gt;
#include &amp;lt;vector&amp;gt;
#include &amp;lt;random&amp;gt;
#include &amp;lt;chrono&amp;gt;
#include &amp;lt;sstream&amp;gt;
#include &amp;lt;iomanip&amp;gt;

typedef std::vector&amp;lt;std::string&amp;gt; DataRow;
typedef std::vector&amp;lt;DataRow&amp;gt; DataGroup;

struct ResponsePackage
{
    std::string ErrorInfo;
    DataRow Head;
    std::string ClientId;
    std::string UUID;
    std::string MsgID;
    std::string SessionID;
    std::string ExtraInfo1;
    std::string ExtraInfo2;
    DataGroup DataBody;
};

// Generate specified length of random string
std::string generateRandomString(size_t length)
{
    const char charset[] = &amp;quot;abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789&amp;quot;;
    const size_t max_index = sizeof(charset) - 1;
    std::string random_string;
    random_string.reserve(length);

    std::random_device rd;
    std::mt19937 generator(rd());
    std::uniform_int_distribution&amp;lt;&amp;gt; distribution(0, max_index);

    for (size_t i = 0; i &amp;lt; length; ++i)
    {
        random_string += charset[distribution(generator)];
    }

    return random_string;
}

void create_large_string()
{
    // Example request package with 50 fields
    ResponsePackage requestPackage;

    requestPackage.Head = {
        &amp;quot;Field1&amp;quot;, &amp;quot;Field2&amp;quot;, &amp;quot;Field3&amp;quot;, &amp;quot;Field4&amp;quot;, &amp;quot;Field5&amp;quot;,
        &amp;quot;Field6&amp;quot;, &amp;quot;Field7&amp;quot;, &amp;quot;Field8&amp;quot;, &amp;quot;Field9&amp;quot;, &amp;quot;Field10&amp;quot;,
        &amp;quot;Field11&amp;quot;, &amp;quot;Field12&amp;quot;, &amp;quot;Field13&amp;quot;, &amp;quot;Field14&amp;quot;, &amp;quot;Field15&amp;quot;,
        &amp;quot;Field16&amp;quot;, &amp;quot;Field17&amp;quot;, &amp;quot;Field18&amp;quot;, &amp;quot;Field19&amp;quot;, &amp;quot;Field20&amp;quot;,
        &amp;quot;Field21&amp;quot;, &amp;quot;Field22&amp;quot;, &amp;quot;Field23&amp;quot;, &amp;quot;Field24&amp;quot;, &amp;quot;Field25&amp;quot;,
        &amp;quot;Field26&amp;quot;, &amp;quot;Field27&amp;quot;, &amp;quot;Field28&amp;quot;, &amp;quot;Field29&amp;quot;, &amp;quot;Field30&amp;quot;,
        &amp;quot;Field31&amp;quot;, &amp;quot;Field32&amp;quot;, &amp;quot;Field33&amp;quot;, &amp;quot;Field34&amp;quot;, &amp;quot;Field35&amp;quot;,
        &amp;quot;Field36&amp;quot;, &amp;quot;Field37&amp;quot;, &amp;quot;Field38&amp;quot;, &amp;quot;Field39&amp;quot;, &amp;quot;Field40&amp;quot;,
        &amp;quot;Field41&amp;quot;, &amp;quot;Field42&amp;quot;, &amp;quot;Field43&amp;quot;, &amp;quot;Field44&amp;quot;, &amp;quot;Field45&amp;quot;,
        &amp;quot;Field46&amp;quot;, &amp;quot;Field47&amp;quot;, &amp;quot;Field48&amp;quot;, &amp;quot;Field49&amp;quot;, &amp;quot;Field50&amp;quot;
    };

    requestPackage.ClientId = &amp;quot;ClientID&amp;quot;;
    requestPackage.UUID = &amp;quot;UUID&amp;quot;;
    requestPackage.MsgID = &amp;quot;MsgID&amp;quot;;
    requestPackage.SessionID = &amp;quot;SessionID&amp;quot;;
    requestPackage.ExtraInfo1 = &amp;quot;ExtraInfo1&amp;quot;;
    requestPackage.ExtraInfo2 = &amp;quot;ExtraInfo2&amp;quot;;

    // Start timing for data generation
    auto start_gen = std::chrono::high_resolution_clock::now();

    // Generate 10,000 rows of data, each with 50 fields
    for (size_t i = 0; i &amp;lt; 10000; ++i)
    {
        DataRow dataRow(50, &amp;quot;This is a test string&amp;quot;);
        requestPackage.DataBody.push_back(dataRow);
    }

    // End timing for data generation
    auto end_gen = std::chrono::high_resolution_clock::now();
    std::chrono::duration&amp;lt;double&amp;gt; duration_gen = end_gen - start_gen;

    // Display result generation time
    std::cout &amp;lt;&amp;lt; &amp;quot;\n----------------------------------------\n&amp;quot;;
    std::cout &amp;lt;&amp;lt; &amp;quot;Data Generation Time: &amp;quot; &amp;lt;&amp;lt; std::fixed &amp;lt;&amp;lt; std::setprecision(3) &amp;lt;&amp;lt; duration_gen.count() &amp;lt;&amp;lt; &amp;quot; seconds.\n&amp;quot;;
    std::cout &amp;lt;&amp;lt; &amp;quot;----------------------------------------\n&amp;quot;;

    // Data merging using different methods
    std::cout &amp;lt;&amp;lt; &amp;quot;\n----------------------------------------\n&amp;quot;;
    std::cout &amp;lt;&amp;lt; &amp;quot;Data Merging Performance:\n&amp;quot;;
    std::cout &amp;lt;&amp;lt; &amp;quot;----------------------------------------\n&amp;quot;;

    {
        // Method 1: Using &#39;+=&#39; string concatenation
        auto start_merge = std::chrono::high_resolution_

```markdown
## Complete Code
{
        // Method 2: Using ostringstream
        auto start_merge = std::chrono::high_resolution_clock::now();
        std::ostringstream bodys;
        for (auto&amp;amp; vec : requestPackage.DataBody)
        {
            std::ostringstream body;
            body &amp;lt;&amp;lt; &amp;quot;This is a test string&amp;quot;;
            for (auto&amp;amp; item : vec)
            {
                body &amp;lt;&amp;lt; item &amp;lt;&amp;lt; &amp;quot; &amp;quot;;
            }
            bodys &amp;lt;&amp;lt; body.str() &amp;lt;&amp;lt; &amp;quot;\n&amp;quot;;
        }
        auto end_merge = std::chrono::high_resolution_clock::now();
        std::chrono::duration&amp;lt;double&amp;gt; duration_merge = end_merge - start_merge;
        std::cout &amp;lt;&amp;lt; &amp;quot;+ ostringstream Data merging took: &amp;quot; &amp;lt;&amp;lt; std::fixed &amp;lt;&amp;lt; std::setprecision(3) &amp;lt;&amp;lt; duration_merge.count() &amp;lt;&amp;lt; &amp;quot; seconds.\n&amp;quot;;
    }

    {
        // Method 3: Pre-allocated memory
        auto start_merge = std::chrono::high_resolution_clock::now();
        std::string bodys;
        bodys.reserve(1000 * 50 * 20); // Pre-allocate enough memory
        for (auto&amp;amp; vec : requestPackage.DataBody)
        {
            std::string body(&amp;quot;This is a test string&amp;quot;);
            body.reserve(50 * 20); // Pre-allocate memory for each row
            for (auto&amp;amp; item : vec)
            {
                body += item + &amp;quot; &amp;quot;;
            }
            bodys += body + &amp;quot;\n&amp;quot;;
        }
        auto end_merge = std::chrono::high_resolution_clock::now();
        std::chrono::duration&amp;lt;double&amp;gt; duration_merge = end_merge - start_merge;
        std::cout &amp;lt;&amp;lt; &amp;quot;+ Pre-reserved Data merging took: &amp;quot; &amp;lt;&amp;lt; std::fixed &amp;lt;&amp;lt; std::setprecision(3) &amp;lt;&amp;lt; duration_merge.count() &amp;lt;&amp;lt; &amp;quot; seconds.\n&amp;quot;;
    }

    {
        // Method 4: Using &#39;bodys = bodys + body + &amp;quot;\n&amp;quot;&#39;
        auto start_merge = std::chrono::high_resolution_clock::now();
        std::string bodys(&amp;quot;&amp;quot;);
        for (auto&amp;amp; vec : requestPackage.DataBody)
        {
            std::string body(&amp;quot;This is a test string&amp;quot;);
            for (auto&amp;amp; item : vec)
            {
                body = body + item + &amp;quot; &amp;quot;; // Note the use of &#39;body = body + item&#39;
            }
            bodys = bodys + body + &amp;quot;\n&amp;quot;; // Again, using &#39;bodys = bodys + body&#39;
        }
        auto end_merge = std::chrono::high_resolution_clock::now();
        std::chrono::duration&amp;lt;double&amp;gt; duration_merge = end_merge - start_merge;
        std::cout &amp;lt;&amp;lt; &amp;quot;+ Data merging (bodys = bodys + body + \&amp;quot;\\n\&amp;quot;) took: &amp;quot; &amp;lt;&amp;lt; std::fixed &amp;lt;&amp;lt; std::setprecision(3) &amp;lt;&amp;lt; duration_merge.count() &amp;lt;&amp;lt; &amp;quot; seconds.\n&amp;quot;;
    }

    std::cout &amp;lt;&amp;lt; &amp;quot;\n----------------------------------------\n&amp;quot;;
    std::cout &amp;lt;&amp;lt; &amp;quot;Data Merging Complete.\n&amp;quot;;
    std::cout &amp;lt;&amp;lt; &amp;quot;----------------------------------------\n&amp;quot;;
}

int main()
{
    try
    {
        create_large_string();
    }
    catch (const std::exception&amp;amp; e)
    {
        std::cerr &amp;lt;&amp;lt; &amp;quot;Caught exception: &amp;quot; &amp;lt;&amp;lt; e.what() &amp;lt;&amp;lt; std::endl;
    }

    std::cout &amp;lt;&amp;lt; &amp;quot;\nProgram finished.\n&amp;quot;;
    return 0;
}
&lt;/code&gt;&lt;/pre&gt;
</description>
        </item>
        <item>
        <title>C&#43;&#43; Lambda Expression Parameter Lifetimes</title>
        <link>https://ttf248.life/en/p/cpp-lambda-parameter-lifetime/</link>
        <pubDate>Wed, 13 Nov 2024 19:23:02 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/cpp-lambda-parameter-lifetime/</guid>
        <description>&lt;p&gt;In C++, lambda expressions are a convenient way to create anonymous functions that can capture external variables and use them within their bodies. This makes lambdas a flexible programming tool. However, the lifetime of parameters in a lambda expression is an aspect that requires careful attention, especially when capturing and passing parameters.&lt;/p&gt;
&lt;h3 id=&#34;1-lambda-expression-parameter-lifetime&#34;&gt;1. Lambda Expression Parameter Lifetime
&lt;/h3&gt;&lt;p&gt;The lifetime of parameters in a lambda expression is typically the same as that of other C++ functions. Parameters exist while the function is being called, and their lifetime ends when the function call terminates. However, due to the possibility of lambdas capturing external variables, the parameter&amp;rsquo;s lifetime is also affected by how it’s captured.&lt;/p&gt;
&lt;h3 id=&#34;2-capturing-the-relationship-with-parameter-lifecycles&#34;&gt;2. Capturing the Relationship with Parameter Lifecycles
&lt;/h3&gt;&lt;h4 id=&#34;21-capturing-external-variables&#34;&gt;2.1 Capturing External Variables
&lt;/h4&gt;&lt;p&gt;C++ lambda expressions allow capturing external variables in two ways:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Capture by Value:&lt;/strong&gt; When capturing by value, the value of the external variable is copied into the lambda&amp;rsquo;s internal scope. The lifetime of this copy is controlled by the lambda’s own lifetime.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Capture by Reference:&lt;/strong&gt; When capturing by reference, a reference to the external variable is retained. The lambda’s reference points to the original external variable, and its lifetime depends on the external variable’s lifetime.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;int x = 10;
auto lambda_by_value = [x]() { std::cout &amp;lt;&amp;lt; x &amp;lt;&amp;lt; std::endl; };  // Captures a copy of x
auto lambda_by_reference = [&amp;amp;x]() { std::cout &amp;lt;&amp;lt; x &amp;lt;&amp;lt; std::endl; };  // Captures a reference to x

lambda_by_value();  // Prints 10
lambda_by_reference();  // Prints 10
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For captured variables, the lifetimes are as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Capture by Value:&lt;/strong&gt; When capturing, the external variable’s value is copied into the lambda, and the copy is destroyed when the lambda’s lifetime ends.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Capture by Reference:&lt;/strong&gt; The lambda holds a reference to the external variable; &lt;strong&gt;the external variable must be valid at the time the lambda is used, or undefined behavior results.&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;22-lambda-parameters&#34;&gt;2.2 Lambda Parameters
&lt;/h4&gt;&lt;p&gt;Lambda parameters are similar to regular function parameters; their lifetime is limited to the lambda function body. That is, lambda parameters are created when the lambda is called and their lifetime ends when the lambda call finishes.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;auto lambda = [](int a, int b) {
    std::cout &amp;lt;&amp;lt; a + b &amp;lt;&amp;lt; std::endl;
};
lambda(5, 10);  // a and b are the parameters of the lambda here
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this example, &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; are the parameters of the lambda expression, they are created when the lambda is called and destroyed after the lambda executes.&lt;/p&gt;
&lt;h3 id=&#34;3-lifecycle-issues-when-capturing-external-variables&#34;&gt;3. Lifecycle Issues When Capturing External Variables
&lt;/h3&gt;&lt;h4 id=&#34;31-whether-captured-variables-can-be-effective-outside-lambda&#34;&gt;3.1 Whether Captured Variables Can Be Effective Outside Lambda
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Value Capture&lt;/strong&gt;: Even if the external variable is destroyed after the lambda call, the lambda internally still holds a copy of the external variable. Therefore, the copy within the lambda can be safely used even if the external variable no longer exists.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;int x = 10;
auto lambda = [x]() { std::cout &amp;lt;&amp;lt; x &amp;lt;&amp;lt; std::endl; };
x = 20;  // x is modified after the lambda call
lambda();  // Prints 10, captures a copy of x
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Reference Capture&lt;/strong&gt;: If the external variable is captured by reference, the lambda&amp;rsquo;s access to this reference depends on the lifetime of the external variable. If the external variable is destroyed before the lambda executes, a dangling reference issue will occur, leading to undefined behavior.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;int x = 10;
auto lambda = [&amp;amp;x]() { std::cout &amp;lt;&amp;lt; x &amp;lt;&amp;lt; std::endl; };
x = 20;  // x is modified before the lambda call
lambda();  // Prints 20, captures a reference to x
&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;It&amp;rsquo;s important to ensure that the external variable is valid when the lambda executes if the execution order of the lambda is uncertain.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
        </item>
        <item>
        <title>Win11 Logitech G431 Headset Driver Installation</title>
        <link>https://ttf248.life/en/p/win11-logitech-g431-headphone-driver-installation/</link>
        <pubDate>Wed, 05 Jun 2024 07:20:17 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/win11-logitech-g431-headphone-driver-installation/</guid>
        <description>&lt;p&gt;Picking up where we left off, I discovered that GitHub had an update, which was a little exciting. The customer service team said the issue with the driver not loading properly was resolved. However, after going through all of this – reinstalling and uninstalling – it still wasn’t working correctly.&lt;/p&gt;
&lt;h2 id=&#34;background&#34;&gt;Background
&lt;/h2&gt;&lt;p&gt;Continuing to contact customer service to inquire about a resolution, I was informed that an engineer could provide remote assistance. However, the engineer’s working hours coincided exactly with my own, leaving me with no option but to abandon the effort. Reviewing the documentation from the previous troubleshooting issue, I decided to attempt a manual driver installation.&lt;/p&gt;
&lt;h2 id=&#34;obtaining-driver-installation-packages&#34;&gt;Obtaining Driver Installation Packages
&lt;/h2&gt;&lt;p&gt;Logitech does not provide separate driver installation packages for devices. How can I obtain the driver files?&lt;/p&gt;
&lt;p&gt;In conjunction with the system image package left over from the previous system reinstallation, we can reinstall the system once in a local virtual machine, and then deploy a clean copy of Ghub in the pure system, inserting the headset device into the virtual machine to find the driver path and copy it out.&lt;/p&gt;
&lt;p&gt;Relevant paths:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;C:\ProgramData\LGHUB&lt;/li&gt;
&lt;li&gt;C:\Windows\System32\DriverStore\FileRepository\logi_audio.inf_amd64_010b035044e24be4&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;device-manager&#34;&gt;Device Manager
&lt;/h2&gt;&lt;p&gt;The focus is on how to find the second path – let’s first briefly outline how to manually manage driver files in a Windows 11 system. This content &lt;strong&gt;is identified using the method of controlling variables by repeatedly plugging and unplugging devices, analyzing device information within Device Manager inside a virtual machine, and identifying three drivers that need to be handled for headphones.&lt;/strong&gt; Two of these drivers are system-provided, while one is provided by Logitech.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;img src=&#34;https://ttf248.life/p/win11-logitech-g431-headphone-driver-installation/20240605073331.png&#34;
	width=&#34;433&#34;
	height=&#34;904&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;Driver Manager&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;47&#34;
		data-flex-basis=&#34;114px&#34;
	
&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;In the second driver shown in the image, it’s provided by Logitech.  Let&amp;rsquo;s analyze the current driver program for the device and then search all driver paths within the virtual machine. Of course, you first need to find files starting with “logi,” then compare the files, which will help you pinpoint the driver folder. Copying the entire folder gives you the driver installation package.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;img src=&#34;https://ttf248.life/p/win11-logitech-g431-headphone-driver-installation/20240605073933.png&#34;
	width=&#34;946&#34;
	height=&#34;645&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;Driver Package&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;146&#34;
		data-flex-basis=&#34;352px&#34;
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;installing-the-driver&#34;&gt;Installing the Driver
&lt;/h2&gt;&lt;p&gt;In the device manager interface, click: Update driver, then click: Browse my computer to find drivers, and you’ll arrive at the following interface:&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;img src=&#34;https://ttf248.life/p/win11-logitech-g431-headphone-driver-installation/20240605074130.png&#34;
	width=&#34;528&#34;
	height=&#34;381&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;Driver Installation&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;138&#34;
		data-flex-basis=&#34;332px&#34;
	
&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Of course, when you open it, you&amp;rsquo;ll only see one driver – the standard USB driver. Select &amp;ldquo;Install from disk&amp;rdquo; and the path is the folder we copied earlier. After installation, you’ll be able to add Logitech-specific drivers in the dropdown list. Switch the device driver to the newly installed driver.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;img src=&#34;https://ttf248.life/p/win11-logitech-g431-headphone-driver-installation/20240605074208.png&#34;
	width=&#34;593&#34;
	height=&#34;423&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;Disk Installation&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;140&#34;
		data-flex-basis=&#34;336px&#34;
	
&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;human-anatomy-device-driven&#34;&gt;Human Anatomy Device-Driven
&lt;/h2&gt;&lt;p&gt;These driver files are provided by the system. You only need to check if there is an exclamation mark preceding the device driver name. If there is, enter the Driver Selection interface, randomly switch to a different type of driver, and then revert it back to restore normal operation.&lt;/p&gt;
&lt;h2 id=&#34;completed&#34;&gt;Completed
&lt;/h2&gt;&lt;p&gt;The microphone volume on the headphones has been restored to normal, and the familiar in-ear functionality has returned.
&lt;img src=&#34;https://ttf248.life/p/win11-logitech-g431-headphone-driver-installation/20240605074823.png&#34;
	width=&#34;485&#34;
	height=&#34;739&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;Side Noise&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;65&#34;
		data-flex-basis=&#34;157px&#34;
	
&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Logitech Headset Driver Installation Failed</title>
        <link>https://ttf248.life/en/p/logitech-headphone-driver-installation-failure/</link>
        <pubDate>Fri, 31 May 2024 21:46:12 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/logitech-headphone-driver-installation-failure/</guid>
        <description>&lt;p&gt;If you completely don&amp;rsquo;t understand these things, contacting official customer service first will also avoid wasting several hours.&lt;/p&gt;
&lt;h2 id=&#34;main-text&#34;&gt;Main Text
&lt;/h2&gt;&lt;p&gt;&lt;img src=&#34;https://ttf248.life/p/logitech-headphone-driver-installation-failure/20240531220709.png&#34;
	width=&#34;693&#34;
	height=&#34;489&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;GHUB&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;141&#34;
		data-flex-basis=&#34;340px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;Recently, the C drive on my desktop computer, which I use for development, had run out of space. I specially took out a 256GB semi-retired solid state hard drive and used it as the C drive. Unfortunately, I kept messing around with it. Since moving to Shanghai, I’ve been busy with various things, and last week I finally took some time to reinstall the system.&lt;/p&gt;
&lt;p&gt;The system reinstallation went smoothly, and installing everyday software and deploying development environments didn&amp;rsquo;t encounter any problems. A few days later, I planned to relax and play a few games when I realized that the drivers for my mouse and headphones hadn’t been installed. Both devices are from Logitech, so I downloaded the GHUB software, which can automatically identify hardware and install drivers.&lt;/p&gt;
&lt;p&gt;However, an unexpected problem occurred. The mouse driver installed successfully, but the headphone driver kept displaying “Loading…”. I suspected that the latest version of Windows 11 might be incompatible with Logitech’s drivers, causing the installation to fail. So, I started searching for information and trying to manually install the drivers, but the problem remained unresolved.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s briefly explain what the functions of these two devices’ drivers are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The mouse driver is mainly used to adjust mouse movement speed and other functions. I rarely use macro functions; I just need to restore previously remembered parameters.&lt;/li&gt;
&lt;li&gt;The headphone driver primarily focuses on the headset function, which is very useful during team voice chats, allowing me to hear my own voice. Although there’s a similar listening function in the system&amp;rsquo;s microphone settings, it doesn’t perform as well as the driver implementation.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Despite repeatedly trying, the headphone driver always failed to load properly. Today, I finally thought of contacting customer support to inquire about the situation and see if there were any solutions. The customer service representative told me that their servers had recently experienced an issue, causing driver downloads to be abnormal. They are currently working on resolving the problem, asking me not to panic, and promising a solution with the next update.&lt;/p&gt;
&lt;p&gt;Although the headphone driver issue hasn’t been resolved yet, at least I now know the cause, and I hope the problem can be solved as soon as possible.&lt;/p&gt;
&lt;h2 id=&#34;mouse-driver-settings&#34;&gt;Mouse Driver Settings
&lt;/h2&gt;&lt;p&gt;&lt;img src=&#34;https://ttf248.life/p/logitech-headphone-driver-installation-failure/20240531220930.png&#34;
	width=&#34;1024&#34;
	height=&#34;768&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;G502&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;133&#34;
		data-flex-basis=&#34;320px&#34;
	
&gt;
&lt;img src=&#34;https://ttf248.life/p/logitech-headphone-driver-installation-failure/20240531220903.png&#34;
	width=&#34;1024&#34;
	height=&#34;768&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;G502&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;133&#34;
		data-flex-basis=&#34;320px&#34;
	
&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Python Dictionary Storage of Custom Objects: The Importance of References vs. Deep Copies</title>
        <link>https://ttf248.life/en/p/python-dictionary-custom-objects-reference-vs-deepcopy/</link>
        <pubDate>Fri, 22 Mar 2024 01:08:05 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/python-dictionary-custom-objects-reference-vs-deepcopy/</guid>
        <description>&lt;p&gt;In Python programming, a dictionary is a very powerful data structure that allows us to associate key-value pairs and efficiently search and manipulate these data. When we try to store custom objects in a dictionary, we often encounter a crucial concept: In Python, object assignment is actually reference assignment, not a deep copy of the object itself. This means that when you put a custom object into a dictionary, the dictionary stores a reference to that object, rather than a brand new copy of the object.&lt;/p&gt;
&lt;h2 id=&#34;basic-example-of-storing-custom-objects&#34;&gt;Basic Example of Storing Custom Objects
&lt;/h2&gt;&lt;p&gt;Let&amp;rsquo;s consider a simple &lt;code&gt;Person&lt;/code&gt; class:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class Person:
    def __init__(self, name, age):
        self.name = name
        self.age = age

# Create a Person object
p1 = Person(&amp;quot;Alice&amp;quot;, 30)

# Store the object in a dictionary
people_dict = {}
people_dict[&amp;quot;alice&amp;quot;] = p1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this example, the &lt;code&gt;people_dict&lt;/code&gt; dictionary now contains an item with a key &lt;code&gt;&amp;quot;alice&amp;quot;&lt;/code&gt; and its value is a reference to the &lt;code&gt;Person&lt;/code&gt; type object &lt;code&gt;p1&lt;/code&gt;. If we modify the properties of this object:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;p1.age = 31
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then when accessing this object through the dictionary, we will find that its age has also been updated:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(people_dict[&amp;quot;alice&amp;quot;].age)  # Output: 31
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is because the dictionary stores references to &lt;code&gt;Person&lt;/code&gt; objects rather than independent copies of them. It stores a reference to the same memory address.&lt;/p&gt;
&lt;h2 id=&#34;deep-copy-vs-shallow-copy&#34;&gt;Deep Copy vs. Shallow Copy
&lt;/h2&gt;&lt;p&gt;This referencing behavior can lead to unexpected results when dealing with nested data structures or custom objects. For example, if a custom object contains mutable attributes (such as lists or another custom object), directly storing such an object in a dictionary and modifying it will affect the object obtained through the dictionary.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class Address:
    def __init__(self, street, city):
        self.street = street
        self.city = city

class Person:
    def __init__(self, name, age, address):
        self.name = name
        self.age = age
        self.address = address

address = Address(&amp;quot;Main St.&amp;quot;, &amp;quot;Springfield&amp;quot;)
p1 = Person(&amp;quot;Bob&amp;quot;, 40, address)
people_dict[&amp;quot;bob&amp;quot;] = p1

# Modify the original address object
address.city = &amp;quot;Shelbyville&amp;quot;

# The person in the dictionary&#39;s address also changed
print(people_dict[&amp;quot;bob&amp;quot;].address.city)  # Output: Shelbyville
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Solution: Deep Copy&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;To avoid problems caused by shared state, we sometimes need to ensure that the dictionary stores a complete copy of the object, rather than a reference to it. Python provides the &lt;code&gt;copy&lt;/code&gt; module&amp;rsquo;s &lt;code&gt;deepcopy&lt;/code&gt; function to achieve this goal:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import copy

# Use deep copy to store objects
people_dict[&amp;quot;bob_deepcopy&amp;quot;] = copy.deepcopy(p1)

# Now even if you modify the original address object, the deep copied object is not affected
address.city = &amp;quot;Capital City&amp;quot;
print(people_dict[&amp;quot;bob&amp;quot;].address.city)  # Output: Capital City
print(people_dict[&amp;quot;bob_deepcopy&amp;quot;].address.city)  # Output: Shelbyville
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In summary, when using dictionaries to store custom objects in Python, be sure to pay attention to the fact that they default to storing object references. For situations where you need to maintain independent states, use &lt;code&gt;deepcopy&lt;/code&gt; to perform a deep copy to avoid unexpected data changes due to shared referencing.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Why does a newly installed gigabit fiber to the home (FTTH) connection only test at 100 Mbps?</title>
        <link>https://ttf248.life/en/p/gigabit-fiber-slow-speed/</link>
        <pubDate>Mon, 18 Mar 2024 00:29:02 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/gigabit-fiber-slow-speed/</guid>
        <description>&lt;p&gt;Want your home network to be lightning fast? The key is understanding cable selection, optical terminals (ONTs), and router configuration, as well as those seemingly insignificant details. This blog post will guide you through easily learning how to build a gigabit network using six types of cables, and how to ensure your network speed isn&amp;rsquo;t restricted by simple device checks and configurations. Let’s explore together and make your home network fly!&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;img src=&#34;https://ttf248.life/p/gigabit-fiber-slow-speed/image.png&#34;
	width=&#34;1001&#34;
	height=&#34;590&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;Manual Repair&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;169&#34;
		data-flex-basis=&#34;407px&#34;
	
&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;chapter-1-an-in-depth-analysis-of-network-transmission-media&#34;&gt;Chapter 1: An In-Depth Analysis of Network Transmission Media
&lt;/h2&gt;&lt;p&gt;When discussing achieving gigabit network access, the carrier that supports high-speed information transmission – cables – plays a crucial role. Below we will provide detailed interpretations of Cat5, Cat6, and Cat7 cables.&lt;/p&gt;
&lt;h3 id=&#34;1-five-category-cables-cat5&#34;&gt;1. &lt;strong&gt;Five-Category Cables (CAT5)&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;CAT5 cables, also known as CAT5, are an earlier and more widely adopted type of twisted pair cable. Each pair of wire pairs is designed with a precise helical structure to reduce crosstalk. It’s primarily used for 10/100Mbps Fast Ethernet, with a maximum transmission frequency of approximately 100MHz. While it was once widely applied, CAT5 cables cannot meet current demands for gigabit and even higher speeds due to physical limitations.&lt;/p&gt;
&lt;h3 id=&#34;2-six-category-cables-cat6&#34;&gt;2. &lt;strong&gt;Six-Category Cables (CAT6)&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;With the development of technology, six-category cables have emerged. Compared to five-category cables, six-cable materials adopted stricter manufacturing standards and more advanced structural designs, significantly improving anti-interference capability and transmission efficiency, supporting data transfer rates up to 1Gbps, and with a transmission distance of up to 100 meters under ideal conditions, which perfectly meets the access requirements of Gigabit networks.&lt;/p&gt;
&lt;h3 id=&#34;3-seven-category-cables-cat7&#34;&gt;3. &lt;strong&gt;Seven-Category Cables (CAT7)&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;Seven-category cables represent the current cutting edge of twisted pair cabling technology. It not only offers a significant leap in transmission rates, theoretically supporting speeds up to 10Gbps, but also incorporates a complete shielding system, including shielding between each pair and overall external shielding, which greatly reduces external electromagnetic interference and near-end crosstalk, ensuring data transmission stability and accuracy. However, CAT7 cables are primarily used for future 10 Gigabit Ethernet or specific high-requirement scenarios.&lt;/p&gt;
&lt;p&gt;When setting up a gigabit home network environment, choosing six-category cables is the most economical and efficient choice to fully unleash the potential of the gigabit fiber optic.  Furthermore, ensuring that all cabling materials meet quality standards and strictly adhering to standard wiring practices are also crucial elements in guaranteeing network performance.&lt;/p&gt;
&lt;h2 id=&#34;chapter-two-deep-dive-into-core-network-devices--the-impact-of-optical-cat-pon-and-router-lan-port-bandwidth&#34;&gt;Chapter Two: Deep Dive into Core Network Devices – The Impact of Optical Cat (PON) and Router LAN Port Bandwidth
&lt;/h2&gt;&lt;h3 id=&#34;the-importance-of-optical-cat-ont-and-its-lan-port-bandwidth&#34;&gt;The Importance of Optical Cat (ONT) and its LAN Port Bandwidth
&lt;/h3&gt;&lt;p&gt;An Optical Cat, or Optical Network Terminal (ONT), is the core device for home broadband access. Its function is to convert optical signals from fiber optic cables into digital signals for use by home network devices. For users with gigabit fiber connections, whether the ONT supports gigabit transmission is particularly important. If the ONT’s WAN port only supports 100 Mbps, even if the incoming fiber rate is high, it will be limited to 100 Mbps due to this bottleneck. Similarly, the ONT’s LAN port also needs to have a gigabit output capability; otherwise, routers or other devices connected to it cannot obtain the true gigabit rate.&lt;/p&gt;
&lt;h3 id=&#34;the-role-of-bandwidth-on-router-lan-ports&#34;&gt;The Role of Bandwidth on Router LAN Ports
&lt;/h3&gt;&lt;p&gt;The router’s LAN ports are responsible for distributing the data received to various terminal devices. When a router&amp;rsquo;s LAN port is only 100 Mbps, even if other devices are configured well, it can only achieve 100 Mbps local network communication. Therefore, when building a Gigabit home network, it’s crucial to ensure that the router’s WAN port can receive 1 Gbps data and that the LAN ports also provide data output capabilities at the Gigabit level, allowing all smart devices in your home to enjoy the smooth experience brought by high-speed networks.&lt;/p&gt;
&lt;p&gt;Furthermore, it&amp;rsquo;s important to note that some older or low-end routers may have a LAN port rate auto-negotiation mechanism, which means that even if the router itself supports 1 Gbps, it might be downgraded to 100 Mbps mode due to cable issues, device compatibility, and other factors. Therefore, correctly configuring router parameters, enabling forced Gigabit mode, and pairing it with a Gigabit switch or direct connection devices are key steps in achieving a full Gigabit network.&lt;/p&gt;
&lt;p&gt;After upgrading to gigabit fiber optic, be sure to check and replace them with a gigabit optical gateway and a gigabit router, ensuring that all device interfaces reach the Gigabit level.&lt;/p&gt;
&lt;h2 id=&#34;chapter-three-the-hidden-mystery--how-a-broken-subline-impacts-gigabit-network-speed&#34;&gt;Chapter Three: The Hidden Mystery – How a Broken Subline Impacts Gigabit Network Speed
&lt;/h2&gt;&lt;h3 id=&#34;line-fault-and-network-performance-degradation&#34;&gt;Line Fault and Network Performance Degradation
&lt;/h3&gt;&lt;p&gt;During the speed tests, the network consistently maintained a connection without any apparent disconnects. As it was a newly deployed broadband for residential customers, the distribution box was cluttered with equipment, and the technician frequently adjusted the fiber optic ONT’s cabling and power adapter placements. This occasionally resulted in speed test results reaching gigabit speeds.&lt;/p&gt;
&lt;p&gt;Based on the previous analysis, we had already investigated and ruled out network cable types and ONT LAN port speeds. Ultimately, the culprit was discovered to be a brown sub-cable within the network cable that had fractured.&lt;/p&gt;
&lt;p&gt;The cause of the break: When the technician installed the crystal head, he applied a little too much force, causing one of the sub-cables to snap in half. It wasn’t completely severed, and subsequent adjustments to the ONT position caused it to eventually break off entirely.&lt;/p&gt;
&lt;h3 id=&#34;six-category-cable-lines-function-analysis&#34;&gt;Six Category Cable Lines Function Analysis
&lt;/h3&gt;&lt;p&gt;Six category cables adhere to the TIA/EIA-568-B standard and contain eight twisted pairs of wires, color-coded as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;White Orange / Orange&lt;/li&gt;
&lt;li&gt;White Green / Green&lt;/li&gt;
&lt;li&gt;White Blue / Blue&lt;/li&gt;
&lt;li&gt;White Brown / Brown&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Under the standard of Gigabit Ethernet (1000BASE-T), these eight lines consist of four pairs working simultaneously, with the following division of labor:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The White Orange and Orange pair of wires (1&amp;amp;2) is used for transmitting data (Tx+/-);&lt;/li&gt;
&lt;li&gt;The White Green and Green pair of wires (3&amp;amp;6) is used for receiving data (Rx+/-);&lt;/li&gt;
&lt;li&gt;The White Blue and Blue pair of wires (4&amp;amp;5) and the White Brown and Brown pair of wires (7&amp;amp;8) were not originally primary in Gigabit Ethernet, but may be enabled in certain advanced applications (such as some PoE power delivery or future technology expansions). In traditional 100 Mbps networks, only four lines – 1, 2, 3, and 6 – could be used.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;impact-of-breakaway-pairs-on-network-speed&#34;&gt;Impact of Breakaway Pairs on Network Speed
&lt;/h3&gt;&lt;p&gt;In the above scenarios, if a brown sub-cable (i.e., a brown or brown-white wire) breaks, theoretically it will cause speed degradation in gigabit networks, as gigabit networks require all four pairs of wires to transmit bidirectionally simultaneously to achieve full speed. However, due to the automatic negotiation function often found in home network devices, when a cable issue is detected, they will revert to a lower operating mode that can still function normally, namely 100 Mbps mode. This explains why even with a broken sub-cable, the network remains connected and operates at 100 Mbps speeds.&lt;/p&gt;
&lt;p&gt;In short, while a broken brown sub-cable does not affect the basic operation of a 100 Mbps network, it can become a key limiting factor for network speed in a gigabit environment. Until thorough diagnostics and repairs are performed, the full potential of a gigabit fiber optic cable cannot be realized. This also reminds us that when encountering similar situations, we should not ignore any potential network infrastructure issues, even seemingly minor faults that do not affect basic connectivity, as they can become hidden obstacles to high-speed network performance.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>WPF UI Thread Blocking Issues and Solutions</title>
        <link>https://ttf248.life/en/p/wpf-ui-thread-and-freezing-solutions/</link>
        <pubDate>Tue, 12 Mar 2024 07:12:21 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/wpf-ui-thread-and-freezing-solutions/</guid>
        <description>&lt;p&gt;When developing desktop applications, particularly when using the Windows Presentation Foundation (WPF) framework to build rich client applications, properly handling the user interface (UI) thread is crucial for ensuring the application’s smoothness and responsiveness. The UI thread, also known as the main thread, is the core thread responsible for processing window and control events, layout calculations, and rendering the UI. Any interaction with UI elements should be executed on the UI thread; this is a fundamental principle followed by WPF and most other GUI frameworks.&lt;/p&gt;
&lt;h2 id=&#34;what-is-the-ui-thread&#34;&gt;What is the UI Thread?
&lt;/h2&gt;&lt;p&gt;The UI thread is created by the operating system when a WPF application starts and initializes the main application window. It’s the only thread within the application that can directly access and modify the state of UI components. This means all user interactions, such as button clicks, text box input, and window size changes, are processed in this thread context. Furthermore, WPF&amp;rsquo;s dependency property system, data binding mechanism, and layout logic are all synchronized on the UI thread.&lt;/p&gt;
&lt;h2 id=&#34;screen-stuttering-and-its-causes&#34;&gt;Screen Stuttering and Its Causes
&lt;/h2&gt;&lt;p&gt;When the UI thread is heavily occupied or blocked for an extended period, such as when performing time-consuming calculations, loading large amounts of data, database queries, or other I/O-intensive tasks, it becomes unable to promptly respond to user interaction requests. This results in the UI freezing – what we commonly refer to as &amp;ldquo;stuttering.&amp;rdquo; In this situation, users will noticeably feel the application&amp;rsquo;s lag and lack of fluidity, and in severe cases, an “Application Not Responding” (ANR) warning may appear.&lt;/p&gt;
&lt;h2 id=&#34;two-basic-rules-for-the-ui-thread&#34;&gt;Two Basic Rules for the UI Thread
&lt;/h2&gt;&lt;p&gt;To avoid the above scenarios, WPF developers should adhere to the following two key rules:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Do not perform time-consuming operations on the UI thread:&lt;/strong&gt; Any operation that could cause the UI thread to block should be moved to a background thread as much as possible to ensure the UI thread can promptly respond to user input and render screen changes.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Do not directly update UI elements from non-UI threads:&lt;/strong&gt; Due to WPF’s security mechanism design, only the UI thread has permission to modify UI elements. Attempting to change UI state directly from another thread will throw an exception. Therefore, even if a background thread completes calculations or data preparation, you must use appropriate cross-thread communication mechanisms to display the results on the UI.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;solutions-asynchronous-programming-and-thread-safe-updates&#34;&gt;Solutions: Asynchronous Programming and Thread-Safe Updates
&lt;/h2&gt;&lt;p&gt;To execute time-consuming tasks while maintaining UI fluency, WPF provides various asynchronous programming models and tools to assist developers in achieving this goal:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Dispatcher Object:&lt;/strong&gt; The WPF Dispatcher class allows you to schedule work items into the UI thread&amp;rsquo;s task queue for execution. You can use the &lt;code&gt;Dispatcher.Invoke&lt;/code&gt; or &lt;code&gt;Dispatcher.BeginInvoke&lt;/code&gt; methods to safely update the UI from a background thread.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;async/await Keywords:&lt;/strong&gt; Leveraging C#’s asynchronous features, you can write asynchronous methods and utilize the &lt;code&gt;await&lt;/code&gt; keyword within them to wait for background tasks to complete, automatically returning to the UI thread to execute subsequent UI update code upon completion.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;case-studies&#34;&gt;Case Studies
&lt;/h2&gt;&lt;h3 id=&#34;updating-the-ui-using-dispatcherinvoke-method&#34;&gt;Updating the UI using &lt;code&gt;Dispatcher.Invoke&lt;/code&gt; method
&lt;/h3&gt;&lt;pre&gt;&lt;code class=&#34;language-csharp&#34;&gt;private void Button_Click(object sender, RoutedEventArgs e)
{
    // Assume this is a time-consuming operation
    Task.Run(() =&amp;gt;
    {
        var result = LongRunningOperation(); // This is a simulated long-running calculation method
        
        // When the time-consuming operation is complete, update the UI on the UI thread
        Application.Current.Dispatcher.Invoke(() =&amp;gt;
        {
            LabelStatus.Text = $&amp;quot;Calculation Result: {result}&amp;quot;;
        });
    });
}

private string LongRunningOperation()
{
    // Simulate a long-running operation
    Thread.Sleep(5000);
    return &amp;quot;Completed&amp;quot;;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;using-the-asyncawait-keyword-with-taskrun&#34;&gt;Using the &lt;code&gt;async/await&lt;/code&gt; keyword with &lt;code&gt;Task.Run&lt;/code&gt;
&lt;/h3&gt;&lt;pre&gt;&lt;code class=&#34;language-csharp&#34;&gt;private async void Button_ClickAsync(object sender, RoutedEventArgs e)
{
    Button button = sender as Button;
    button.IsEnabled = false; // Prevent duplicate clicks by the user

    try
    {
        // Start a background task
        var result = await Task.Run(() =&amp;gt; LongRunningOperation());

        // Automatically switch back to the UI thread to update the UI after the background task completes
        LabelStatus.Text = $&amp;quot;Calculation Result: {result}&amp;quot;;
    }
    catch (Exception ex)
    {
        MessageBox.Show($&amp;quot;An error occurred: {ex.Message}&amp;quot;);
    }
    finally
    {
        button.IsEnabled = true; // Re-enable the button
    }
}
&lt;/code&gt;&lt;/pre&gt;
</description>
        </item>
        <item>
        <title>Upgrading the GCC version caused program crashes: hidden issues due to code non-compliance.</title>
        <link>https://ttf248.life/en/p/gcc-upgrade-causes-program-crash-code-irregularities/</link>
        <pubDate>Sun, 10 Mar 2024 23:19:06 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/gcc-upgrade-causes-program-crash-code-irregularities/</guid>
        <description>&lt;p&gt;In the same business code scenario, the program compiled and ran normally in a CentOS 7 environment. However, when switching to CentOS 8 and using an updated version of GCC for compilation, the program crashed. It’s worth noting that the issue only occurs in &lt;strong&gt;Release mode&lt;/strong&gt;, while &lt;strong&gt;Debug mode&lt;/strong&gt; does not exhibit any problems. This is the first time we&amp;rsquo;ve encountered a situation like this; after three days of investigation, we finally identified the root cause.&lt;/p&gt;
&lt;h3 id=&#34;problem-identification&#34;&gt;Problem Identification
&lt;/h3&gt;&lt;p&gt;After investigation, the root cause of the issue was &lt;strong&gt;the function lacked a return value&lt;/strong&gt;. In Release mode, new versions of GCC perform more optimizations, which caused an unknown logic to occur within the function that originally did not have an explicit return value during execution, ultimately triggering a crash. Our conclusion is that &lt;strong&gt;compiler warnings should not be ignored, especially in legacy projects where some warnings may be dismissed, but it’s also important to avoid suppressing all warnings.&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&#34;environment-details&#34;&gt;Environment Details
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;CentOS 7 GCC Version:&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CentOS 8 GCC Version:&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;crash-phenomena&#34;&gt;Crash Phenomena
&lt;/h3&gt;&lt;p&gt;When analyzing the stack information for program crashes, we observed the following stack details:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[New LWP 1385902]
[Thread debugging using libthread_db enabled]
Using host libthread_db library &amp;quot;/lib64/libthread_db.so.1&amp;quot;.
Core was generated by `./pstack_main`.
Program terminated with signal SIGSEGV, Segmentation fault.
#0  0x00007ffe894b4420 in ?? ()
(gdb) bt
#0  0x00007ffe894b4420 in ?? ()
#1  0x00000000004008e9 in main ()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This stack doesn&amp;rsquo;t appear intuitive; the crash function’s stack information shows a &lt;code&gt;??&lt;/code&gt;, which makes troubleshooting even more complex.&lt;/p&gt;
&lt;h3 id=&#34;code-example&#34;&gt;Code Example
&lt;/h3&gt;&lt;p&gt;To better understand the issue, here is a minimal code example that reproduces the crash:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;#include &amp;lt;iostream&amp;gt;
#include &amp;lt;map&amp;gt;

int test() {
    std::cout &amp;lt;&amp;lt; &amp;quot;1&amp;quot; &amp;lt;&amp;lt; std::endl;
}

int main() {
    test();
    return 0;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;test()&lt;/code&gt; function in this code clearly does not explicitly return a value, and its return type is &lt;code&gt;int&lt;/code&gt;. According to the C++ standard, when a function is declared as an &lt;code&gt;int&lt;/code&gt; type, it must have a return value, otherwise it may lead to undefined behavior.&lt;/p&gt;
&lt;h3 id=&#34;compilation-warning&#34;&gt;Compilation Warning
&lt;/h3&gt;&lt;p&gt;In our project, the CMake script suppresses many compile-time warnings, including the following:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;/root/pstack/main.cpp: In function ‘int test()’:
/root/pstack/main.cpp:7:1: warning: no return statement in function returning non-void [-Wreturn-type]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This warning indicates that the &lt;code&gt;test()&lt;/code&gt; function does not return a value, which is the root cause of the problem. Newer versions of GCC (such as 8.5.0) may make unstable optimizations with this undefined behavior when optimizing code, potentially leading to program crashes.&lt;/p&gt;
&lt;h3 id=&#34;assembly-code-differences&#34;&gt;Assembly Code Differences
&lt;/h3&gt;&lt;p&gt;To explain the differences in GCC compiler optimization behavior, we compared assembly code generated by different versions of GCC:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;GCC 4.8.5 Generated Assembly Code:&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The assembly code is relatively verbose and includes handling logic for standard output streams (such as &lt;code&gt;std::cout&lt;/code&gt;). This indicates that the compiler performed more conservative optimizations, not optimizing excessively for the missing return value issue in the &lt;code&gt;test()&lt;/code&gt; function, possibly to avoid a crash.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;GCC 8.5.0 Generated Assembly Code:&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The new version of GCC performed more optimizations, reducing the code volume. However, this optimization may have resulted in unpredictable behavior when executing functions without returning values, leading to program crashes.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;conclusion&#34;&gt;Conclusion
&lt;/h3&gt;&lt;p&gt;Through this troubleshooting process, we deeply realized that in C++, &lt;strong&gt;function return values must be explicit&lt;/strong&gt;, particularly when a function is declared as &lt;code&gt;int&lt;/code&gt;, a return value must be provided. When upgrading from older versions of compilers to newer versions of GCC, more optimization and stricter warning mechanisms may be encountered. Therefore, we recommend not &lt;strong&gt;disabling all warnings&lt;/strong&gt; during compilation, but rather selectively addressing them, especially common issues such as function return values and type matching.
Ultimately, by adding a return value to the &lt;code&gt;test()&lt;/code&gt; function, the problem was resolved, and the program returned to normal operation.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>VMware Virtual Machine CPU Resource Usage Anomaly</title>
        <link>https://ttf248.life/en/p/vmware-virtual-machine-cpu-usage-anomaly/</link>
        <pubDate>Sun, 10 Mar 2024 22:14:59 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/vmware-virtual-machine-cpu-usage-anomaly/</guid>
        <description>&lt;p&gt;Background: The business system, running in Windows version, is deployed locally and consumes approximately 5% of CPU resources. The Linux version of the business system, deployed within a VMware-installed CentOS8 environment, exhibits abnormal resource consumption.&lt;/p&gt;
&lt;h2 id=&#34;problem-description&#34;&gt;Problem Description
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;Host Machine: Windows 10 Enterprise&lt;/li&gt;
&lt;li&gt;VMware: 17.5&lt;/li&gt;
&lt;li&gt;Virtual Machine: CentOS8
The virtual machine resource allocation is &lt;code&gt;4C8GB&lt;/code&gt;, running the business system. The business system is deployed in the Linux system within the virtual machine, and the internal top command observes system resource usage. CPU utilization is not high, while the external Windows system’s Task Manager shows very high CPU resource consumption. Examining processes reveals that the VMware process consumes a large amount of CPU resources.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;+&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;+
|         Windows           |
|                           |
|   +&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;ndash;+  |
|   |      VMware        |  |
|   |      Program       |  |
|   +&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;ndash;+  |
|                           |
+&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;+&lt;/p&gt;
&lt;h2 id=&#34;key-concepts&#34;&gt;Key Concepts
&lt;/h2&gt;&lt;p&gt;Troubleshooting this issue wasn’t smooth, as the root cause wasn&amp;rsquo;t the business system itself but the virtual machine. How to shift thinking from conventional business code to system load, then from abnormal load data to pinpoint a soft interrupt, and finally arrive at the critical point – what factors affect VMware soft interrupt efficiency? This article will first introduce various concepts and then provide solutions.&lt;/p&gt;
&lt;h3 id=&#34;hyper-v&#34;&gt;Hyper-V
&lt;/h3&gt;&lt;p&gt;The virtualization technology for Windows operating systems underwent a significant transformation. When Microsoft initially released WSL, enabling the Hyper-V service would prevent VMware virtual machines from working simultaneously. It wasn&amp;rsquo;t until subsequent versions that VMware could be compatible with the Hyper-V service.&lt;/p&gt;
&lt;h3 id=&#34;system-load&#34;&gt;System Load
&lt;/h3&gt;&lt;p&gt;In Linux systems, &amp;ldquo;load&amp;rdquo; refers to the number of processes currently running or waiting to be executed. The load is typically represented by three numbers: the average process count in the run queue over 1 minute, 5 minutes, and 15 minutes respectively. These numbers can be viewed by running the &lt;code&gt;uptime&lt;/code&gt; command or the &lt;code&gt;top&lt;/code&gt; command.&lt;/p&gt;
&lt;p&gt;Specifically, these three numbers represent:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;1-minute load:&lt;/strong&gt; The average number of processes in the run queue over the past 1 minute.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;5-minute load:&lt;/strong&gt; The average number of processes in the run queue over the past 5 minutes.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;15-minute load:&lt;/strong&gt; The average number of processes in the run queue over the past 15 minutes.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The meaning of the load is the number of processes waiting to be executed within the system. If this number exceeds the logical CPU count for the system, it indicates a high system load, meaning many processes are waiting for processor resources. This can cause the system to become slow or unresponsive, depending on the severity of the load and the configuration and performance of the system.&lt;/p&gt;
&lt;p&gt;Ideally, the load should remain within the logical CPU count range to optimize system performance. If the load consistently exceeds the CPU count, it may be necessary to further analyze processes in the system to identify the cause of the high load and take appropriate measures to adjust system resource allocation or optimize how processes run.&lt;/p&gt;
&lt;h3 id=&#34;analyzing-load-with-mpstat&#34;&gt;Analyzing Load with mpstat
&lt;/h3&gt;&lt;p&gt;The &lt;code&gt;mpstat&lt;/code&gt; command is used to report multiple pieces of information about one or more processors, including average load, CPU utilization, interrupts, and context switches. Within the &lt;code&gt;sysstat&lt;/code&gt; package, &lt;code&gt;mpstat&lt;/code&gt; is a valuable tool for analyzing system load conditions.  Here&amp;rsquo;s how to perform load analysis using &lt;code&gt;mpstat&lt;/code&gt;:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Install sysstat:&lt;/strong&gt;
If &lt;code&gt;sysstat&lt;/code&gt; isn’t installed on your system, use your system&amp;rsquo;s package manager to install it.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Run mpstat:&lt;/strong&gt;
Use the &lt;code&gt;mpstat&lt;/code&gt; command to view CPU usage and load. By default, &lt;code&gt;mpstat&lt;/code&gt; displays CPU utilization averages once per second. You can adjust the output frequency by specifying an interval. For example, to run &lt;code&gt;mpstat&lt;/code&gt; at a rate of one time per second, use the following command: &lt;code&gt;mpstat -P ALL 2&lt;/code&gt;, where &lt;code&gt;irq&lt;/code&gt; represents interrupt resource usage.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Analyze Output:&lt;/strong&gt;
The output from &lt;code&gt;mpstat&lt;/code&gt; includes CPU utilization for each processor, as well as the system&amp;rsquo;s average load. Pay particular attention to the average load and the utilization of each CPU to understand the system’s load conditions. If the load is high, further analysis can be done to determine which processes are causing it and whether there are any performance bottlenecks.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Combine with Other Tools:&lt;/strong&gt;
In addition to &lt;code&gt;mpstat&lt;/code&gt;, you can use tools like &lt;code&gt;sar&lt;/code&gt;, &lt;code&gt;pidstat&lt;/code&gt;, and &lt;code&gt;iostat&lt;/code&gt; to comprehensively analyze system performance. By combining the outputs of multiple tools, you can gain a more complete understanding of the system’s load conditions and identify the root causes of performance issues.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;interrupt&#34;&gt;Interrupt
&lt;/h3&gt;&lt;p&gt;This section doesn&amp;rsquo;t elaborate on the content too much,
Recommended: &lt;a class=&#34;link&#34; href=&#34;https://www.codedump.info/post/20200522-sgfap-softirq/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;System Guide for Application Developers - CPU Part - Soft Interrupt&lt;/a&gt;
Frequent triggering of soft interrupts will also be reflected in system load.&lt;/p&gt;
&lt;h2 id=&#34;troubleshooting&#34;&gt;Troubleshooting
&lt;/h2&gt;&lt;p&gt;Considering that analysis solely from the CPU perspective couldn’t pinpoint the issue, should we start to suspect that the system had become abnormal? It might be due to excessive load on the Linux operating system, causing VMware to consume an unusually high amount of CPU resources. By using &lt;code&gt;mpstat&lt;/code&gt; to analyze local virtual machines, we found that &lt;code&gt;irq&lt;/code&gt; utilization was abnormally high, approaching 25% per core, while in normal circumstances, when business processes were idle, &lt;code&gt;irq&lt;/code&gt; should have accounted for approximately 5%.&lt;/p&gt;
&lt;p&gt;In a colleague’s development environment within the group, his CentOS 7 was deployed on VMware with normal resource usage. Conversely, in the Shanghai development environment, although also running on VMware, we couldn&amp;rsquo;t directly observe the host machine’s CPU resource situation. At this point, we faced multiple variables: VMware virtual machines, the Linux operating system, and the GCC version.&lt;/p&gt;
&lt;p&gt;Shifting our focus to the test environment, the Shenzhen test environment was deployed on a physical machine running low-version GCC compiled services and was running on CentOS 8. Interestingly, in the Shenzhen environment, &lt;code&gt;irq&lt;/code&gt; utilization was normal.&lt;/p&gt;
&lt;p&gt;To investigate potential issues introduced by the GCC version, we deployed a program compiled with a high-version GCC to the Shenzhen environment for testing, which also yielded normal results.&lt;/p&gt;
&lt;p&gt;The problem seemed to become clearer, and we began to suspect that the operating system might be experiencing an issue. After all, CentOS 8 is no longer officially supported. Even after deploying clean CentOS 7 and CentOS 8, the problem persisted.&lt;/p&gt;
&lt;p&gt;At this point, we started to suspect the only remaining uncertainty: the VMware virtual machine software itself. Suddenly, a flash of insight occurred – could we have inadvertently enabled Hyper-V previously without fully disabling it, thereby causing this issue? After all, interrupts are also implemented through virtualization software. Do different virtualization technologies have bugs? These questions deserved in-depth consideration and investigation.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion
&lt;/h2&gt;&lt;p&gt;According to the Microsoft official documentation, after completely disabling the Hyper-V service on the machine as described, VMware recovered normal operation on the host. This finally resolved the issue. As initially stated, this experience was convoluted and arduous, requiring comprehensive analysis and judgment. It was also our first time troubleshooting and pinpointing the problem down to the virtual machine level.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;Disable-WindowsOptionalFeature -Online -FeatureName Microsoft-HyperV-Hypervisor
bcdedit /set hypervisorlaunchtype off
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://learn.microsoft.com/zh-cn/troubleshoot/windows-client/application-management/virtualization-apps-not-work-with-hyper-v&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://learn.microsoft.com/zh-cn/troubleshoot/windows-client/application-management/virtualization-apps-not-work-with-hyper-v&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>C&#43;&#43; Programming Traps: A Detailed Explanation of Program Crashes Caused by Improper Use of `std::map`</title>
        <link>https://ttf248.life/en/p/cpp-programming-traps-std-map-crash-details/</link>
        <pubDate>Sun, 10 Mar 2024 22:03:06 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/cpp-programming-traps-std-map-crash-details/</guid>
        <description>e&gt;
&lt;h2 id=&#34;text&#34;&gt;Text
&lt;/h2&gt;&lt;p&gt;In the C++ standard library, &lt;code&gt;std::map&lt;/code&gt; is an associative container that stores elements in ascending order of keys (key) and provides efficient keyword lookup functionality. However, novice developers sometimes fall into trouble because they misunderstand the behavior of the square bracket operator &lt;code&gt;[]&lt;/code&gt; in &lt;code&gt;std::map&lt;/code&gt;. In fact, when using &lt;code&gt;[]&lt;/code&gt; to access a non-existent key, &lt;code&gt;std::map&lt;/code&gt; inserts a new key-value pair, and the default constructor will be used to initialize the value type corresponding to that key.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;#include &amp;lt;iostream&amp;gt;
#include &amp;lt;map&amp;gt;

int main() {
    std::map&amp;lt;std::string, int&amp;gt; myMap;

    // Incorrect usage: assuming here that we are trying to access a non-existent key and assume it will return 0
    std::cout &amp;lt;&amp;lt; &amp;quot;Value for &#39;nonexistent_key&#39;: &amp;quot; &amp;lt;&amp;lt; myMap[&amp;quot;nonexistent_key&amp;quot;] &amp;lt;&amp;lt; std::endl;

    // In fact, the above line of code creates a new key-value pair, where the value is initialized by the default constructor of int (usually 0)
    return 0;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Although the above code does not directly cause the program to crash, this implicit insertion behavior can lead to unexpected side effects in some cases, such as resource leaks or changes that do not meet expectations. Worse still, in a multithreaded environment, concurrent access to uninitialized memory areas may even cause the program to crash.&lt;/p&gt;
&lt;p&gt;To prevent these problems, it is recommended to use &lt;code&gt;std::map::find()&lt;/code&gt; or &lt;code&gt;std::map::count()&lt;/code&gt; methods to check if the key exists, or explicitly insert elements using &lt;code&gt;std::map::insert()&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;std::map&amp;lt;std::string, int&amp;gt; safeMap;
if (safeMap.count(&amp;quot;nonexistent_key&amp;quot;) == 0) {
    std::cout &amp;lt;&amp;lt; &amp;quot;Key does not exist.&amp;quot; &amp;lt;&amp;lt; std::endl;
} else {
    std::cout &amp;lt;&amp;lt; &amp;quot;Value for existing key: &amp;quot; &amp;lt;&amp;lt; safeMap[&amp;quot;nonexistent_key&amp;quot;] &amp;lt;&amp;lt; std::endl;
}

// Or explicitly insert a key-value pair, specifying the initial value
safeMap.insert({ &amp;quot;new_key&amp;quot;, 0 });
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If the map container stores objects of pointer type, the implicit insertion behavior will save an uninitialized pointer, and any operation on this pointer will cause the program to crash.&lt;/p&gt;</description>
        </item>
        <item>
        <title>pstack troubleshoot a hung process</title>
        <link>https://ttf248.life/en/p/pstack-troubleshooting-process-hangs/</link>
        <pubDate>Sat, 24 Feb 2024 23:55:59 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/pstack-troubleshooting-process-hangs/</guid>
        <description>&lt;p&gt;In software development and operations, deadlocked processes are frequently encountered. This situation can lead to performance degradation or service unavailability. This article introduces how to use the pstack tool to troubleshoot deadlocked process issues by analyzing process stack information to identify the root cause and resolve it.&lt;/p&gt;
&lt;p&gt;Background: A child service within the risk control system experienced a deadlocked state, resulting in the unavailability of the risk control service. Due to the lack of service availability monitoring, the deadlocked process situation was not detected in a timely manner, leading to system unavailability.&lt;/p&gt;
&lt;h2 id=&#34;text&#34;&gt;Text
&lt;/h2&gt;&lt;p&gt;A hung process refers to a process that has stopped responding but hasn&amp;rsquo;t exited. This situation can be caused by various reasons, such as deadlocks, resource exhaustion, or exceptions. To resolve these issues, we can use the &lt;code&gt;pstack&lt;/code&gt; tool to analyze the process’s stack information and identify the root cause.&lt;/p&gt;
&lt;h2 id=&#34;steps&#34;&gt;Steps
&lt;/h2&gt;&lt;p&gt;&lt;code&gt;pstack&lt;/code&gt; is a commonly used tool, often provided alongside &lt;code&gt;gdb&lt;/code&gt; (GNU Debugger). You can install it using the following command:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;sudo apt-get install gdb
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Obtain Process ID: First, we need to obtain the process ID (PID) of the zombie process. We can use the &lt;code&gt;ps&lt;/code&gt; command to list all processes and find the PID of the process we want to investigate.&lt;/p&gt;
&lt;p&gt;Use the &lt;code&gt;pstack&lt;/code&gt; tool to analyze the process stack. Once you have obtained the process ID, you can use the &lt;code&gt;pstack&lt;/code&gt; tool to retrieve the stack information for that process. Run the following command:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pstack &amp;lt;PID&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will output the stack information of the process, displaying the sequence of function calls currently being executed. By analyzing this information, you can identify where the process is stuck and subsequently pinpoint the problem.&lt;/p&gt;
&lt;p&gt;Analyze Stack Information: By examining the stack information, you can find the cause of the process becoming zombie. You may discover deadlock situations, infinite loops, or other abnormal conditions. Take appropriate measures based on the specific situation, such as releasing locks, fixing code logic, etc.&lt;/p&gt;
&lt;h2 id=&#34;case-study&#34;&gt;Case Study
&lt;/h2&gt;&lt;p&gt;Simple demo, after the main function starts, a child thread is created and the actual function enters an infinite loop, causing the program to fail to terminate normally and enter a state of false death.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cmake_minimum_required(VERSION 3.0.0)
project(pstack_main VERSION 0.1.0 LANGUAGES C CXX)

include(CTest)
enable_testing()

# Find the Threads library
find_package(Threads REQUIRED)

add_executable(pstack_main main.cpp)

# Link with the Threads library
target_link_libraries(pstack_main PRIVATE Threads::Threads)

set(CPACK_PROJECT_NAME ${PROJECT_NAME})
set(CPACK_PROJECT_VERSION ${PROJECT_VERSION})
include(CPack)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;#include &amp;lt;iostream&amp;gt;
#include &amp;lt;thread&amp;gt;
#include &amp;lt;chrono&amp;gt;

void infiniteLoop() {
    while (true) {
        // Main thread enters an infinite loop
    }
}

int main() {
    std::thread thread(infiniteLoop); // Create a thread to execute the infinite loop function
    thread.join(); // Wait for the thread to end
    return 0;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Running the program, and examining the &lt;code&gt;pstack&lt;/code&gt; results:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;Thread 2 (Thread 0x7eff3619b700 (LWP 1315017)):
#0  infiniteLoop () at /root/pstack/main.cpp:6
#1  0x0000000000402ca9 in std::__invoke_impl&amp;lt;void, void (*)()&amp;gt; (__f=@0x2260eb8: 0x4029a6 &amp;lt;infiniteLoop()&amp;gt;) at /usr/include/c++/8/bits/invoke.h:60
#2  0x0000000000402b02 in std::__invoke&amp;lt;void (*)()&amp;gt; (__fn=@0x2260eb8: 0x4029a6 &amp;lt;infiniteLoop()&amp;gt;) at /usr/include/c++/8/bits/invoke.h:95
#3  0x0000000000403150 in std::thread::_Invoker&amp;lt;std::tuple&amp;lt;void (*)()&amp;gt; &amp;gt;::_M_invoke&amp;lt;0ul&amp;gt; (this=0x2260eb8) at /usr/include/c++/8/thread:244
#4  0x0000000000403126 in std::thread::_Invoker&amp;lt;std::tuple&amp;lt;void (*)()&amp;gt; &amp;gt;::operator() (this=0x2260eb8) at /usr/include/c++/8/thread:253
#5  0x000000000040310a in std::thread::_State_impl&amp;lt;std::thread::_Invoker&amp;lt;std::tuple&amp;lt;void (*)()&amp;gt; &amp;gt; &amp;gt;::_M_run (this=0x2260eb0) at /usr/include/c++/8/thread:196
#6  0x00007eff36bceb23 in execute_native_thread_routine () from /lib64/libstdc++.so.6
#7  0x00007eff36ea91ca in start_thread () from /lib64/libpthread.so.0
#8  0x00007eff361d58d3 in clone () from /lib64/libc.so.6
Thread 1 (Thread 0x7eff372e1740 (LWP 1315016)):
#0  0x00007eff36eaa6cd in __pthread_timedjoin_ex () from /lib64/libpthread.so.0
#1  0x00007eff36bceda7 in std::thread::join() () from /lib64/libstdc++.so.6
#2  0x00000000004029d2 in main () at /root/pstack/main.cpp:13
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It can be seen that the program is in a false death state because of the infinite loop, the main thread enters an infinite loop, and the child thread cannot&lt;/p&gt;
</description>
        </item>
        <item>
        <title>C&#43;&#43; Function Call Latency</title>
        <link>https://ttf248.life/en/p/cpp-function-call-timing/</link>
        <pubDate>Wed, 24 Jan 2024 14:11:33 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/cpp-function-call-timing/</guid>
        <description>&lt;p&gt;Designed a行情 SDK, implementing different callback function implementations, and performed an extensive test. Recently I’ve been looking into C++ function programming, where functions have become first-class citizens, flowing within the program internally – what&amp;rsquo;s the difference in performance?&lt;/p&gt;
&lt;p&gt;Previous article link: &lt;a class=&#34;link&#34; href=&#34;https://ttf248.life/en/p/compiler-callback-performance-testing/&#34; &gt;Compiler, Callback Functions, Performance Testing&lt;/a&gt;
&lt;code&gt;leimao&lt;/code&gt;大佬 also did similar tests, so I borrowed their code.&lt;/p&gt;
&lt;h2 id=&#34;main-content&#34;&gt;Main Content
&lt;/h2&gt;&lt;p&gt;The execution platform remains our old friend, &lt;a class=&#34;link&#34; href=&#34;https://wandbox.org/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://wandbox.org/&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;#include &amp;lt;cassert&amp;gt;
#include &amp;lt;chrono&amp;gt;
#include &amp;lt;functional&amp;gt;
#include &amp;lt;iostream&amp;gt;
#include &amp;lt;vector&amp;gt;

int add_one(int input) { return input + 1; }

bool validate_vector_add_one(std::vector&amp;lt;int&amp;gt; const&amp;amp; input_vector,
                             std::vector&amp;lt;int&amp;gt; const&amp;amp; output_vector)
{
    bool is_valid{true};
    for (size_t i{0}; i &amp;lt; input_vector.size(); ++i)
    {
        if (output_vector.at(i) != input_vector.at(i) + 1)
        {
            is_valid = false;
            break;
        }
    }
    return is_valid;
}

void reset_vector(std::vector&amp;lt;int&amp;gt;&amp;amp; input_vector)
{
    for (size_t i{0}; i &amp;lt; input_vector.size(); ++i)
    {
        input_vector.at(i) = 0;
    }
}

template &amp;lt;typename T, typename Func&amp;gt;
void unitary_function_pass_by_lambda_function(T&amp;amp; output, T const&amp;amp; input,
                                              Func const func)
{
    output = func(input);
}

template &amp;lt;typename T&amp;gt;
void unitary_function_pass_by_std_function_value(T&amp;amp; output, T const&amp;amp; input,
                                                 std::function&amp;lt;T(T)&amp;gt; const func)
{
    output = func(input);
}

template &amp;lt;typename T&amp;gt;
void unitary_function_pass_by_std_function_reference(
    T&amp;amp; output, T const&amp;amp; input, std::function&amp;lt;T(T)&amp;gt; const&amp;amp; func)
{
    output = func(input);
}

template &amp;lt;typename T&amp;gt;
void unitary_function_pass_by_function_pointer(T&amp;amp; output, T const&amp;amp; input,
                                               T (*func)(T))
{
    output = func(input);
}

int main()
{
    // Set floating point format std::cout with 3 decimal places.
    std::cout.precision(3);

    size_t const num_elements{10000000};
    std::vector&amp;lt;int&amp;gt; input_vector(num_elements, 0);
    std::vector&amp;lt;int&amp;gt; output_vector(num_elements, 0);

    auto const lambda_function_add_one{[](int const&amp;amp; input) -&amp;gt; int
                                       { return input + 1; }};
    std::function&amp;lt;int(int)&amp;gt; const std_function_add_one{lambda_function_add_one};

    std::cout &amp;lt;&amp;lt; &amp;quot;The size of a function pointer: &amp;quot; &amp;lt;&amp;lt; sizeof(&amp;amp;add_one)
              &amp;lt;&amp;lt; std::endl;
    std::cout &amp;lt;&amp;lt; &amp;quot;The size of a std::function pointer: &amp;quot;
              &amp;lt;&amp;lt; sizeof(&amp;amp;std_function_add_one) &amp;lt;&amp;lt; std::endl;
    std::cout &amp;lt;&amp;lt; &amp;quot;The size of a std::function: &amp;quot; &amp;lt;&amp;lt; sizeof(std_function_add_one)
              &amp;lt;&amp;lt; std::endl;

    // Call function frequently in a vanilla way.
    // The compiler knows what function to call at compile time and can optimize
    // the code.
    // This is the best performance we could get.
    std::chrono::steady_clock::time_point const time_start_vanilla{
        std::chrono::steady_clock::now()};
    for (size_t i{0}; i &amp;lt; num_elements; ++i)
    {
        output_vector.at(i) = add_one(input_vector.at(i));
    }
    std::chrono::steady_clock::time_point const time_end_vanilla{
        std::chrono::steady_clock::now()};
    auto const time_elapsed_vanilla{
        std::chrono::duration_cast&amp;lt;std::chrono::nanoseconds&amp;gt;(time_end_vanilla -
                                                             time_start_vanilla)
            .count()};
    float const latency_vanilla{time_elapsed_vanilla /
                                static_cast&amp;lt;float&amp;gt;(num_elements)};
    std::cout &amp;lt;&amp;lt; &amp;quot;Latency Pass Vanilla: &amp;quot; &amp;lt;&amp;lt; latency_vanilla &amp;lt;&amp;lt; &amp;quot; ns&amp;quot;
              &amp;lt;&amp;lt; std::endl;
    assert(validate_vector_add_one(input_vector, output_vector));
    reset_vector(output_vector

```markdown
## Text
// Sometimes, we don&#39;t know what function to call at compile time.
// We can use `std::function` to pass a function as an argument.
// In this case, we pass the `std::function` by value.
// Because the size of a `std::function` is 32 bytes, passing by value
// results in a lot of copying and bad performance.
std::chrono::steady_clock::time_point const
    time_start_pass_by_std_function_value{std::chrono::steady_clock::now()};
for (size_t i{0}; i &amp;lt; num_elements; ++i)
{
    unitary_function_pass_by_std_function_value(
        output_vector.at(i), input_vector.at(i), std_function_add_one);
}
std::chrono::steady_clock::time_point const
    time_end_pass_by_std_function_value{std::chrono::steady_clock::now()};
auto const time_elapsed_pass_by_std_function_value{
    std::chrono::duration_cast&amp;lt;std::chrono::nanoseconds&amp;gt;(
        time_end_pass_by_std_function_value -
        time_start_pass_by_std_function_value)
        .count()};
float const latency_pass_by_std_function_value{
    time_elapsed_pass_by_std_function_value /
    static_cast&amp;lt;float&amp;gt;(num_elements)};
std::cout &amp;lt;&amp;lt; &amp;quot;Latency Pass By Std Function Value: &amp;quot;
          &amp;lt;&amp;lt; latency_pass_by_std_function_value &amp;lt;&amp;lt; &amp;quot; ns&amp;quot; &amp;lt;&amp;lt; std::endl;
assert(validate_vector_add_one(input_vector, output_vector));
reset_vector(output_vector);

// Instead of passing the `std::function` by value, we can pass it by
// reference (pointer). In this case, object copying is eliminated. The
// performance is better than passing the `std::function` by value. However,
// the performance is still not as good as the vanilla way.
std::chrono::steady_clock::time_point const
    time_start_pass_by_std_function_reference{
        std::chrono::steady_clock::now()};
for (size_t i{0}; i &amp;lt; num_elements; ++i)
{
    unitary_function_pass_by_std_function_reference(
        output_vector.at(i), input_vector.at(i), std_function_add_one);
}
std::chrono::steady_clock::time_point const
    time_end_pass_by_std_function_reference{
        std::chrono::steady_clock::now()};
auto const time_elapsed_pass_by_std_function_reference{
    std::chrono::duration_cast&amp;lt;std::chrono::nanoseconds&amp;gt;(
        time_end_pass_by_std_function_reference -
        time_start_pass_by_std_function_reference)
        .count()};
float const latency_pass_by_std_function_reference{
    time_elapsed_pass_by_std_function_reference /
    static_cast&amp;lt;float&amp;gt;(num_elements)};
std::cout &amp;lt;&amp;lt; &amp;quot;Latency Pass By Std Function Reference: &amp;quot;
          &amp;lt;&amp;lt; latency_pass_by_std_function_reference &amp;lt;&amp;lt; &amp;quot; ns&amp;quot; &amp;lt;&amp;lt; std::endl;
assert(validate_vector_add_one(input_vector, output_vector));
reset_vector(output_vector);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;## Text

// `std::function` is a general-purpose wrapper for function pointers,
// callable objects, and lambda functions. Because it&#39;s general purpose,
// it&#39;s not as efficient as a function pointer. In this case, we pass a
// function pointer to a function. The performance is better than passing
// the `std::function` by reference.
std::chrono::steady_clock::time_point const
    time_start_pass_by_function_pointer{std::chrono::steady_clock::now()};
for (size_t i{0}; i &amp;lt; num_elements; ++i)
{
    unitary_function_pass_by_function_pointer(output_vector.at(i),
                                                  input_vector.at(i), &amp;amp;add_one);
}
std::chrono::steady_clock::time_point const
    time_end_pass_by_function_pointer{std::chrono::steady_clock::now()};
auto const time_elapsed_pass_by_function_pointer{
    std::chrono::duration_cast&amp;lt;std::chrono::nanoseconds&amp;gt;(
        std::chrono::steady_clock::now() -
        time_start_pass_by_function_pointer)
        .count()};
float const latency_pass_by_function_pointer{
    time_elapsed_pass_by_function_pointer /
    static_cast&amp;lt;float&amp;gt;(num_elements)};
std::cout &amp;lt;&amp;lt; &amp;quot;Latency Pass By Function Pointer: &amp;quot;
          &amp;lt;&amp;lt; latency_pass_by_function_pointer &amp;lt;&amp;lt; &amp;quot; ns&amp;quot; &amp;lt;&amp;lt; std::endl;
assert(validate_vector_add_one(input_vector, output_vector));
reset_vector(output_vector);

// We can also pass a lambda function to a function.
// The compiler knows what function to call at compile time and can optimize
// the code. The performance is also better than passing the `std::function`
// by reference.
std::chrono::steady_clock::time_point const
    time_start_pass_by_lambda_function{std::chrono::steady_clock::now()};
for (size_t i{0}; i &amp;lt; num_elements; ++i)
{
    unitary_function_pass_by_lambda_function(
        output_vector.at(i), input_vector.at(i), lambda_function_add_one);
}
std::chrono::steady_clock::time_point const
    time_end_pass_by_lambda_function{std::chrono::steady_clock::now()};
auto const time_elapsed_pass_by_lambda_function{
    std::chrono::duration_cast&amp;lt;std::chrono::nanoseconds&amp;gt;(
        std::chrono::steady_clock::now() -
        time_start_pass_by_lambda_function)
        .count()};
float const latency_pass_by_lambda_function{
    time_elapsed_pass_by_lambda_function /
    static_cast&amp;lt;float&amp;gt;(num_elements)};
std::cout &amp;lt;&amp;lt; &amp;quot;Latency Pass By Lambda Function: &amp;quot;
          &amp;lt;&amp;lt; latency_pass_by_lambda_function &amp;lt;&amp;lt; &amp;quot; ns&amp;quot; &amp;lt;&amp;lt; std::endl;
assert(validate_vector_add_one(input_vector, output_vector));
reset_vector(output_vector);
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;body&#34;&gt;Body
&lt;/h2&gt;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# The default optimization for the team is to enable O2, and the compiler selected was gcc13. Performance and execution times vary slightly between different versions of gcc, with higher versions resulting in better lambda performance.
# Function pointer size: 8
# std::function pointer size: 8
# std::function size: 32
# Vanilla Pass Latency: 0.418 ns
# Latency Pass By Std Function Value: 3.47 ns
# Latency Pass By Std Function Reference: 1.36 ns
# Latency Pass By Function Pointer: 0.396 ns
# Latency Pass By Lambda Function: 0.44 ns
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;references&#34;&gt;References
&lt;/h2&gt;&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://leimao.github.io/blog/CPP-Function-Call-Performance/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://leimao.github.io/blog/CPP-Function-Call-Performance/&lt;/a&gt;&lt;/p&gt;</description>
        </item>
        <item>
        <title>How to Anti-Debug</title>
        <link>https://ttf248.life/en/p/program-anti-debug/</link>
        <pubDate>Tue, 23 Jan 2024 19:46:36 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/program-anti-debug/</guid>
        <description>&lt;p&gt;Suddenly, I was feeling the urge to browse for new wallpapers, sticking with my usual black series, with some areas colored in, and the desktop needing to display icons. Other color schemes would result in blurry icons.&lt;/p&gt;
&lt;p&gt;I stared at the assembly code, trying to figure it out, but couldn&amp;rsquo;t understand it. I tried throwing it to an &lt;code&gt;AI&lt;/code&gt;, explaining the instructions, but it failed to explain the context – clearly, this was a command used for a specific scenario. Regular code isn’t like that.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;AI&lt;/code&gt; was no longer as useful as a search engine at this point; its knowledge base of assembly language was insufficient.&lt;/p&gt;
&lt;h2 id=&#34;wallpaper&#34;&gt;Wallpaper
&lt;/h2&gt;&lt;p&gt;&lt;img src=&#34;https://ttf248.life/p/program-anti-debug/Snipaste_2024-01-23_19-50-53.png&#34;
	width=&#34;1020&#34;
	height=&#34;470&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;Black Assembly Wallpaper&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;217&#34;
		data-flex-basis=&#34;520px&#34;
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;assembly-code&#34;&gt;Assembly Code
&lt;/h2&gt;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;PUSHFD
MOV DWORD PTR [ESP],0X100
POPFD
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Actual Application Scenario&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;bool IsDebugged()
{
    __try
    {
        __asm
        {
            pushfd
            mov dword ptr [esp], 0x100
            popfd
            nop
        }
        return true;
    }
    __except(GetExceptionCode() == EXCEPTION_SINGLE_STEP
        ? EXCEPTION_EXECUTE_HANDLER
        : EXCEPTION_CONTINUE_EXECUTION)
    {
        return false;
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;explanation&#34;&gt;Explanation
&lt;/h2&gt;&lt;p&gt;&lt;code&gt;TrapFlag&lt;/code&gt; is a flag bit in the register file. When this flag is set, it throws an exception &lt;code&gt;SINGLE_STEP&lt;/code&gt;. Because when we trace the code, this flag will be cleared by the debugger, so we won&amp;rsquo;t see this exception.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In actual testing, if you directly step over detecting functions, debugging will not be detected. Only when entering the detection function to execute will it be detected (based on research materials, yet to be verified in practice).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;references&#34;&gt;References
&lt;/h2&gt;&lt;p&gt;Chinese related materials are based on the English articles from websites, which introduce many anti-debugging techniques.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://anti-debug.checkpoint.com/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://anti-debug.checkpoint.com/&lt;/a&gt; techniques/assembly.html&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://song-10.gitee.io/2021/08/08/Reverse-2021-08-08-anti-debug/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://song-10.gitee.io/2021/08/08/Reverse-2021-08-08-anti-debug/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>How to Download Focus News/CCTV Video Files</title>
        <link>https://ttf248.life/en/p/how-to-download-focus-interview-cctv-videos/</link>
        <pubDate>Tue, 23 Jan 2024 19:23:35 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/how-to-download-focus-interview-cctv-videos/</guid>
        <description>&lt;p&gt;Recently, someone asked how to download Focus Interview videos. My mind immediately went to the usual – eight or nine out of ten times it’s encrypted using an &lt;code&gt;m3u8&lt;/code&gt; method, and a bit of simple processing is all it takes.&lt;/p&gt;
&lt;h2 id=&#34;downloader&#34;&gt;Downloader
&lt;/h2&gt;&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/nilaoda/N_m3u8DL-CLI&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/nilaoda/N_m3u8DL-CLI&lt;/a&gt;
&lt;code&gt;m3u8 downloader&lt;/code&gt; is an open-source command-line m3u8/HLS/dash downloader that supports ordinary AES-128-CBC decryption, multi-threading, custom request headers, etc. Supports Simplified Chinese, Traditional Chinese and English. English Supported.&lt;/p&gt;
&lt;h2 id=&#34;browser-extensions&#34;&gt;Browser Extensions
&lt;/h2&gt;&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://microsoftedge.microsoft.com/addons/detail/ngjclnbcdbahekojpkhancmiobdahemb&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Live Stream Downloader&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;honeyed-confidence&#34;&gt;Honeyed Confidence
&lt;/h2&gt;&lt;p&gt;Getting the address, assuming it was solved, turned out to be nothing – unable to parse segments normally, query information, and discover that the official had processed the download address, requiring manual replacement of the &lt;code&gt;key&lt;/code&gt; parsed by the plugin into the following links.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;https://newcntv.qcloudcdn.com/asp/hls/2000/0303000a/3/default/***********************/2000.m3u8
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As of January 2024, the address is still valid; if there are any changes in the future, analyze the webpage independently.
Historical Address Backup: &lt;code&gt;https://hlswx.cntv.kcdnvip.com/asp/hls/main/0303000a/3/default/一串字符/main.m3u8?maxbr=2000&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References
&lt;/h2&gt;&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;http://jln.cn/post/517.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;http://jln.cn/post/517.html&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Bulk Modify SQL Server Database Disk File Permissions</title>
        <link>https://ttf248.life/en/p/bulk-modify-sqlserver-database-disk-permissions/</link>
        <pubDate>Tue, 23 Jan 2024 19:06:36 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/bulk-modify-sqlserver-database-disk-permissions/</guid>
        <description>&lt;p&gt;The company adjusted its security policies. Ultimately, ‘Mechanical Mini’ was relocated back home as a backup server, along with a full system reinstallation. Ubuntu switched to Windows Server; due to an irregular activation method – used at home – it seemed like it wouldn&amp;rsquo;t be activated, and that was fine. An unconventional activation triggered Microsoft’s detection (running normally for half a month), the server would automatically shut down after running for one hour. After reviewing the system logs, it was discovered that this was due to using a pirated version.&lt;/p&gt;
&lt;p&gt;There wasn’t much else to do, so the system was reinstalled again, and SQL Server also needed to be reinstalled – it&amp;rsquo;s always a bit of a pain each time. File permission control is very strict, making it impossible to attach the database normally.&lt;/p&gt;
&lt;h2 id=&#34;error-message&#34;&gt;Error Message
&lt;/h2&gt;&lt;p&gt;After the system reinstallation, &lt;code&gt;SqlServer&lt;/code&gt; may encounter error 5120, an operating system access denied error, when attaching a database.&lt;/p&gt;
&lt;h2 id=&#34;processing-script&#34;&gt;Processing Script
&lt;/h2&gt;&lt;p&gt;Referencing the previous link: &lt;a class=&#34;link&#34; href=&#34;https://ttf248.life/en/p/bulk-update-local-git-and-legacy-permissions/&#34; &gt;Batch Update Local Git Repository&lt;/a&gt;, it’s that familiar script all over again – modified to, we iterate through folders while modifying file permissions. Currently used with full editing permissions.&lt;/p&gt;
&lt;p&gt;Most tutorials online have you manually modify files. They only need to change a few files each time? I always have to process batches of files; doing everything manually is going to drive me crazy.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$currentUserName = [System.Security.Principal.WindowsIdentity]::GetCurrent().Name
[Console]::OutputEncoding = [System.Text.Encoding]::UTF8
$rootDirectory = &amp;quot;D:\data\2013_RujiaInfo&amp;quot;

Get-ChildItem -Path $rootDirectory -Recurse | ForEach-Object {
    $itemPath = $_.FullName

    if ($_ -is [System.IO.DirectoryInfo]) {
        $icaclsResult = icacls $itemPath /setowner &amp;quot;$currentUserName&amp;quot; 2&amp;gt;&amp;amp;1
        if ($LASTEXITCODE -eq 0) {
            Write-Host &amp;quot;Changed the owner of folder $itemPath to $currentUserName&amp;quot;
            # Grant current user write permissions
            Invoke-Expression &amp;quot;icacls `&amp;quot;$itemPath`&amp;quot; /grant `&amp;quot;$($currentUserName):(OI)(CI)F`&amp;quot;&amp;quot;
            Write-Host &amp;quot;Granted $currentUserName editing permissions for the folder&amp;quot;
        } else {
            Write-Host &amp;quot;Unable to change the owner of folder $itemPath. Error message: $icaclsResult&amp;quot;
        }
    } else {
        $takeownResult = icacls $itemPath /setowner &amp;quot;$currentUserName&amp;quot; 2&amp;gt;&amp;amp;1
        if ($LASTEXITCODE -eq 0) {
            # Grant current user write permissions
            Invoke-Expression &amp;quot;icacls `&amp;quot;$itemPath`&amp;quot; /grant `&amp;quot;$($currentUserName):(F)`&amp;quot;&amp;quot;
            Write-Host &amp;quot;Granted $currentUserName editing permissions for the file&amp;quot;
        } else {
            Write-Host &amp;quot;Unable to change the owner of file $itemPath. Error message: $takeownResult&amp;quot;
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;
</description>
        </item>
        <item>
        <title>Linux System Benchmark Test</title>
        <link>https://ttf248.life/en/p/linux-system-benchmark-test/</link>
        <pubDate>Tue, 09 Jan 2024 10:56:23 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/linux-system-benchmark-test/</guid>
        <description>&lt;p&gt;Windows platform has RuMaster (Entertainment Master), which isn&amp;rsquo;t known for highly accurate data, but it’s still useful as a reference. Of course, there are other professional benchmarking software options available. When it comes to Linux systems, there haven’t seemed to be any particularly suitable benchmarking software found.&lt;/p&gt;
&lt;p&gt;Sysbench is a versatile benchmark testing tool that can be used to test CPU, memory, file I/O, thread performance, and more. You can use Sysbench to execute various performance testing tasks.&lt;/p&gt;
&lt;p&gt;I currently have three machines available for testing: the Mechanical Artist mini laptop, a local small host machine, an Alibaba Cloud Dev development cloud server, and a Huawei Cloud Dev server.&lt;/p&gt;
&lt;h2 id=&#34;installing-sysbench&#34;&gt;Installing Sysbench
&lt;/h2&gt;&lt;p&gt;On most Linux distributions, you can use the package manager to install Sysbench. For example, on CentOS 8, you can use the following command:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;sudo dnf install sysbench
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;sysbench-usage-examples&#34;&gt;Sysbench Usage Examples
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;Testing CPU performance: &lt;code&gt;sysbench --test=cpu run&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Testing memory read performance: &lt;code&gt;sysbench --test=memory run&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Testing file I/O performance:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;sysbench --test=fileio --file-test-mode=rndrw prepare
sysbench --test=fileio --file-test-mode=rndrw run
sysbench --test=fileio --file-test-mode=rndrw cleanup
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Testing multi-threaded performance: &lt;code&gt;sysbench --test=threads --num-threads=4 run&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Testing MySQL database performance (requires adjusting the maximum connection number):&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;sysbench --test=oltp --db-driver=mysql --mysql-db=test --mysql-user=yourusername --mysql-password=yourpassword --oltp-table-size=1000000 prepare
sysbench --test=oltp --db-driver=mysql --mysql-db=test --mysql-user=yourusername --mysql-password=yourpassword --max-time=60 --oltp-read-only=off --oltp-test-mode=complex --max-requests=0 run
sysbench --test=oltp --db-driver=mysql --mysql-db=test --mysql-user=yourusername --mysql-password=yourpassword cleanup
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;score-report&#34;&gt;Score Report
&lt;/h2&gt;&lt;meta http-equiv=&#34;Content-Type&#34; content=&#34;text/html; charset=utf-8&#34;&gt;&lt;link type=&#34;text/css&#34; rel=&#34;stylesheet&#34; href=&#34;resources/sheet.css&#34; &gt;
&lt;h2 id=&#34;run-score-report&#34;&gt;Run Score Report
&lt;/h2&gt;&lt;style type=&#34;text/css&#34;&gt;.ritz .waffle a { color: inherit; }.ritz .waffle .s0{background-color:#ffffff;text-align:left;color:#000000;font-family:&#39;Arial&#39;;font-size:10pt;vertical-align:bottom;white-space:nowrap;direction:ltr;padding:2px 3px 2px 3px;}&lt;/style&gt;&lt;div class=&#34;ritz grid-container&#34; dir=&#34;ltr&#34;&gt;&lt;table class=&#34;waffle&#34; cellspacing=&#34;0&#34; cellpadding=&#34;0&#34;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th class=&#34;row-header freezebar-origin-ltr&#34;&gt;&lt;/th&gt;&lt;th id=&#34;0C0&#34; style=&#34;width:100px;&#34; class=&#34;column-headers-background&#34;&gt;A&lt;/th&gt;&lt;th id=&#34;0C1&#34; style=&#34;width:421px;&#34; class=&#34;column-headers-background&#34;&gt;B&lt;/th&gt;&lt;th id=&#34;0C2&#34; style=&#34;width:398px;&#34; class=&#34;column-headers-background&#34;&gt;C&lt;/th&gt;&lt;th id=&#34;0C3&#34; style=&#34;width:422px;&#34; class=&#34;column-headers-background&#34;&gt;D&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr style=&#34;height: 20px&#34;&gt;&lt;th id=&#34;0R0&#34; style=&#34;height: 20px;&#34; class=&#34;row-headers-background&#34;&gt;&lt;div class=&#34;row-header-wrapper&#34; style=&#34;line-height: 20px&#34;&gt;1&lt;/div&gt;&lt;/th&gt;&lt;td&gt;&lt;/td&gt;&lt;td class=&#34;s0&#34; dir=&#34;ltr&#34;&gt;Local Technician&lt;/td&gt;&lt;td class=&#34;s0&#34; dir=&#34;ltr&#34;&gt;Alibaba Cloud&lt;/td&gt;&lt;td class=&#34;s0&#34; dir=&#34;ltr&#34;&gt;Huawei Cloud&lt;/td&gt;&lt;/tr&gt;&lt;tr style=&#34;height: 20px&#34;&gt;&lt;th id=&#34;0R1&#34; style=&#34;height: 20px;&#34; class=&#34;row-headers-background&#34;&gt;&lt;div class=&#34;row-header-wrapper&#34; style=&#34;line-height: 20px&#34;&gt;2&lt;/div&gt;&lt;/th&gt;&lt;td class=&#34;s0&#34; dir=&#34;ltr&#34;&gt;System Configuration&lt;/td&gt;&lt;td class=&#34;s0&#34; dir=&#34;ltr&#34;&gt;System Information&lt;br&gt;  Operating System              Ubuntu 23.04&lt;br&gt;  Kernel                        Linux 6.2.0-36-generic x86_64&lt;br&gt;  Model                         Machenike Machenike DT Computer&lt;br&gt;  Motherboard                   Machenike Machenike DT Computer&lt;br&gt;  BIOS                          American Megatrends International, LLC.&lt;br&gt;DB19V012&lt;br&gt;&lt;br&gt;CPU Information&lt;br&gt;  Name                          Intel Core i7-12650H&lt;br&gt;  Topology                      1 Processor, 10 Cores, 16 Threads&lt;br&gt;  Identifier                    GenuineIntel Family 6 Model 154 Stepping 3&lt;br&gt;  Base Frequency                4.60 GHz&lt;br&gt;  L1 Instruction Cache          32.0 KB x 8&lt;br&gt;  L1 Data Cache                 48.0 KB x 8&lt;br&gt;  L2 Cache                      1.25 MB x 2&lt;br&gt;  L3 Cache                      24.0 MB&lt;br&gt;&lt;br&gt;Memory Information&lt;br&gt;  Size                          62.6 GB&lt;/td&gt;&lt;td class=&#34;s0&#34; dir=&#34;ltr&#34;&gt;System Information&lt;br&gt;  Operating System              CentOS Stream 8&lt;br&gt;  Kernel                        Linux 4.18.0-513.el8.x86_64 x86_64&lt;br&gt;  Model                         Alibaba Cloud Alibaba Cloud ECS&lt;br&gt;  Motherboard                   N/A&lt;br&gt;  BIOS                          SeaBIOS 449e491&lt;br&gt;&lt;br&gt;CPU Information&lt;br&gt;  Name                          Intel(R) Xeon(R) Platinum&lt;br&gt;  Topology                      1 Processor, 1 Core, 2 Threads&lt;br&gt;  Identifier                    GenuineIntel Family 6 Model 85 Stepping 4&lt;br&gt;  Base Frequency                2.50 GHz&lt;br&gt;  L1 Instruction Cache          32.0 KB&lt;br&gt;  L1 Data Cache                 32.0 KB&lt;br&gt;  L2 Cache                      1.00 MB&lt;br&gt;  L3 Cache                      33.0 MB&lt;br&gt;&lt;br&gt;Memory Information&lt;br&gt;  Size                          1.65 GB&lt;/td&gt;&lt;td class=&#34;s0&#34; dir=&#34;ltr&#34;&gt;System Information&lt;br&gt;  Operating System              Ubuntu 22.04.1 LTS&lt;br&gt;  Kernel                        Linux 5.15.0-60-generic x86_64&lt;br&gt;  Model                         OpenStack Foundation OpenStack Nova - 64 GB
&lt;tr&gt;&lt;th id=&#34;0R2&#34; style=&#34;height: 20px;&#34; class=&#34;row-headers-background&#34;&gt;&lt;div class=&#34;row-header-wrapper&#34; style=&#34;line-height: 20px&#34;&gt;3&lt;/div&gt;&lt;/th&gt;&lt;td class=&#34;s0&#34; dir=&#34;ltr&#34;&gt;CPU&lt;/td&gt;&lt;td class=&#34;s0&#34; dir=&#34;ltr&#34;&gt;sysbench 1.0.20 (using
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Benchmark Data Report
system LuaJIT 2.1.0-beta3&lt;br&gt;&lt;br&gt;Running the test with following options:&lt;br&gt;Number of threads: 1&lt;br&gt;Initializing random number generator from current time&lt;br&gt;&lt;br&gt;&lt;br&gt;Prime numbers limit: 10000&lt;br&gt;&lt;br&gt;Initializing worker threads&amp;hellip;&lt;br&gt;&lt;br&gt;Threads started!&lt;br&gt;&lt;br&gt;CPU speed:&lt;br&gt;    events per second:  4032.48&lt;br&gt;&lt;br&gt;General statistics:&lt;br&gt;    total time:                          10.0004s&lt;br&gt;    total number of events:              40330&lt;br&gt;&lt;br&gt;Latency (ms):&lt;br&gt;         min:                                    0.25&lt;br&gt;         avg:                                    0.25&lt;br&gt;         max:                                    0.73&lt;br&gt;         95th percentile:                        0.25&lt;br&gt;         sum:                                 9997.55&lt;br&gt;&lt;br&gt;Threads fairness:&lt;br&gt;    events (avg/stddev):           40330.0000/0.00&lt;br&gt;    execution time (avg/stddev):   9.9975/0.00&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;sysbench 1.0.20 (using system LuaJIT 2.1.0-beta3)&lt;br&gt;&lt;br&gt;Running the test with following options:&lt;br&gt;Number of threads: 1&lt;br&gt;Initializing random number generator from current time&lt;br&gt;&lt;br&gt;&lt;br&gt;Prime numbers limit: 10000&lt;br&gt;&lt;br&gt;Initializing worker threads&amp;hellip;&lt;br&gt;&lt;br&gt;Threads started!&lt;br&gt;&lt;br&gt;CPU speed:&lt;br&gt;    events per second:  1062.51&lt;br&gt;&lt;br&gt;General statistics:&lt;br&gt;    total time:                          10.0008s&lt;br&gt;    total number of events:              10628&lt;br&gt;&lt;br&gt;Latency (ms):&lt;br&gt;         min:                                    0.91&lt;br&gt;         avg:                                    0.94&lt;br&gt;         max:                                   22.84&lt;br&gt;         95th percentile:                        1.06&lt;br&gt;         sum:                                 9993.46&lt;br&gt;&lt;br&gt;Threads fairness:&lt;br&gt;    events (avg/stddev):           10628.0000/0.00&lt;br&gt;    execution time (avg/stddev):   9.9935/0.00&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;sysbench 1.0.20 (using system LuaJIT 2.1.0-beta3)&lt;br&gt;&lt;br&gt;Running the test with following options:&lt;br&gt;Number of threads: 1&lt;br&gt;Initializing random number generator from current time&lt;br&gt;&lt;br&gt;&lt;br&gt;Prime numbers limit: 10000&lt;br&gt;&lt;br&gt;Initializing worker threads&amp;hellip;&lt;br&gt;&lt;br&gt;Threads started!&lt;br&gt;&lt;br&gt;CPU speed:&lt;br&gt;    events per second:  1125.56&lt;br&gt;&lt;br&gt;General statistics:&lt;br&gt;    total time:                          10.0005s&lt;br&gt;    total number of events:              11258&lt;br&gt;&lt;br&gt;Latency (ms):&lt;br&gt;         min:                                    0.86&lt;br&gt;         avg:                                    0.89&lt;br&gt;         max:                                    1.70&lt;br&gt;         95th percentile:                        0.99&lt;br&gt;         sum:                                 9995.40&lt;br&gt;&lt;br&gt;Threads fairness:&lt;br&gt;    events (avg/stddev):           11258.0000/0.00&lt;br&gt;    execution time (avg/stddev):   9.9954/0.00&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Running the test with following options:&lt;br&gt;Number of threads: 1&lt;br&gt;Initializing random number generator from current time&lt;br&gt;&lt;br&gt;&lt;br&gt;Running memory speed test with the following options:&lt;br&gt;  block size: 1KiB&lt;br&gt;  total size: 102400MiB&lt;br&gt;  operation: write&lt;br&gt;  scope: global&lt;br&gt;&lt;br&gt;Initializing worker threads&amp;hellip;&lt;br&gt;&lt;br&gt;Threads started!&lt;br&gt;&lt;br&gt;Total operations: 101993199 (10198146.52 per second)&lt;br&gt;&lt;br&gt;99602.73 MiB transferred (9959.13 MiB/sec)&amp;lt; - Benchmark Data Report
system LuaJIT 2.1.0-beta3&lt;br&gt;&lt;br&gt;Running the test with following options:&lt;br&gt;Number of threads: 1&lt;br&gt;Initializing random number generator from current time&lt;br&gt;&lt;br&gt;&lt;br&gt;Prime numbers limit: 10000&lt;br&gt;&lt;br&gt;Initializing worker threads&amp;hellip;&lt;br&gt;&lt;br&gt;Threads started!&lt;br&gt;&lt;br&gt;CPU speed:&lt;br&gt;    events per second:  4032.48&lt;br&gt;&lt;br&gt;General statistics:&lt;br&gt;    total time:                          10.0004s&lt;br&gt;    total number of events:              40330&lt;br&gt;&lt;br&gt;Latency (ms):&lt;br&gt;         min:                                    0.25&lt;br&gt;         avg:                                    0.25&lt;br&gt;         max:                                    0.73&lt;br&gt;         95th percentile:                        0.25&lt;br&gt;         sum:                                 9997.55&lt;br&gt;&lt;br&gt;Threads fairness:&lt;br&gt;    events (avg/stddev):           40330.0000/0.00&lt;br&gt;    execution time (avg/stddev):   9.9975/0.00&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;sysbench 1.0.20 (using system LuaJIT 2.1.0-beta3)&lt;br&gt;&lt;br&gt;Running the test with following options:&lt;br&gt;Number of threads: 1&lt;br&gt;Initializing random number generator from current time&lt;br&gt;&lt;br&gt;&lt;br&gt;Prime numbers limit: 10000&lt;br&gt;&lt;br&gt;Initializing worker threads&amp;hellip;&lt;br&gt;&lt;br&gt;Threads started!&lt;br&gt;&lt;br&gt;CPU speed:&lt;br&gt;    events per second:  1062.51&lt;br&gt;&lt;br&gt;General statistics:&lt;br&gt;    total time:                          10.0008s&lt;br&gt;    total number of events:              10628&lt;br&gt;&lt;br&gt;Latency (ms):&lt;br&gt;         min:                                    0.91&lt;br&gt;         avg:                                    0.94&lt;br&gt;         max:                                   22.84&lt;br&gt;         95th percentile:                        1.06&lt;br&gt;         sum:                                 9993.46&lt;br&gt;&lt;br&gt;Threads fairness:&lt;br&gt;    events (avg/stddev):           10628.0000/0.00&lt;br&gt;    execution time (avg/stddev):   9.9935/0.00&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;sysbench 1.0.20 (using system LuaJIT 2.1.0-beta3)&lt;br&gt;&lt;br&gt;Running the test with following options:&lt;br&gt;Number of threads: 1&lt;br&gt;Initializing random number generator from current time&lt;br&gt;&lt;br&gt;&lt;br&gt;Prime numbers limit: 10000&lt;br&gt;&lt;br&gt;Initializing worker threads&amp;hellip;&lt;br&gt;&lt;br&gt;Threads started!&lt;br&gt;&lt;br&gt;CPU speed:&lt;br&gt;    events per second:  1125.56&lt;br&gt;&lt;br&gt;General statistics:&lt;br&gt;    total time:                          10.0005s&lt;br&gt;    total number of events:              11258&lt;br&gt;&lt;br&gt;Latency (ms):&lt;br&gt;         min:                                    0.86&lt;br&gt;         avg:                                    0.89&lt;br&gt;         max:                                    1.70&lt;br&gt;         95th percentile:                        0.99&lt;br&gt;         sum:                                 9995.40&lt;br&gt;&lt;br&gt;Threads fairness:&lt;br&gt;    events (avg/stddev):           11258.0000/0.00&lt;br&gt;    execution time (avg/stddev):   9.9954/0.00&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Running the test with following options:&lt;br&gt;Number of threads: 1&lt;br&gt;Initializing random number generator from current time&lt;br&gt;&lt;br&gt;&lt;br&gt;Running memory speed test with the following options:&lt;br&gt;  block size: 1KiB&lt;br&gt;  total size: 102400MiB&lt;br&gt;  operation: write&lt;br&gt;  scope: global&lt;br&gt;&lt;br&gt;Initializing worker threads&amp;hellip;&lt;br&gt;&lt;br&gt;Threads started!&lt;br&gt;&lt;br&gt;Total operations: 101993199 (10198146.52 per second)&lt;br&gt;&lt;br&gt;99602.73 MiB transferred (9959.13 MiB/sec)&amp;lt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Run Score Reports
random number generator from current time&lt;br&gt;&lt;br&gt;&lt;br&gt;Running memory speed test with the following options:&lt;br&gt;  block size: 1KiB&lt;br&gt;  total size: 102400MiB&lt;br&gt;  operation: write&lt;br&gt;  scope: global&lt;br&gt;&lt;br&gt;Initializing worker threads&amp;hellip;&lt;br&gt;&lt;br&gt;Threads started!&lt;br&gt;&lt;br&gt;Total operations: 48418803 (4841004.79 per second)&lt;br&gt;&lt;br&gt;47283.99 MiB transferred (4727.54 MiB/sec)&lt;br&gt;&lt;br&gt;&lt;br&gt;General statistics:&lt;br&gt;    total time:                          10.0001s&lt;br&gt;    total number of events:              48418803&lt;br&gt;&lt;br&gt;Latency (ms):&lt;br&gt;         min:                                    0.00&lt;br&gt;         avg:                                    0.00&lt;br&gt;         max:                                   25.26&lt;br&gt;         95th percentile:                        0.00&lt;br&gt;         sum:                                 4578.95&lt;br&gt;&lt;br&gt;Threads fairness:&lt;br&gt;    events (avg/stddev):           48418803.0000/0.00&lt;br&gt;    execution time (avg/stddev):   4.5789/0.00&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Machine Learning&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Neural Networks - Run Score Reports
random number generator from current time&lt;br&gt;&lt;br&gt;&lt;br&gt;Running memory speed test with the following options:&lt;br&gt; block size: 1KiB&lt;br&gt; total size: 102400MiB&lt;br&gt; operation: write&lt;br&gt; scope: global&lt;br&gt;&lt;br&gt;Initializing worker threads&amp;hellip;&lt;br&gt;&lt;br&gt;Threads started!&lt;br&gt;&lt;br&gt;Total operations: 48418803 (4841004.79 per second)&lt;br&gt;&lt;br&gt;47283.99 MiB transferred (4727.54 MiB/sec)&lt;br&gt;&lt;br&gt;&lt;br&gt;General statistics:&lt;br&gt; total time:                          10.0001s&lt;br&gt; total number of events:              48418803&lt;br&gt;&lt;br&gt;Latency (ms):&lt;br&gt; min:                                    0.00&lt;br&gt; avg:                                    0.00&lt;br&gt; max:                                   25.26&lt;br&gt; 95th percentile:                        0.00&lt;br&gt; sum:                                 4578.95&lt;br&gt;&lt;br&gt;Threads fairness:&lt;br&gt; events (avg/stddev):           48418803.0000/0.00&lt;br&gt; execution time (avg/stddev):   4.5789/0.00&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Machine Learning&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Neural Networks&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;score-report-data&#34;&gt;Score Report Data
&lt;/h2&gt;&lt;p&gt;enabled, calling fsync() each 100 requests.&lt;br&gt;Calling fsync() at the end of test, Enabled.&lt;br&gt;Using synchronous I/O mode&lt;br&gt;Doing random r/w test&lt;br&gt;Initializing worker threads&amp;hellip;&lt;br&gt;&lt;br&gt;Threads started!&lt;br&gt;&lt;br&gt;&lt;br&gt;File operations:&lt;br&gt;    reads/s:                      1593.12&lt;br&gt;    writes/s:                     1062.08&lt;br&gt;    fsyncs/s:                     3406.64&lt;br&gt;&lt;br&gt;Throughput:&lt;br&gt;    read, MiB/s:                  24.89&lt;br&gt;    written, MiB/s:               16.60&lt;br&gt;&lt;br&gt;General statistics:&lt;br&gt;    total time:                          10.0164s&lt;br&gt;    total number of events:              60600&lt;br&gt;&lt;br&gt;Latency (ms):&lt;br&gt;         min:                                    0.00&lt;br&gt;         avg:                                    0.16&lt;br&gt;         max:                                   31.32&lt;br&gt;         95th percentile:                        0.54&lt;br&gt;         sum:                                 9956.30&lt;br&gt;&lt;br&gt;Threads fairness:&lt;br&gt;    events (avg/stddev):           60600.0000/0.00&lt;br&gt;    execution time (avg/stddev):   9.9563/0.00&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;2147483648 bytes written in 18.29 seconds (111.98 MiB/sec).&lt;br&gt;&lt;br&gt;Running the test with following options:&lt;br&gt;Number of threads: 1&lt;br&gt;Initializing random number generator from current time&lt;br&gt;&lt;br&gt;&lt;br&gt;Extra file open flags: (none)&lt;br&gt;128 files, 16MiB each&lt;br&gt;2GiB total file size&lt;br&gt;Block size 16KiB&lt;br&gt;Number of IO requests: 0&lt;br&gt;Read/Write ratio for combined random IO test: 1.50&lt;br&gt;Periodic FSYNC enabled, calling fsync() each 100 requests.&lt;br&gt;Calling fsync() at the end of test, Enabled.&lt;br&gt;Using synchronous I/O mode&lt;br&gt;Doing random r/w test&lt;br&gt;Initializing worker threads&amp;hellip;&lt;br&gt;&lt;br&gt;Threads started!&lt;br&gt;&lt;br&gt;&lt;br&gt;File operations:&lt;br&gt;    reads/s:                      1665.88&lt;br&gt;    writes/s:                     1110.59&lt;br&gt;    fsyncs/s:                     3563.77&lt;br&gt;&lt;br&gt;Throughput:&lt;br&gt;    read, MiB/s:                  26.03&lt;br&gt;    written, MiB/s:               17.35&lt;br&gt;&lt;br&gt;General statistics:&lt;br&gt;    total time:                          10.0112s&lt;br&gt;    total number of events:              63355&lt;br&gt;&lt;br&gt;Latency (ms):&lt;br&gt;         min:                                    0.00&lt;br&gt;         avg:                                    0.16&lt;br&gt;         max:                                  205.01&lt;br&gt;         95th percentile:                        0.78&lt;br&gt;         sum:                                 9972.64&lt;br&gt;&lt;br&gt;Threads fairness:&lt;br&gt;    events (avg/stddev):           63355.0000/0.00&lt;br&gt;    execution time (avg/stddev):   9.9726/0.00&lt;/p&gt;
&lt;hr&gt;
&lt;tr style=&#34;height: 20px&#34;&gt;&lt;th id=&#34;0R5&#34; style=&#34;height: 20px;&#34; class=&#34;row-headers-background&#34;&gt;&lt;div class=&#34;row-header-wrapper&#34; style=&#34;line-height: 20px&#34;&gt;6&lt;/div&gt;&lt;/th&gt;&lt;td class=&#34;s0&#34; dir=&#34;ltr&#34;&gt;Multi-threaded&lt;/td&gt;&lt;td class=&#34;s0&#34; dir=&#34;ltr&#34;&gt;Running the test with following options:&lt;br&gt;Number of threads: 4&lt;br&gt;Initializing random number generator from current time&lt;br&gt;&lt;br&gt;&lt;br&gt;Initializing worker threads...&lt;br&gt;&lt;br&gt;Threads started!&lt;br&gt;&lt;br&gt;&lt;br&gt;General statistics:&lt;br&gt;    total time:                          10.0002s&lt;br&gt;    total number of events:              197956&lt;br&gt;&lt;br&gt;Latency (ms):&lt;br&gt;         min:                                    0
&lt;h2 id=&#34;score-report-1&#34;&gt;Score Report
&lt;/h2&gt;&lt;pre&gt;&lt;code&gt;sum:                                40050.41&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;Threads fairness:&amp;lt;br&amp;gt;    events (avg/stddev):           4590.0000/94.36&amp;lt;br&amp;gt;    execution time (avg/stddev):   10.0126/0.00

Running the test with following options:
Number of threads: 4
Initializing random number generator from current time&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;
Initializing worker threads...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;
Threads started!&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;
General statistics:
    total time:                          10.0004s
    total number of events:              28536&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;
Latency (ms):
         min:                                    0.23
         avg:                                    1.40
         max:                                    3.56
         95th percentile:                        1.47
         sum:                                39975.16&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;
Threads fairness:&amp;lt;br&amp;gt;    events (avg/stddev):           7134.0000/39.87
    execution time (avg/stddev):   9.9938/0.01
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;epilogue&#34;&gt;Epilogue
&lt;/h2&gt;&lt;p&gt;Whether &lt;code&gt;ChatGPT&lt;/code&gt; is a good thing or not, the table above could not be arranged according to the previously mastered &lt;code&gt;Markdown&lt;/code&gt;, and it was not made into a table to display, which would result in a very poor effect. Customizing the theme limited the maximum page width, and a series of page configurations were adjusted accordingly, changing the width to percentage limits.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A simple method is to use tools like TablesGenerator to generate HTML tables (content complexity is not suitable).&lt;/li&gt;
&lt;li&gt;Or write it using Google Docs online and then download and save it as an HTML document, directly copy it into the blog (simple and direct, ultimately adopted).
Ensure that the config configuration is enabled with unsafe configuration items, and independently configure the page width.
In Hugo, you can set the width of a page individually. This can be achieved by adding a custom parameter in the page&amp;rsquo;s Front Matter.  Here’s an example:&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;In your Markdown page&amp;rsquo;s Front Matter section (usually at the beginning of the file), add a custom parameter such as &lt;code&gt;custom_width&lt;/code&gt;:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;---
title: &amp;quot;My Page&amp;quot;
date: 2024-01-09
custom_width: &amp;quot;800px&amp;quot;  # Set width to 800 pixels
---

Content...
&lt;/code&gt;&lt;/pre&gt;
&lt;ol&gt;
&lt;li&gt;In your Hugo theme, find or create the corresponding single page template file (e.g., &lt;code&gt;layouts/_default/single.html&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;In the single page template, check if there is a &lt;code&gt;custom_width&lt;/code&gt; parameter in the Front Matter and apply it to the appropriate HTML elements, such as &lt;code&gt;div&lt;/code&gt;:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-html&#34;&gt;{{ define &amp;quot;main&amp;quot; }}
  &amp;lt;div style=&amp;quot;max-width: {{ with .Params.custom_width }}{{ . }}{{ else }}100%{{ end }}; margin: 0 auto;&amp;quot;&amp;gt;
    {{ .Content }}
  &amp;lt;/div&amp;gt;
{{ end }}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this example, we used inline styles (the &lt;code&gt;style&lt;/code&gt; attribute) to set the &lt;code&gt;max-width&lt;/code&gt; property for the &lt;code&gt;div&lt;/code&gt; element when no &lt;code&gt;custom_width&lt;/code&gt; parameter is specified, defaulting the width to 100%. &lt;code&gt;margin: 0 auto;&lt;/code&gt; centers the &lt;code&gt;div&lt;/code&gt; element.&lt;/p&gt;
&lt;p&gt;Please note that in actual applications, you may need to adjust this example based on your theme structure and CSS styling details. Ensure that when adjusting styles, you maintain consistency and readability with the theme.&lt;/p&gt;
&lt;p&gt;Due to the slight difference in the enabled theme, the site&amp;rsquo;s custom &lt;code&gt;CSS&lt;/code&gt; configuration was finally adjusted.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Git Disable HTTP Repository</title>
        <link>https://ttf248.life/en/p/git-disable-http-repositories/</link>
        <pubDate>Mon, 08 Jan 2024 21:22:04 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/git-disable-http-repositories/</guid>
        <description>&lt;p&gt;Updated habit software version, unsure which Git version to start from, prohibiting fetching code from &lt;code&gt;Http&lt;/code&gt; repositories.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;fatal: Unencrypted HTTP is not supported for GitLab. Ensure the repository remote URL is using HTTPS
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;background-introduction&#34;&gt;Background Introduction
&lt;/h2&gt;&lt;p&gt;Environment: Windows platform, I’ve always used Tiny Turtle to operate Git, and key configuration was also handled through it. I previously created a script to batch update local repositories.
Previous article link: &lt;a class=&#34;link&#34; href=&#34;https://ttf248.life/en/p/bulk-update-local-git-and-legacy-permissions/&#34; &gt;Batch Update Local Git Repository&lt;/a&gt;
Today when I went home to execute the code update, the previous error occurred, and the repository could no longer be updated normally. I was planning to use &lt;code&gt;Git&lt;/code&gt;’s configuration to continue using the &lt;code&gt;http&lt;/code&gt; protocol to update the repository, but I searched everywhere without finding the corresponding configuration item.
The simplest solution is of course to switch to the &lt;code&gt;ssh&lt;/code&gt; protocol to update the repository, as the &lt;code&gt;gitlab&lt;/code&gt; configured by the company will not provide the &lt;code&gt;https&lt;/code&gt; protocol in the short term.&lt;/p&gt;
&lt;h2 id=&#34;legacy-issues&#34;&gt;Legacy Issues
&lt;/h2&gt;&lt;p&gt;When writing the batch update local repository script previously, we initially planned to use &lt;code&gt;ssh&lt;/code&gt; to pull the repository and didn&amp;rsquo;t investigate thoroughly. The &lt;code&gt;git&lt;/code&gt; configuration information configured via Small Turtle was not synchronized to the config file, resulting in a &amp;ldquo;permission denied&amp;rdquo; error when executing with the command line:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git pull # prompts for permission issues and cannot update the repository normally
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Checking the key configuration using the command was correct: &lt;code&gt;ssh -T git@gitlab.yintech.net&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;If you can successfully pull code using Small Turtle (TortoiseGit), but receive a &amp;ldquo;key not recognized&amp;rdquo; error when using the &lt;code&gt;git pull&lt;/code&gt; command from the command line, this may be because Small Turtle uses PuTTY&amp;rsquo;s SSH key while the command line uses OpenSSH&amp;rsquo;s SSH key.&lt;/p&gt;
&lt;p&gt;The key configuration in Small Turtle does not read key file information from the system &lt;code&gt;.ssh&lt;/code&gt; folder; instead, it configures the path to the key file separately through the interface when configuring the repository. There’s a useful trick: configuring the first pulled repository with its key allows other repositories to reuse that same key file. PuTTY loads the key and doesn&amp;rsquo;t immediately exit but starts a proxy service.&lt;/p&gt;
&lt;p&gt;By adjusting the global configuration to not use the system default &lt;code&gt;ssh&lt;/code&gt; configuration, Git Bash will use TortoisePlink for SSH operations. This configuration is suitable for situations where you are using TortoiseGit with the built-in PuTTY tool.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git config --global core.sshCommand &amp;quot;&#39;C:/Program Files/TortoiseGit/bin/TortoisePlink.exe&#39; -batch -ssh&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Please modify the execution file path in the above configuration to match your Small Turtle installation path. Since the full path is configured, no need to add it to the system environment variables.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>AI Programming and Task Decomposition</title>
        <link>https://ttf248.life/en/p/ai-programming-and-task-decomposition/</link>
        <pubDate>Fri, 22 Dec 2023 08:44:26 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/ai-programming-and-task-decomposition/</guid>
        <description>&lt;p&gt;Two years ago, I added a copy function to the site, which took me half a day of tinkering. Ultimately, the rendering effect wasn&amp;rsquo;t quite satisfactory, and as a somewhat mediocre frontend developer, I didn’t plan on improving it – if it worked, it was good enough. This year, I used &lt;code&gt;AI&lt;/code&gt; to develop a mini-program, which has also made me more familiar with frontend development. I refactored it (the &lt;code&gt;AI&lt;/code&gt; redesigned it).&lt;/p&gt;
&lt;p&gt;Realization: Breaking down tasks really helps; many small tasks are quite natural for the &lt;code&gt;AI&lt;/code&gt; to develop. For the closed mini-program, 75% of the code was provided by &lt;code&gt;AI&lt;/code&gt;, broken down into multiple tasks delivered, and manually assembled the project code.&lt;/p&gt;
&lt;h2 id=&#34;previous-context&#34;&gt;Previous Context
&lt;/h2&gt;&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://ttf248.github.io/post/2022/001/%E4%BB%A5%E4%B8%BA%E5%BE%88%E7%AE%80%E5%8D%95%E7%9A%84%E4%BA%8B%E6%83%85%E5%A2%9E%E5%8A%A0%E4%B8%80%E4%B8%AA%E4%BB%A3%E7%A0%81%E5%A4%8D%E5%88%B6%E6%8C%89%E9%92%AE/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Making simple things easier: Adding a code copy button&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;main-text&#34;&gt;Main Text
&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;promot&lt;/strong&gt;: Design CSS and JavaScript code to locate webpage elements with the class &amp;ldquo;highlight&amp;rdquo;, and within their container, add a copy button to enable copying of the content.&lt;/p&gt;
&lt;p&gt;The first draft of the code was immediately problematic; I generated a revised version that is largely functional, but the copied content is incorrect, containing redundant text. Further guidance led to optimization, specifying core color design and effects.&lt;/p&gt;
&lt;p&gt;Having learned your specific requirements for button size and color, we redesigned the button styles to accommodate these demands.  Here’s the adjusted CSS style sheet, incorporating your provided color scheme and optimizing the button size:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-css&#34;&gt;.copy-btn {
  position: absolute;
  top: 2px; /* Adjust position */
  right: 2px;
  padding: 3px 6px; /* Reduce size */
  font-size: 12px; /* Decrease font size */
  background-color: #8FD1E1; /* Button background color */
  color: #272822; /* Text color */
  border: 1px solid #272822; /* Border color */
  border-radius: 3px; /* Rounded corners */
  cursor: pointer;
  opacity: 0.7; /* Default semi-transparent */
  transition: opacity 0.3s ease, background-color 0.3s ease; /* Smooth transition effect */
}

.copy-btn:hover {
  opacity: 1; /* Unhide on hover */
  background-color: #A3D4E5; /* Background color on hover */
}

.highlight {
  position: relative;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this style sheet, I reduced the button&amp;rsquo;s size and font size to create a more refined look. The button’s default state is semi-transparent, becoming fully opaque when hovered over, with a slightly brighter background color for visual feedback. This design meets your color requirements while maintaining modern and user-friendly characteristics.&lt;/p&gt;
&lt;p&gt;Here’s a simpler &lt;code&gt;JavaScript&lt;/code&gt; implementation:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;document.addEventListener(&#39;DOMContentLoaded&#39;, function() {
  // Find all .highlight elements
  const highlights = document.querySelectorAll(&#39;.highlight&#39;);

  highlights.forEach(function(highlight) {
    // Create copy button
    const copyBtn = document.createElement(&#39;button&#39;);
    copyBtn.textContent = &#39;Copy&#39;;
    copyBtn.classList.add(&#39;copy-btn&#39;);

    // Add the button to each .highlight element
    highlight.appendChild(copyBtn);

    // Add click event listener
    copyBtn.addEventListener(&#39;click&#39;, function() {
      // Assume the first &amp;lt;code&amp;gt; element within .highlight contains the code to copy
      const code = highlight.querySelector(&#39;code&#39;) ? highlight.querySelector(&#39;code&#39;).textContent : &#39;&#39;;

      navigator.clipboard.writeText(code).then(() =&amp;gt; {
        // Display success message
        copyBtn.textContent = &#39;Copied!&#39;;
        setTimeout(() =&amp;gt; {
          copyBtn.textContent = &#39;Copy&#39;;
        }, 2000);
      }).catch(err =&amp;gt; {
        console.error(&#39;Copy failed: &#39;, err);
      });
    });
  });
});
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;epilogue&#34;&gt;Epilogue
&lt;/h2&gt;&lt;p&gt;While I had hoped to see a copy button added in the top-right corner of the code area, due to my lack of familiarity with frontend development, I was unable to accurately describe the functionality based on keywords and spent several rounds iterating with &lt;code&gt;AI&lt;/code&gt; without achieving the desired results.&lt;/p&gt;
&lt;p&gt;It remains to be seen what impact &lt;code&gt;AI&lt;/code&gt; will have on software development as time goes on.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Cloud Servers and Script Kiddies</title>
        <link>https://ttf248.life/en/p/cloud-servers-and-script-kids/</link>
        <pubDate>Mon, 20 Nov 2023 15:08:53 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/cloud-servers-and-script-kids/</guid>
        <description>&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Data Mining&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Deep Learning&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Neural Network&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;An Alibaba Cloud server was recently purchased during the Double Eleven event: an economy version with a price of 99 per year and low configuration, which is used as a jumpboard to proxy home services, it’s also a decent option. The activity lasts until 2026.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Specifically, a Shanghai region server was selected to minimize latency when proxying home machines. Windows 11 and Windows Server 2022 were installed, with the server version being deployed later.  Suddenly, a &amp;ldquo;access denied&amp;rdquo; message appeared, initially assuming it was due to a server update that would resolve itself. After five minutes, attempting to connect again still resulted in denial of login.  Searching for related error messages indicated that someone was attempting to log in, and excessive incorrect password attempts were preventing access.
I had previously encountered security attack scripts, so I immediately suspected a brute-force login attempt by malicious actors. The firewall settings were simplified, without enabling whitelisting, exposing ports 3389 for two machines publicly, much like bait in a fish pond.  Once identified as being targeted by script kiddies, the next steps were straightforward: setting up a firewall whitelist to allow only the company’s and home network IP addresses to access the proxy service.
&lt;code&gt;frps&lt;/code&gt; proxy server previously had no logging configuration, but after enabling logging, it was quite amusing – all sorts of domestic and international IP addresses were attempting to log into the home server. Fortunately, there was one server version running, which made me realize that if the Windows 11 machine hadn’t been targeted, it would have eventually been compromised due to the relatively simple password settings.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;2023/11/17 16:51:14 [I] [proxy.go:204] [639d8947325142ac] [host-remote] get a user connection [101.43.98.211:50486]
2023/11/17 16:51:14 [I] [proxy.go:204] [639d8947325142ac] [host-remote] get a user connection [218.93.202.63:56970]
2023/11/17 16:51:14 [I] [proxy.go:204] [639d8947325142ac] [host-remote] get a user connection [222.179.106.174:60812]
2023/11/17 16:51:15 [I] [proxy.go:204] [639d8947325142ac] [host-remote] get a user connection [58.16.204.238:2839]
2023/11/17 16:51:15 [I] [proxy.go:204] [639d8947325142ac] [host-remote] get a user connection [124.223.47.24:50274]
2023/11/17 16:51:16 [I] [proxy.go:204] [639d8947325142ac] [host-remote] get a user connection [43.248.128.22:55883]
2023/11/17 16:51:16 [I] [proxy.go:204] [639d8947325142ac] [host-remote] get a user connection [43.143.53.138:56955]
2023/11/17 16:51:16 [I] [proxy.go:204] [639d8947325142ac] [host-remote] get a user connection [43.228.7.250:61550]
2023/1

```shell
Nov 16 04:46:34 aliyun-sh sshd[156625]: Failed password for root from 120.55.164.64 port 53410 ssh2
Nov 16 04:46:34 aliyun-sh sshd[156623]: Failed password for root from 111.16.215.122 port 36548 ssh2
Nov 16 04:46:58 aliyun-sh sshd[156630]: Failed password for invalid user share from 139.9.233.78 port 53872 ssh2
Nov 16 04:47:23 aliyun-sh sshd[156634]: Failed password for invalid user spark from 139.9.233.78 port 36134 ssh2
Nov 16 04:47:26 aliyun-sh sshd[156636]: Failed password for root from 120.55.164.64 port 46142 ssh2
Nov 16 04:47:47 aliyun-sh sshd[156640]: Failed password for root from 111.16.215.122 port 42962 ssh2
Nov 16 04:48:24 aliyun-sh sshd[156652]: Failed password for root from 120.55.164.64 port 38868 ssh2
Nov 16 04:48:25 aliyun-sh sshd[156654]: Failed password for root from 111.16.215.122 port 46164 ssh2
Nov 16 04:48:39 aliyun-sh sshd[156657]: Failed password for invalid user test from 139.9.233.78 port 39386 ssh2
Nov 16 04:48:50 aliyun-sh sshd[156659]: Failed password for root from 111.16.215.122 port 38892 ssh2
Nov 16 04:48:53 aliyun-sh sshd[156662]: Failed password for root from 120.55.164.64 port 49348 ssh2
Nov 16 04:48:53 aliyun-sh sshd[156664]: Failed password for invalid user test from 139.9.233.78 port 49864 ssh2
Nov 16 04:50:02 aliyun-sh sshd[156672]: Failed password for root from 111.16.215.122 port 45294 ssh2
Nov 16 04:50:30 aliyun-sh sshd[156680]: Failed password for invalid user zabbix from 139.9.233.78 port 52206 ssh2
Nov 16 04:50:50 aliyun-sh sshd[156683]: Failed password for root from 120.55.164.64 port 34820 ssh2
Nov 16 04:50:51 aliyun-sh sshd[156685]: Failed password for root from 111.16.215.122 port 58978 ssh2
Nov 16 04:51:18 aliyun-sh sshd[156689]: Failed password for root from 120.55.164.64 port 45306 ssh2
Nov 16 04:51:25 al

## Epilogue
Developing a self-hosted server requires setting up a whitelist for public Windows access, and on Linux systems, it’s recommended to disable password logins and enable key file login.&lt;/code&gt;&lt;/pre&gt;</description>
        </item>
        <item>
        <title>- Batch update local Git repositories and resolve legacy permission issues.</title>
        <link>https://ttf248.life/en/p/bulk-update-local-git-and-legacy-permissions/</link>
        <pubDate>Thu, 19 Oct 2023 14:16:22 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/bulk-update-local-git-and-legacy-permissions/</guid>
        <description>&lt;p&gt;The projects within the team have dependencies on each other, and due to historical reasons, submodules haven’t been used to manage these project dependencies. Daily development requires manually updating the repository code one by one, otherwise various strange issues may arise.&lt;/p&gt;
&lt;p&gt;Referring to online resources, the structure is generally similar. A local manual repository directory (&lt;strong&gt;git_list.txt&lt;/strong&gt;) is maintained, and a script iterates through the directories to perform an update in one go. Before starting each project, this script needs to be executed.&lt;/p&gt;
&lt;h2 id=&#34;linux&#34;&gt;linux
&lt;/h2&gt;&lt;p&gt;create new file: batch_pull.sh&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;#!/bin/bash

echo &amp;quot;============ Updating Repository ===================&amp;quot;

# Check if git_list.txt exists
if [ ! -f &amp;quot;git_list.txt&amp;quot; ]; then
  echo &amp;quot;git_list.txt file does not exist! Please create and add the Git repository URLs to pull.&amp;quot;
  exit 1
else
  echo &amp;quot;============ Detected Git Repository List File ===================&amp;quot;
fi

# Read each URL from git_list.txt and execute the pull operation
while read -r url; do
  if [ -d &amp;quot;$url&amp;quot; ]; then
    cd &amp;quot;$url&amp;quot; || continue
    git pull
    cd ..
    echo &amp;quot;Pull $url completed!&amp;quot;
    echo &amp;quot;========================================&amp;quot;
  else
    echo &amp;quot;Directory $url does not exist, skipping pull.&amp;quot;
  fi
done &amp;lt; &amp;quot;git_list.txt&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;windows&#34;&gt;Windows
&lt;/h2&gt;&lt;p&gt;Create a new file: batch_pull.bat&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-batch&#34;&gt;@echo off
chcp 65001 &amp;gt; nul
rem Enter the directory of the script
cd /d &amp;quot;%~dp0&amp;quot;

rem Check if git_list.txt exists
if not exist &amp;quot;git_list.txt&amp;quot; (
  echo git_list.txt file does not exist! Please create and add the Git repository URLs you want to pull.
  exit /b 1
) else (
  echo ============ Detected Git repository list file =========
)

rem Read each URL from git_list.txt and execute the pull operation
for /f %%i in (git_list.txt) do (
  if exist &amp;quot;%%i&amp;quot; (
    pushd &amp;quot;%%i&amp;quot;
    git pull
    popd
    echo Pull %%i completed!
    echo ========================================
  ) else (
    echo Directory %%i does not exist, skipping pull.
  )
)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;historical-issues&#34;&gt;Historical Issues
&lt;/h3&gt;&lt;p&gt;Also addressed the &lt;code&gt;git&lt;/code&gt; folder permission files encountered after reinstalling the system: &lt;strong&gt;Fatal error &amp;ldquo;unsafe repository (&amp;rsquo;/home/repon&amp;rsquo; is owned by someone else)&amp;rdquo;&lt;/strong&gt;.
Most suggested solutions online originate from &lt;code&gt;stack overflow&lt;/code&gt;:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Add trust to the repository directory: &lt;code&gt;git config --global --add safe.directory /home/repon&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Manually modify the configuration file &lt;code&gt;.gitconfig&lt;/code&gt;, specifying the directory to add trust&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;[safe]
    directory = /home/repon
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After using this method, repository updates are normal, but there are many warning messages displayed in the console every time &lt;code&gt;git pull&lt;/code&gt; is executed, indicating owner errors.&lt;/p&gt;
&lt;h3 id=&#34;desktop-system-reinstallation&#34;&gt;Desktop System Reinstallation
&lt;/h3&gt;&lt;p&gt;Machines developed for a long time without system reinstallation, the system partition contained an explosion of garbage files, so I took some time to reinstall the system and encountered this permission issue again. Previous scripts would not run because the permissions were incomplete.&lt;/p&gt;
&lt;p&gt;Using the new approach, directly add &lt;code&gt;*&lt;/code&gt;, so that &lt;code&gt;git&lt;/code&gt; automatically trusts all directories.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git config --global --add safe.directory &amp;quot;*&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;p&gt;It is suspected to be a user permission problem, or whether everyone has not adapted to the &lt;code&gt;windows&lt;/code&gt; platform. There are similar &lt;code&gt;chown&lt;/code&gt; commands.  You can modify folder ownership. Of course, if your directories are few, manually modifying ownership also works. However, this work computer has added domain information. I don&amp;rsquo;t know if it&amp;rsquo;s an abnormal domain deployed by the company or whether there is an anomaly in the local system settings. The user list cannot find the user used for login, and finally processed through command-line operations.&lt;/p&gt;
&lt;p&gt;With administrator permissions, execute the &lt;code&gt;powershell&lt;/code&gt; script &lt;code&gt;change_ower.ps1&lt;/code&gt;, remember to adjust the script file encoding to &lt;code&gt;gbk&lt;/code&gt; so that it doesn&amp;rsquo;t display garbled characters in Chinese operating systems.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-powershell&#34;&gt;# Get the current user&#39;s username
$currentUserName = [System.Security.Principal.WindowsIdentity]::GetCurrent().Name

# Set PowerShell&#39;s character encoding to UTF-8
[Console]::OutputEncoding = [System.Text.Encoding]::UTF8

# The root directory path to change ownership
$rootDirectory = &amp;quot;G:\workspace&amp;quot;  # Replace with the actual directory path

# Recursively iterate through directories and change file and folder owners
Get-ChildItem -Path $rootDirectory -Recurse | ForEach-Object {
    $itemPath = $_.FullName

    # Check if it&#39;s a file or a folder
    if ($_ -is [System.IO.DirectoryInfo]) {
        # If it&#39;s a folder, use icacls to change the owner permission
        $icaclsResult = icacls $itemPath /setowner &amp;quot;$currentUserName&amp;quot; 2&amp;gt;&amp;amp;1
        if ($LASTEXITCODE -eq 0) {
            Write-Host &amp;quot;Changed the owner of folder $itemPath to $currentUserName&amp;quot;
        } else {
            Write-Host &amp;quot;Unable to change the owner of folder $itemPath. Error information: $icaclsResult&amp;quot;
        }
    } else {
        # If it&#39;s a file, use icacls to change the owner permission
        $takeownResult = icacls $itemPath /setowner &amp;quot;$currentUserName&amp;quot; 2&amp;gt;&amp;amp;1
        if ($LASTEXITCODE -eq 0) {
            # Write-Host &amp;quot;Changed the owner of file $itemPath to $currentUserName&amp;quot;
        } else {
            Write-Host &amp;quot;Unable to change the owner of file $itemPath. Error information: $takeownResult&amp;quot;
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Unexpected situations still occurred, and the Chinese information output by the script was garbled. I tried setting the console character encoding and adjusting the script encoding, but the output was all garbled.  It is likely that my brain wasn&amp;rsquo;t clear at all. I tried enabling the beta feature in Control Panel - Region - Language Settings to globally enable Unicode encoding, and the script executed normally. Several development software programs could not work properly. Later, when reviewing materials, I remembered to adjust the script file encoding to &lt;code&gt;gbk&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&#34;resources&#34;&gt;Resources
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://ganzhixiong.com/p/f1b9f4fc/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://ganzhixiong.com/p/f1b9f4fc/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://stackoverflow.com/questions/71901632/fatal-error-unsafe-repository-home-repon-is-owned-by-someone-else&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://stackoverflow.com/questions/71901632/fatal-error-unsafe-repository-home-repon-is-owned-by-someone-else&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
        </item>
        <item>
        <title>WPF Learning Resources</title>
        <link>https://ttf248.life/en/p/wpf-learning-resources/</link>
        <pubDate>Tue, 17 Oct 2023 10:49:24 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/wpf-learning-resources/</guid>
        <description>&lt;p&gt;The potholes in the mini-program development haven’t been filled, and we’ve just dug a new one with &lt;strong&gt;WPF&lt;/strong&gt;. Recently, the company has been experiencing some turbulence, and remote collaboration communication is invariably less efficient than desired. So, we&amp;rsquo;ve taken on the development of client interfaces.&lt;/p&gt;
&lt;h2 id=&#34;wpf&#34;&gt;WPF
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://learn.microsoft.com/zh-cn/dotnet/desktop/wpf/introduction-to-wpf?view=netframeworkdesktop-4.8&amp;amp;preserve-view=true&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;WPF Microsoft Official Learning Resources&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.cnblogs.com/zh7791/p/11502696.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;WPF Basic Summary (Learning Suggestions)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;http://www.wpfsoft.com/introduction&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;WPF Chinese Website&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://zhuanlan.zhihu.com/p/638815741&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;WPF Personal Summary and Learning Recommendations&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;WPF&lt;/code&gt; interface design uses many concepts similar to web frontend design, striving to isolate UI design from business logic as much as possible, which is the desired division of labor in internet companies. This year I just finished tinkering with Mini Programs, and many concepts are common, making it relatively easy to get started. These are considered the “Dao” in modern UI design – mastering the basic framework concepts makes the path less prone to deviation.&lt;/p&gt;
&lt;p&gt;For readers who have previous &lt;code&gt;WinForms&lt;/code&gt; development experience, it is recommended to read: &lt;a class=&#34;link&#34; href=&#34;https://www.cnblogs.com/zh7791/p/11502696.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;WPF Basic Summary (Learning Suggestions)&lt;/a&gt;. The content is not long and is suitable for experienced readers to plan their learning path.&lt;/p&gt;
&lt;p&gt;For beginners, it is recommended to read: &lt;a class=&#34;link&#34; href=&#34;http://www.wpfsoft.com/introduction&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;WPF Chinese Website&lt;/a&gt;, which introduces basic concepts, the development history, and the logical reasoning of underlying classes from scratch. This website happens to be quite lucky – the author just released it in August this year, timed perfectly with my content to attract readers to purchase courses. If I waited any later, I’d probably have no chance.&lt;/p&gt;
&lt;p&gt;For the most authoritative learning materials, of course, Microsoft&amp;rsquo;s official resources are preferred, but they can be a bit dry, and new learners need patience.
Classic electronic books also exist, but they are not recommended; there isn&amp;rsquo;t much time to sit down and read them in daily work, so it’s more suitable to practice with projects.&lt;/p&gt;
&lt;h2 id=&#34;c-and-net-release-history&#34;&gt;C# and .NET Release History
&lt;/h2&gt;&lt;p&gt;Regarding previous learning languages, there have been a number of new features released in recent years, and the versioning of syntax has been iterating annually.
&lt;a class=&#34;link&#34; href=&#34;https://en.wikipedia.org/wiki/C_Sharp_%28programming_language%29&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://en.wikipedia.org/wiki/C_Sharp_(programming_language)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Official Learning Resources:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://learn.microsoft.com/zh-cn/dotnet/csharp/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://learn.microsoft.com/zh-cn/dotnet/csharp/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://learn.microsoft.com/zh-cn/dotnet/core/tutorials/with-visual-studio?pivots=dotnet-7-0&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://learn.microsoft.com/zh-cn/dotnet/core/tutorials/with-visual-studio?pivots=dotnet-7-0&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
        </item>
        <item>
        <title>ZeroTier VPN</title>
        <link>https://ttf248.life/en/p/zero-tier-remote-lan/</link>
        <pubDate>Tue, 19 Sep 2023 04:58:03 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/zero-tier-remote-lan/</guid>
        <description>&lt;ul&gt;
&lt;li&gt;I recently got a &lt;code&gt;mini&lt;/code&gt; host for the office, thinking it would be convenient to configure an environment and have occasional access at home. I temporarily deployed internal network penetration using &lt;code&gt;frp&lt;/code&gt; – specifying port forwarding, which requires a public server with a connection quality dependent on its bandwidth. Instead, I experimented with a fresh &lt;code&gt;Zerotier&lt;/code&gt; virtual LAN, similar to a &lt;code&gt;VPN&lt;/code&gt;, where I created a virtual network card locally and all machines joined it into a single virtual network.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;what-is-zerotier&#34;&gt;What is ZeroTier
&lt;/h2&gt;&lt;p&gt;&lt;code&gt;ZeroTier&lt;/code&gt; is a software-defined wide area network (SD-WAN) solution that allows users to create secure virtual networks between devices in different geographic locations. Through &lt;code&gt;ZeroTier&lt;/code&gt;, you can easily connect multiple computers, servers, and devices into a virtual, encrypted network – as if they were on the same local network. This helps developers and IT professionals securely share data and resources across different locations without complex network setups or VPN configurations.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;ZeroTier Networks&lt;/strong&gt;: A ZeroTier network is a virtual, global LAN that allows different devices to connect together over the internet, as if they were on the same physical network. This network can contain multiple subnets, with all devices connected through ZeroTier technology.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Planet Servers&lt;/strong&gt;: Planet servers are a key component of the ZeroTier network. They are global and responsible for maintaining and managing the entire ZeroTier network topology, routing information, and network status. The planet server acts as a central control center for the global network, without directly transferring data. User devices need to connect to at least one planet server to participate in the ZeroTier network.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Transit Servers&lt;/strong&gt;: Transit servers are auxiliary nodes within the ZeroTier network that help establish direct communication channels between devices. When devices cannot connect directly, they can use transit servers to transmit data. This helps improve network reachability and performance. Transit servers are typically located around the world, acting as data transmission hubs.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In essence, ZeroTier uses the assistance of planet servers and transit servers to enable devices to create virtual local networks globally, achieving secure and fast communication between devices. The planet server is responsible for global network management, while the transit server helps devices establish connections when needed.&lt;/p&gt;
&lt;h2 id=&#34;installation--deployment&#34;&gt;Installation &amp;amp; Deployment
&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;Visit the ZeroTier official website (&lt;a class=&#34;link&#34; href=&#34;https://www.zerotier.com/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://www.zerotier.com/&lt;/a&gt;) to obtain installation files and documentation.&lt;/li&gt;
&lt;li&gt;Download and install the ZeroTier One client according to your operating system. It supports Windows, macOS, Linux, and many other platforms.&lt;/li&gt;
&lt;li&gt;Launch the ZeroTier One client after installation.&lt;/li&gt;
&lt;li&gt;Register a ZeroTier account if you don&amp;rsquo;t already have one. You can create an account within the client.&lt;/li&gt;
&lt;li&gt;Log in with your ZeroTier account and create a new network. This network will have a unique 16-character ID, which you need to remember.&lt;/li&gt;
&lt;li&gt;Join this network on your device. You can either enter the network ID in the client or use the QR code scanning feature.&lt;/li&gt;
&lt;li&gt;Devices installed and configured with the ZeroTier client will be added to the same virtual network. These devices can now communicate directly with each other as if they were on the same local area network.&lt;/li&gt;
&lt;li&gt;You can manage network settings, add devices, and monitor network traffic in the ZeroTier control panel.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;installing-and-deploying-moon&#34;&gt;Installing and Deploying Moon
&lt;/h2&gt;&lt;p&gt;Many domestic operators have banned &lt;code&gt;UDP&lt;/code&gt; tunneling, and the &lt;code&gt;frp&lt;/code&gt; service is stable. Due to using the &lt;code&gt;TCP&lt;/code&gt; protocol, deploying &lt;code&gt;Zerotier&lt;/code&gt; intermediate servers can achieve similar effects. The firewall needs to open &lt;code&gt;udp 9993&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;curl -s https://install.zerotier.com/ | sudo bash
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Check installation success:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;zerotier-cli info
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Join the local network:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;zerotier-cli join network-id
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Create &lt;code&gt;moon&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd /var/lib/zerotier-one &amp;amp;&amp;amp; sudo zerotier-idtool initmoon identity.public &amp;gt; moon.json
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Edit the configuration file, adjust the &lt;code&gt;stableEndpoints&lt;/code&gt; node, &amp;ldquo;server public IP/9993&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Generate a signature configuration, create the &lt;code&gt;moons.d&lt;/code&gt; folder, move the previous files to this folder, and restart the service:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;sudo zerotier-idtool genmoon moon.json
mkdir moons.d &amp;amp;&amp;amp; mv 000000eb444ec0d8.moon moons.d/
systemctl restart zerotier-one.service
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Client nodes join the &lt;code&gt;moon&lt;/code&gt; server, taking the ID from the JSON configuration file&amp;rsquo;s &lt;code&gt;id&lt;/code&gt; field:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;zerotier-cli.bat orbit ztaddr ztaddr
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# Observe whether new moon nodes appear, with IDs and information matching the server configuration
[root@idv-36f9d5 ~]# zerotier-cli listpeers
200 listpeers &amp;lt;ztaddr&amp;gt; &amp;lt;path&amp;gt; &amp;lt;latency&amp;gt; &amp;lt;version&amp;gt; &amp;lt;role&amp;gt;
200 listpeers 0cccb***** 35.236.*.*/64393;110;10726 327 1.6.3 LEAF
200 listpeers 3a46f***** 185.180.*.*/9993;110;757 -1 - PLANET
200 listpeers 3ed7c***** 39.97.*.*/9993;172;79 32 1.6.3 MOON
200 listpeers 4f838***** - -1 - LEAF
200 listpeers 62f86***** 50.7.*.*/9993;110;4796 351 - PLANET
200 listpeers 778cd***** 103.195.*.*/9993;5148;4887 253 - PLANET
200 listpeers 992fc***** 195.181.*.*/9993;10161;4921 226 - PLANET
200 listpeers 9d2b5***** - -1 - LEAF
&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;On the Windows platform, start the terminal with administrator privileges and use the &lt;code&gt;zerotier-cli.bat&lt;/code&gt; command-line interface. On the Linux platform, use the &lt;code&gt;zerotier-cli&lt;/code&gt; interface. The &lt;code&gt;listpeers&lt;/code&gt; subcommand displays connected nodes and shows all nodes when using &lt;code&gt;listpeers&lt;/code&gt;, indicating a successful join.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;uninstalling&#34;&gt;Uninstalling
&lt;/h2&gt;&lt;p&gt;How to uninstall on the &lt;code&gt;Windows&lt;/code&gt; platform is beyond the scope of this document, as it follows standard operating procedures – typically through the Control Panel. We will focus on the &lt;code&gt;Ubuntu&lt;/code&gt; instructions:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Remove the zerotier-one service using dpkg:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;sudo dpkg -P zerotier-one
&lt;/code&gt;&lt;/pre&gt;
&lt;ol&gt;
&lt;li&gt;Delete the zerotier-one directory, which stores the address information; deleting it will result in a new address upon reinstallation:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;sudo rm -rf /var/lib/zerotier-one/
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;epilogue&#34;&gt;Epilogue
&lt;/h2&gt;&lt;p&gt;They were originally all decommissioned, and when the servers arrived, there weren’t suitable services as proxy nodes. Alibaba was doing sales promotion, providing development trial servers with low configurations, priced affordably in 1999, and used them for two years. What was valued was the bandwidth provided by the servers.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.wnark.com/archives/152.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://www.wnark.com/archives/152.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.cnblogs.com/Yogile/p/12642423.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://www.cnblogs.com/Yogile/p/12642423.html&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>VMware Virtual Machine Disk Space Optimization</title>
        <link>https://ttf248.life/en/p/vmware-virtual-machine-disk-space-optimization/</link>
        <pubDate>Wed, 21 Jun 2023 18:35:41 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/vmware-virtual-machine-disk-space-optimization/</guid>
        <description>&lt;p&gt;When installing a development system with VMware virtual machines, it’s generally recommended to allocate extra disk space. Over time, the local disk space consumed by the VM will far exceed the actual size of its files.&lt;/p&gt;
&lt;h2 id=&#34;scenario-description&#34;&gt;Scenario Description
&lt;/h2&gt;&lt;p&gt;The &lt;code&gt;df -h&lt;/code&gt; command revealed that the current machine was using 60GB of disk space, and after deleting all snapshots and clone images, the local virtual machine still occupied significantly more than 60GB, further straining the already limited hard drive.&lt;/p&gt;
&lt;h2 id=&#34;prerequisites&#34;&gt;Prerequisites
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;When installing the virtual machine, do not select &amp;ldquo;pre-allocate disk.&amp;rdquo;&lt;/li&gt;
&lt;li&gt;The local hard drive where the virtual machine is stored must have sufficient free disk space greater than the currently used space of the virtual machine.&lt;/li&gt;
&lt;li&gt;If there isn&amp;rsquo;t enough space, consider temporarily moving the virtual machine to a portable hard drive, optimize the disk, and then migrate it back.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;tools&#34;&gt;Tools
&lt;/h2&gt;&lt;p&gt;The official provides the &lt;code&gt;open-vm-tools&lt;/code&gt; package, which can be installed via yum or by installing the VMware-Tools image package.&lt;/p&gt;
&lt;h2 id=&#34;commands&#34;&gt;Commands
&lt;/h2&gt;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;vmware-toolbox-cmd disk shrink /
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After execution, the virtual machine will automatically shut down, and the VMware host program will perform disk compression. The execution time depends on the volume of the virtual machine and the speed of the disk access.&lt;/p&gt;
&lt;p&gt;The effect is quite good; the disk space occupied by the virtual machine is essentially equal to the disk information as shown in &lt;code&gt;df -h&lt;/code&gt;.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Stable Diffusion – The Love, Hate, and Drama of Installing it from Scratch</title>
        <link>https://ttf248.life/en/p/stable-diffusion-zero-install-story/</link>
        <pubDate>Thu, 13 Apr 2023 00:23:54 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/stable-diffusion-zero-install-story/</guid>
        <description>&lt;p&gt;Domestic resources are basically all recommending &lt;strong&gt;Autumn Leaf&lt;/strong&gt;’s one-click deployment package, thinking that they are open-source projects based on &lt;code&gt;Python&lt;/code&gt;, so deployment wouldn&amp;rsquo;t be very complicated, let&amp;rsquo;s try to start from scratch.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;I was messing around with AI-generated images and specifically changed my graphics card, a beginner version of the &lt;code&gt;3060 12g&lt;/code&gt;; the seven-year-old &lt;code&gt;960&lt;/code&gt; retired gloriously.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The core &lt;code&gt;pytorch cuda&lt;/code&gt; installation, which I previously encountered issues with when writing Python game helper scripts (I had installed it locally before), still presented problems – the &lt;code&gt;cuda&lt;/code&gt; encryption consistently failed to activate.&lt;/p&gt;
&lt;h2 id=&#34;to-do&#34;&gt;To Do
&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;Replan the article structure, first introduce PyTorch, version correspondence, and how to check versions.&lt;/li&gt;
&lt;li&gt;How to create a new virtual environment from scratch locally and deploy PyTorch.&lt;/li&gt;
&lt;li&gt;Translate the manuscript from scratch: &lt;a class=&#34;link&#34; href=&#34;https://stable-diffusion-art.com/install-windows/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://stable-diffusion-art.com/install-windows/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Organize reference materials&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;steps&#34;&gt;Steps
&lt;/h2&gt;&lt;p&gt;Step-by-step installation tutorials in Chinese may not be readily available. When you search in English on &lt;code&gt;Google&lt;/code&gt;, you’ll find many similar tutorials starting from scratch. After a brief introduction, we need to install &lt;code&gt;git&lt;/code&gt; and then explain the need to install &lt;code&gt;python&lt;/code&gt;. Then, you go ahead and download the repository – simply double-clicking the script does the trick.&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/AUTOMATIC1111/stable-diffusion-webui&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/AUTOMATIC1111/stable-diffusion-webui&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;For detailed usage and Q&amp;amp;A, consult the &lt;code&gt;issues&lt;/code&gt;, &lt;a class=&#34;link&#34; href=&#34;https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki&lt;/a&gt;. I don’t know why no one explains what this repository is for. Actually, the name itself isn&amp;rsquo;t difficult to understand – it’s a graphical control console that makes it easier to use. During installation, it downloads the official repository content and obtains the actual &lt;code&gt;SD&lt;/code&gt; code.&lt;/p&gt;
&lt;p&gt;The repository also provides an installation and startup script that automatically recognizes the current folder and whether there is a &lt;code&gt;Python&lt;/code&gt; virtual environment. If one exists, it defaults to using the Python in the current path.&lt;/p&gt;
&lt;p&gt;For beginner users, we recommend checking out: &lt;a class=&#34;link&#34; href=&#34;https://stable-diffusion-art.com/install-windows/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://stable-diffusion-art.com/install-windows/&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;pytorch&#34;&gt;PyTorch
&lt;/h2&gt;&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://pytorch.org/get-started/locally/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://pytorch.org/get-started/locally/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Here’s what I wanted to talk about today. Don&amp;rsquo;t just follow their steps and run the script directly. Python uses requirement files to install dependencies, which is a minor issue. The core thing is your GPU version and driver version, which need to match PyTorch. Many people have discussed this relationship online – you can find it by searching.&lt;/p&gt;
&lt;p&gt;Refer to: &lt;a class=&#34;link&#34; href=&#34;https://blog.csdn.net/weixin_40660408/article/details/129896700&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://blog.csdn.net/weixin_40660408/article/details/129896700&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Creating a virtual environment is like creating an empty virtual environment, where you first execute the official script and install PyTorch within it.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -c &amp;quot;import torch; print(torch.version.cuda)&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -c &amp;quot;import torch; print(torch.__version__, torch.cuda.is_available())&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The above two scripts can check the CUDA version you need to install and also check if PyTorch has been installed successfully.&lt;/p&gt;
&lt;p&gt;It’s not recommended to do fancy operations here – just follow the logic on the official page and copy it over directly to install.  Directly using &lt;code&gt;pip&lt;/code&gt; to install PyTorch is likely to fail or won&amp;rsquo;t activate CUDA.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Key point: Don’t use messy folder names, as this could cause PyTorch to not work correctly. I spent a lot of time installing and reinstalling, trying to upgrade to version 2.0 because the official documentation said it would be faster.  I hadn&amp;rsquo;t used it much before, and wasn&amp;rsquo;t sure if Python versions had an impact. I also reviewed the official manual, which recommended using version 3.8. This created a small conflict since I’d previously used a one-click installation package that contained version 3.10. Finally, I started from scratch by creating a new folder, creating a virtual environment, and ensuring PyTorch was installed successfully.&lt;/p&gt;
&lt;p&gt;Then I moved the newly installed virtual environment into the web UI folder.  At this point, running the script to install other dependencies didn’t cause any problems.&lt;/p&gt;
&lt;p&gt;After moving it, you need to execute: &lt;code&gt;python -m pip install --upgrade --force-reinstall pip&lt;/code&gt; to fix Pip.&lt;/p&gt;
&lt;p&gt;It might seem a bit strange, but I spent quite a long time troubleshooting this because it couldn&amp;rsquo;t correctly recognize my PyTorch.  I realized that by installing it first, then installing other dependencies, I could eliminate all sources of interference.&lt;/p&gt;
&lt;h2 id=&#34;xformers&#34;&gt;Xformers
&lt;/h2&gt;&lt;p&gt;It is recommended to enable this, which can accelerate image generation and reduce memory usage. However, a side effect is that &lt;strong&gt;generated images are relatively less stable&lt;/strong&gt; with the same set of parameters.
&lt;a class=&#34;link&#34; href=&#34;https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Xformers&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;stable-diffusion-webui:Xformers&lt;/a&gt;
&lt;a class=&#34;link&#34; href=&#34;https://huggingface.co/docs/diffusers/optimization/xformers&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;huggingface optimization&lt;/a&gt;
| 100.00% | 2m 57.03s | 7440/10058 MiB | 12288/12288 MiB (100.0%) |&lt;/p&gt;
&lt;h2 id=&#34;xformers-1&#34;&gt;Xformers
&lt;/h2&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Optimization Ratio&lt;/th&gt;
&lt;th&gt;Time taken&lt;/th&gt;
&lt;th&gt;Torch active/reserved&lt;/th&gt;
&lt;th&gt;Sys VRAM&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;51.02%&lt;/td&gt;
&lt;td&gt;1m 29.21s&lt;/td&gt;
&lt;td&gt;4547/7164 MiB&lt;/td&gt;
&lt;td&gt;9298/12288 MiB (75.67%)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;xformers-2&#34;&gt;Xformers
&lt;/h2&gt;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;((masterpiece)),((best quality)),((high detail)),((realistic,))
Industrial age city, deep canyons in the middle, Chinese architectural streets, bazaars, Bridges, (rainy days:1.2), (steampunk:0.8), Chinese architecture
Negative prompt: nsfw,((cowboy)),(((pubic))), ((((pubic_hair))))sketch, duplicate, ugly, huge eyes, text, logo, monochrome, worst face, (bad and mutated hands:1.3), (worst quality:2.0), (low quality:2.0), (blurry:2.0), horror, geometry, bad_prompt, (bad hands), (missing fingers), multiple limbs, bad anatomy, (interlocked fingers:1.2), Ugly Fingers, (extra digit and hands and fingers and legs and arms:1.4), crown braid, ((2girl)), (deformed fingers:1.2), (long fingers:1.2), succubus wings,horn,succubus horn,succubus hairstyle, (bad-artist-anime), bad-artist, bad hand, borrowed character, text focus, watermark, sample watermark, character watermark, lofter username, photo date watermark, movie poster, magazine cover, journal, cover, cover page, doujin cover, album cover, manga cover, brand name imitation, EasyNegative,Tights, silk stockings,shorts
Steps: 35, Sampler: DPM adaptive, CFG scale: 5.5, Seed: 2223996555, Size: 1088x1088, Model hash: 543bcbc212, Model: base_Anything-V3.0-pruned, Clip skip: 2, ENSD: 31337
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;epilogue&#34;&gt;Epilogue
&lt;/h2&gt;&lt;p&gt;We didn’t recommend the one-click deployment package because it contained some settings that were customized by the author and differed from the official, out-of-the-box configuration. If you&amp;rsquo;re a beginner, you might not understand why those parameters are optimal; it’s generally best to start with the official version. As you use it more and more, take time to read the official documentation, and you’ll learn which parameters need adjustment.&lt;/p&gt;
&lt;h2 id=&#34;gpu-selection&#34;&gt;GPU Selection
&lt;/h2&gt;&lt;p&gt;Following the cryptocurrency mining boom, GPU prices have become relatively less high; for entry-level players choosing between the &lt;code&gt;3060&lt;/code&gt; and &lt;code&gt;3060ti&lt;/code&gt;, it’s generally recommended to opt for the 12G version of the 3060 due to its larger VRAM, as it can generate larger resolution images. Why do you need a higher resolution? Because you can increase the resolution during generation, which will result in clearer and more detailed images. If you only want to generate small images, then 8GB of VRAM is sufficient.&lt;/p&gt;
&lt;p&gt;There’s also the &lt;strong&gt;Super Resolution Upscaling&lt;/strong&gt; option, which enhances details and makes the image richer in detail, requiring more VRAM.&lt;/p&gt;
&lt;p&gt;Below is a summary table of the single-precision (FP32), half-precision (FP16), and double-precision (FP64) floating-point computing capabilities of NVIDIA GeForce GTX 970, GeForce RTX 3060 Ti, GeForce RTX 3060, GeForce RTX 3080, and GeForce RTX 3080 Ti:&lt;/p&gt;
&lt;p&gt;| GeForce GTX 970 | 2014 | 3.49 | 87.2 | 0.109 |&lt;/p&gt;
&lt;h2 id=&#34;graphics-card-selection&#34;&gt;Graphics Card Selection
&lt;/h2&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Graphics Card Model&lt;/th&gt;
&lt;th&gt;Release Year&lt;/th&gt;
&lt;th&gt;Single-Precision Floating Point Compute Capability (TFLOPS)&lt;/th&gt;
&lt;th&gt;Half-Precision Floating Point Compute Capability (TFLOPS)&lt;/th&gt;
&lt;th&gt;Double-Precision Floating Point Compute Capability (TFLOPS)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;/table&gt;
&lt;h2 id=&#34;graphics-card-selection-1&#34;&gt;Graphics Card Selection
&lt;/h2&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Graphics Card Model&lt;/th&gt;
&lt;th&gt;Release Year&lt;/th&gt;
&lt;th&gt;Single-Precision Floating-Point Compute Capability (TFLOPS)&lt;/th&gt;
&lt;th&gt;Half-Precision Floating-Point Compute Capability (TFLOPS)&lt;/th&gt;
&lt;th&gt;Double-Precision Floating-Point Compute Capability (TFLOPS)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;/table&gt;
&lt;h2 id=&#34;graphics-card-selection-2&#34;&gt;Graphics Card Selection
&lt;/h2&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Graphics Card Model&lt;/th&gt;
&lt;th&gt;Release Year&lt;/th&gt;
&lt;th&gt;Single-Precision Floating-Point Compute Capability (TFLOPS)&lt;/th&gt;
&lt;th&gt;Half-Precision Floating-Point Compute Capability (TFLOPS)&lt;/th&gt;
&lt;th&gt;Double-Precision Floating-Point Compute Capability (TFLOPS)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;/table&gt;
&lt;h2 id=&#34;graphics-card-selection-3&#34;&gt;Graphics Card Selection
&lt;/h2&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Graphics Card Model&lt;/th&gt;
&lt;th&gt;Release Year&lt;/th&gt;
&lt;th&gt;Single-Precision Floating-Point Compute Capability (TFLOPS)&lt;/th&gt;
&lt;th&gt;Half-Precision Floating-Point Compute Capability (TFLOPS)&lt;/th&gt;
&lt;th&gt;Double-Precision Floating-Point Compute Capability (TFLOPS)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;/table&gt;
&lt;h2 id=&#34;gpu-selection-1&#34;&gt;GPU Selection
&lt;/h2&gt;&lt;p&gt;Excerpted from &lt;a class=&#34;link&#34; href=&#34;https://docs.google.com/spreadsheets/d/1Zlv4UFiciSgmJZncCujuXKHwc4BcxbjbSBg71-SdeNk/edit#gid=0&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;various GPU performance test data&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;updates&#34;&gt;Updates
&lt;/h2&gt;&lt;p&gt;Every six months, I originally planned to revisit and refine the installation steps, and explain more basic concepts. However, I discovered that most people using AI image generation are simply adjusting parameters based on images provided by experts, or re-rendering existing images with formatting changes.&lt;/p&gt;
&lt;p&gt;I had previously attempted a project using AI to generate UI materials for mini programs, but after struggling for half a day, the results were unsatisfactory compared to just pulling resource images directly from the official mini program documentation.&lt;/p&gt;</description>
        </item>
        <item>
        <title>Program optimization should not attempt to fight against hardware.</title>
        <link>https://ttf248.life/en/p/program-optimization-dont-fight-hardware/</link>
        <pubDate>Fri, 07 Apr 2023 16:30:15 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/program-optimization-dont-fight-hardware/</guid>
        <description>&lt;p&gt;&lt;code&gt;one loop thread&lt;/code&gt;, the time taken has already been at the &lt;strong&gt;microsecond&lt;/strong&gt; level, switching servers resulted in a backlog of up to 60,000 packets, to almost none.&lt;/p&gt;
&lt;p&gt;In single-threaded loop processing data scenarios, the CPU performance depends on factors such as clock frequency, cache size, and instruction set architecture. Generally, CPUs with higher clock frequencies, larger caches, and more advanced instruction set architectures perform better in single-threaded data processing.&lt;/p&gt;
&lt;h2 id=&#34;single-threaded&#34;&gt;Single-Threaded
&lt;/h2&gt;&lt;p&gt;Performance improvements aren&amp;rsquo;t always achieved by adding threads; it’s not necessary to overcomplicate things.  Refine the project workflow, identify time-consuming bottlenecks, and determine if a single thread can meet the requirements. Considering single-threaded approaches reduces complexity and minimizes potential issues.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;It’s often a bit misguided to jump straight into suggesting threading.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;events&#34;&gt;Events
&lt;/h2&gt;&lt;p&gt;All processed &lt;strong&gt;market data, latency sensitive&lt;/strong&gt;.
Working late into the night to release a new optimized version, local API removal for testing, speed was okay, tps: 42,000
Deployed to server, tps dropped significantly: 21,000, went home to try on a desktop, tps: 79,000, started suspecting that the internal service virtual machines might have some issues, initially suspected frequency-related problems, the difference between the home desktop and the server’s &lt;code&gt;CPU&lt;/code&gt; is the biggest, namely the frequency.&lt;/p&gt;
&lt;p&gt;Test Server A&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;processor	: 7
vendor_id	: GenuineIntel
cpu family	: 6
model		: 47
model name	: Intel(R) Xeon(R) CPU E7- 4807  @ 1.87GHz
stepping	: 2
microcode	: 0x34
cpu MHz		: 1866.733
cache size	: 18432 KB
physical id	: 1
siblings	: 4
core id		: 3
cpu cores	: 4
apicid		: 7
initial apicid	: 7
fpu		: yes
fpu_exception	: yes
cpuid level	: 11
wp		: yes
flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts mmx fxsr sse sse2 ss syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts nopl xtopology tsc_reliable nonstop_tsc cpuid aperfmperf pni pclmulqdq ssse3 cx16 sse4_1 sse4_2 popcnt aes hypervisor lahf_lm pti dtherm arat
bugs		: clflush_monitor cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs itlb_multihit
bogomips	: 3733.46
clflush size	: 64
cache_alignment	: 64
address sizes	: 40 bits physical, 48 bits virtual
power management:
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Test Server B&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;processor	: 7
vendor_id	: GenuineIntel
cpu family	: 6
model		: 63
model name	: Intel(R) Xeon(R) CPU E5-2640 v3 @ 2.60GHz
stepping	: 2
microcode	: 0x3c
cpu MHz		: 2599.998
cache size	: 20480 KB
physical id	: 14
siblings	: 1
core id		: 0
cpu cores	: 1
apicid		: 14
initial apicid	: 14
fpu		: yes
fpu_exception	: yes
cpuid level	: 15
wp		: yes
flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts mmx fxsr sse sse2 ss syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts nopl xtopology tsc_reliable nonstop_tsc cpuid aperfmperf pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cpuid_fault epb pti fsgsbase smep xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts
bugs		: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs itlb_multihit
bogomips	: 5199.99
clflush size	: 64
cache_alignment	: 64
address sizes	: 40 bits physical, 48 bits virtual
power management:
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;consulting-openai&#34;&gt;Consulting OpenAI
&lt;/h2&gt;&lt;p&gt;Initially, regarding core count and clock frequency, the Intel(R) Xeon(R) CPU E7-4807 @ 1.87GHz has 6 physical cores and 12 logical cores with a clock speed of 1.87GHz; while the Intel(R) Xeon(R) CPU E5-2640 v3 @ 2.60GHz has 8 physical cores and 16 logical cores with a clock speed of 2.60GHz. Therefore, from the perspective of core count and clock frequency, the E5-2640 v3 should be more advantageous in single-threaded data processing compared to the E7-4807.&lt;/p&gt;
&lt;p&gt;Secondly, considering cache size, the E7-4807 has a cache size of 12MB, while the E5-2640 v3 has a cache size of 20MB. Therefore, the E5-2640 v3 possesses larger cache space, which can improve data access speed and cache hit rate.&lt;/p&gt;
&lt;p&gt;Finally, regarding architecture, the E5-2640 v3 utilizes the more advanced Haswell architecture, while the E7-4807 employs the older Nehalem architecture. The Haswell architecture offers performance improvements compared to the Nehalem architecture, which may also influence the E5-2640 v3’s performance in single-threaded data processing.&lt;/p&gt;
&lt;p&gt;Therefore, considering everything comprehensively, in scenarios involving single-threaded loop processing of data, the E5-2640 v3 should exhibit better performance than the E7-4807. However, specific performance differences will be influenced by various factors including data processing algorithms, memory bandwidth, system load, and so on, requiring a case-by-case analysis.&lt;/p&gt;
&lt;h2 id=&#34;hardware-specifications&#34;&gt;Hardware Specifications
&lt;/h2&gt;&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.cpubenchmark.net/compare/3786.2vs2365.2/%5BDual-CPU%5D-Intel-Xeon-E7%e2%80%934807-vs-%5BDual-CPU%5D-Intel-Xeon-E5-2640-v3&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://www.cpubenchmark.net/compare/3786.2vs2365.2/%5BDual-CPU%5D-Intel-Xeon-E7–4807-vs-%5BDual-CPU%5D-Intel-Xeon-E5-2640-v3&lt;/a&gt;
| Price | Search Online  $78 - BUY |  |&lt;/p&gt;
&lt;h2 id=&#34;hardware-specifications-1&#34;&gt;Hardware Specifications
&lt;/h2&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Specification&lt;/th&gt;
&lt;th&gt;Xeon E7-4807 (LGA1567)&lt;/th&gt;
&lt;th&gt;Xeon E5-2640 v3 (LGA2011-v3)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;/table&gt;
&lt;h2 id=&#34;hardware-specifications-2&#34;&gt;Hardware Specifications
&lt;/h2&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Specification&lt;/th&gt;
&lt;th&gt;Xeon E7-4807 (LGA1567)&lt;/th&gt;
&lt;th&gt;Xeon E5-2640 v3 (LGA2011-v3)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;/table&gt;
&lt;h2 id=&#34;hardware-specifications-3&#34;&gt;Hardware Specifications
&lt;/h2&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Specification&lt;/th&gt;
&lt;th&gt;Xeon E7-4807 (LGA1567)&lt;/th&gt;
&lt;th&gt;Xeon E5-2640 v3 (LGA2011-v3)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;/table&gt;
&lt;h2 id=&#34;hardware-specifications-4&#34;&gt;Hardware Specifications
&lt;/h2&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Specification&lt;/th&gt;
&lt;th&gt;Xeon E7-4807 (LGA1567)&lt;/th&gt;
&lt;th&gt;Xeon E5-2640 v3 (LGA2011-v3)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Turbo Speed&lt;/td&gt;
&lt;td&gt;Not Supported&lt;/td&gt;
&lt;td&gt;Up to 3.4 GHz&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;hardware-specifications-5&#34;&gt;Hardware Specifications
&lt;/h2&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Specification&lt;/th&gt;
&lt;th&gt;Xeon E7-4807 (LGA1567)&lt;/th&gt;
&lt;th&gt;Xeon E5-2640 v3 (LGA2011-v3)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Number of Physical Cores&lt;/td&gt;
&lt;td&gt;6 (Threads: 12)&lt;/td&gt;
&lt;td&gt;8 (Threads: 16)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;hardware-specifications-6&#34;&gt;Hardware Specifications
&lt;/h2&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Specification&lt;/th&gt;
&lt;th&gt;Xeon E7-4807 (LGA1567)&lt;/th&gt;
&lt;th&gt;Xeon E5-2640 v3 (LGA2011-v3)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;/table&gt;
&lt;h2 id=&#34;hardware-specifications-7&#34;&gt;Hardware Specifications
&lt;/h2&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Specification&lt;/th&gt;
&lt;th&gt;Xeon E7-4807 (LGA1567)&lt;/th&gt;
&lt;th&gt;Xeon E5-2640 v3 (LGA2011-v3)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Max TDP&lt;/td&gt;
&lt;td&gt;95W x 2&lt;/td&gt;
&lt;td&gt;90W x 2&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;hardware-specifications-8&#34;&gt;Hardware Specifications
&lt;/h2&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Specification&lt;/th&gt;
&lt;th&gt;Xeon E7-4807 (LGA1567)&lt;/th&gt;
&lt;th&gt;Xeon E5-2640 v3 (LGA2011-v3)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Yearly Running Cost&lt;/td&gt;
&lt;td&gt;$34.68&lt;/td&gt;
&lt;td&gt;$32.85&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;hardware-specifications-9&#34;&gt;Hardware Specifications
&lt;/h2&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Specification&lt;/th&gt;
&lt;th&gt;Xeon E7-4807 (LGA1567)&lt;/th&gt;
&lt;th&gt;Xeon E5-2640 v3 (LGA2011-v3)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;/table&gt;
&lt;h2 id=&#34;hardware-specifications-10&#34;&gt;Hardware Specifications
&lt;/h2&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Specification&lt;/th&gt;
&lt;th&gt;Xeon E7-4807 (LGA1567)&lt;/th&gt;
&lt;th&gt;Xeon E5-2640 v3 (LGA2011-v3)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;First Seen on Chart&lt;/td&gt;
&lt;td&gt;Q3 2020&lt;/td&gt;
&lt;td&gt;Q3 2014&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;hardware-specifications-11&#34;&gt;Hardware Specifications
&lt;/h2&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Specification&lt;/th&gt;
&lt;th&gt;Xeon E7-4807 (LGA1567)&lt;/th&gt;
&lt;th&gt;Xeon E5-2640 v3 (LGA2011-v3)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;# of Samples&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;46&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;hardware-specifications-12&#34;&gt;Hardware Specifications
&lt;/h2&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Specification&lt;/th&gt;
&lt;th&gt;Xeon E7-4807 (LGA1567)&lt;/th&gt;
&lt;th&gt;Xeon E5-2640 v3 (LGA2011-v3)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;CPU Value&lt;/td&gt;
&lt;td&gt;69.1&lt;/td&gt;
&lt;td&gt;225.6&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;hardware-specifications-13&#34;&gt;Hardware Specifications
&lt;/h2&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Specification&lt;/th&gt;
&lt;th&gt;Xeon E7-4807 (LGA1567)&lt;/th&gt;
&lt;th&gt;Xeon E5-2640 v3 (LGA2011-v3)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Single Thread Rating&lt;/td&gt;
&lt;td&gt;721 (-59.2%)&lt;/td&gt;
&lt;td&gt;1767 (0.0%)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;hardware-specifications-14&#34;&gt;Hardware Specifications
&lt;/h2&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Specification&lt;/th&gt;
&lt;th&gt;Xeon E7-4807 (LGA1567)&lt;/th&gt;
&lt;th&gt;Xeon E5-2640 v3 (LGA2011-v3)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;CPU Mark&lt;/td&gt;
&lt;td&gt;6223 (-64.6%)&lt;/td&gt;
&lt;td&gt;17600 (0.0%)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
</description>
        </item>
        <item>
        <title>Prompt Engineer</title>
        <link>https://ttf248.life/en/p/prompt-engineer/</link>
        <pubDate>Sun, 26 Mar 2023 20:46:53 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/prompt-engineer/</guid>
        <description>&lt;p&gt;Just as we needed to learn the techniques of searching engines back then, we also need to learn some techniques for communicating with &lt;code&gt;AI&lt;/code&gt;, providing reasonable and sufficient constraints, and efficiently obtaining the answers we need.&lt;/p&gt;
&lt;p&gt;If you look at it from a different angle, current &lt;code&gt;AI&lt;/code&gt; is like a very good student with excellent memory – it has the ability to memorize everything. What we need to do is learn how to communicate with &lt;code&gt;AI&lt;/code&gt; correctly, effectively, and precisely describe our needs to help &lt;code&gt;AI&lt;/code&gt; generate the expected results.&lt;/p&gt;
&lt;p&gt;##科普
The incredibly popular &lt;code&gt;AI&lt;/code&gt; – specifically &lt;code&gt;Generative Pre-Training&lt;/code&gt; – literally translates to generative pre-training. It’s a deep learning model that trains text generation based on internet-available data, used for tasks like question answering, text summarization generation, machine translation, classification, code generation, and conversational AI. Currently, there have been various versions of models such as GPT-1, GPT-2, GPT-3, and GPT-4, each larger and more powerful than the previous one.&lt;/p&gt;
&lt;h2 id=&#34;does-it-really-have-intelligence&#34;&gt;Does it Really Have Intelligence?
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;Similarity is high, accuracy increases accordingly.&lt;/li&gt;
&lt;li&gt;Basic, repetitive tasks, after specific training, no longer require human intervention.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Generative AI is a technology that utilizes existing data such as text, audio, and images to create new content. It can be used for various tasks including text generation, speech synthesis, image generation, and dialogue systems. The logical reasoning of Generative AI depends on its training data and model structure. Generally, Generative AI can follow grammar, logic, and common sense to a certain extent, but it may also produce errors, biases, or unrealistic content. Therefore, the output of Generative AI requires human judgment and verification; it should not be blindly trusted or used.&lt;/p&gt;
&lt;h2 id=&#34;prompt-engineer&#34;&gt;Prompt Engineer
&lt;/h2&gt;&lt;p&gt;Time flows only forward, and people need to learn to adapt to the current. You can think of &lt;code&gt;AI&lt;/code&gt; as not intelligent, lacking logic, and often producing unusable code.&lt;/p&gt;
&lt;p&gt;If you shift your perspective, the current &lt;code&gt;AI&lt;/code&gt; is like a child with excellent memory – it has the ability to memorize after reading something repeatedly, much like copying homework. What we need to do is learn how to communicate correctly, effectively, and with &lt;code&gt;AI&lt;/code&gt;, precisely describing our needs to help &lt;code&gt;AI&lt;/code&gt; generate the desired results.&lt;/p&gt;
&lt;h2 id=&#34;dialogue-patterns&#34;&gt;Dialogue Patterns
&lt;/h2&gt;&lt;p&gt;Two years ago, when GitHub Copilot was released, no one could have imagined that two years later, OpenAI would appear out of nowhere, allowing humans to recognize the capabilities of large language models.&lt;/p&gt;
&lt;p&gt;Based on &lt;strong&gt;comment-based programming&lt;/strong&gt; and &lt;strong&gt;conversational programming&lt;/strong&gt;, the interaction logic is completely different. The dialogue pattern is friendly for novice users, and it’s necessary to mention NewBing providing follow-up prompts after each question. Microsoft is trying to guide users to obtain more content from the AI knowledge base.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Example Scenario
&lt;strong&gt;Scene:&lt;/strong&gt; A long high-speed train journey, coding is a good choice. With limited traffic, processing a few images, obtaining image materials, downloading software to process images is a bit extravagant, and network conditions don’t allow it.
&lt;strong&gt;Requirement:&lt;/strong&gt; There are a batch of picture materials with transparent background images used as navigation bars for small programs. The selected state is colored, and the unselected state is black and white. The materials are all in color, and they need to be preprocessed.
&lt;strong&gt;Prompt:&lt;/strong&gt; Write a python code snippet to read all pictures from a folder, convert the pictures into black and white, maintain the transparent background, save the pictures, optimize the image quality, reduce file size, and display it in the console, comparing the sizes before and after optimization.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import necessary libraries
import os
from PIL import Image

# Define a function to convert images to black and white, maintain transparency, save images, and return file sizes
def convert_and_save(image_file):
    # Open the image
    image = Image.open(image_file)
    # Get the image mode, if it&#39;s RGBA mode, indicating a transparent background
    mode = image.mode
    if mode == &amp;quot;RGBA&amp;quot;:
        # Create a white background image with the same size as the original image
        background = Image.new(&amp;quot;RGB&amp;quot;, image.size, (255, 255, 255))
        # Paste the original image onto the white background, ignoring transparent pixels
        background.paste(image, mask=image.split()[3])
        # Convert the composite image to grayscale mode
        gray_image = background.convert(&amp;quot;L&amp;quot;)
        # Convert the grayscale image back to RGBA mode to preserve transparency
        final_image = gray_image.convert(&amp;quot;RGBA&amp;quot;)
    else:
        # If it&#39;s not RGBA mode, directly convert the image to grayscale mode
        final_image = image.convert(&amp;quot;L&amp;quot;)
    # Get the original image file name and extension
    file_name, file_ext = os.path.splitext(image_file)
    # Define the new image file name with _bw suffix for black and white
    new_file_name = file_name + &amp;quot;_bw&amp;quot; + file_ext
    # Save the new image and optimize quality, reduce file size
    final_image.save(new_file_name, optimize=True)
    # Get the original and new image file sizes and return
    old_size = os.path.getsize(image_file)
    new_size = os.path.getsize(new_file_name)
    return old_size, new_size

# Define a folder path to store all images
folder_path = &amp;quot;images&amp;quot;
# Iterate through all files in the folder
for file in os.listdir(folder_path):
    # Concatenate the full file path
    file_path = os.path.join(folder_path, file)
    # Check if the file is an image file based on extension (can add or modify extensions as needed)
    if file_path.endswith((&amp;quot;.png&amp;quot;, &amp;quot;.jpg&amp;quot;, &amp;quot;.jpeg&amp;quot;, &amp;quot;.gif&amp;quot;)):
        # Call the function to convert and save the image, and get the file size
        old_size, new_size = convert_and_save(file_path)
        # Display the comparison of file sizes before and after optimization in the console
        print(f&amp;quot;{file}: {old_size} bytes -&amp;gt; {new_size} bytes&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;AI&lt;/code&gt; quickly provided an answer, looked very beautiful, and worked well, experienced programmers would find that the code below was not very generic, and couldn’t be put into production:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Fixed parameters&lt;/li&gt;
&lt;li&gt;No log files, all information is printed to the console&lt;/li&gt;
&lt;li&gt;The task of image processing is suitable for a multi-threaded model
&lt;code&gt;NewBing&lt;/code&gt; appeared, based on the prompt, chose a question: &lt;strong&gt;What can be improved in this code?&lt;/strong&gt;
The answer given this time was far beyond expectations, and most programmers would think of optimization measures, &lt;code&gt;AI&lt;/code&gt; gave corresponding suggestions.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;This code may still have some improvements, such as:

You can use the argparse library to allow users to specify the folder path and extension through command-line parameters instead of hardcoding them in the code.
You can use the logging library to record the running status of the program instead of printing it directly to the console.
You can use try-except statements to catch potential exceptions, such as file opening failure, image conversion failure, etc., and handle them

## Let&#39;s illustrate with an example

```python
# Import necessary libraries
import argparse
import logging
import multiprocessing
import os

from PIL import Image


# Define a function to convert and save the image, maintaining transparent background and optimizing file size
def convert_and_save(image_file):
    # Open the image
    try:
        image = Image.open(image_file)
    except Exception as e:
        logging.error(f&amp;quot;Failed to open {image_file}: {e}&amp;quot;)
        return None, None
    # Get the image mode; if it&#39;s RGBA, it has a transparent background
    mode = image.mode
    if mode == &amp;quot;RGBA&amp;quot;:
        # Create a white background image with the same size
        background = Image.new(&amp;quot;RGB&amp;quot;, image.size, (255, 255, 255))
        # Paste the original image onto the white background, ignoring transparent pixels
        background.paste(image, mask=image.split()[3])
        # Convert the combined image to grayscale mode
        gray_image = background.convert(&amp;quot;L&amp;quot;)
        # Convert the grayscale image back to RGBA mode to preserve the transparent background
        final_image = gray_image.convert(&amp;quot;RGBA&amp;quot;)
    else:
        # If it&#39;s not RGBA, directly convert the image to grayscale mode
        final_image = image.convert(&amp;quot;L&amp;quot;)
    # Get the original image file name and extension
    file_name, file_ext = os.path.splitext(image_file)
    # Define the new image file name with _bw suffix for black and white
    new_file_name = file_name + &amp;quot;_bw&amp;quot; + file_ext
    # Save the new image, optimizing quality to reduce file size
    try:
        final_image.save(new_file_name, optimize=True)
    except Exception as e:
        logging.error(f&amp;quot;Failed to save {new_file_name}: {e}&amp;quot;)
        return None, None
    # Get the original image and new image file sizes, and return
    old_size = os.path.getsize(image_file)
    new_size = os.path.getsize(new_file_name)
    return file_name, old_size, new_size

# Define a function to parse command-line arguments and return the folder path and extension list
def parse_args():
    # Create an argument parser object
    parser = argparse.ArgumentParser(description=&amp;quot;Convert images to black and white and optimize quality.&amp;quot;)
    # Add a required positional argument, representing the folder path
    parser.add_argument(&amp;quot;folder_path&amp;quot;, help=&amp;quot;The path of the folder that contains the images.&amp;quot;)
    # Add an optional argument, representing the extension list, defaulting to png, jpg, jpeg, and gif
    parser.add_argument(&amp;quot;-e&amp;quot;, &amp;quot;--extensions&amp;quot;, nargs=&amp;quot;+&amp;quot;, default=[&amp;quot;.png&amp;quot;, &amp;quot;.jpg&amp;quot;, &amp;quot;.jpeg&amp;quot;, &amp;quot;.gif&amp;quot;], help=&amp;quot;The extensions of the image files.&amp;quot;)
    # Parse command-line arguments and return the result object
    args = parser.parse_args()
    return args.folder_path, args.extensions

# Define a function to print the comparison of file sizes before and after optimization
def print_result(result):
    # If the result is not empty, indicating successful conversion and saving
    if result:
        # Unpack the result into a tuple of filename and file size tuple
        if len(result) == 3:
            file, old_size, new_size = result
            # Display the comparison of file sizes before and after optimization in the console
            logging.info(f&amp;quot;{file}: {old_size} bytes -&amp;gt; {new_size} bytes&amp;quot;)
        else:
            # Log the result if it&#39;s not a tuple of 3 elements
            logging.info(f&amp;quot;{result}&amp;quot;)

# Configure the logger, outputting logs to the console and files, setting the log level to INFO
logging.basicConfig(level=logging.INFO,
                    format=&amp;quot;%(asctime)s %(levelname)s %(message)s&amp;quot;,
                    handlers=[logging.StreamHandler(),
                              logging.FileHandler(&amp;quot;log.txt&amp;quot;)])

# Call the function to get the folder path and extension list
folder_path, extensions = parse_args()

if __name__ == &amp;quot;__main__&amp;quot;:
    # Windows needs this function because Windows lacks the fork() function (not entirely accurate).
    # Therefore, on Windows, forking is simulated by creating a new process, and the code - Additionally, this new process is instructed to run the code passed through a pipe by passing the `--multiprocessing-fork` command-line argument to it.
- If you examine the implementation of the `freeze_support()` function, its task is to check which process it&#39;s running in and whether it should execute the code passed through a pipe.
- `multiprocessing.freeze_support()`

- A process pool is created, automatically allocating processes based on the number of cores on the computer.
- An empty list, `results`, is created to store the result objects of asynchronous tasks.
- The script iterates through all files in the folder:
- The full file path is constructed using `os.path.join()`.
- It checks if the file ends with any of the extensions specified in the `extensions` list (you can modify this list as needed).
- If it&#39;s an image file, the `convert_and_save` function is called to convert and save the image asynchronously, without blocking the main process. The file size is also obtained.  The result object is then added to the `results` list using `pool.apply_async()` with the callback function `print_result`.
- The process pool is closed to stop accepting new tasks.
- `pool.join()` is called to wait for all tasks in the pool to complete.

## Epilogue
Due to local development being on a `windows` system, the first answer given by `AI` did not include the `main` function and also lacked `multiprocessing.freeze_support`. The code was fixed after following up and encountering an error. 
Just as learning the techniques of search engines required skill, we also need to learn how to communicate with `AI`, providing reasonable and sufficient constraints to efficiently obtain the desired answers.
Note: **If you are a programming beginner, if you still don&#39;t understand certain parts of the code based on the given comments, please continue to ask related questions.**&lt;/code&gt;&lt;/pre&gt;
</description>
        </item>
        <item>
        <title>WeChat Mini Program Background and Development Environment</title>
        <link>https://ttf248.life/en/p/wechat-mini-program-background-and-development-environment/</link>
        <pubDate>Fri, 24 Mar 2023 21:59:11 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/wechat-mini-program-background-and-development-environment/</guid>
        <description>&lt;p&gt;WeChat Mini Program Introduction and Development Preparation&lt;/p&gt;
&lt;h2 id=&#34;why-mini-programs-exist&#34;&gt;Why Mini Programs Exist
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Better Experience:&lt;/strong&gt; Slow web loading and blank screens; native app experience is faster.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Standards &amp;amp; Management:&lt;/strong&gt; For WeChat, onboarding and management
Before mini programs were released, WeChat published an SDK called &lt;code&gt;JSDk&lt;/code&gt; to open up some of the original WeChat capabilities: WeChat Pay, coupons. However, developers used web development languages to develop logic, bypassing some WeChat regulations. Mini Programs have their own description language.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;what-are-mini-programs&#34;&gt;What are Mini Programs?
&lt;/h2&gt;&lt;p&gt;Mini programs are applications that can be used without needing to download and install them. They realize the dream of having applications &lt;strong&gt;within easy reach&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Users can open an application simply by scanning a code or searching for it. This also embodies the concept of &lt;strong&gt;use-it-and-leave-it&lt;/strong&gt; – users don’t need to worry about installing too many apps.  Applications will be ubiquitous and available at any time, but &lt;strong&gt;without needing to install or uninstall&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;differences-between-mini-programs-and-mobile-applications&#34;&gt;Differences Between Mini Programs and Mobile Applications
&lt;/h2&gt;&lt;p&gt;No installation required, doesn&amp;rsquo;t occupy memory, easy to spread: QR code scanning, mini program cards, Search One Search (Sohu Search)&lt;/p&gt;
&lt;h2 id=&#34;what-mini-programs-can-do&#34;&gt;What Mini Programs Can Do
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;Content Tools: Zhihu Hot Rankings, Weibo Hot Topics, Mobike Bikes, Today Headline, Tencent Maps, Tencent Translate&lt;/li&gt;
&lt;li&gt;Retail: Pinduoduo, JD.com Shopping, Mugujie, Daily Fresh, Xiaomi Mall, Watsons&lt;/li&gt;
&lt;li&gt;Games: Jump Rope, Happy Mahjong, Happy Dou Dizhu, Duoyue Live Streaming, YY Live Streaming&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Course content is from 2018; some application vendors have already gone bankrupt now.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;development-preparation&#34;&gt;Development Preparation
&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;Register a Mini Program Account: Simply fill in the information normally, and click the activation link on the email.&lt;/li&gt;
&lt;li&gt;Information Registration&lt;/li&gt;
&lt;li&gt;Log into the Mini Program Management Backstage&lt;/li&gt;
&lt;li&gt;Complete Mini Program Information&lt;/li&gt;
&lt;li&gt;Bind Developer: For individual developers, use the WeChat account you’re logged in with as the administrator account; no additional binding is required.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Email has certain restrictions – it needs a new email address, but you can apply aliases for QQ emails, and the WeChat background will not verify them. After trying this, the name of the mini program is quite troublesome. As soon as it involves trademarks, it’s prone to audit failure.&lt;/p&gt;
&lt;p&gt;You can select service categories and also add custom ones; a Mini Program can add five categories.&lt;/p&gt;
&lt;p&gt;In Settings, you can view the Mini Program&amp;rsquo;s &lt;code&gt;ID&lt;/code&gt; information and enable message push notifications.  Enabling message push allows you to use the Message Template function.&lt;/p&gt;
&lt;h2 id=&#34;developer-tools-as-described-by-the-author&#34;&gt;Developer Tools (as described by the author)
&lt;/h2&gt;&lt;p&gt;Normal download and installation, with no special notes – just a basic understanding. Simply enter in Guest Mode, and if you want to enable mobile debugging (i.e., view the mini-program development version on your phone), you need to log into the mini-program developer account, then click Settings, and switch to the specified mini-program’s &lt;code&gt;ID&lt;/code&gt; within Project Details.&lt;/p&gt;
&lt;h2 id=&#34;code-structure&#34;&gt;Code Structure
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;js: Interaction logic&lt;/li&gt;
&lt;li&gt;json: Data configuration&lt;/li&gt;
&lt;li&gt;wxml: UI elements&lt;/li&gt;
&lt;li&gt;wxss: UI styles&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>Office migration, the servers are inaccessible.</title>
        <link>https://ttf248.life/en/p/office-move-server-inaccessible/</link>
        <pubDate>Sat, 11 Mar 2023 01:42:05 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/office-move-server-inaccessible/</guid>
        <description>&lt;p&gt;Administrative notice, office relocation from the second floor to the fifteenth floor – a standard, routine desk move.&lt;/p&gt;
&lt;h2 id=&#34;design-sense&#34;&gt;Design Sense
&lt;/h2&gt;&lt;p&gt;&lt;img src=&#34;https://ttf248.life/p/office-move-server-inaccessible/20230311014537.png&#34;
	width=&#34;511&#34;
	height=&#34;916&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;Office Building&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;55&#34;
		data-flex-basis=&#34;133px&#34;
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;migration&#34;&gt;Migration
&lt;/h2&gt;&lt;p&gt;Closing up shop, packing everything away, a familiar route, a new workstation – adjusting computer cabling, finding a comfortable posture to start working.
(ÒωÓױ)! – Connecting the network cable, the servers frequently used by the team were inaccessible. I tried switching to wireless networking, and access was normal again.&lt;/p&gt;
&lt;p&gt;Initially, I thought it was a problem with the server’s IP address range settings. The wired network at the new workstation wasn&amp;rsquo;t included in the firewall configuration list. After contacting IT colleagues to adjust it, the issue was resolved. This IP address range wasn’t just for one server; when trying to access other servers, they were all functioning normally. Gradually, I began to feel confused? Let professional people handle professional matters – eventually, the operations department colleague identified that this server had &lt;code&gt;docker&lt;/code&gt; deployed, and the default network of the &lt;code&gt;docker0&lt;/code&gt; service conflicted with the wired network configuration of the office, causing data packets sent to it to not receive responses and being routed to the &lt;code&gt;docker&lt;/code&gt; service.&lt;/p&gt;
&lt;p&gt;Other servers didn’t have the &lt;code&gt;docker&lt;/code&gt; service deployed, so only this one was affected. I frequently used it, and occasionally used containers to deploy some testing services – I never expected to encounter such a scenario. Later, thinking about it in detail, since the entire group was located within the same office building, IT colleagues divided IP address ranges using addresses starting with &lt;code&gt;172&lt;/code&gt;, which wasn’t unusual.&lt;/p&gt;
&lt;h2 id=&#34;docker0&#34;&gt;docker0
&lt;/h2&gt;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# vim /etc/docker/daemon.json
{
    &amp;quot;bip&amp;quot;:&amp;quot;172.200.0.1/24&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Restart the service and switch to the new network; the server recovers normal access.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References
&lt;/h2&gt;&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://yeasy.gitbook.io/docker_practice/advanced_network/docker0&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Docker from Beginner to Advanced - docker0&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Embedded Systems – Getting Started Part 1 – Technical Terms</title>
        <link>https://ttf248.life/en/p/embedded-entry-professional-terms/</link>
        <pubDate>Tue, 07 Mar 2023 13:42:36 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/embedded-entry-professional-terms/</guid>
        <description>&lt;p&gt;Here’s the English translation:&lt;/p&gt;
&lt;p&gt;“When I think of embedded systems, my mind immediately goes back to the 51-pin single-chip microcomputers and Texas Instruments we used in university labs.&lt;/p&gt;
&lt;p&gt;LPA3399Pro is a visual host developed on the HiSilicon RK3399Pro platform, designed for portable computing hosts that require extensive visual computation. It features an integrated NPU compute unit with 3.0 TOPS of processing power and supports various algorithm models.&lt;/p&gt;
&lt;p&gt;RV1109 is HiSilicon’s SoC for machine vision within its AI branch, featuring a dedicated NPU. RV1109 provides 1TOPS of processing power.”&lt;/p&gt;
&lt;h2 id=&#34;system-on-chip&#34;&gt;System on Chip
&lt;/h2&gt;&lt;p&gt;SoC stands for System on a Chip, meaning “chiplet system.” It’s a technology that integrates multiple electronic systems onto a single chip. This technology can significantly reduce the size and weight of electronic products while simultaneously improving performance and lowering power consumption.&lt;/p&gt;
&lt;p&gt;SoC (System on a Chip) and CPU (Central Processing Unit) are both important components of computer systems, but they have some distinctions between them.&lt;/p&gt;
&lt;p&gt;The CPU is the core processor within a computer system, responsible for executing program instructions. It typically only contains basic components such as arithmetic logic units, control units, and registers.&lt;/p&gt;
&lt;p&gt;An SoC, on the other hand, integrates multiple electronic systems onto a single chip, including a CPU, along with other components like memory, graphics processors, and input/output interfaces. This allows for significant reductions in product size and weight while boosting performance and reducing power consumption.&lt;/p&gt;
&lt;p&gt;In essence, the CPU is a component of an SoC, whereas the SoC represents a more complex and highly integrated electronic system.&lt;/p&gt;
&lt;h2 id=&#34;microcontroller-unit&#34;&gt;Microcontroller Unit
&lt;/h2&gt;&lt;p&gt;SoC (System on a Chip) and MCU (Microcontroller Unit) are technologies that integrate multiple electronic systems onto a single chip, but there are also some differences between them.&lt;/p&gt;
&lt;p&gt;An MCU is a microcontroller, typically containing a CPU, memory, input/output interfaces, and other basic components. It’s commonly used to control other electronic devices such as household appliances and automotive electronics systems.&lt;/p&gt;
&lt;p&gt;An SoC, on the other hand, integrates more electronic systems onto a single chip, in addition to the basic components of an MCU, it can also integrate other components like graphics processors and wireless communication modules. This significantly reduces the size and weight of electronic products while improving performance and reducing power consumption.&lt;/p&gt;
&lt;p&gt;In essence, an MCU is a simple microcontroller, whereas an SoC is a more complex and highly integrated electronic system.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>AI-assisted programming, the evolution of productivity</title>
        <link>https://ttf248.life/en/p/ai-assisted-programming-productivity-evolution/</link>
        <pubDate>Tue, 28 Feb 2023 17:05:17 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/ai-assisted-programming-productivity-evolution/</guid>
        <description>&lt;p&gt;“GitHub Copilot” was released less than two years ago, and “ChatGPT” has appeared. I don’t fully understand the underlying principles; I&amp;rsquo;ve been using them for a while. The two tools’ levels of assistance are completely different, but they have both achieved a significant increase in productivity.&lt;/p&gt;
&lt;p&gt;For things that are too complex, AI still can’t do it, because they lack logic, or they rely on patterns or fixed frameworks – the training data is sufficient, and AI’s effect is nine out of ten.&lt;/p&gt;
&lt;h2 id=&#34;github-copilot&#34;&gt;GitHub Copilot
&lt;/h2&gt;&lt;p&gt;When publishing, I looked at the official introduction’s “demo” and found it wasn&amp;rsquo;t very smart. After trying it out, it wasn’t great, so I gave up on it.&lt;/p&gt;
&lt;p&gt;Before 2022, when I had some free time, I switched to the new version to try it out, and the effect was already quite good. The only issue was that the speed in China was a bit slow, likely due to network reasons. For languages like &lt;code&gt;Python&lt;/code&gt;, the code generated based on comments was pretty good, and much faster than researching and writing code myself.&lt;/p&gt;
&lt;h2 id=&#34;chatgpt&#34;&gt;ChatGPT
&lt;/h2&gt;&lt;p&gt;&lt;code&gt;Github Copilot&lt;/code&gt; still focuses on developers, i.e., from a development perspective when designing the product, requiring you to understand how to program and guide AI assistance through comments.
&lt;code&gt;ChatGPT&lt;/code&gt; has moved to the next level, &lt;strong&gt;allowing non-experts to quickly get started&lt;/strong&gt;.  A conversational chat interface where you can pose questions, describe requirements, and ask for clarification if needed.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s take an example: You know about tokenization but don’t know how to code, wanting to calculate the similarity of strings:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;Python 分词计算 維天運通 LOGORY 和 合肥維天運通信息科技股份有限公司 两个字符串的相似度；
比较前，统一处理字符串，英文转换为小写；
第二个字符中如果能找到第一个字符的首字符，将第二个字符串截位，截位的长度为第一个字符串的长度
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Hello, this is Bing. 😊&lt;/p&gt;
&lt;p&gt;According to online information 123, Python tokenization can be implemented using the &lt;code&gt;jieba&lt;/code&gt; library. The &lt;code&gt;jieba&lt;/code&gt; library supports three segmentation modes: precise mode, full mode, and search engine mode. You can choose an appropriate mode based on your needs.&lt;/p&gt;
&lt;p&gt;To compare two strings, you first need to tokenize them and then calculate their word vectors or edit distance metrics. You can use libraries like NumPy or SciPy for mathematical operations.&lt;/p&gt;
&lt;p&gt;To standardize string processing, converting English to lowercase, you can use Python&amp;rsquo;s built-in &lt;code&gt;lower()&lt;/code&gt; function.&lt;/p&gt;
&lt;p&gt;To slice the second string, you can use Python’s slicing operator [].&lt;/p&gt;
&lt;p&gt;Here is a possible code example:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import jieba
import numpy as np
from scipy.spatial.distance import cosine

# Define two strings
str1 = &amp;quot;維天運通 LOGORY&amp;quot;
str2 = &amp;quot;合肥維天運通信息科技股份有限公司&amp;quot;

# Standardize the string, convert English to lowercase
str1 = str1.lower()
str2 = str2.lower()

# Find the position of the first character of the first string in the second string
index = str2.find(str1[0])

# If found, slice the second string
if index != -1:
    str2 = str2[index:index+len(str1)]

# Tokenize the two strings using the jieba library
words1 = list(jieba.cut(str1))
words2 = list(jieba.cut(str2))

# Calculate the word vectors of the two strings (using frequency as weights)
vector1 = np.zeros(len(words1))
vector2 = np.zeros(len(words1))

for i, word in enumerate(words1):
    vector1[i] += 1 # Count the number of times each word appears in the first string
    if word in words2:
        vector2[i] += 1 # Count the number of times each word appears in the second string

# Calculate the similarity between the two strings (using cosine distance as an index)
similarity = 1 - cosine(vector1, vector2)

print(&amp;quot;The similarity between the two strings is:&amp;quot;, similarity)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Furthermore, consult the results of running and congratulations, you have discovered a problem. The result is fabricated, it does not actually run the code, but will give you a good result.&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&#34;example&#34;&gt;Example
&lt;/h3&gt;&lt;p&gt;Question: SQL statement to retrieve records from the product code table where the product code is within the range 80000-89999.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;Hello, this is Bing. 😊

According to search result 12, you can use the SQL BETWEEN operator to select values within a given range. The BETWEEN operator includes both the start and end values.

You need to first specify the table name and column name that you want to query, and then use the BETWEEN operator and the range value in the WHERE clause.

For example, if you want to retrieve records from the product code table where the product code is between 80000 and 89999, you can use the following statement:

SELECT * FROM product_code
WHERE code BETWEEN 80000 AND 89999;
Here, &#39;*&#39; represents selecting all columns [3]. If you only want to select specific columns, such as name and price, you can separate them with commas, as shown below:

SELECT name, price FROM product_code
WHERE code BETWEEN 80000 AND 89999;

I hope this helps.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Continue questioning, negating it: The product code is a string, your code is wrong&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;Sorry, I didn&#39;t notice that the product code is a string. If that’s the case, you need to add single quotes around both ends of the range values, as follows:

SELECT * FROM product_code
WHERE code BETWEEN &#39;80000&#39; AND &#39;89999&#39;;

Please forgive my mistake. 🙏

Congratulations, it will recognize mistakes.
&lt;/code&gt;&lt;/pre&gt;
</description>
        </item>
        <item>
        <title>A Collection of Less Commonly Used Git Commands</title>
        <link>https://ttf248.life/en/p/less-common-git-commands-summary/</link>
        <pubDate>Fri, 17 Feb 2023 10:08:57 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/less-common-git-commands-summary/</guid>
        <description>&lt;p&gt;Less commonly used, but very useful &lt;code&gt;git&lt;/code&gt; commands&lt;/p&gt;
&lt;h3 id=&#34;reinstalling-the-system-caused-changes-to-folder-ownership-on-windows&#34;&gt;Reinstalling the system caused changes to folder ownership on &lt;code&gt;Windows&lt;/code&gt;.
&lt;/h3&gt;&lt;p&gt;The new version of &lt;code&gt;git&lt;/code&gt; has added security checks, prompting an unsafe state and preventing further operation.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git config --global --add safe.directory
git config --global --add safe.directory &amp;quot;*&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;new-computer-saving-account-password-information&#34;&gt;New Computer Saving Account Password Information
&lt;/h3&gt;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git config --global credential.helper store
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If the saved information needs to be updated, first clear out the old credentials&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git config --system --unset credential.helper
&lt;/code&gt;&lt;/pre&gt;
</description>
        </item>
        <item>
        <title>- Compiler
- Callback Function
- Performance Testing</title>
        <link>https://ttf248.life/en/p/compiler-callback-performance-testing/</link>
        <pubDate>Wed, 15 Feb 2023 13:59:25 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/compiler-callback-performance-testing/</guid>
        <description>&lt;ul&gt;
&lt;li&gt;Last year, an SDK was designed to handle encapsulating certain events and provide a class interface externally. During service initialization, the caller implements the corresponding classes and passes the object pointer to the module.&lt;/li&gt;
&lt;li&gt;Familiarity with C11 piqued my curiosity, leading me to explore what would happen if these interfaces were implemented using lambda function objects instead of pure virtual function definitions. Compared to the traditional interface definition method, it seemed more flexible.&lt;/li&gt;
&lt;li&gt;The question arose: with two different syntaxes, which one is faster from a performance perspective? Not understanding compiler principles, I decided to try out some code to find out.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction
&lt;/h2&gt;&lt;p&gt;Online website, allowing you to select different compilers, compilation parameters, run code on the &lt;code&gt;linux&lt;/code&gt; platform, or view corresponding assembly code.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://wandbox.org/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://wandbox.org/&lt;/a&gt; : Sometimes useful for technical validation; executing small code snippets in a web browser is very convenient.&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://godbolt.org/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://godbolt.org/&lt;/a&gt; : Using different colors to distinguish the corresponding assembly code for each line, it’s more convenient than using a local debugger.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;body&#34;&gt;Body
&lt;/h2&gt;&lt;p&gt;The Standards Committee established grammatical rules, and at the compilation level, how it’s implemented depends on each compiler vendor. It&amp;rsquo;s worth noting that Microsoft’s compiler is quite powerful here. Syntax sugar isn’t a panacea; callback interfaces aren’t abundant, using &lt;code&gt;lambda&lt;/code&gt; expressions is more convenient and eliminates the need to define empty callback function interfaces. When callback interfaces become numerous, traditional virtual functions are advantageous for unifying business interface definitions.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;On the &lt;code&gt;windows&lt;/code&gt; platform, performance is close, with little difference.&lt;/li&gt;
&lt;li&gt;On the &lt;code&gt;linux&lt;/code&gt; platform, a comparison of virtual functions and &lt;code&gt;lambda&lt;/code&gt; reveals a 1.35ns overhead per call.
In typical business system development, this level of performance loss can be ignored; introducing &lt;code&gt;lambda&lt;/code&gt; brings greater convenience in design. This is particularly noticeable when dealing with multi-signal processing – the underlying event triggering mechanism, along with handling functions for logging objects when events are triggered. When more business processing interfaces are needed, &lt;code&gt;lambda&lt;/code&gt; objects are stored in a &lt;code&gt;vector&lt;/code&gt;, and they’re iterated through sequentially during event triggering, similar to Qt&amp;rsquo;s signals and slots; logging, monitoring, business 1, and business 2 are completely decoupled.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;code&#34;&gt;Code
&lt;/h2&gt;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;Counter: 1000000
Time: 3966us
Counter: 1000000
Time: 5316us
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;#include &amp;lt;iostream&amp;gt;
#include &amp;lt;chrono&amp;gt;
#include &amp;lt;memory&amp;gt;
#include &amp;lt;functional&amp;gt;
#include &amp;lt;atomic&amp;gt;
#include &amp;lt;string&amp;gt;

std::atomic_int64_t counter = 0;

// Define the callback interface
class UserInterface
{
public:
    virtual void name() = 0;
    virtual void full_name() = 0;
};

class User : public UserInterface
{
public:
    void name() {}
    void full_name() { counter++; }
};

void to_string(UserInterface* user)
{
    user-&amp;gt;name();
    user-&amp;gt;full_name();
}

using name_handler = std::function&amp;lt;void()&amp;gt;;
using full_name_handler = std::function&amp;lt;void()&amp;gt;;

class Test
{
    name_handler name_;
    full_name_handler full_name_;

public:
    void set_name_handler(name_handler name)
    {
        name_ = name;
    }

    void set_full_name_handler(full_name_handler full_name)
    {
        full_name_ = full_name;
    }

    void to_string()
    {
        name_();
        full_name_();
    }
};

int main()
{
    User user;

    auto start = std::chrono::high_resolution_clock::now();

    for (int i = 0; i &amp;lt; 1000000; i++)
    {
        to_string(&amp;amp;user);
    }

    auto end = std::chrono::high_resolution_clock::now();
    std::cout &amp;lt;&amp;lt; &amp;quot;Counter: &amp;quot; &amp;lt;&amp;lt; counter &amp;lt;&amp;lt; std::endl;
    std::cout &amp;lt;&amp;lt; &amp;quot;Time: &amp;quot; &amp;lt;&amp;lt; std::chrono::duration_cast&amp;lt;std::chrono::microseconds&amp;gt;(end - start).count() &amp;lt;&amp;lt; &amp;quot;us&amp;quot; &amp;lt;&amp;lt; std::endl;

    counter = 0;
    auto name = []() {};
    auto full_name = []() { counter++; };

    Test test;
    test.set_name_handler(name);
    test.set_full_name_handler(full_name);

    start = std::chrono::high_resolution_clock::now();

    for (int i = 0; i &amp;lt; 1000000; i++)
    {
        test.to_string();
    }

    end = std::chrono::high_resolution_clock::now();
    std::cout &amp;lt;&amp;lt; &amp;quot;Counter: &amp;quot; &amp;lt;&amp;lt; counter &amp;lt;&amp;lt; std::endl;
    std::cout &amp;lt;&amp;lt; &amp;quot;Time: &amp;quot; &amp;lt;&amp;lt; std::chrono::duration_cast&amp;lt;std::chrono::microseconds&amp;gt;(end - start).count() &amp;lt;&amp;lt; &amp;quot;us&amp;quot; &amp;lt;&amp;lt; std::endl;

    return 0;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;epilogue&#34;&gt;Epilogue
&lt;/h2&gt;&lt;p&gt;While researching, I came across similar code snippets &lt;a class=&#34;link&#34; href=&#34;https://gist.githubusercontent.com/benloong/8050171/raw/fa577ec923b460862078b8b40233a42a1c619eeb/functionperformance.cpp&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;functionperformance.cpp&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;#include &amp;lt;iostream&amp;gt;
#include &amp;lt;chrono&amp;gt;
#include &amp;lt;memory&amp;gt;
#include &amp;lt;functional&amp;gt;

using namespace std;
using namespace std::chrono;

class Base
{
public:
	Base(){}
	~Base(){}
	virtual int func(int i) = 0;
};

class Derived : public Base
{
public:
	Derived(int base = 10) : base{base}
	{

	}
	~Derived(){}

	virtual int func(int i)
	{
		return i*base;
	}
private:
	int base;
};

struct Func
{
	int base;
	int operator()(int i)
	{
		return i*base;
	}
	Func(int base) : base {base}
	{

	}
};
const int base = 10;
int calculate(int i)
{
	return base*i;
}

int main()
{
	const int num = 10000;
	Base *p = new Derived{10};
	int total = 0;
	auto start = high_resolution_clock::now();
	for (int i = 0; i &amp;lt; num; ++i)
	{
		total += p-&amp;gt;func(i);
	}
	auto end = high_resolution_clock::now();
	std::cout&amp;lt;&amp;lt;&amp;quot;result: &amp;quot;&amp;lt;&amp;lt;total&amp;lt;&amp;lt;&amp;quot;\nvirtual call elapsed: \t&amp;quot;&amp;lt;&amp;lt;duration_cast&amp;lt;nanoseconds&amp;gt;(end-start).count()&amp;lt;&amp;lt;&amp;quot; nanoseconds.\n&amp;quot;&amp;lt;&amp;lt;std::endl;

	total = 0;
	start = high_resolution_clock::now();
	for (int i = 0; i &amp;lt; num; ++i)
	{
		total += calculate(i);
	}
	end = high_resolution_clock::now();
	std::cout&amp;lt;&amp;lt;&amp;quot;result: &amp;quot;&amp;lt;&amp;lt;total&amp;lt;&amp;lt;&amp;quot;\ndirect function call elapsed: \t&amp;quot;&amp;lt;&amp;lt;duration_cast&amp;lt;nanoseconds&amp;gt;(end-start).count()&amp;lt;&amp;lt;&amp;quot; nanoseconds.\n&amp;quot;&amp;lt;&amp;lt;std::endl;

	Func functor{10};
	total = 0;
	start = high_resolution_clock::now();
	for (int i = 0; i &amp;lt; num; ++i)
	{
		total += functor(i);
	}
	end = high_resolution_clock::now();
	std::cout&amp;lt;&amp;lt;&amp;quot;result: &amp;quot;&amp;lt;&amp;lt;total&amp;lt;&amp;lt;&amp;quot;\nfunctor call elapsed: \t&amp;quot;&amp;lt;&amp;lt;duration_cast&amp;lt;nanoseconds&amp;gt;(end-start).count()&amp;lt;&amp;lt;&amp;quot; nanoseconds.\n&amp;quot;&amp;lt;&amp;lt;std::endl;
	int base = 10;
	function&amp;lt;int(int)&amp;gt; lambda = [base](int i)
	{
		return i*base;
	};
	total = 0;
	start = high_resolution_clock::now();
	for (int i = 0; i &amp;lt; num; ++i)
	{
		total += lambda(i);
	}
	end = high_resolution_clock::now();
	std::cout&amp;lt;&amp;lt;&amp;quot;result: &amp;quot;&amp;lt;&amp;lt;total&amp;lt;&amp;lt;&amp;quot;\nlambda call elapsed: \t&amp;quot;&amp;lt;&amp;lt;duration_cast&amp;lt;nanoseconds&amp;gt;(end-start).count()&amp;lt;&amp;lt;&amp;quot; nanoseconds.\n&amp;quot;&amp;lt;&amp;lt;std::endl;
	return 0;
}

/*
test on mac mini i7 2.7GHz
clang++ -std=c++11 chronotest.cpp -O0
output:
result: 499950000
virtual call elapsed: 	43171 nanoseconds.

result: 499950000
direct function call elapsed: 	31379 nanoseconds.

result: 499950000
functor call elapsed: 	41497 nanoseconds.

result: 499950000
lambda call elapsed: 	207416 nanoseconds.
===================================================
clang++ -std=c++11 chronotest.cpp -O1
output:
result: 499950000
virtual call ```
/*
Here are two modes that have been added: a regular function and a functor. They provide an interface callback mechanism and comparison with direct calls, resulting in a qualitative difference in performance loss. Functors perform close to functions, and sometimes even outperform them. The compilation principle is somewhat of a knowledge blind spot; I suspect it&#39;s due to the addresses of accessed variables and the proximity of functions, which benefits `CPU` processing.

Attached is the output from running with `wandbox`.
*/
``` ## Epilogue
While searching for materials, I came across a similar code snippet [functionperformance.cpp](https://gist.githubusercontent.com/benloong/8050171/raw/fa577ec923b460862078b8b40233a42a1c619eeb/functionperformance.cpp)

```c++
#include &amp;lt;iostream&amp;gt;
#include &amp;lt;chrono&amp;gt;
#include &amp;lt;memory&amp;gt;
#include &amp;lt;functional&amp;gt;

using namespace std;
using namespace std::chrono;

class Base
{
public:
	Base(){}
	virtual ~Base(){}
	virtual int func(int i) = 0;
};

class Derived : public Base
{
public:
	Derived(int base = 10) : base{base}
	{

	}
	~Derived(){}

	virtual int func(int i)
	{
		return i*base;
	}
private:
	int base;
};

struct Func
{
	int base;
	int operator()(int i)
	{
		return i*base;
	}
	Func(int base) : base {base}
	{

	}
};
const int base = 10;
int calculate(int i)
{
	return base*i;
}

int main()
{
	const int num = 10000;
	Base *p = new Derived{10};
	int total = 0;
	auto start = high_resolution_clock::now();
	for (int i = 0; i &amp;lt; num; ++i)
	{
		total += p-&amp;gt;func(i);
	}
	auto end = high_resolution_clock::now();
	std::cout&amp;lt;&amp;lt;&amp;quot;result: &amp;quot;&amp;lt;&amp;lt;total&amp;lt;&amp;lt;&amp;quot;\nvirtual call elapsed: \t&amp;quot;&amp;lt;&amp;lt;duration_cast&amp;lt;nanoseconds&amp;gt;(end-start).count()&amp;lt;&amp;lt;&amp;quot; nanoseconds.\n&amp;quot;&amp;lt;&amp;lt;std::endl;

	total = 0;
	start = high_resolution_clock::now();
	for (int i = 0; i &amp;lt; num; ++i)
	{
		total += calculate(i);
	}
	end = high_resolution_clock::now();
	std::cout&amp;lt;&amp;lt;&amp;quot;result: &amp;quot;&amp;lt;&amp;lt;total&amp;lt;&amp;lt;&amp;quot;\ndirect function call elapsed: \t&amp;quot;&amp;lt;&amp;lt;duration_cast&amp;lt;nanoseconds&amp;gt;(end-start).count()&amp;lt;&amp;lt;&amp;quot; nanoseconds.\n&amp;quot;&amp;lt;&amp;lt;std::endl;

	Func functor{10};
	total = 0;
	start = high_resolution_clock::now();
	for (int i = 0; i &amp;lt; num; ++i)
	{
		total += functor(i);
	}
	end = high_resolution_clock::now();
	std::cout&amp;lt;&amp;lt;&amp;quot;result: &amp;quot;&amp;lt;&amp;lt;total&amp;lt;&amp;lt;&amp;quot;\nfunctor call elapsed: \t&amp;quot;&amp;lt;&amp;lt;duration_cast&amp;lt;nanoseconds&amp;gt;(end-start).count()&amp;lt;&amp;lt;&amp;quot; nanoseconds.\n&amp;quot;&amp;lt;&amp;lt;std::endl;
	int base = 10;
	function&amp;lt;int(int)&amp;gt; lambda = [base](int i)
	{
		return i*base;
	};
	total = 0;
	start = high_resolution_clock::now();
	for (int i = 0; i &amp;lt; num; ++i)
	{
		total += lambda(i);
	}
	end = high_resolution_clock::now();
	std::cout&amp;lt;&amp;lt;&amp;quot;result: &amp;quot;&amp;lt;&amp;lt;total&amp;lt;&amp;lt;&amp;quot;\nlambda call elapsed: \t&amp;quot;&amp;lt;&amp;lt;duration_cast&amp;lt;nanoseconds&amp;gt;(end-start).count()&amp;lt;&amp;lt;&amp;quot; nanoseconds.\n&amp;quot;&amp;lt;&amp;lt;std::endl;
	return 0;
}

/*
test on mac mini i7 2.7GHz
clang++ -std=c++11 chronotest.cpp -O0
output:
result: 499950000
virtual call elapsed: 	43171 nanoseconds.

result: 499950000
direct function call elapsed: 	31379 nanoseconds.

result: 499950000
functor call elapsed: 	41497 nanoseconds.

result: 499950000
lambda call elapsed: 	207416 nanoseconds.
===================================================
clang++ -std=c++11 chronotest.cpp -O1
output:
result: 49995000

## Epilogue

```shell
result: 499950000
virtual call elapsed: 6143 nanoseconds.

result: 499950000
direct function call elapsed: 30 nanoseconds.

result: 499950000
functor call elapsed: 31 nanoseconds.

result: 499950000
lambda call elapsed: 15134 nanoseconds.
&lt;/code&gt;&lt;/pre&gt;
</description>
        </item>
        <item>
        <title>Host byte, network byte, observe directly through debugger</title>
        <link>https://ttf248.life/en/p/host-network-byte-order-debugger/</link>
        <pubDate>Tue, 10 Jan 2023 14:18:12 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/host-network-byte-order-debugger/</guid>
        <description>&lt;p&gt;Throughout the history of computer development, there has been no unified standard for data storage. There are two commonly used rules for byte arrangement. For example, if the low-order bits of a multi-digit number are placed at smaller addresses and the high-order bits are placed at larger addresses, it is referred to as little-endian; conversely, it is called big-endian. In network applications, byte order is a factor that must be considered because different types of machines may adopt different standards, so they are all converted according to the network standard.
According to reading habits, big-endian byte order is more consistent with the left-to-right reading order.&lt;/p&gt;
&lt;h2 id=&#34;processor-architecturehttpszhwikipediaorgwikie5ad97e88a82e5ba8f&#34;&gt;&lt;a class=&#34;link&#34; href=&#34;https://zh.wikipedia.org/wiki/%E5%AD%97%E8%8A%82%E5%BA%8F&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Processor Architecture&lt;/a&gt;
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;Processors such as x86, MOS Technology 6502, Z80, VAX, and PDP-11 use little-endian byte order.&lt;/li&gt;
&lt;li&gt;Processors like Motorola 6800, Motorola 68000, PowerPC 970 use big-endian byte order.&lt;/li&gt;
&lt;li&gt;The byte order of processors such as ARM, PowerPC (excluding PowerPC 970), DEC Alpha, SPARC V9, MIPS, PA-RISC and IA64 is configurable.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;network-byte-order&#34;&gt;Network Byte Order
&lt;/h2&gt;&lt;p&gt;Network transmission generally uses big-endian byte order, also known as network byte order or network order. The IP protocol defines big-endian as the network byte order.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;Berkeley&lt;/code&gt; sockets API defined a set of conversion functions to convert 16 and 32-bit integers between network byte order and host byte order.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;#include &amp;lt;arpa/inet.h&amp;gt;

uint32_t htonl(uint32_t hostlong); // Converts a uint32_t from host byte order to network byte order
uint16_t htons(uint16_t hostshort); // Converts a uint16_t from host byte order to network byte order
uint32_t ntohl(uint32_t netlong); // Converts a uint32_t from network byte order to host byte order
uint16_t ntohs(uint16_t netshort); // Converts a uint16_t from network byte order to host byte order
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If using &lt;code&gt;asio&lt;/code&gt; as the networking library, its internal namespace provides cross-platform adaptation functions with different names:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;boost::asio::detail::socket_ops::network_to_host_long&lt;/li&gt;
&lt;li&gt;boost::asio::detail::socket_ops::network_to_host_short&lt;/li&gt;
&lt;li&gt;boost::asio::detail::socket_ops::host_to_network_long&lt;/li&gt;
&lt;li&gt;boost::asio::detail::socket_ops::host_to_network_short&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;visual-studio-debugger&#34;&gt;Visual Studio Debugger
&lt;/h2&gt;&lt;p&gt;In debugging mode, select the Debug menu, Window, and checkmark the Memory window. Within &lt;code&gt;Visual Studio&lt;/code&gt;, you can directly view data in memory using the debugger, as shown in the following image:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ttf248.life/p/host-network-byte-order-debugger/Snipaste_2023-01-10_14-44-00.png&#34;
	width=&#34;535&#34;
	height=&#34;147&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;Debugger Menu&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;363&#34;
		data-flex-basis=&#34;873px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;ways-to-view-memory&#34;&gt;Ways to View Memory
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;The window directly outputs &amp;amp;variable name and jumps to the corresponding variable address.&lt;/li&gt;
&lt;li&gt;If the variable is originally a pointer, double-click on the variable to select it and drag it to the memory window to display the content at the corresponding address.&lt;/li&gt;
&lt;li&gt;If the variable is not a pointer, add it to the calculation window to obtain its address, then manually copy it to the memory window.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;lets-illustrate-with-an-example&#34;&gt;Let&amp;rsquo;s illustrate with an example
&lt;/h3&gt;&lt;p&gt;Receive a data segment, stored in the &lt;code&gt;buffer&lt;/code&gt; object, convert network byte order to host byte order, resulting in &lt;code&gt;body_length&lt;/code&gt; being 30. The server allocates four bytes for transmitting this data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;bool NetworkMessage::decode_header()
{
    // Network byte order to host byte order
    body_length_ = boost::asio::detail::socket_ops::network_to_host_long(*(int *)buffer_.data());
    return auto_reserve(body_length_);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Big-endian byte order&lt;/strong&gt;: When observing the &lt;code&gt;buffer_&lt;/code&gt; content in memory,
&lt;img src=&#34;https://ttf248.life/p/host-network-byte-order-debugger/buffer_.png&#34;
	width=&#34;603&#34;
	height=&#34;318&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;buffer_&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;189&#34;
		data-flex-basis=&#34;455px&#34;
	
&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Little-endian byte order&lt;/strong&gt;: When observing the content of &lt;code&gt;body_length_&lt;/code&gt; in memory,
&lt;img src=&#34;https://ttf248.life/p/host-network-byte-order-debugger/body_length_.png&#34;
	width=&#34;581&#34;
	height=&#34;333&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;body_length_&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;174&#34;
		data-flex-basis=&#34;418px&#34;
	
&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>C11: sleep for vs yield</title>
        <link>https://ttf248.life/en/p/c11-sleep-for-vs-yield/</link>
        <pubDate>Tue, 20 Sep 2022 20:54:51 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/c11-sleep-for-vs-yield/</guid>
        <description>&lt;p&gt;While reviewing the code, &lt;code&gt;std::this_thread::yield()&lt;/code&gt; suddenly popped into my view, a syntax sugar from &lt;code&gt;C11&lt;/code&gt; that I’d used quite a bit, but hadn&amp;rsquo;t noticed before.&lt;/p&gt;
&lt;p&gt;I didn’t consult the manual; first, I thought it had something to do with asynchronous operations – the word was used in the coroutine implementation of the Boost library.  Clearly, it wasn’t related to coroutines; it’s about controlling logic within a regular thread.&lt;/p&gt;
&lt;h2 id=&#34;documentation&#34;&gt;Documentation
&lt;/h2&gt;&lt;h3 id=&#34;yield&#34;&gt;yield
&lt;/h3&gt;&lt;p&gt;The accuracy of this function depends on the implementation, particularly the OS scheduler mechanism and system state used. For example, the First-Come, First-Served (FCFS) real-time scheduler (Linux’s SCHED_FIFO) will suspend the current thread and place it at the end of the queue for other threads with the same priority that are ready to run (and has no effect if there are no other threads with the same priority).&lt;/p&gt;
&lt;h3 id=&#34;sleep_for&#34;&gt;sleep_for
&lt;/h3&gt;&lt;p&gt;Blocks the current thread&amp;rsquo;s execution for at least the specified &lt;code&gt;sleep_duration&lt;/code&gt;.
This function may block longer than &lt;code&gt;sleep_duration&lt;/code&gt; due to scheduling or resource contention delays.
The standard library recommends measuring durations using a monotonic clock. If implementing with system time instead, waiting times may be sensitive to clock adjustments.&lt;/p&gt;
&lt;h3 id=&#34;analysis&#34;&gt;Analysis
&lt;/h3&gt;&lt;p&gt;Both functions are designed to release the current thread and its associated resources, with the actual effect depending on the platform. I’m still a bit unclear at this point, so let&amp;rsquo;s run the code to see the execution results.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;ThinkPad laptop (Visual Studio Community 2022), Tencent Cloud S2 Standard Server (gcc8.5)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;analysis-1&#34;&gt;Analysis
&lt;/h3&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Execution Platform&lt;/th&gt;
&lt;th&gt;Function&lt;/th&gt;
&lt;th&gt;First/us&lt;/th&gt;
&lt;th&gt;Second/us&lt;/th&gt;
&lt;th&gt;Third/us&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;/table&gt;
&lt;h3 id=&#34;analysis-2&#34;&gt;Analysis
&lt;/h3&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Execution Platform&lt;/th&gt;
&lt;th&gt;Function&lt;/th&gt;
&lt;th&gt;First Time / us&lt;/th&gt;
&lt;th&gt;Second Time / us&lt;/th&gt;
&lt;th&gt;Third Time / us&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;/table&gt;
&lt;h3 id=&#34;analysis-3&#34;&gt;Analysis
&lt;/h3&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Execution Platform&lt;/th&gt;
&lt;th&gt;Function&lt;/th&gt;
&lt;th&gt;First/us&lt;/th&gt;
&lt;th&gt;Second/us&lt;/th&gt;
&lt;th&gt;Third/us&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;/table&gt;
&lt;h3 id=&#34;analysis-4&#34;&gt;Analysis
&lt;/h3&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Execution Platform&lt;/th&gt;
&lt;th&gt;Function&lt;/th&gt;
&lt;th&gt;First/us&lt;/th&gt;
&lt;th&gt;Second/us&lt;/th&gt;
&lt;th&gt;Third/us&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;/table&gt;
&lt;h3 id=&#34;analysis-5&#34;&gt;Analysis
&lt;/h3&gt;&lt;p&gt;From the results, it’s clear that due to differences in operating system implementations, the stability of high-precision sleep varies greatly. If you want high-precision sleep, using &lt;code&gt;yield&lt;/code&gt; is more appropriate.&lt;/p&gt;
&lt;p&gt;When the time precision is increased to &lt;code&gt;ms&lt;/code&gt;, the difference between the two methods is not significant.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://qingcms.gitee.io/cppreference/20210212/zh/cpp/header/thread.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://qingcms.gitee.io/cppreference/20210212/zh/cpp/header/thread.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://qingcms.gitee.io/cppreference/20210212/zh/cpp/thread/sleep_for.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://qingcms.gitee.io/cppreference/20210212/zh/cpp/thread/sleep_for.html&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
        </item>
        <item>
        <title>Linux Server, Reset MySQL Password</title>
        <link>https://ttf248.life/en/p/linux-server-reset-mysql-password/</link>
        <pubDate>Tue, 20 Sep 2022 14:27:35 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/linux-server-reset-mysql-password/</guid>
        <description>&lt;p&gt;I had an idle Tencent Cloud server that was expiring at the end of the year and I hadn&amp;rsquo;t planned to renew it. So, I decided to deploy a MySQL database for development purposes. When reinstalling the system, I wanted to save time and chose a third-party image provided by Tencent Cloud, which already had MySQL installed. I thought the system should include a Readme file or similar documentation explaining the password and deployment path.&lt;/p&gt;
&lt;p&gt;The Tencent Cloud server reinstalled very quickly, taking about a minute. Once logged in, &lt;code&gt;systemctl status mysql&lt;/code&gt; showed that the service was running. I started searching for the password but couldn&amp;rsquo;t find it anywhere, and I began to panic.&lt;/p&gt;
&lt;p&gt;Then, I thought, since I had already accessed the server with root privileges, there must be a way to reset the password. I searched through documentation and found a forum post on Alibaba Cloud’s forum, continuing to tinker.&lt;/p&gt;
&lt;h2 id=&#34;reset-password&#34;&gt;Reset Password
&lt;/h2&gt;&lt;p&gt;Edit the configuration file &lt;code&gt;vim /etc/my.cnf&lt;/code&gt;, add the following configuration to the &lt;code&gt;mysqld&lt;/code&gt; node: &lt;code&gt;skip-grant-tables&lt;/code&gt;, and execute the command to restart the database: &lt;code&gt;systemctl restart mysql&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Then, log in directly to the data using &lt;code&gt;mysql&lt;/code&gt;, after which everything will proceed normally. To reset the &lt;code&gt;root&lt;/code&gt; user password and enable allowing remote login simultaneously:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;USE mysql;
UPDATE user SET authentication_string = password(&#39;pass&#39;) WHERE User = &#39;root&#39;;
GRANT ALL PRIVILEGES ON *.* TO &#39;root&#39;@&#39;%&#39; IDENTIFIED BY &#39;pass&#39; WITH GRANT OPTION;
FLUSH PRIVILEGES;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To revert the modified configuration file, restart the database, and you’re done.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://help.aliyun.com/document_detail/42520.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://help.aliyun.com/document_detail/42520.html&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
        </item>
        <item>
        <title>A Brief Overview of Automated Testing</title>
        <link>https://ttf248.life/en/p/automated-testing-overview/</link>
        <pubDate>Thu, 04 Aug 2022 11:39:18 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/automated-testing-overview/</guid>
        <description>&lt;p&gt;The investment in testing for financial trading systems far exceeded that of other systems, with tedious test steps repeatedly executed and a low &lt;code&gt;ROI&lt;/code&gt;. As projects and personnel changed, uncontrollable factors inevitably introduced, a common situation being the modification of a field output from Interface A impacting the results of Interface B. With each version release, risk also accumulated.&lt;/p&gt;
&lt;h2 id=&#34;theoretical-knowledge&#34;&gt;Theoretical Knowledge
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;How to Measure the Value of Automation?&lt;/strong&gt;
An automation testing ROI = (Manual Execution Time) * (Number of Runs) / (Development Cost + Maintenance Cost)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Which Features Should Be Automated?&lt;/strong&gt;
Frequently used features that are unlikely to change. Writing automated test code for this type of interface yields the highest returns.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Why Choose This Timing to Drive Automation Testing?&lt;/strong&gt;
Not appropriate near project launch – distant water doesn’t quench immediate thirst; automation is a long-term return model. It&amp;rsquo;s most suitable when the project is already in a &lt;strong&gt;production environment&lt;/strong&gt; and within a stable release cycle.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;framework-selection&#34;&gt;Framework Selection
&lt;/h2&gt;&lt;p&gt;Given the task of automation testing without prior practical experience, a typical starting point is to open a search engine and find tools and frameworks that can be used with the current system’s &lt;strong&gt;technology stack&lt;/strong&gt;, review the user manuals, and get started. If you can immediately find a suitable tool, congratulations, &lt;strong&gt;perfect start&lt;/strong&gt;!&lt;/p&gt;
&lt;p&gt;Let me preface this by saying I might be wrong. After reviewing relevant materials, it&amp;rsquo;s not that these frameworks don’t exist; rather, they are too complex and consume excessive resources. For beginners, what’s needed is something small, streamlined, and concise. Consulting with colleagues in the testing group led to the suggestion of a &lt;code&gt;Python&lt;/code&gt; self-built framework – essentially, wrapping existing unit testing frameworks into an automated testing framework.&lt;/p&gt;
&lt;p&gt;Referencing the design thinking for this project: &lt;a class=&#34;link&#34; href=&#34;https://github.com/wintests/pytestDemo&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/wintests/pytestDemo&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;why-use-a-framework&#34;&gt;Why Use a Framework?
&lt;/h2&gt;&lt;p&gt;Services have multiple different deployment environments – development, testing, and live testing environments. The purpose of a framework is to act as an abstraction layer, separating test cases and data. This allows for configuring different case data according to various environment configurations, and it also supports shared data.&lt;/p&gt;
&lt;p&gt;The core logic is focused on increasing the utilization of automation. When scenarios become more complex, the data between different environments is completely unrelated and has no bearing on each other. Simply add a &lt;code&gt;label&lt;/code&gt; tag when specifying case data to indicate the supported environment.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References
&lt;/h2&gt;&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://time.geekbang.org/column/article/496850&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Best Value Automation Testing&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Why Do We Need to Learn a New Language?</title>
        <link>https://ttf248.life/en/p/why-learn-a-new-language/</link>
        <pubDate>Thu, 04 Aug 2022 11:27:30 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/why-learn-a-new-language/</guid>
        <description>&lt;p&gt;Starting from my academic years, I’ve been working with &lt;code&gt;C++&lt;/code&gt; for over ten years. So, why do I need to learn other programming languages?&lt;/p&gt;
&lt;p&gt;Work experience: Lacking experience in elegant module design, &lt;code&gt;C++&lt;/code&gt; syntax is freeform. Learning other languages helps me guide the development of more elegant designs.&lt;/p&gt;
&lt;p&gt;I often use them when writing some tools. The design principles for low-level libraries and business modules are also becoming clearer.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Visual Studio Compilation Character Set [Converted]</title>
        <link>https://ttf248.life/en/p/visual-studio-character-set/</link>
        <pubDate>Thu, 04 Aug 2022 10:51:43 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/visual-studio-character-set/</guid>
        <description>&lt;p&gt;&lt;code&gt;C++&lt;/code&gt; cross-platform development. Commonly encountered on Chinese operating systems: &lt;code&gt;error C2001&lt;/code&gt; - constant contains a newline character.&lt;/p&gt;
&lt;h2 id=&#34;visual-studio&#34;&gt;Visual Studio
&lt;/h2&gt;&lt;p&gt;&lt;code&gt;cmake&lt;/code&gt; organizes the project compilation script, generating a temporary solution under the &lt;code&gt;windows&lt;/code&gt; system for development. The reason for cross-platform compatibility is that the file encoding is chosen as &lt;code&gt;utf-8&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The cited reference provides a detailed explanation of the causes of the problem from first principles.&lt;/p&gt;
&lt;p&gt;Regarding encoding, &lt;code&gt;MSVC&lt;/code&gt; has dedicated compilation options &lt;code&gt;/source-charset&lt;/code&gt; and &lt;code&gt;/execution-charset&lt;/code&gt;. The former indicates the encoding of the file itself, and the latter indicates what encoding the byte array inside the compiled character set is. Encoding issues can basically be solved using these two options.&lt;/p&gt;
&lt;p&gt;For example, the &lt;code&gt;windows&lt;/code&gt; &lt;code&gt;cmd&lt;/code&gt; console defaults to displaying &lt;code&gt;gbk&lt;/code&gt; encoding. However, the code file itself is &lt;code&gt;utf-8&lt;/code&gt;. Because of cross-platform considerations and it&amp;rsquo;s inconvenient to directly convert it to &lt;code&gt;gbk&lt;/code&gt;, we don’t include the method of writing encoding conversion code for different platforms. On &lt;code&gt;Win10&lt;/code&gt;, we can set these compilation options to &lt;code&gt;/source-charset:utf-8 /execution-charset:gbk&lt;/code&gt;, indicating that the compiler reads in with &lt;code&gt;utf-8&lt;/code&gt; encoding and then converts it to &lt;code&gt;gbk&lt;/code&gt; to store in the array, so &lt;code&gt;printf&lt;/code&gt; can display Chinese characters normally in the &lt;code&gt;cmd&lt;/code&gt; console.&lt;/p&gt;
&lt;h2 id=&#34;cmake-configuration-for-visual-studio&#34;&gt;CMake Configuration for Visual Studio
&lt;/h2&gt;&lt;pre&gt;&lt;code class=&#34;language-cmake&#34;&gt;if(WIN32)
    message(STATUS &amp;quot;Configuring trade on WIN32&amp;quot;)
    set(CMAKE_CXX_FLAGS &amp;quot;${CMAKE_CXX_FLAGS} /source-charset:utf-8 /execution-charset:gbk&amp;quot;)
endif()
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;references&#34;&gt;References
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://zhuanlan.zhihu.com/p/146543940&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://zhuanlan.zhihu.com/p/146543940&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>Quickly calculate folder size on the Windows platform</title>
        <link>https://ttf248.life/en/p/windows-platform-quick-folder-size-statistics/</link>
        <pubDate>Mon, 01 Aug 2022 19:54:18 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/windows-platform-quick-folder-size-statistics/</guid>
        <description>&lt;p&gt;The Linux platform is very simple: &lt;code&gt;du -sh *&lt;/code&gt; – just one line of code solves the problem. What about Windows? With many disks and a desire to clean up, with numerous files, the system’s built-in Resource Manager is too slow to calculate folder sizes, making you want to give up.&lt;/p&gt;
&lt;h2 id=&#34;everything&#34;&gt;Everything
&lt;/h2&gt;&lt;p&gt;For developers working on the &lt;code&gt;windows&lt;/code&gt; platform, you probably haven&amp;rsquo;t personally used &lt;code&gt;everything&lt;/code&gt;, and should at least have heard of it. Its search speed far exceeds that of the built-in file explorer. Given that system-level support for fast indexing exists, we can certainly find similar tools that build file indexes while also calculating file sizes.&lt;/p&gt;
&lt;h2 id=&#34;wiztree&#34;&gt;WizTree
&lt;/h2&gt;&lt;p&gt;Website: &lt;a class=&#34;link&#34; href=&#34;https://www.diskanalyzer.com/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://www.diskanalyzer.com/&lt;/a&gt;
Use the standard installation method or the green version to unzip and run.
It’s fast, with a wide variety of data display types – the left side features a tree diagram mode, the right side displays file types, and there&amp;rsquo;s also graphical visualization available at the bottom of the software.&lt;/p&gt;
&lt;h2 id=&#34;spacesniffer-update-2023-no-longer-maintained&#34;&gt;SpaceSniffer (Update 2023 No Longer Maintained)
&lt;/h2&gt;&lt;p&gt;Software Website: &lt;a class=&#34;link&#34; href=&#34;http://www.uderzo.it/main_products/space_sniffer/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;http://www.uderzo.it/main_products/space_sniffer/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The operation is very simple – select the corresponding drive letter, and the software will display folder sizes in a graphical way, with larger volumes resulting in larger matrices in the images.  Other operations are easily understood by simply clicking on them. It supports inputting filter conditions for files:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;File size filtering&lt;/li&gt;
&lt;li&gt;File date filtering&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;&lt;img src=&#34;https://ttf248.life/p/windows-platform-quick-folder-size-statistics/space_sniffer.gif&#34;
	width=&#34;960&#34;
	height=&#34;540&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;Basic Usage&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;177&#34;
		data-flex-basis=&#34;426px&#34;
	
&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;img src=&#34;https://ttf248.life/p/windows-platform-quick-folder-size-statistics/advance.gif&#34;
	width=&#34;960&#34;
	height=&#34;540&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;Advanced Usage&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;177&#34;
		data-flex-basis=&#34;426px&#34;
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References
&lt;/h2&gt;&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://moe.best/software/spacesniffer.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://moe.best/software/spacesniffer.html&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>How to Copy Webpage Stylesheets (CSS): Element Inspector</title>
        <link>https://ttf248.life/en/p/how-to-copy-webpage-css-element-inspect/</link>
        <pubDate>Sun, 31 Jul 2022 23:36:48 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/how-to-copy-webpage-css-element-inspect/</guid>
        <description>&lt;p&gt;Static blog themes, the mainstream is based on foreign templates, modified and adjusted without much consideration for Chinese content layout.&lt;/p&gt;
&lt;h2 id=&#34;main-text&#34;&gt;Main Text
&lt;/h2&gt;&lt;p&gt;About half a month ago, I adjusted the blog’s stylesheet – as I&amp;rsquo;ve been developing backend services for many years, I’m a pure newbie in frontend. After struggling with it for half a day, the design wasn’t quite reasonable. Suddenly an idea struck me: I looked at the technical blogs I often read – infoq and OpenChina – and their layouts look really good. Could I borrow some of those? After reviewing the source files, I got lost trying to locate the relevant elements.&lt;/p&gt;
&lt;p&gt;Frontend developers might be laughing at this point… I didn’t understand how to locate the specified elements, but that&amp;rsquo;s okay; I have plenty of free time on weekends, so I stopped and thought about it. It reminded me of when I wrote Python crawlers – I had used something similar before.&lt;/p&gt;
&lt;h3 id=&#34;element-inspection&#34;&gt;Element Inspection
&lt;/h3&gt;&lt;p&gt;That’s right, it’s the browser&amp;rsquo;s built-in element inspection tool – copying stylesheets, locating specific elements, all done in minutes. &lt;code&gt;selector&lt;/code&gt; to locate elements, &lt;code&gt;hugo&lt;/code&gt; to create a “user define css”&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Copy element&lt;/li&gt;
&lt;li&gt;Copy outerHTML&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Copy selector&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Copy JS path&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Copy styles&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Copy XPath&lt;/li&gt;
&lt;li&gt;Copy full XPath&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>Editing Extremely Large Files on the Windows Platform: EmEditor (Text Editor)</title>
        <link>https://ttf248.life/en/p/windows-platform-edit-large-files-emeditor-text-editor/</link>
        <pubDate>Sun, 31 Jul 2022 23:21:24 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/windows-platform-edit-large-files-emeditor-text-editor/</guid>
        <description>&lt;p&gt;The Shanghai GuoAn database incident, which caused a huge stir within the black hacking circles, remains unclear whether it’s true or false. Let&amp;rsquo;s see what we remember in two years and look back on it then. Based on past experience, I updated a batch of local social engineering databases, and I encountered a massive SQL file: 17.9G. A regular text editor couldn&amp;rsquo;t even preview it, let alone open it. Chatting with netizens, someone mentioned EmEditor.&lt;/p&gt;
&lt;h2 id=&#34;text&#34;&gt;Text
&lt;/h2&gt;&lt;p&gt;Website: &lt;a class=&#34;link&#34; href=&#34;https://www.emeditor.com/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://www.emeditor.com/&lt;/a&gt;
I took a look at it over the weekend and found it quite convenient. The design supports editing large files, and when sufficient memory is available, the entire file is loaded into memory, resulting in fast search and editing speeds. It also supports splitting files.&lt;/p&gt;</description>
        </item>
        <item>
        <title>Adding a code copy button for seemingly simple things</title>
        <link>https://ttf248.life/en/p/add-copy-button-for-simple-task/</link>
        <pubDate>Fri, 25 Feb 2022 01:23:39 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/add-copy-button-for-simple-task/</guid>
        <description>&lt;blockquote&gt;
&lt;p&gt;Spent four hours on this, and when I saw the sentence, it was hilarious. How could it have taken so long? Finally looked at the time: three hours.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This was the first draft of the year 2022, and it wasn’t complicated – exactly as the title said. (At that time, I was still quite young), I thought simply copying &lt;a class=&#34;link&#34; href=&#34;https://ouuan.github.io/post/from-hexo-to-hugo/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;作业&lt;/a&gt; would be enough, putting it in my favorites folder, and letting it sit for over a month before finally remembering the task.&lt;/p&gt;
&lt;p&gt;When migrating to &lt;code&gt;hugo&lt;/code&gt;, I always felt that the plugins were too few, couldn’t copy code, which made copying notes from Evernote to the blog very cumbersome, seriously hindering my motivation for writing a casual blog.&lt;/p&gt;
&lt;h2 id=&#34;foreword&#34;&gt;Foreword
&lt;/h2&gt;&lt;p&gt;First, carefully read the original author’s draft, read it through completely and flip through their introduction. Wow, I ran into a big shot – Tsinghua University undergraduate, has been exposed to computers since early on, huh? Just a facade, let&amp;rsquo;s take a look at this blog first, completely forgot what he was supposed to do. Also, check out the author’s &lt;code&gt;Github&lt;/code&gt; repository; this modified ‘even’ theme is much prettier than now and has more features, let’s get started, merge the relevant code into it.
&lt;img src=&#34;https://ttf248.life/p/add-copy-button-for-simple-task/2022-02-25-02-08-19.png&#34;
	width=&#34;215&#34;
	height=&#34;150&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;143&#34;
		data-flex-basis=&#34;344px&#34;
	
&gt; &lt;img src=&#34;https://ttf248.life/p/add-copy-button-for-simple-task/2022-02-25-02-08-40.png&#34;
	width=&#34;217&#34;
	height=&#34;167&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;1&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;129&#34;
		data-flex-basis=&#34;311px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;new-features-view-article-history-associate-submission-records&#34;&gt;New Features: View Article History, Associate Submission Records
&lt;/h3&gt;&lt;p&gt;The effect is still good, and you can experience it by dragging it to the end of the article.&lt;/p&gt;
&lt;p&gt;Before taking action, I didn’t carefully examine the author&amp;rsquo;s original repository history, assuming a simple merge would fix everything.  Ultimately, I merged a huge amount of code with numerous conflicts and N times of manual overrides – all of which were frontend and rendering template code, using the one that matched my requirements.&lt;/p&gt;
&lt;p&gt;Repository Address: &lt;a class=&#34;link&#34; href=&#34;https://github.com/TianlongXiang/hugo-theme-even&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/TianlongXiang/hugo-theme-even&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;A Chinese pitfall: if &lt;code&gt;git&lt;/code&gt; doesn’t adjust this parameter, it will cause the generated link to not be able to obtain the current article&amp;rsquo;s &lt;code&gt;commit hash&lt;/code&gt;, resulting in history link generation failure.  When generating the complete historical record of the article, you also need to modify the automatic integration script and remember to pull the entire historical record of the current repository.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/TianlongXiang/TianlongXiang.github.io/commit/0d860154de9b335ed88a57d62eedd4b6e3ccbeee&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;feat: Attempt to fetch the full GitHub repository to dynamically update the last modification record of the article&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/TianlongXiang/TianlongXiang.github.io/commit/1b5d719966737f16a8c67880370dc2722adea0b3&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;chore: Path contains Chinese, hugo GitInfo needs to enable this setting&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;name: Build Github
run: git config --global core.quotePath false &amp;amp;&amp;amp; hugo -b &amp;quot;https://www.xiangtianlong.com/&amp;quot; -d &amp;quot;github_public&amp;quot; &amp;amp;&amp;amp; ls
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;style-adjustments&#34;&gt;Style Adjustments
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Adjust website content width, the previous design was suitable for both mobile and desktop devices; in reality, no one actually viewed it on their phones, and I personally view it on my computer.&lt;/li&gt;
&lt;li&gt;The directory bar should support automatic expansion/contraction.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;body&#34;&gt;Body
&lt;/h2&gt;&lt;p&gt;Referenced &lt;code&gt;ouuan&lt;/code&gt;’s code records for half an hour or so, and still couldn&amp;rsquo;t quite understand how to increase the copy button.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Time travel, a month later, it came back to this matter again&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;text&#34;&gt;Text
&lt;/h2&gt;&lt;p&gt;Since I didn’t understand this assignment, I switched to copying from another source – it was definitely easier to grasp. The results of my search were surprisingly helpful; a forum post on the official &lt;code&gt;hugo&lt;/code&gt; website detailed how to add a copy button. After checking it out, I realized that the code block styles generated by &lt;code&gt;even&lt;/code&gt; differed from the descriptions in the documentation – this part was quite complicated. So, I’m just going to record this for reference.&lt;/p&gt;
&lt;p&gt;Because I don&amp;rsquo;t really understand front-end development, when I encounter something I don’t get, I open my browser’s “Inspect” tool and analyze the code alongside the style information on the right; I use JavaScript logs to help me understand the logic. There were many things I didn’t understand at first. Taking a deep breath and carefully breaking down the logic helped me find a solution eventually.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The &lt;code&gt;&amp;lt;pre&amp;gt;&lt;/code&gt; nodes are multiple, this refers to a single code block. The theme itself rendered line numbers, which resulted in the copy button appearing twice.&lt;/li&gt;
&lt;li&gt;I wanted to disable the theme’s built-in code highlighting; unfortunately, I wasn&amp;rsquo;t familiar with the settings for this theme.&lt;/li&gt;
&lt;li&gt;I consulted the &lt;code&gt;hugo&lt;/code&gt; website and read some documentation – it was a bit confusing at first, but I learned that the &lt;code&gt;markup&lt;/code&gt; setting could control code highlighting.&lt;/li&gt;
&lt;li&gt;I adjusted the configuration file, but it didn’t work; the rendered output differed from my expectations.&lt;/li&gt;
&lt;li&gt;I discovered this bunch of settings called &lt;code&gt;pygmentsOptions&lt;/code&gt;, so I continued to consult documentation and adjust the settings, first removing the line numbers.&lt;/li&gt;
&lt;li&gt;I configured a custom CSS stylesheet and a custom JavaScript script.&lt;/li&gt;
&lt;li&gt;Since I’d already done so much, my brain suddenly remembered seeing a nice color palette recently, so I modified the button styles: Let&amp;rsquo;s go with the traditional Chinese sky blue!&lt;/li&gt;
&lt;/ol&gt;
&lt;img class=&#34;special-img-class&#34; style=&#34;width:20%&#34; src=&#34;2022-02-25-02-01-22.png&#34; /&gt;
&lt;img class=&#34;special-img-class&#34; style=&#34;width:20%&#34; src=&#34;2022-02-25-02-01-33.png&#34; /&gt;
&lt;p&gt;I spent four hours on this, and when I saw that sentence, I even found it funny – how could it take so long? Finally, I looked at the time: It was only three hours.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://ouuan.github.io/post/from-hexo-to-hugo/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://ouuan.github.io/post/from-hexo-to-hugo/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://gohugobrasil.netlify.app/content-management/syntax-highlighting/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://gohugobrasil.netlify.app/content-management/syntax-highlighting/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://gohugo.io/getting-started/configuration-markup#highlight&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://gohugo.io/getting-started/configuration-markup#highlight&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.dannyguo.com/blog/how-to-add-copy-to-clipboard-buttons-to-code-blocks-in-hugo/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://www.dannyguo.com/blog/how-to-add-copy-to-clipboard-buttons-to-code-blocks-in-hugo/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>Chaos Engineering</title>
        <link>https://ttf248.life/en/p/chaos-engineering/</link>
        <pubDate>Wed, 28 Jul 2021 14:35:07 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/chaos-engineering/</guid>
        <description>&lt;p&gt;A pattern of disruption to test system stability.&lt;/p&gt;
&lt;h2 id=&#34;main-text&#34;&gt;Main Text
&lt;/h2&gt;&lt;p&gt;The domestic internet industry is always fond of playing with new things, often introducing terms that most people wouldn’t be able to guess what they are.
After reading some articles, this definition specifically for the early stages of Chaos Engineering is relatively easy to accept:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Early exploration of Chaos Engineering has actually been ongoing within the industry, previously existing under the guise of fault testing and disaster recovery exercises. As microservice architectures continue to develop and distributed systems grow ever larger, Chaos Engineering has begun to emerge and gain increasing importance. Following Netflix’s formal proposal of the Chaos Engineering concept, related theories have also rapidly enriched. Netflix&amp;rsquo;s practices have also demonstrated the significant impact Chaos Engineering has on stability.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;reference-links&#34;&gt;Reference Links
&lt;/h2&gt;&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.infoq.cn/article/gsqtykoa3uvrtqi1kkmo&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;ByteDance Chaos Engineering Practices Summary&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Kubernetes paused pod</title>
        <link>https://ttf248.life/en/p/kubernetes-pause-pod/</link>
        <pubDate>Mon, 12 Jul 2021 11:23:09 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/kubernetes-pause-pod/</guid>
        <description>&lt;p&gt;Deployment controllers implement a crucial function within a Kubernetes cluster: the ability to horizontally scale and shrink Pods. This capability was essential for traditional cloud-era platforms.&lt;/p&gt;
&lt;p&gt;Encountering a business scenario where you need to modify data in a database, restarting Pod nodes after adjustments. However, during Pod operation, table fields are continuously modified, requiring temporary pausing of application updates to tables, adjusting the data, and then restoring the Pod.&lt;/p&gt;
&lt;p&gt;Besides abruptly deleting the Deployment, are there other ways to achieve a similar pause effect?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;kubectl scale --replicas=0 deployment/&amp;lt;your-deployment&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Before seeing the answer, many people might have thought that simply operating processes was the way to go, stuck in the mindset of directly manipulating business processes instead of realizing the solution.&lt;/p&gt;
&lt;h2 id=&#34;reference-links&#34;&gt;Reference Links
&lt;/h2&gt;&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://stackoverflow.com/questions/54821044/how-to-stop-pause-a-pod-in-kubernetes&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;How to Stop/Pause a Pod in Kubernetes&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Docker Basics, Intermediate, and Advanced</title>
        <link>https://ttf248.life/en/p/docker-two-three-things/</link>
        <pubDate>Thu, 21 Jan 2021 09:26:07 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/docker-two-three-things/</guid>
        <description>&lt;p&gt;Having worked with CentOS for many years, content may not apply to macOS or Ubuntu users in some cases.&lt;/p&gt;
&lt;p&gt;You can refer to the documentation from Tsinghua University for installation guidance: &lt;a class=&#34;link&#34; href=&#34;https://mirrors.tuna.tsinghua.edu.cn/help/docker-ce/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://mirrors.tuna.tsinghua.edu.cn/help/docker-ce/&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;installation&#34;&gt;Installation
&lt;/h2&gt;&lt;p&gt;Due to unknown mysterious forces, domestic Docker installation is recommended to set the cloud vendor&amp;rsquo;s repository address. Here we recommend using &lt;strong&gt;Alibaba Cloud&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;set-repository-source-address&#34;&gt;Set Repository Source Address
&lt;/h3&gt;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;yum install yum-utils device-mapper-persistent-data lvm2 &amp;amp;&amp;amp; \
sudo yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;deploy-the-latest-version&#34;&gt;Deploy the Latest Version
&lt;/h3&gt;&lt;p&gt;Docker is a commonly used background service, we recommend setting it to start on boot. The following command applies to CentOS 7:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;sudo yum install -y docker-ce docker-ce-cli containerd.io &amp;amp;&amp;amp; systemctl enable --now docker
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;deploying-a-specific-version&#34;&gt;Deploying a Specific Version
&lt;/h3&gt;&lt;p&gt;The releases of &lt;code&gt;kubernetes&lt;/code&gt; and &lt;code&gt;docker&lt;/code&gt; are not fully synchronized. If you need to deploy &lt;code&gt;kubernetes&lt;/code&gt; subsequently, refer to the &lt;code&gt;kubernetes&lt;/code&gt; deployment instructions and install a specific version of &lt;code&gt;docker&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;yum list docker-ce --showduplicates | sort -r
sudo yum install -y docker-ce-18.09.2-3.el7 docker-ce-cli-18.09.2-3.el7 containerd.io-18.09.2-3.el7 &amp;amp;&amp;amp; systemctl enable --now docker
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;adding-docker-permissions-for-regular-users&#34;&gt;Adding Docker Permissions for Regular Users
&lt;/h3&gt;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;sudo usermod -aG docker ${USER}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;uninstall&#34;&gt;Uninstall
&lt;/h3&gt;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;sudo yum erase -y docker-ce docker-ce-cli containerd.io
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;everyday-use&#34;&gt;Everyday Use
&lt;/h2&gt;&lt;h3 id=&#34;mirror-acceleration&#34;&gt;Mirror Acceleration
&lt;/h3&gt;&lt;p&gt;There’s still an unknown mysterious force that causes slow image pulls. At this time, domestic cloud vendors have emerged and provided many acceleration services, which are still recommended – &lt;strong&gt;Alibaba Cloud&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The acceleration addresses can be managed by you registering an Alibaba Cloud account; this service is free. Alibaba Cloud also offers a free image build service.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cat &amp;gt; /etc/docker/daemon.json &amp;lt;&amp;lt;EOF
{
  &amp;quot;registry-mirrors&amp;quot;: [
    &amp;quot;https://docker.nju.edu.cn&amp;quot;,
    &amp;quot;https://mirror.baidubce.com&amp;quot;,
    &amp;quot;https://docker.m.daocloud.io&amp;quot;,
    &amp;quot;https://docker.mirrors.sjtug.sjtu.edu.cn&amp;quot;
  ]
}
EOF
systemctl daemon-reload &amp;amp;&amp;amp; \
systemctl restart docker
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;recommended-control-panels&#34;&gt;Recommended Control Panels
&lt;/h3&gt;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;docker volume create portainer_data &amp;amp;&amp;amp; \
docker run -d --name=portainer --restart=always -p 9000:9000 -v /var/run/docker.sock:/var/run/docker.sock -v portainer_data:/data portainer/portainer-ce:2.20.3-alpine
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;frequently-used-image-pull-list&#34;&gt;Frequently Used Image Pull List
&lt;/h3&gt;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;docker pull rancher/rancher:stable &amp;amp;&amp;amp; docker pull portainer/portainer-ce:2.0.1 &amp;amp;&amp;amp; \
docker pull centos:7 &amp;amp;&amp;amp; docker pull ubuntu:20.04 &amp;amp;&amp;amp; docker pull ubuntu:18.04 &amp;amp;&amp;amp; \
docker pull redis:5 &amp;amp;&amp;amp; docker pull redis:6 &amp;amp;&amp;amp; \
docker pull alpine:3.11 &amp;amp;&amp;amp; docker pull busybox:1.32 &amp;amp;&amp;amp; \
docker pull rabbitmq:3.7-management &amp;amp;&amp;amp; \
docker pull mariadb:10.2 &amp;amp;&amp;amp; \
docker pull nginx:1.18 &amp;amp;&amp;amp; docker pull nginx:1.19 &amp;amp;&amp;amp; \
docker pull mysql:5.6 &amp;amp;&amp;amp; docker pull mysql:8 &amp;amp;&amp;amp; \
docker pull elasticsearch:6.8.11 &amp;amp;&amp;amp; docker pull logstash:6.8.11 &amp;amp;&amp;amp; docker pull kibana:6.8.11 &amp;amp;&amp;amp; \
docker pull zookeeper:3.4 &amp;amp;&amp;amp; \
docker pull influxdb:1.7 &amp;amp;&amp;amp; docker pull grafana/grafana:7.3.1 &amp;amp;&amp;amp; \
docker pull percona:8 &amp;amp;&amp;amp; docker pull percona:5.6 &amp;amp;&amp;amp; \
docker pull cloverzrg/frps-docker:0.34.3 &amp;amp;&amp;amp; docker pull cloverzrg/frpc-docker:0.34.3
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;common-command-combinations&#34;&gt;Common Command Combinations
&lt;/h3&gt;&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://docs.docker.com/engine/reference/commandline/docker/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://docs.docker.com/engine/reference/commandline/docker/&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;View container running status, append the &lt;code&gt;format&lt;/code&gt; parameter to view detailed container information, and ignore image information.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;docker ps --format &amp;quot;{{.Names}}: {{.Ports}}: {{.Size}}&amp;quot;
#portainer: 0.0.0.0:8000-&amp;gt;8000/tcp, 0.0.0.0:9000-&amp;gt;9000/tcp: 0B (virtual 172MB)
#influxdb: 0.0.0.0:8086-&amp;gt;8086/tcp: 183B (virtual 311MB)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;p&gt;Stop all containers with one command&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;docker stop $(docker ps -a -q)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Delete all images with one command&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;docker rmi $(docker images -a -q)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;p&gt;Export image&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;docker save &amp;lt;IMAGE NAME&amp;gt;:&amp;lt;IMAGE TAG&amp;gt; &amp;gt; XXX.tar
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Export image and compress it&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;docker save &amp;lt;IMAGE NAME&amp;gt;:&amp;lt;IMAGE TAG&amp;gt; | gzip &amp;gt; XXX.tar
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Import image&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;docker load -i XXX.tar
&lt;/code&gt;&lt;/pre&gt;
</description>
        </item>
        <item>
        <title>Linux Setup JMeter Test Environment</title>
        <link>https://ttf248.life/en/p/linux-setup-jmeter-testing-environment/</link>
        <pubDate>Tue, 22 Dec 2020 10:12:50 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/linux-setup-jmeter-testing-environment/</guid>
        <description>&lt;p&gt;The author has a strong interest in hardware and used JMeter to conduct load testing, documenting the process of deploying JMeter, InfluxDB, and Grafana on CentOS 7. They shared installation and command usage for JMeter, InfluxDB’s features and Docker installation method, as well as a simple deployment and configuration for Grafana. They summarized experience and references related to high-performance programming patterns.&lt;/p&gt;
&lt;h2 id=&#34;background&#34;&gt;Background
&lt;/h2&gt;&lt;p&gt;As widely known, I have a very strong interest in hardware. By chance, the test team was using &lt;code&gt;JMeter&lt;/code&gt; to perform load tests and discovered that performance wasn&amp;rsquo;t improving. As a curious individual, I decisively took action to see how the company conducted its testing. There’s also a small story: at some point in the distant past, I read a post on OpenChina about how to create more impressive-looking performance test graphs – after observing &lt;code&gt;Windows&lt;/code&gt; versions execute tests and achieving visualized &lt;code&gt;TPS&lt;/code&gt; data display, what&amp;rsquo;s the use of configuring a web panel?&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Thinking is all well and good, but you have to try it yourself to understand.
Don’t use GUI mode for load testing! only for Test creation and Test debugging.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;background-1&#34;&gt;Background
&lt;/h2&gt;&lt;p&gt;Officially, it’s recommended to obtain test reports via the command line and display them using a GUI, which introduces data errors.  I don&amp;rsquo;t have deep knowledge of JMeter – at least I found a reason to tinker with a &lt;code&gt;Linux&lt;/code&gt; version console panel. The openchinese post’s core component deployment isn’t friendly; you need to follow their WeChat channel to download the required files, so as a young millennial, of course I used &lt;code&gt;Docker&lt;/code&gt; instead. Basically, my server is located domestically, and accessing the overseas source addresses is very slow – at least using an image service, Alibaba Cloud has a free acceleration.&lt;/p&gt;
&lt;p&gt;Regarding &lt;code&gt;docker&lt;/code&gt; installation and deployment, this will not be elaborated on here; please refer to previous articles for recommendations.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;The following content focuses on two main areas: setting up the basic test environment components and a simple explanation of each component.&lt;/p&gt;
&lt;h2 id=&#34;jmeter&#34;&gt;JMeter
&lt;/h2&gt;&lt;p&gt;Apache JMeter is a Java-based load testing tool developed by the Apache Software Foundation. It’s used to perform stress tests on software, initially designed for web application testing but later expanded to other testing domains. It can be used to test static and dynamic resources, such as static files, Java microservices, CGI scripts, Java objects, databases, FTP servers, etc. JMeter can simulate massive loads from various stress categories to test the strength of servers, networks, or objects and analyze overall performance. Furthermore, JMeter can perform functional/regression testing on applications by creating scripts with assertions to verify that your program returns the expected results. To maximize flexibility, JMeter allows using regular expressions to create assertions.&lt;/p&gt;
&lt;p&gt;Apache jmeter can be used to perform performance tests on static and dynamic resources (files, Servlets, Perl scripts, Java objects, databases and queries, FTP servers, etc.). It can be used to simulate heavy loads on servers, networks, or objects to test their strength or analyze overall performance under different stress types. You can use it for performance graphing or large concurrent load testing of your server/script/object.&lt;/p&gt;
&lt;h3 id=&#34;jmeter-deployment-on-centos7&#34;&gt;Jmeter Deployment on CentOS7
&lt;/h3&gt;&lt;p&gt;Install the &lt;code&gt;JDK&lt;/code&gt; runtime environment, download the &lt;code&gt;Jmeter&lt;/code&gt; installation package:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;yum install java-1.8.0-openjdk -y &amp;amp;&amp;amp; \
wget https://mirrors.bfsu.edu.cn/apache//jmeter/binaries/apache-jmeter-5.4.tgz &amp;amp;&amp;amp; tar -xf apache-jmeter-5.4.tgz
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Configure environment variables:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;export JMETER_HOME=$HOME/jmeter/apache-jmeter-5.4
export PATH=$JMETER_HOME/bin:$PATH
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;jmeter-commands&#34;&gt;JMeter Commands
&lt;/h3&gt;&lt;p&gt;Finally, it will be connected to the &lt;code&gt;Grafana&lt;/code&gt; dashboard, and you don&amp;rsquo;t need to input the &lt;code&gt;-l&lt;/code&gt; parameter to observe data in the &lt;code&gt;web&lt;/code&gt; console.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;jmeter -n -t /tmp/order-500-10s.jmx -l /tmp/jmeter-order-report-20200109/order-500-10s.jtl
# Generally, don&#39;t use test results and test reports to simplify the command
jmeter -n -t /tmp/order-500-10s.jmx
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;influxdb&#34;&gt;InfluxDB
&lt;/h2&gt;&lt;p&gt;InfluxDB is an open-source distributed time series database written in Go. It requires no external dependencies. The database is now primarily used for storing large volumes of timestamped data such as DevOps monitoring data, app metrics, IoT sensor data, and real-time analytics data.&lt;/p&gt;
&lt;h3 id=&#34;influxdb-features&#34;&gt;InfluxDB Features
&lt;/h3&gt;&lt;p&gt;InfluxDB’s features can be summarized into the following 9 aspects:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Schema-less (Schemaless):&lt;/strong&gt; Can contain an arbitrary number of columns;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Metric Retention Time Setting:&lt;/strong&gt;  Allows setting the retention time for metrics;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Support for Time-Related Functions:&lt;/strong&gt; Supports functions related to time (such as min, max, sum, count, mean, median, etc.) for convenient statistical analysis;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Storage Policy Support:&lt;/strong&gt; Can be used for data deletion and modification (InfluxDB does not provide methods for deleting or modifying data);&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Continuous Query Support:&lt;/strong&gt;  Automatically scheduled sets of statements that run within the database, combined with storage policies to reduce InfluxDB’s system footprint;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Native HTTP Support:&lt;/strong&gt; Built-in HTTP API;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Support for Similar SQL Syntax:&lt;/strong&gt; Supports a syntax similar to SQL;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Support for Setting Data Replica Count in Clusters:&lt;/strong&gt; Allows setting the number of replicas for data within clusters;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Support for Periodic Sampling of Data:&lt;/strong&gt;  Writes data to another measurement, facilitating granular data storage.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;influxdb-docker-installation&#34;&gt;InfluxDB Docker Installation
&lt;/h3&gt;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;mkdir influxdb &amp;amp;&amp;amp; cd influxdb &amp;amp;&amp;amp; \
docker run -p 8086:8086 -d --name influxdb -v $PWD:/var/lib/influxdb influxdb:1.7
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;docker exec -it influxdb /bin/bash&lt;/code&gt; enters the container, executes commands, and manually creates a database&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;root@bce0a55bbc72:/# influx
Connected to http://localhost:8086 version 1.7.10
InfluxDB shell version: 1.7.10
&amp;gt; Execute commands in the interactive shell
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;influxdb-database-and-user-creation&#34;&gt;InfluxDB Database and User Creation
&lt;/h3&gt;&lt;p&gt;Create database: &lt;code&gt;create database jmeter_t2&lt;/code&gt;
View databases: &lt;code&gt;show databases&lt;/code&gt;
Switch to database: &lt;code&gt;use jmeter_t2&lt;/code&gt;
Create user: &lt;code&gt;create user &amp;quot;admin&amp;quot; with password &#39;admin&#39; with all privileges&lt;/code&gt;
View users: &lt;code&gt;show users&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;&amp;gt; show users
user  admin
----  -----
admin true
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If the user permissions for &lt;code&gt;admin&lt;/code&gt; are displayed as &lt;code&gt;true&lt;/code&gt;, the database setup is complete.&lt;/p&gt;
&lt;h2 id=&#34;grafana&#34;&gt;Grafana
&lt;/h2&gt;&lt;p&gt;When writing test cases, it was found that the chart visualization effect is not very necessary; the &lt;code&gt;tps&lt;/code&gt; data from the interface can be observed when executed in the command line, and more importantly, we wanted to know the internal timing of the program.&lt;/p&gt;
&lt;p&gt;A simple deployment of the &lt;code&gt;grafana&lt;/code&gt; console panel, importing a configuration file to connect with &lt;code&gt;InfluxDB&lt;/code&gt;, was performed. The console supports filtering test results through tags; generally, only one &lt;code&gt;InfluxDB&lt;/code&gt; database needs to be configured:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Application Name&lt;/li&gt;
&lt;li&gt;Test Case Name&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://ttf248.life/p/linux-setup-jmeter-testing-environment/Snipaste_2021-03-09_19-44-22.png&#34;
	width=&#34;861&#34;
	height=&#34;357&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;grafana&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;241&#34;
		data-flex-basis=&#34;578px&#34;
	
&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;docker run -d --name=grafana -p 3000:3000 grafana/grafana:7.3.1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Due to the sampling interval in the web version, the calculated &lt;code&gt;TPS&lt;/code&gt; and related values do not match the aggregated report from &lt;code&gt;JMeter&lt;/code&gt;.  Refer to this link for reference: &lt;a class=&#34;link&#34; href=&#34;https://www.vinsguru.com/jmeter-real-time-results-influxdb-grafana/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://www.vinsguru.com/jmeter-real-time-results-influxdb-grafana/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The documentation also describes how to customize the &lt;code&gt;listener&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&#34;epilogue&#34;&gt;Epilogue
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;High-performance program patterns invariably are one-loop thread; any locks, enqueueing, and dequeueing will cause unnecessary performance loss.&lt;/li&gt;
&lt;li&gt;The time spent on core business logic is greater than the time spent introducing other code, concurrency can effectively improve efficiency; if the core latency is sufficient, be cautious about introducing other code.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;references&#34;&gt;References
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://my.oschina.net/u/4617935/blog/4680856&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;JMeter Series - JMeter + Grafana + InfluxDB Real-time Monitoring&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://hub.docker.com/_/influxdb&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;InfluxDB Official Image&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://hub.docker.com/r/grafana/grafana&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Grafana Official Image&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://jmeter.apache.org/download_jmeter.cgi&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;JMeter Website&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://medium.com/@jasonli.studio/to-install-apache-jmeter-in-centos7-294bc72a97ba&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;To install Apache JMeter in CentOS7&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>Red Hat and CentOS Lifecycle</title>
        <link>https://ttf248.life/en/p/redhat-centos-lifecycle/</link>
        <pubDate>Tue, 21 Jul 2020 20:02:35 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/redhat-centos-lifecycle/</guid>
        <description>&lt;p&gt;Production environment operating systems, with Red Hat and CentOS being the mainstream choices. The documentation includes links to two system lifecycles and shares experience upgrading from CentOS 8 to CentOS Stream 8.&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction
&lt;/h2&gt;&lt;p&gt;In the current domestic environment, Red Hat and CentOS are the mainstream choices for production environments. After experiencing the retirement of Red Hat 6 two years ago, this record includes the official website links for the lifecycles of both systems.&lt;/p&gt;
&lt;h2 id=&#34;main-content&#34;&gt;Main Content
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://access.redhat.com/support/policy/updates/errata&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Red Hat Enterprise Linux Life Cycle&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://wiki.centos.org/zh/About/Product&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;CentOS Product Specifications&lt;/a&gt;
Red Hat Enterprise Linux (RHEL) and CentOS are the mainstream choices for enterprise servers. RHEL provides stable support and update cycles, suitable for enterprise applications. CentOS as RHEL&amp;rsquo;s community edition, offers similar functionality and stability but without official support.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;follow-up&#34;&gt;Follow-up
&lt;/h2&gt;&lt;p&gt;When publishing this article, I didn’t expect to update it two years later. Just a few days ago, I upgraded my daily virtual machine from CentOS 8 to CentOS 8 Stream. I can&amp;rsquo;t say much about what to choose in production – I prefer to keep the latest version in my local environment.&lt;/p&gt;
&lt;p&gt;CentOS 8 Stream is a rolling release version that offers faster updates and new features than traditional CentOS, making it suitable for development and testing environments.&lt;/p&gt;</description>
        </item>
        <item>
        <title>Building PCs</title>
        <link>https://ttf248.life/en/p/computer-assembly/</link>
        <pubDate>Sat, 18 Jul 2020 14:33:46 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/computer-assembly/</guid>
        <description>&lt;p&gt;The author has long had an interest in building computers from a young age, and began to delve into hardware assembly after university. They recommended websites for comparing hardware performance and offered purchasing suggestions, including CPU, solid-state drives, hard disk drives, and memory frequencies. They also shared their experience and advice regarding hardware selection and important considerations.&lt;/p&gt;
&lt;h3 id=&#34;wonder--unspeakable&#34;&gt;Wonder – Unspeakable
&lt;/h3&gt;&lt;p&gt;Ever since I was young, I’ve dreamed of building my own computer, but unfortunately, economic conditions didn&amp;rsquo;t allow it. Finally, after a lot of hard work, I reached university and built a laptop for portability. If I had to pinpoint a specific time when the idea started, it would be when I first began thinking about assembling computers at my hometown’s library. After all, it was a sizable city-level library, with not only an electronic reading room (though I never actually used it – it was billed by the hour) but also a magazine reading room, where I discovered magazines like &lt;em&gt;Popular Science&lt;/em&gt; and &lt;em&gt;Computer News&lt;/em&gt;. For someone who had limited exposure to computers, these were practically divine科普资料 (scientific popularization materials). When I read about players “doing dungeons” and “killing monsters,” I thought about building my own computer to do the same, acting as the main output. And when I saw “black technology,” I fantasized about replicating what was described in the books, hoping to achieve similar effects (regarding the use of hacking tools).&lt;/p&gt;
&lt;p&gt;Even though I had a heavy workload in high school, with my limited understanding at the time, reading and playing were both important. It could be said that we lived a &amp;ldquo;naive and carefree&amp;rdquo; life, using the excuse of going to the library to read as an alibi, and I would often carry a small bag and walk there. The city wasn&amp;rsquo;t that big, so it was generally a walk. Arriving at the library, I enjoyed the air conditioning, reading novels, comics, and game magazines, occasionally even delving into more serious books.&lt;/p&gt;
&lt;p&gt;It’s easy for older people to forget things, and this is where the library sparked the initial seed. When I was in junior high school, relatives had assembled a computer, but I don&amp;rsquo;t know what it was used for back then. The operating system was Windows 2003, and there were built-in games like &lt;em&gt;Paper Cards&lt;/em&gt; + &lt;em&gt;Empire&lt;/em&gt;. We all thought about “outsmarting” each other to steal keys, playing games together with my cousin.&lt;/p&gt;
&lt;p&gt;When I entered junior high school, the school offered basic computer training, and later I transitioned to a computer competition concept. I even qualified for NOIP (National Olympiad in Informatics) once. Speaking of this, it’s worth mentioning the power of our alumni. The high school&amp;rsquo;s computer building was donated by alumni, including a computer teaching room + library. At that time, it was also the initial wave of China’s internet boom. School leaders supported participation in computer competitions, as several senior classmates had been admitted to key universities through computer science.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;I have never reflected on my relationship with computers like this way. It&amp;rsquo;s no wonder that after graduating, I stubbornly switched from automation to computer science – the seed had already been planted, and those within the circle didn’t realize it. Having encountered more things from a young age, I thought I was very skilled, but in reality, I only understood the surface level. My biggest advantage was the initial passion.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;hardware-assembly&#34;&gt;Hardware Assembly
&lt;/h3&gt;&lt;p&gt;Browse through stores like Carda, Chiphell, and Zhihu’s computer assembly forums – newcomers can relatively easily put together a list of the components they need. After 2019, when purchasing CPUs with limited financial resources, prioritize AMD for higher performance.&lt;/p&gt;
&lt;p&gt;I recommend a commonly used hardware performance comparison website: &lt;a class=&#34;link&#34; href=&#34;https://cpu.userbenchmark.com/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://cpu.userbenchmark.com/&lt;/a&gt;.  You can compare prices with US-imported parts on Xianyu (a Chinese online marketplace). True experts can find great deals by buying secondhand on Xianyu – it’s significantly cheaper. If you&amp;rsquo;re not very familiar, I don’t recommend buying from Xianyu; I purchased fake memory myself, and while it hasn’t caused any problems so far, I’m not entirely sure about its reliability – the model and specifications don’t match at all.&lt;/p&gt;
&lt;h4 id=&#34;sn550-vs-sn750&#34;&gt;SN550 VS SN750
&lt;/h4&gt;&lt;p&gt;The difference between the SN550 1TB and the SN750 1TB is that they consistently read and write slower by a factor of two – one reads at 850MB, while the other reads at 1.6GB. However, for everyday use, there’s no noticeable difference because both support 4K equally. Of course, this refers to the 1TB SN550; speeds are significantly slower in sequential read/write operations with the 500G and 250G versions. In my opinion, if you&amp;rsquo;re not a spendthrift, buying the SN550 is sufficient – my main reason for not purchasing it wasn’t its sequential read/write speed, but rather that it only comes in a 1TB capacity, while the SN750 offers 2TB. For me, given the circumstances and without needing to expand further, my motherboard&amp;rsquo;s M.2 NVMe interface is more valuable than these SSD differences.&lt;/p&gt;
&lt;p&gt;Based on a consensus of online user feedback, purchasing an adapter board – a B150 motherboard can also support M.2 interfaces for SSDs.&lt;/p&gt;
&lt;h4 id=&#34;hard-disk-drive-selection&#34;&gt;Hard Disk Drive Selection
&lt;/h4&gt;&lt;p&gt;Currently, the prices of hard disk drives are relatively stable. For users with large storage needs, it is necessary to select a suitable mechanical hard drive. Users who frequently download resources are recommended to choose enterprise-grade hard drives. Common ones include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Western Blue Disc&lt;/li&gt;
&lt;li&gt;Seagate Exos&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Large capacity mechanical hard drives are recommended to be partitioned, and frequent download operations should be fixed on a specific partition. If bad sectors appear later, they can be concentrated in one partition, and the current partition can be discarded, which can effectively extend the lifespan of the mechanical hard drive.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Seagate series &lt;a class=&#34;link&#34; href=&#34;https://www.seagate.com/cn/zh/enterprise-storage/exos-drives/exos-e-drives/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;official introduction&lt;/a&gt;
&lt;img src=&#34;https://ttf248.life/Seagate_Exos1.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;Seagate Enterprise Hard Drive Overview&#34;
	
	
&gt;
&lt;img src=&#34;https://ttf248.life/Seagate_Exos2.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;Seagate Enterprise Hard Drive Overview&#34;
	
	
&gt;&lt;/p&gt;
&lt;h4 id=&#34;memory-frequency&#34;&gt;Memory Frequency
&lt;/h4&gt;&lt;p&gt;From a daily usage perspective, frequency will not have a significant impact on performance.
&lt;img src=&#34;https://ttf248.life/memory.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;Memory Timing&#34;
	
	
&gt; &lt;img src=&#34;https://ttf248.life/yanyu-memory.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;YanYu Memory&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;Memory timings (also known as RAM timings) are four parameters that describe the performance of Synchronous Dynamic Random Access Memory (SDRAM): CL, TRCD, TRP, andTRAS, measured in clock cycles. They are typically written as four digits separated by hyphens, for example 7-8-8-24. The fourth parameter (RAS) is often omitted, and sometimes a fifth parameter: Command rate (command rate), usually 2T or 1T, also written as 2N, 1N is added. These parameters specify the latency (delay time) that affects the speed of random access memory. Lower numbers generally indicate faster performance. The final element determining system performance is the actual latency, typically measured in nanoseconds.&lt;/p&gt;
&lt;p&gt;When converting memory timings to actual latency, it’s important to note that they are measured in clock cycles. Without knowing the clock cycle time, it&amp;rsquo;s impossible to determine whether a set of numbers is faster or slower than another set.&lt;/p&gt;
&lt;p&gt;For example, DDR3-2000 memory with a clock frequency of 1000 MHz has a clock cycle of 1 ns. Based on this 1 ns clock, CL=7 gives an absolute latency of 7 ns. A faster DDR3-2666 (clock 1333 MHz, each cycle 0.75 ns) might use a larger CL=9, but the generated absolute latency of 6.75 ns is shorter.&lt;/p&gt;
&lt;p&gt;Modern DIMMs include a Serial Presence Detect (SPD) ROM chip that contains recommended memory timings for automatic configuration. The BIOS on the PC may allow users to adjust timings to improve performance (with the risk of instability), or in some cases increase stability (such as using suggested timings).&lt;/p&gt;
&lt;p&gt;Note: Memory bandwidth is a measure of throughput for memory and is typically limited by transfer rates rather than latency. By interleaving access to multiple internal banks of SDRAM, it&amp;rsquo;s possible to transmit at peak rates continuously. Increasing latency may be used to increase bandwidth. Specifically, each new generation of DDR memory has higher transmission rates, but absolute latency has not changed significantly, especially in the first-generation products on the market, which often have longer delays than the previous generation.&lt;/p&gt;
&lt;p&gt;Even with increased memory latency, increasing memory bandwidth can improve the performance of computer systems with multiple processors or multiple execution threads. Higher bandwidth will also boost the performance of integrated graphics cards that do not have dedicated video memory.
&lt;img src=&#34;https://ttf248.life/memory-timing-parameters-explanation.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;Memory Timing Parameters Explanation&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://en.wikipedia.org/wiki/Memory_time_series_parameter&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Memory Time Series Parameter Explanation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>Website Acceleration and Domain Settings</title>
        <link>https://ttf248.life/en/p/website-acceleration-and-domain-setup/</link>
        <pubDate>Sat, 20 Jun 2020 10:36:27 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/website-acceleration-and-domain-setup/</guid>
        <description>&lt;p&gt;Due to slow access to GitHub Pages from within the country, the author applied for a personal domain and purchased CDN acceleration services from a domestic cloud host provider. During the configuration process, the author encountered an issue where the www subdomain could not be accessed, which was ultimately resolved by deleting the generic domain DNS record and setting up a second-level domain separately. The author also shared the principles and configuration experience of CDN acceleration, as well as their attempts and lessons learned using reverse proxy with Nginx.&lt;/p&gt;
&lt;h3 id=&#34;background&#34;&gt;Background
&lt;/h3&gt;&lt;p&gt;The website is hosted on GitHub Pages, and due to some well-known reasons, accessing GitHub Pages internally can be slow. Therefore, I applied for a personal domain name and purchased CDN acceleration services from a domestic cloud host provider. When configuring the acceleration service, I thought about my development machine, which has Docker, frp, k8s, and other services deployed on it – all with their respective dashboards. Following the principle of not wasting anything, I configured several reverse proxies, all using subdomains.&lt;/p&gt;
&lt;p&gt;When I was happily using these subdomains, I discovered that the www subdomain could no longer be accessed. In the Alibaba Cloud console, I had configured DNS to resolve both &lt;a class=&#34;link&#34; href=&#34;https://www.xiangtianlong.com&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;www.xiangtianlong.com&lt;/a&gt; and xiangtianlong.com, and hadn’t yet enabled CDN acceleration; at that time, both domains were working normally.&lt;/p&gt;
&lt;p&gt;When configuring CDN acceleration, due to a large number of subdomains, I enabled generic domain rules, which unified routed all traffic to the development machine. As a result, the www subdomain also went down – yes, you read that right, the “www” prefix was a subdomain. In reality, the website is deployed on GitHub Pages, and the development machine has no caching information for the site.&lt;/p&gt;
&lt;p&gt;As for why the website wasn’t deployed on the development machine, it&amp;rsquo;s because it’s a static blog, paired with GitHub’s action to automatically integrate publishing – truly delicious!&lt;/p&gt;
&lt;h3 id=&#34;domain&#34;&gt;Domain
&lt;/h3&gt;&lt;p&gt;Non-professional web development, the understanding of domains does not involve SEO or cross-origin issues. As a blog site, a bare domain easily highlights the blogger&amp;rsquo;s site, such as myself who uses Chinese pinyin as my domain name, and given that mobile access is now more prevalent, it’s preferable to input fewer characters.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Keyboard shortcuts can be used on desktop to avoid entering “www” and “com”.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;cdn&#34;&gt;CDN
&lt;/h3&gt;&lt;p&gt;I&amp;rsquo;ve used both Alibaba Cloud and Tencent Cloud, and it’s easy for newcomers to get started. Tencent Cloud also has dedicated video explanations of the related concepts. The principle of CDN acceleration is similar to that of a JD.com warehouse: new products are pre-distributed to warehouses across China, and when delivery requests are triggered, they&amp;rsquo;re distributed locally.&lt;/p&gt;
&lt;p&gt;Origin Address: The address where the original website resources are stored.&lt;/p&gt;
&lt;p&gt;Cache file settings: Using F12 in your browser’s developer console to analyze static and dynamic resources.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;All 0 days validity&lt;/li&gt;
&lt;li&gt;&lt;code&gt;.php;.jsp;.asp;.aspx&lt;/code&gt; 0 days validity&lt;/li&gt;
&lt;li&gt;&lt;code&gt;.jpg;.png;.js;.css;.woff2&lt;/code&gt; 1 day validity&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Tencent Cloud configuration rules:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You can configure up to 10 cache expiration rules.&lt;/li&gt;
&lt;li&gt;The priority of multiple cache expiration rules is bottom-first.&lt;/li&gt;
&lt;li&gt;Cache expiration time can be set up to a maximum of 365 days.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;miserable-confession&#34;&gt;Miserable Confession
&lt;/h3&gt;&lt;p&gt;I had never used Nginx before, assuming that just searching for a website would reveal the configuration for reverse proxy. The result was quite confusing, and I spent half a day without even getting a 302 redirect to work. So, I decided to take a brute-force approach – deleting wildcard domain resolution patterns in DNS parsing and setting up individual second-level domains. Suddenly, I noticed that Alibaba Cloud DNS had a feature called &amp;ldquo;Display URL Redirect,&amp;rdquo; which worked perfectly as a 302 redirect.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;I set up the first second-level domain normally, and then when I tried to set up the second one, it didn&amp;rsquo;t work. I was starting to doubt my sanity. After waiting for a while, it suddenly started working – apparently, Alibaba Cloud DNS sometimes has hiccups with its DNS propagation.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;references&#34;&gt;References
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.zhihu.com/question/20414602&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Why are more and more website domain names not prefixed with &amp;ldquo;www&amp;rdquo;?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.cloudxns.net/Support/detail/id/918.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;What&amp;rsquo;s the difference between domains with and without &amp;ldquo;www&amp;rdquo;?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://gythialy.github.io/Docker-nginx-reverse-proxy/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Docker nginx reverse proxy setup&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>Blog IDE Environment and Ramblings</title>
        <link>https://ttf248.life/en/p/blog-ide-environment-and-ramblings/</link>
        <pubDate>Tue, 31 Mar 2020 13:54:27 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/blog-ide-environment-and-ramblings/</guid>
        <description>&lt;p&gt;This article introduces the basic concepts of Markdown and its applications in various software, recommends using VSCode as an IDE, and lists recommended plugins. The author shares their experience switching from Hexo to Hugo, emphasizing Hugo’s flexibility and customization capabilities. Finally, it provides some suggestions for quickly getting started with new technologies and shares a trick for resolving the issue of Hugo theme styles not updating.&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction
&lt;/h2&gt;&lt;h3 id=&#34;markdown&#34;&gt;Markdown
&lt;/h3&gt;&lt;blockquote&gt;
&lt;p&gt;A lightweight markup language that allows people to write documents in an easy-to-read and -write plain text format.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;markdown-1&#34;&gt;Markdown
&lt;/h3&gt;&lt;p&gt;Detailed Markdown syntax will not be elaborated upon in this document. We recommend an ebook, &lt;a class=&#34;link&#34; href=&#34;https://markdown-zh.readthedocs.io/en/latest/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;click here&lt;/a&gt;. Many software applications on the market now support MD as a writing format. The csdn blog system has launched an online editor that supports MD syntax; the default article when first using it is an introduction to MD syntax, which I think is quite good. Evernote added support for MD notes in 2018, with various MD markers available in the shortcut bar, making it almost as easy to use as editing a regular article, and the overall interaction process is friendly to beginners.&lt;/p&gt;
&lt;h3 id=&#34;ide-recommendations&#34;&gt;IDE Recommendations
&lt;/h3&gt;&lt;p&gt;When writing this article, it’s already 2020 – you’ve undoubtedly heard of VS Code, after all, anyone thinking about using Git Page to build a blog system is an industry professional. In the early years, Sublime and Atom were also good choices. Thanks to two years of promotion by the open-source community, VS Code has developed rapidly and has gradually become the preferred choice for newcomers.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The relationship between Microsoft’s giant corporation and the open-source community has successfully transitioned from a state of division into a honeymoon phase: embracing open source. My company has also actively introduced the Java ecosystem in recent two years, meaning that in business development, Java&amp;rsquo;s ecosystem is currently very fragrant domestically.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;vs-code-plugin-recommendations&#34;&gt;VS Code Plugin Recommendations
&lt;/h3&gt;&lt;p&gt;&lt;img src=&#34;https://ttf248.life/p/blog-ide-environment-and-ramblings/2020-03-31-14-07-17.png&#34;
	width=&#34;310&#34;
	height=&#34;306&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;Plugin List&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;101&#34;
		data-flex-basis=&#34;243px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;All plugins have their own Readme files, introducing basic usage, core functions, and some authors even provide demo screenshots.
&lt;code&gt;Paste Image&lt;/code&gt;, combined with Hugo&amp;rsquo;s image plugin method, is very convenient for importing images.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Don’t remember the shortcuts, open the VS Code shortcut management menu, search for “md”, read it several times; review it again to see the plugin usage instructions.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;hugo&#34;&gt;Hugo
&lt;/h2&gt;&lt;p&gt;I switched from Hexo to Hugo, as I love tinkering – it’s just my nature! Ultimately, I couldn&amp;rsquo;t resist the urge to quietly write articles.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Hugo supports placing images and Markdown documents in a single folder.&lt;/li&gt;
&lt;li&gt;The Academic theme supports various article styles in its design.&lt;/li&gt;
&lt;li&gt;Various convenient customization extensions.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;academic&#34;&gt;academic
&lt;/h2&gt;&lt;p&gt;The default website is &lt;code&gt;exampleSite&lt;/code&gt;, and menu introduction uses the &lt;code&gt;#component&lt;/code&gt; approach. It’s recommended to use &lt;code&gt;url&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The URL pattern allows for single-page navigation when clicking on navigation links, rather than scrolling to the homepage. This is purely a matter of personal preference.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Style: Notebooks, Speeches, eBooks&lt;/li&gt;
&lt;li&gt;Flexibility: Customization of overall style and custom CSS styles&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This theme’s Chinese support isn&amp;rsquo;t fully complete; primarily from a visual perspective, the font sizes don’t align well with Chinese reading habits. However, Hexo’s development team is largely comprised of Chinese developers, which is an advantage over Hugo in this regard.  Nevertheless, “you get what you build” – manually customize your browser elements. When locating an element to determine the CSS style name to modify, clicking &lt;strong&gt;Insert Style Rule Below&lt;/strong&gt; allows you to easily obtain the node name even with nested CSS layers.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://sourcethemes.com/academic/docs/customization/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Introduce &lt;code&gt;custom.css&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://sourcethemes.com/academic/docs/customization/#add-scripts-js&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Introduce &lt;code&gt;custom_js&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;The theme’s built-in syntax highlighting settings can be found here: &lt;a class=&#34;link&#34; href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/#highlighting-options.&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Official Link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion
&lt;/h2&gt;&lt;p&gt;The kids are complaining again, saying you talk in such a vague and unclear way, without mentioning any details.&lt;/p&gt;
&lt;p&gt;What I want to say is that with these things, you have enough to work with:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Official Manuals&lt;/li&gt;
&lt;li&gt;Plugin Documentation&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;When quickly getting started with new technologies, it’s recommended to first read the official website documentation, &lt;strong&gt;scanning through&lt;/strong&gt; – not aiming for a single thorough reading.  The results you get from search engines may not always be consistent with the latest version and could potentially mislead you. Reviewing a new book in the same way is also advisable: first look at the table of contents to understand what the author intends to cover, and sometimes it’s beneficial to read the introduction – particularly with translated foreign books, the preface often covers the core content of the book and its scope.&lt;/p&gt;
&lt;h2 id=&#34;easter-eggs&#34;&gt;Easter Eggs
&lt;/h2&gt;&lt;p&gt;Switching the Hugo Academic built-in style settings and publishing to the site, the style didn&amp;rsquo;t change when accessed. A clever teammate already figured it out – clearing local browser cache solved the problem.  I, being equally ingenious, used: F12 developer mode, switched to &lt;code&gt;network&lt;/code&gt;, checked &lt;code&gt;disable cache&lt;/code&gt;, refreshed, and voila!&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ttf248.life/p/blog-ide-environment-and-ramblings/2020-03-31-14-27-15.png&#34;
	width=&#34;399&#34;
	height=&#34;142&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;network&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;280&#34;
		data-flex-basis=&#34;674px&#34;
	
&gt;&lt;/p&gt;</description>
        </item>
        <item>
        <title>Automatic System Switchover</title>
        <link>https://ttf248.life/en/p/auto-integration-system-switch/</link>
        <pubDate>Sun, 29 Mar 2020 02:11:33 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/auto-integration-system-switch/</guid>
        <description>&lt;p&gt;Use GitHub Actions to automatically deploy your Hugo blog to GitHub Pages and Gitee.&lt;/p&gt;
&lt;h4 id=&#34;background-introduction&#34;&gt;Background Introduction
&lt;/h4&gt;&lt;p&gt;Yesterday while updating the blog, I discovered that the travis service was unavailable. Upon checking the travis webpage, I noticed the progress was stuck during source code retrieval, and a flash of insight occurred – I thought about GitHub’s previously launched action service.&lt;/p&gt;
&lt;p&gt;Due to being busy at the time and also needing to apply for access to use action, it has now officially gone live. With some free weekend time, I decided to try out a new toy?&lt;/p&gt;
&lt;p&gt;Official documentation can be found by entering the website yourself; this article won&amp;rsquo;t provide further reprints. If you’ve used Kubernetes before, you’ll find that the action YAML file configuration is very similar.&lt;/p&gt;
&lt;p&gt;Regarding introductory tutorials or Chinese introduction materials, I recommend searching for &lt;strong&gt;阮一峰’s blog&lt;/strong&gt;. There are two articles – the first introduces the basic syntax, and the second provides a practical case study.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;#### Content

Required Knowledge Points
- GitHub Secrets
- Action Syntax
Core jobs use existing components to complete, pushing to domestic Gitee uses command implementation, this command part is quite brutal, all are forced pushes, inherited from the logic used with Travis.

```yaml
name: github pages and gitee pages

on:
  push:
    branches:
      - hugo

jobs:
  deploy:
    runs-on: ubuntu-18.04
    steps:
      - uses: actions/checkout@v2
        with:
          submodules: true

      - name: Setup Hugo
        uses: peaceiris/actions-hugo@v2
        with:
          hugo-version: &#39;latest&#39;
          extended: true

      - name: Build Github and Gitee ## Single step can only write one run command
        run: hugo -b &amp;quot;https://www.xiangtianlong.com/&amp;quot; -d &amp;quot;github_public&amp;quot; &amp;amp;&amp;amp; hugo -b &amp;quot;https://www.xiangtianlong.com/&amp;quot; -d &amp;quot;gitee_public&amp;quot; &amp;amp;&amp;amp; ls

      - name: Deploy Github
        uses: peaceiris/actions-gh-pages@v3
        with:
          github_token: ${{ secrets.BLOG_TOKEN }}
          publish_dir: ./github_public
          publish_branch: master
          cname: xiangtianlong.com

      - name: Deploy Gitee
        run: cd ./gitee_public &amp;amp;&amp;amp; git init &amp;amp;&amp;amp; git config user.name &amp;quot;TianlongXiang&amp;quot; &amp;amp;&amp;amp; git config user.email &amp;quot;tianlongxiang51@gmail.com&amp;quot; &amp;amp;&amp;amp; git add . &amp;amp;&amp;amp; git commit -m &amp;quot;Update TianlongXiang&#39;s Blog&amp;quot; &amp;amp;&amp;amp; git push --force &amp;quot;https://xiangtianlong:${{ secrets.GITEE_PASSWORD }}@gitee.com/xiangtianlong/xiangtianlong.git&amp;quot; master:master
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
#### Epilogue

Based on the actions provided by the official market, currently there are quite a few supported playstyles. After building a Docker image, it’s no longer dependent on services offered by Docker Hub. 

Reviewing the Hugo issues, using GitHub Actions to automatically deploy Git Pages results in the final published website needing to be on the master branch. If deployed from another branch, the settings interface will prompt an error indicating that the deployed webpage has syntax problems.

This is simply because Hugo’s source files are located on the master branch, and GitHub treats it as the Jelly blog&#39;s source code for detection, unable to check and resolve any errors resulting in the error message. 

The solution is straightforward: move Hugo source files to another branch and publish static files to the master branch.&lt;/code&gt;&lt;/pre&gt;
</description>
        </item>
        <item>
        <title>Standard Library Container Memory Allocators: allocator</title>
        <link>https://ttf248.life/en/p/standard-library-container-memory-allocator/</link>
        <pubDate>Mon, 30 Dec 2019 13:26:19 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/standard-library-container-memory-allocator/</guid>
        <description>&lt;p&gt;A custom allocator can improve performance, increase memory utilization efficiency, and address the issue of frequent, small memory allocations.&lt;/p&gt;
&lt;h4 id=&#34;antecedent&#34;&gt;Antecedent
&lt;/h4&gt;&lt;p&gt;Recently, I&amp;rsquo;ve been working on the development of network data packets, requiring frequent allocation and release of small blocks of memory. Initially, I considered using a memory pool, reviewing several existing ones and discovering this:
&lt;a class=&#34;link&#34; href=&#34;https://github.com/cacay/MemoryPool&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/cacay/MemoryPool&lt;/a&gt;
When looking at the interface, I was quite puzzled by how the memory pool&amp;rsquo;s implementation was a bit strange. The &lt;code&gt;MemoryPool&lt;/code&gt; implementation logic involves allocating fixed-size memory blocks. Having reviewed Boost’s memory pool interface, it provides a template that is instantiated when used. Fortunately, this library already had an article describing it, mentioning the concept of an ‘allocator’.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;#### [wiki](https://zh.wikipedia.org/wiki/%E5%88%86%E9%85%8D%E5%99%A8_(C%2B%2B))

In C++ programming, an allocator is a key component of the C++ standard library. The C++ standard library defines various data structures commonly referred to as &amp;quot;containers&amp;quot; (such as linked lists, sets, etc.). A common feature of these containers is that their size can be changed at runtime; therefore, dynamic memory allocation becomes necessary to achieve this. The allocator is used to handle memory allocation and deallocation requests made by the containers. In other words, the allocator encapsulates the low-level details of memory management for Standard Template Library (STL) containers. By default, the C++ standard library uses its built-in generic allocator, but programmers can customize allocators to replace it as needed.

The allocator was originally invented by Alexander Stepanov as part of the C++ Standard Template Library (STL), with the intention of creating a mechanism that would &amp;quot;make the library more flexible and allow independent use of low-level data models,&amp;quot; and enable programmers to utilize custom pointer and reference types within the library; however, when the STL was incorporated into the C++ standard, the C++ standards committee realized that complete abstraction of the data model would result in unacceptable performance penalties. To compromise, restrictions on the allocator were made more stringent, and compared to Stepanov&#39;s original vision, the degree to which the current standard describes allocators is greatly limited.

Although customization of the allocator is somewhat restricted, it is still needed in many cases, typically for encapsulating access to different types of memory spaces (such as shared memory and reclaimed memory), or for improving performance when using a memory pool for memory allocation. In addition, from the perspective of memory usage and execution time, in programs that frequently perform small amounts of memory allocation, introducing a dedicated allocator can also yield benefits.
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;- Data Mining
- Deep Learning
- Neural Network

#### [Usage Requirements](https://zh.wikipedia.org/wiki/%E5%88%86%E9%85%8D%E5%99%A8_(C%2B%2B))

The primary reason for defining custom allocators is to improve performance. Utilizing a dedicated custom allocator can increase program performance, or improve memory usage efficiency, or both [4][8]. The default allocator uses the `new` operator to allocate storage [Reference 5], which is often implemented using the C language heap allocation function (malloc()) [9]. Because heap allocation functions are optimized for occasional large memory allocations, the default allocator generally works well when allocating memory for containers that require a single large memory allocation at once, such as vectors and doubly-ended queues [8]. However, for associative containers and linked lists that frequently allocate small amounts of memory, using the default allocator typically results in low efficiency [4][9]. In addition, the malloc()-based default allocator also has many problems, such as poor reference locality [4], and may cause memory fragmentation [4][9].

In short, this section (…)(like) is a “Dream” speech for this standard regarding allocators. Before dreams come true, programmers concerned with portability will be limited to using stateless custom allocators.
— Scott Meyers, *Effective STL*

Given this, in this situation, people often use memory pool-based allocators to solve the problem of frequent small allocations [8]. Unlike the default “on-demand” allocation method, when using a memory pool-based allocator, the program pre-allocates large blocks of memory (i.e., &amp;quot;memory pool&amp;quot;) and then the custom allocator simply returns a pointer to an available memory location in the pool to the requester. When objects are destructed, it does not actually deallocate memory; instead, it is deferred until the lifetime of the memory pool ends [Note 1][8].

In the topic of “custom allocators,” many C++ experts and authors have participated in discussions, such as Scott Meyers’s *Effective STL* and Andrej Alexandrescu’s *Modern C++ Design*, which mention it. Meyers realized that if an allocator instance is required to be equal for a specific type T, the portable allocator instance must not contain state. Although the C++ standard encourages library implementers to support stateful allocators [Reference 4], Meyers said that this paragraph is “(seemingly) a wonderful view,” but it’s almost nonsense, and he considered the restrictions on allocators “too strict” [4]. For example, the list in STL allows the `splice` method, which means a node in one list object A can be directly moved into another list object B, which requires that the memory allocated by A&#39;s allocator be released by B&#39;s allocator, thus deducing that the allocator instances of A and B must be equal. Meyers’s conclusion is that allocators should be defined as types using static methods. For example, according to the C++ standard, the allocator must provide a `rebind` method other class template.

Furthermore, in *C++ Programming Language*, Bjørn Strubø suggests “‘restricting the allocator to avoid different information for each object’ is obviously not a problem” (roughly), and points out that most allocators do not need state, or even perform better without state. He proposed three use cases for custom allocators: memory pool-type allocators, shared memory-type allocators, and garbage collection-type allocators, and demonstrated an implementation of an allocator using an internal memory pool to quickly allocate/deallocate small amounts of memory [3]. However, he also mentioned that such optimization may already be implemented in the sample allocator he provided.

Another use for custom allocators is to debug memory-related errors [10]. To do this, you can write an allocator that allocates extra memory when allocating and stores debugging information. This type of allocator not only ensures that memory is allocated/deallocated by the same type of allocator, but also can protect the program to some extent from cache overflows [11].
&lt;/code&gt;&lt;/pre&gt;
</description>
        </item>
        
    </channel>
</rss>
