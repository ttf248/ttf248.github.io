<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Quantization on Uncle Xiang&#39;s Notebook</title>
        <link>https://ttf248.life/en/tags/quantization/</link>
        <description>Recent content in Quantization on Uncle Xiang&#39;s Notebook</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en</language>
        <lastBuildDate>Thu, 04 Dec 2025 22:08:05 +0800</lastBuildDate><atom:link href="https://ttf248.life/en/tags/quantization/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>Is AI Getting “Dumber”? Uncovering the Game Between Parameter Accuracy and Inference Costs</title>
        <link>https://ttf248.life/en/p/is-ai-getting-dumber-uncovering-the-game-between-parameter-precision-and-inference-costs/</link>
        <pubDate>Thu, 04 Dec 2025 21:58:09 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/is-ai-getting-dumber-uncovering-the-game-between-parameter-precision-and-inference-costs/</guid>
        <description>&lt;p&gt;Recently mingling in various programming large model communication circles, the most complained about thing is model degradation.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Models deployed on local desktop computers are quantized models, essentially downgraded versions.&lt;/li&gt;
&lt;li&gt;With “vibe coding” so popular, could it be that the content output by current large models is the most valuable product – code?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This round of prompts received one optimization, which coincided with model degradation, and the large model’s solution was to optimize the prompt, with more detailed task planning and clearer output requirements.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Addressing this issue, provide suitable prompts: Many large factories have now provided large model services, and users sometimes feedback that models are degrading; from a professional perspective, considering parameter precision and inference cost, write an article;科普文 (科普 article), don’t be too long.
Optimized version:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;You are a senior AI industry technical column writer. Your goal is to write a professional but accessible Chinese科普 article for general readers.

Please step by step think and write:
1. Outline drafting: First, please draft a clear three-paragraph article structure outline (e.g., Introduction, Precision Analysis, Cost &amp;amp; Architecture Analysis, Conclusion).
2. Content focus: The article must deeply and clearly explain the phenomenon of “model degradation” reported by users, and the core analysis points must revolve around **parameter precision (quantization)** and **inference operational cost**.
3. Word count and format: The final article length must be controlled at 600 words or less. Please use clear `##` headings to divide paragraphs, and use **bold** to emphasize key professional terms (such as: quantization, MoE, FP16).
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction
&lt;/h2&gt;&lt;p&gt;To ensure clarity and align with a科普 (explanatory) approach, I’ve established the following structure:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Introduction:&lt;/strong&gt; Confirming user perception of reality and introducing the core conflict – in the context of rapidly growing user scales, service providers must balance “intellectual peak” with “service availability.”&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Precision Analysis (Core Point 1):&lt;/strong&gt; Explaining the relationship between &lt;strong&gt;parameters&lt;/strong&gt; and &lt;strong&gt;precision&lt;/strong&gt;. By comparing &lt;strong&gt;FP16&lt;/strong&gt; with &lt;strong&gt;quantization&lt;/strong&gt; (INT8/INT4), we use the analogy of &amp;ldquo;image compression&amp;rdquo; to illustrate why a drop in precision leads to degraded logic.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Cost &amp;amp; Architecture (Core Point 2):&lt;/strong&gt; Analyzing &lt;strong&gt;inference cost&lt;/strong&gt;. We introduce the &lt;strong&gt;MoE&lt;/strong&gt; architecture and &lt;strong&gt;sparse activation&lt;/strong&gt; mechanisms, explaining how models “slack off” to save money and accelerate performance.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Conclusion:&lt;/strong&gt; Summarizing that this &amp;ldquo;degradation of intelligence&amp;rdquo; is an inevitable path for commercialization and widespread adoption (the impossibility triangle).&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;article-body&#34;&gt;Article Body
&lt;/h2&gt;&lt;p&gt;Recently, many users have reported that large models seem less “intelligent” than before: logic has become simpler, instruction following has deteriorated, and even empty talk (“car轱辘话”) has emerged. This isn’t entirely an illusion. From a technical perspective, this “decline in intelligence” phenomenon is often a technological compromise made by major companies when facing massive user bases, balancing &lt;strong&gt;inference costs&lt;/strong&gt; with service speed.&lt;/p&gt;
&lt;h2 id=&#34;precision-shrinking-the-cost-of-4k-to-720p&#34;&gt;Precision “Shrinking”: The Cost of 4K to 720P
&lt;/h2&gt;&lt;p&gt;The “intelligence” of large models is primarily stored in trillions of &lt;strong&gt;parameters&lt;/strong&gt;. Ideally, these parameters run in high-precision &lt;strong&gt;FP16&lt;/strong&gt; (16-bit floating-point) format, capturing extremely subtle semantic differences. However, this high precision means significant &lt;strong&gt;memory&lt;/strong&gt; usage and slow computation speeds.&lt;/p&gt;
&lt;p&gt;To allow hundreds of millions of users to use the service smoothly, service providers generally adopt &lt;strong&gt;quantization&lt;/strong&gt; technology. This is a technique that compresses parameter precision from &lt;strong&gt;FP16&lt;/strong&gt; to &lt;strong&gt;INT8&lt;/strong&gt; or even &lt;strong&gt;INT4&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;This is like compressing a 4K high-definition movie into a 720P stream: although the plot (overall logic) hasn&amp;rsquo;t changed, the visual details (small logical associations, complex instruction execution details) are lost. This “lossy compression” directly leads to a decline in model performance when handling complex tasks, giving users the sensation of “getting dumber.”&lt;/p&gt;
&lt;h2 id=&#34;cost-pressure-letting-the-brain-partially-rest&#34;&gt;Cost Pressure: Letting the Brain “Partially Rest”
&lt;/h2&gt;&lt;p&gt;Besides accuracy, &lt;strong&gt;the operational cost of inference&lt;/strong&gt; is another key factor. Every time you ask an AI a question, servers perform massive matrix calculations, leading to exorbitant electricity bills and hardware wear and tear.&lt;/p&gt;
&lt;p&gt;To reduce costs, modern large models often adopt &lt;strong&gt;MoE&lt;/strong&gt; (Mixture of Experts) architecture. Unlike traditional models that activate all neurons every time, &lt;strong&gt;MoE&lt;/strong&gt; employs a &lt;strong&gt;sparse activation&lt;/strong&gt; strategy: for your question, the system only activates a small subset of relevant “expert” networks, while the rest remain dormant.&lt;/p&gt;
&lt;p&gt;Although this significantly reduces computational load, if the &lt;strong&gt;routing algorithm&lt;/strong&gt; (Router) distributes your complex mathematical problems to “literary experts” for load balancing or power saving, or sacrifices the number of expert calls in favor of speed, the output quality will inevitably fluctuate and decline.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion
&lt;/h2&gt;&lt;p&gt;Therefore, what is being referred to as “diminishment” – in essence, is the inevitable pain associated with AI’s transition from the laboratory to large-scale commercialization. The restraint on &lt;strong&gt;parameter precision&lt;/strong&gt; and the efficient design of &lt;strong&gt;model architecture&lt;/strong&gt; are what manufacturers seek to balance between “absolute intelligence” and “cost efficiency,” allowing everyone to affordably use AI.&lt;/p&gt;</description>
        </item>
        <item>
        <title>Global Stock Markets Plunge for Days: Fed, AI, and Quantitative Easing</title>
        <link>https://ttf248.life/en/p/global-stock-markets-plunge-for-days-fed-ai-and-quantitative-easing/</link>
        <pubDate>Sat, 22 Nov 2025 00:40:07 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/global-stock-markets-plunge-for-days-fed-ai-and-quantitative-easing/</guid>
        <description>&lt;ul&gt;
&lt;li&gt;Still doing nothing, Xiaomi didn’t fall below my current price level. It has fallen consecutively for two days, and was finally stabilized by a buyback. The US market last night was interesting – it started to rally, but the next day it completely crashed. Today, both A-shares and Hong Kong stocks are basically worthless.&lt;/li&gt;
&lt;li&gt;At this point, falling so much is actually good; I didn’t continue following the broader market, just laid down and relaxed, waiting for a better opportunity.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;why-the-decline&#34;&gt;Why the Decline
&lt;/h2&gt;&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://wallstreetcn.com/articles/3759889&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://wallstreetcn.com/articles/3759889&lt;/a&gt;
Other viewpoints may or may not be correct; quantitative funds are increasingly influencing domestic stock markets. Individual stocks often closely align with index movements. The original text is lengthy, and Bobo (豆包 - a common online nickname) has extracted the core content.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;In November 2025, global risk assets will experience a major decline.  According to Rich Privorotsky, a senior trader at Goldman Sachs, this downturn was the result of multi-layered transmission and ultimately evolved into systemic selling, driven by four key factors.&lt;/p&gt;
&lt;h2 id=&#34;fed-shifts-to-hawks-the-starting-point-of-the-downtrend&#34;&gt;Fed Shifts to Hawks: The Starting Point of the Downtrend
&lt;/h2&gt;&lt;p&gt;Recent employment data presents contradictory signals: job growth remains robust, but the unemployment rate has risen to 4.44% (primarily due to a surge in 16-24 year olds entering the workforce), and three-month average new hires only totaled 62,000, with potential new hires at just 39,000. August data was also revised downwards. Adding to this, there’s a trend of corporate layoffs, leading market expectations for the Fed to adopt a dovish stance. However, the Fed has maintained a hawkish tone, effectively withdrawing bets on December rate cuts, with the probability of cuts now essentially zero – triggering the first domino in the downward slide. Privorotsky bluntly stated that this hawkish position in light of the employment backdrop is a “policy error.”&lt;/p&gt;
&lt;h2 id=&#34;ai-narrative-restructuring-google-drives-a-winner-takes-all-landscape&#34;&gt;AI Narrative Restructuring: Google Drives a Winner-Takes-All Landscape
&lt;/h2&gt;&lt;p&gt;Key logical divisions are emerging within the AI sector, with Nvidia’s strong financial results no longer making it the core investment focus. Google&amp;rsquo;s breakthrough progress with the Gemini-3 model is reshaping the AI investment ecosystem. This “disruptive model” has forced other companies to delay product cycles and increase capital expenditures, leading to increased uncertainty regarding returns on investment, resulting in companies like Oracle failing to follow the upward trend. A distinct &amp;ldquo;winner-takes-all&amp;rdquo; landscape is forming within the market, shifting AI from a broad “adoption” phase to one dominated by a select few, triggering adjustments across related sectors.&lt;/p&gt;
&lt;h2 id=&#34;crypto-flash-crash-retail-investor-risk-appetite-declines&#34;&gt;Crypto Flash Crash: Retail Investor Risk Appetite Declines
&lt;/h2&gt;&lt;p&gt;The volatile swings in the cryptocurrency market triggered a chain reaction. Retail investors, who had steadfastly held their positions over the past two years – often referred to as “diamond hands” – have transitioned into “selling hands” due to factors such as large-scale dumps by whale accounts. This panic sentiment spilled over into non-profit tech stocks and AI-related equities, exemplified by Palantir, which plummeted 6% during trading hours after rising 5.5%, marking a significant downgrade in retail risk appetite and a fundamental shift in market structure.&lt;/p&gt;
&lt;h2 id=&#34;quantitative-funds-selling-off-the-key-driver-of-accelerated-decline&#34;&gt;Quantitative Funds Selling Off: The Key Driver of Accelerated Decline
&lt;/h2&gt;&lt;p&gt;Since August, trend-following funds (CTAs) have held over $500 billion in long positions, triggering concentrated liquidations after the index broke key levels. Simultaneously, rising volatility prompted volatility control strategy funds to sell off, compounded by capital flows into VIX ETNs creating a “short-tailed, short-convex” allocation, which amplified the downward effect. The previously stable “low volatility structure” collapsed, with quantitative and systematic funds initiating a “mechanical selling,” leading to a sudden and significant plunge in the market without any major events.&lt;/p&gt;
&lt;h2 id=&#34;fundamental-concerns-and-stabilization-conditions&#34;&gt;Fundamental Concerns and Stabilization Conditions
&lt;/h2&gt;&lt;p&gt;From a fundamental perspective, AI investment faces a “capital bottleneck”: a wave of corporate bond issuance is imminent, with AI data centers relying on debt expansion, while rising capital costs may slow down the pace of AI expansion. This risk has not been fully priced in previously. Goldman Sachs predicts that the S&amp;amp;P 500 mini contract could fall to 6500 points, and believes that the long-term value of AI remains unchanged; true winners are labor-intensive companies that achieve margin expansion through automation.&lt;/p&gt;
&lt;p&gt;Market stabilization requires fulfillment of three conditions: CTA position liquidation is complete, retail investors with excessive long positions have been squeezed out, and at least two of the following conditions are met: cryptocurrency stability, a dovish Fed policy shift, and AI capital expenditure support.&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
