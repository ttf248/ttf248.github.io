<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Deepseek on Uncle Xiang&#39;s Notebook</title>
        <link>https://ttf248.life/en/tags/deepseek/</link>
        <description>Recent content in Deepseek on Uncle Xiang&#39;s Notebook</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en</language>
        <lastBuildDate>Sun, 25 May 2025 02:57:45 +0800</lastBuildDate><atom:link href="https://ttf248.life/en/tags/deepseek/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>Deploy DeepSeek-R1 locally</title>
        <link>https://ttf248.life/en/p/ollama-local-deployment-deepseek-r1/</link>
        <pubDate>Fri, 07 Feb 2025 22:41:02 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/ollama-local-deployment-deepseek-r1/</guid>
        <description>&lt;p&gt;Ollama is an open-source AI tool designed to enable users to run and deploy large language models (LLMs) locally. Its goal is to provide a convenient and efficient way for developers to use models like GPT on their local machines without relying on cloud services. Ollama supports various models and focuses on optimizing performance, ensuring that even resource-constrained devices can run these models smoothly.&lt;/p&gt;
&lt;p&gt;With Ollama, users can use text-based AI applications and interact with locally deployed models without worrying about data privacy or high API usage fees. You can call different models through the command-line interface (CLI) to perform tasks such as natural language processing and question answering.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Ollama is suitable for trying out different models. The Windows version, after testing, doesn&amp;rsquo;t fully utilize the hardware&amp;rsquo;s performance; this may be due to the Windows version itself. The Linux version might be better. When deploying a 32b parameter model, with low memory and GPU load, the response speed is very slow.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;hardware-overview&#34;&gt;Hardware Overview
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;Operating system: win11&lt;/li&gt;
&lt;li&gt;CPU：i7-10700K&lt;/li&gt;
&lt;li&gt;Memory: 40GB&lt;/li&gt;
&lt;li&gt;Graphics card: RTX 3060 12GB&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;environmental-preparation&#34;&gt;Environmental preparation
&lt;/h2&gt;&lt;p&gt;Add a new system environment variable for convenient use later&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;set OLLAMA_MODELS=E:\ollama&lt;/code&gt;&lt;/strong&gt;&lt;br&gt;
This variable specifies the storage path for the Ollama model. &lt;code&gt;E:\ollama&lt;/code&gt; is a folder path indicating that all local model files are stored in this directory. Ollama will load and use your downloaded or deployed language models based on this path. You can store the model files in other locations, just change this path.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;set OLLAMA_HOST=127.0.0.1:8000&lt;/code&gt;&lt;/strong&gt;&lt;br&gt;
This environment variable sets the host and port for the Ollama service&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;127.0.0.1&lt;/code&gt; is the local address (localhost), meaning that the Ollama service will only listen for requests from the local machine&lt;/li&gt;
&lt;li&gt;The port number 8000 is the designated port, indicating that the Ollama service will listen for and process requests on port 8000. You can change the port number as needed, but make sure it is not occupied by other applications.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;set OLLAMA_ORIGINS=*&lt;/code&gt;&lt;/strong&gt;&lt;br&gt;
This environment variable controls which sources of requests are allowed to access the Ollama service&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;code&gt;*&lt;/code&gt; indicates that any source (i.e., all domains and IP addresses) is allowed to access the Ollama service. This is typically used in development and debugging environments; in production, you would usually specify stricter source control, limiting access only to specific domains or IPs to improve security.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;deepseek-r1-model-deployment&#34;&gt;DeepSeek-R1 Model Deployment
&lt;/h2&gt;&lt;p&gt;Ollama installation is straightforward and will not be elaborated on here&lt;/p&gt;
&lt;p&gt;Post-installation verification&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;C:\Users\core&amp;gt;ollama -v
ollama version is 0.5.11
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Model deployment, refer to the official website model page and select the corresponding parameters for the model: &lt;code&gt;ollama run deepseek-r1:14b&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;The 14B parameter model can effectively remember conversation context, while smaller parameter versions cannot. The 32B parameter version is very slow when deployed locally, so I didn&amp;rsquo;t conduct further testing.&lt;/p&gt;
&lt;h2 id=&#34;reference-materials&#34;&gt;Reference materials
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.ollama.com/library/deepseek-r1&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://www.ollama.com/library/deepseek-r1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/SPEvYTmTBxhoEkJqm1yPmw&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://mp.weixin.qq.com/s/SPEvYTmTBxhoEkJqm1yPmw&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://blog.csdn.net/x18990027/article/details/145368094&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://blog.csdn.net/x18990027/article/details/145368094&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
        </item>
        <item>
        <title>DeepSeek suddenly surged before the Spring Festival, Nvidia stock plummeted: institutional operations and large model thinking chains</title>
        <link>https://ttf248.life/en/p/deepseek-chinese-new-year-nvidia-stock-drop-institutional-operations-large-language-model-chain/</link>
        <pubDate>Fri, 07 Feb 2025 20:36:05 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/deepseek-chinese-new-year-nvidia-stock-drop-institutional-operations-large-language-model-chain/</guid>
        <description>&lt;p&gt;Ahead of the Spring Festival, DeepSeek briefly became a hot topic, garnering widespread attention on social media in just a few days. This sudden surge in popularity was not only surprising but also triggered a chain reaction in the market. At the same time, Nvidia&amp;rsquo;s stock experienced a sharp decline, raising concerns about its prospects among many investors. Some institutions conducted large-scale short selling during this period, suggesting everything pointed towards a &amp;ldquo;well-planned&amp;rdquo; scenario.&lt;/p&gt;
&lt;h3 id=&#34;deepseeks-rapid-rise-to-prominence-quickly-becoming-the-focus-of-attention&#34;&gt;DeepSeek&amp;rsquo;s rapid rise to prominence: quickly becoming the focus of attention
&lt;/h3&gt;&lt;p&gt;DeepSeek is an AI-based tool focused on optimizing deep learning models, particularly in the fields of natural language processing (NLP) and image generation. In the days leading up to the Spring Festival, this project suddenly garnered significant attention from a large number of investors and technical professionals. The performance of its team and the technological achievements demonstrated have sparked strong interest among many people. Discussions about DeepSeek dominate all topics in the tech circle, whether on developer communities or social media platforms.&lt;/p&gt;
&lt;p&gt;However, DeepSeek&amp;rsquo;s sudden surge in popularity is not accidental. After analysis, many people suspect that some institutions may be involved behind the scenes. Especially after its rise, Nvidia’s stock price experienced a noticeable decline, and there are clearly factors driving this change.&lt;/p&gt;
&lt;h3 id=&#34;nvidia-stock-plummets-the-force-behind-the-short-selling-operation&#34;&gt;Nvidia stock plummets: The force behind the short-selling operation
&lt;/h3&gt;&lt;p&gt;Nvidia, a global leader in graphics processing units (GPUs), has long been a key hardware provider for many large language models and AI computing. With the rapid development of the AI market, Nvidia&amp;rsquo;s stock has consistently performed strongly and even become a favored investment target for many investors. However, with the rise of DeepSeek and the market’s high attention to its technology, Nvidia&amp;rsquo;s stock experienced a sharp decline.&lt;/p&gt;
&lt;p&gt;Behind this phenomenon, there may be involved the short-selling strategies of institutional investors. In recent years, with the popularization of AI technology, Nvidia&amp;rsquo;s stock price has been highly driven up, and many investors began to believe that its stock price was overhyped. Especially after DeepSeek’s explosive success, some institutions may have gained substantial profits by shorting Nvidia&amp;rsquo;s stock. By seizing precise market timing and predicting the influence of DeepSeek, these institutions successfully profited from it.&lt;/p&gt;
&lt;h3 id=&#34;exploring-large-model-thinking-chains-from-results-to-process&#34;&gt;Exploring Large Model Thinking Chains: From &amp;ldquo;Results&amp;rdquo; to &amp;ldquo;Process&amp;rdquo;
&lt;/h3&gt;&lt;p&gt;In traditional AI applications, many practitioners and investors focus more on the &amp;ldquo;results&amp;rdquo; of AI models—such as generated images, text, and other direct outputs. However, in discussions related to DeepSeek, an increasing number of people are realizing that the thinking chain hidden behind large models is the core content that deserves more attention. In the past, we could only see the results of model output, but now, we need to understand the underlying logic, algorithms, and how to optimize model performance by adjusting these factors.&lt;/p&gt;
&lt;p&gt;This shift in thinking is, in essence, a deep reflection on AI research and application. Moving from simple black-box operations to truly understanding the internal workings of models has led many technical personnel and investors to begin reassessing the future direction of artificial intelligence. DeepSeek&amp;rsquo;s popularity exemplifies this breakthrough application, prompting people to focus on the entire model construction and optimization process, rather than just the final output.&lt;/p&gt;
&lt;h3 id=&#34;summary&#34;&gt;Summary
&lt;/h3&gt;&lt;p&gt;DeepSeek&amp;rsquo;s sudden surge in popularity, Nvidia’s stock plunge, and the short-selling operations of institutions behind the market – all this seems to be part of a carefully designed scheme. Through an in-depth understanding of large language model thinking chains, we can see that the application of AI technology is not merely a superficial accumulation of features but rather a deep exploration and optimization of the model&amp;rsquo;s internal logic. As technology advances, we may witness more innovative tools like DeepSeek, driving AI research and applications to higher levels.&lt;/p&gt;
&lt;p&gt;This phenomenon not only shows us the immense potential of AI technology but also prompts us to begin thinking about the commercial competition and capital operations behind the technology. The future trend of the market will be a continued focus of the interplay between technology and capital.&lt;/p&gt;</description>
        </item>
        
    </channel>
</rss>
