<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Deepseek on Uncle Xiang&#39;s Notebook</title>
        <link>https://ttf248.life/en/tags/deepseek/</link>
        <description>Recent content in Deepseek on Uncle Xiang&#39;s Notebook</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en</language>
        <lastBuildDate>Wed, 28 May 2025 09:47:38 +0800</lastBuildDate><atom:link href="https://ttf248.life/en/tags/deepseek/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>Deploy DeepSeek-R1 locally</title>
        <link>https://ttf248.life/en/p/ollama-local-deployment-deepseek-r1/</link>
        <pubDate>Fri, 07 Feb 2025 22:41:02 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/ollama-local-deployment-deepseek-r1/</guid>
        <description>&lt;p&gt;Ollama is an open-source AI tool designed to enable users to run and deploy large language models (LLMs) locally. It aims to provide a convenient and efficient way for developers to use models like GPT on their local machines without relying on cloud services. Ollama supports various models and focuses on optimizing performance so that even resource-constrained devices can run them smoothly.&lt;/p&gt;
&lt;p&gt;With Ollama, users can utilize text-based AI applications and interact with locally deployed models without concerns about data privacy or high API usage fees. You can call different models through a command-line interface (CLI) for tasks like natural language processing and question answering.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Ollama is good for trying out different models. The Windows version doesn&amp;rsquo;t fully utilize hardware performance, likely due to Windows itself. The Linux version might be better. Deploying a 32B parameter model results in slow responses even with low memory and GPU load.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;hardware-overview&#34;&gt;Hardware Overview
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;Operating system: win11&lt;/li&gt;
&lt;li&gt;CPU：i7-10700K&lt;/li&gt;
&lt;li&gt;Memory: 40GB&lt;/li&gt;
&lt;li&gt;Graphics card: RTX 3060 12GB&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;environment-setup&#34;&gt;Environment Setup
&lt;/h2&gt;&lt;p&gt;Added system environment variable for future use&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;set OLLAMA_MODELS=E:\ollama&lt;/code&gt;&lt;/strong&gt;&lt;br&gt;
This variable specifies the storage path for Ollama models. &lt;strong&gt;INLINE_CODE_0&lt;/strong&gt; is a folder path where all local model files are stored. Ollama loads and uses your downloaded or deployed language models based on this path. You can store the model files in other locations by changing this path.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;set OLLAMA_HOST=127.0.0.1:8000&lt;/code&gt;&lt;/strong&gt;&lt;br&gt;
This environment variable sets the host and port for the Ollama service&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;127.0.0.1&lt;/code&gt; is a local address (localhost), meaning that the Ollama service will only listen for requests from the local machine&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;INLINE_CODE_0&lt;/strong&gt; is the designated port number, indicating that the Ollama service will listen for and process requests on port 8000. You can change the port number as needed, but ensure it&amp;rsquo;s not occupied by another application.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;set OLLAMA_ORIGINS=*&lt;/code&gt;&lt;/strong&gt;&lt;br&gt;
This environment variable controls which sources of requests are allowed to access the Ollama service&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;INLINE_CODE_0&lt;/strong&gt; indicates that any origin (i.e., all domains and IP addresses) is allowed to access the Ollama service. This is typically used in development and debugging environments; in production, more restrictive source control is usually specified to limit access to specific domains or IPs for enhanced security.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;deepseek-r1-model-deployment&#34;&gt;DeepSeek-R1 Model Deployment
&lt;/h2&gt;&lt;p&gt;Ollama installation is straightforward; details are omitted here&lt;/p&gt;
&lt;p&gt;Post-installation verification&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;C:\Users\core&amp;gt;ollama -v
ollama version is 0.5.11
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Model deployment, refer to the official model page and select the corresponding parameters for the model: &lt;strong&gt;INLINE_CODE_0&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;A 14B parameter model effectively remembers conversation context; smaller versions do not. The 32B parameter version is too slow for local deployment, so further testing was not conducted.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.ollama.com/library/deepseek-r1&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://www.ollama.com/library/deepseek-r1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/SPEvYTmTBxhoEkJqm1yPmw&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://mp.weixin.qq.com/s/SPEvYTmTBxhoEkJqm1yPmw&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://blog.csdn.net/x18990027/article/details/145368094&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://blog.csdn.net/x18990027/article/details/145368094&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
        </item>
        <item>
        <title>DeepSeek unexpectedly surged before the Spring Festival, causing Nvidia stock to plummet: Institutional maneuvering and large model reasoning chains</title>
        <link>https://ttf248.life/en/p/deepseek-chinese-new-year-nvidia-stock-drop-institutional-operations-large-language-model-chain/</link>
        <pubDate>Fri, 07 Feb 2025 20:36:05 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/deepseek-chinese-new-year-nvidia-stock-drop-institutional-operations-large-language-model-chain/</guid>
        <description>&lt;p&gt;Ahead of the Spring Festival, DeepSeek quickly became a hot topic, garnering widespread attention on social media in just a few days. This sudden surge not only surprised many but also triggered a market chain reaction. Simultaneously, Nvidia&amp;rsquo;s stock plummeted, raising concerns about its future and prompting significant short-selling activity from some institutions, suggesting a potentially orchestrated scenario.&lt;/p&gt;
&lt;h3 id=&#34;please-provide-the-chinese-text-you-want-me-to-translate-i-am-ready-when-you-are&#34;&gt;Please provide the Chinese text you want me to translate. I am ready when you are!
&lt;/h3&gt;&lt;p&gt;DeepSeek is an AI-powered tool focused on optimizing deep learning models, particularly in natural language processing (NLP) and image generation. In the days leading up to Chinese New Year, the project unexpectedly gained significant attention from investors and technical professionals. The team&amp;rsquo;s performance and demonstrated technological achievements have sparked considerable interest. Discussions about DeepSeek dominate tech circles across developer communities and social media platforms.&lt;/p&gt;
&lt;p&gt;However, DeepSeek&amp;rsquo;s sudden surge in popularity wasn&amp;rsquo;t accidental. Analysis suggests potential involvement from certain institutions. Notably, Nvidia’s stock price has since seen a clear decline, indicating some factors are driving this change.&lt;/p&gt;
&lt;h3 id=&#34;please-provide-the-chinese-text-you-want-me-to-translate-i-am-ready-when-you-are-1&#34;&gt;Please provide the Chinese text you want me to translate. I am ready when you are!
&lt;/h3&gt;&lt;p&gt;Nvidia, a leading manufacturer of graphics processing units (GPUs), has long been a key hardware provider for large language models and AI computing. While the company&amp;rsquo;s stock has consistently performed strongly alongside the rapid growth of the AI market, it recently experienced a sharp decline following DeepSeek’s surge in popularity and increased market attention to its technology.&lt;/p&gt;
&lt;p&gt;This phenomenon may involve short-selling strategies by institutional investors. In recent years, as AI technology has become widespread, Nvidia&amp;rsquo;s stock price has been highly inflated, leading many investors to believe its valuation is overhyped. Particularly after the surge in popularity of DeepSeek, some institutions may have profited handsomely by shorting Nvidia’s stock. These institutions successfully capitalized on precise market timing and an understanding of DeepSeek’s influence.&lt;/p&gt;
&lt;h3 id=&#34;please-provide-the-chinese-text-you-want-me-to-translate-i-am-ready-when-you-are-2&#34;&gt;Please provide the Chinese text you want me to translate. I am ready when you are!
&lt;/h3&gt;&lt;p&gt;In traditional AI applications, many practitioners and investors focus on the &amp;ldquo;results&amp;rdquo; of AI models—such as generated images or text. However, discussions surrounding DeepSeek reveal a growing recognition that the underlying reasoning chain behind large language models is the more valuable core element. Previously, we could only see model outputs; now, we need to understand their logic, algorithms, and how to optimize performance by adjusting these factors.&lt;/p&gt;
&lt;p&gt;This shift in thinking represents a deeper consideration of AI research and application. Moving from simple &amp;ldquo;black box&amp;rdquo; operations to genuinely understanding the internal workings of models is prompting many technical experts and investors to re-evaluate the future direction of artificial intelligence. DeepSeek&amp;rsquo;s popularity exemplifies this breakthrough, encouraging attention towards the entire model construction and optimization process, rather than just the final output.&lt;/p&gt;
&lt;h3 id=&#34;please-provide-the-chinese-text-you-want-me-to-translate-i-am-ready-when-you-are-3&#34;&gt;Please provide the Chinese text you want me to translate. I am ready when you are!
&lt;/h3&gt;&lt;p&gt;DeepSeek&amp;rsquo;s sudden rise, Nvidia’s stock plunge, and the market manipulation behind it all appear to be part of a carefully orchestrated scheme. A deep understanding of large language model thinking chains reveals that applying AI technology isn&amp;rsquo;t just about superficial accumulation; it requires in-depth exploration and optimization of internal model logic. As technology advances, we may see more innovative tools like DeepSeek emerge, driving AI research and application to new heights.&lt;/p&gt;
&lt;p&gt;This phenomenon not only reveals the immense potential of AI technology but also prompts us to consider the underlying business competition and capital operations. The future market trends will be a continued focus of this interplay between technology and capital.&lt;/p&gt;</description>
        </item>
        
    </channel>
</rss>
