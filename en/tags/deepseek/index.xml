<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Deepseek on Uncle Xiang&#39;s Notebook</title>
        <link>https://ttf248.life/en/tags/deepseek/</link>
        <description>Recent content in Deepseek on Uncle Xiang&#39;s Notebook</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en</language>
        <lastBuildDate>Mon, 02 Jun 2025 20:54:02 +0800</lastBuildDate><atom:link href="https://ttf248.life/en/tags/deepseek/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>ollama local deployment of deepseek-R1</title>
        <link>https://ttf248.life/en/p/ollama-local-deployment-deepseek-r1/</link>
        <pubDate>Fri, 07 Feb 2025 22:41:02 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/ollama-local-deployment-deepseek-r1/</guid>
        <description>&lt;p&gt;Ollama is an open-source AI tool designed to enable users to run and deploy large language models (LLMs) locally. Its goal is to provide a convenient and efficient way for developers to use models like GPT on their local machines without relying on cloud services. Ollama supports multiple models and focuses on optimizing performance, allowing even resource-constrained devices to smoothly run these models.&lt;/p&gt;
&lt;p&gt;Through Ollama, users can utilize text-based AI applications and interact with locally deployed models without worrying about data privacy or high API usage fees. You can invoke different models via a command-line interface (CLI) for tasks such as natural language processing and question answering.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Ollama is suitable for experimenting with various models; after testing the Windows version, it couldn&amp;rsquo;t fully leverage the hardware’s performance, possibly due to the Windows version. When deploying 32b parameter models, with low memory and GPU load, the response speed is slow.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;hardware-overview&#34;&gt;Hardware Overview
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;Operating System: Windows 11&lt;/li&gt;
&lt;li&gt;CPU: i7-10700K&lt;/li&gt;
&lt;li&gt;Memory: 40GB&lt;/li&gt;
&lt;li&gt;Graphics Card: RTX 3060 12GB&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;environment-setup&#34;&gt;Environment Setup
&lt;/h2&gt;&lt;p&gt;Add the system environment variable to facilitate subsequent use:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;set OLLAMA_MODELS=E:\ollama&lt;/code&gt;&lt;/strong&gt;
This variable specifies the location where Ollama models are stored. &lt;code&gt;E:\ollama&lt;/code&gt; is a folder path indicating that all local model files will be stored in this directory. Ollama will load and use the language models you download or deploy based on this path. You can store model files in other locations by simply changing this path.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;set OLLAMA_HOST=127.0.0.1:8000&lt;/code&gt;&lt;/strong&gt;
This environment variable sets the host and port for the Ollama service.
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;127.0.0.1&lt;/code&gt; is the localhost address, meaning the Ollama service will only listen for requests from the local machine.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;8000&lt;/code&gt; is the specified port number, indicating that the Ollama service will wait for and process requests on port 8000. You can change the port number if needed, but make sure it&amp;rsquo;s not already in use by another application.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;set OLLAMA_ORIGINS=*&lt;/code&gt;&lt;/strong&gt;
This environment variable controls which origins are allowed to access the Ollama service.
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;*&lt;/code&gt; indicates that all origins (i.e., all domains and IP addresses) can access the Ollama service. This is typically used in development and debugging environments, and in production environments, it&amp;rsquo;s usually necessary to specify stricter origin control, limiting only specific domains or IPs to access your service for enhanced security.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;deepseek-r1-model-deployment&#34;&gt;DeepSeek-R1 Model Deployment
&lt;/h2&gt;&lt;p&gt;ollama installation is straightforward, so we won&amp;rsquo;t detail it here.&lt;/p&gt;
&lt;p&gt;Post-installation verification:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;C:\Users\core&amp;gt;ollama -v
ollama version is 0.5.11
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To deploy the model, refer to the official model page and select the appropriate parameter model: &lt;code&gt;ollama run deepseek-r1:14b&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;The 14b parameter version effectively remembers conversation context; smaller parameter versions cannot retain context. The 32b parameter version is very sluggish when deployed locally and hasn&amp;rsquo;t been further tested.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.ollama.com/library/deepseek-r1&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://www.ollama.com/library/deepseek-r1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/SPEvYTmTBxhoEkJqm1yPmw&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://mp.weixin.qq.com/s/SPEvYTmTBxhoEkJqm1yPmw&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://blog.csdn.net/x18990027/article/details/145368094&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://blog.csdn.net/x18990027/article/details/145368094&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
        </item>
        <item>
        <title>DeepSeek suddenly went viral before the Spring Festival, NVIDIA stock plummeted: The underlying institutional operations and the chain of thought for large models.</title>
        <link>https://ttf248.life/en/p/deepseek-explodes-before-chinese-new-year-nvidia-stock-plummets-behind-the-scenes-and-large-language-model-reasoning/</link>
        <pubDate>Fri, 07 Feb 2025 20:36:05 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/deepseek-explodes-before-chinese-new-year-nvidia-stock-plummets-behind-the-scenes-and-large-language-model-reasoning/</guid>
        <description>&lt;p&gt;Prior to the Spring Festival, DeepSeek had become a hot topic, attracting widespread attention on social media in just a few days. This sudden surge in popularity was surprising and triggered a chain reaction in the market. Meanwhile, NVIDIA’s stock price plummeted, raising concerns among many investors, with some institutions conducting large-scale short selling during this period, seemingly pointing to a “carefully orchestrated” situation.&lt;/p&gt;
&lt;h3 id=&#34;deepseeks-sudden-surge-in-popularity-rapidly-becoming-a-focus&#34;&gt;&lt;strong&gt;DeepSeek&amp;rsquo;s Sudden Surge in Popularity: Rapidly Becoming a Focus&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;DeepSeek is an AI-powered tool focused on optimizing deep learning models, particularly in the application of natural language processing (NLP) and image generation. In the days leading up to Chinese New Year, this project suddenly attracted significant attention from investors and tech professionals alike. The performance of its team and the demonstrated technical results sparked strong interest among many. Discussions about DeepSeek dominated virtually every topic within the developer community and on social media platforms.&lt;/p&gt;
&lt;p&gt;However, DeepSeek’s sudden surge in popularity was not accidental. After analysis, many began to suspect that certain entities were involved behind it. Notably, after its rise to prominence, NVIDIA&amp;rsquo;s stock experienced a significant decline, suggesting that some factors were driving this change.&lt;/p&gt;
&lt;h3 id=&#34;nvidia-stock-plummets-the-hand-behind-the-short-selling-operation&#34;&gt;&lt;strong&gt;Nvidia Stock Plummets: The Hand Behind the Short Selling Operation&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;Nvidia, a global leader in graphics processing units (GPUs), has long been a key hardware provider for many large models and AI computations. With the rapid development of the AI market, Nvidia’s stock has performed strongly over the long term and even become a favorite among many investors. However, with the explosive popularity of DeepSeek and the market&amp;rsquo;s intense focus on its technology, Nvidia’s stock experienced a dramatic plunge.&lt;/p&gt;
&lt;p&gt;The reason behind this phenomenon may involve institutional investor short selling strategies. Over the past few years, as AI technology has spread, Nvidia’s stock price has been significantly inflated, leading many investors to believe that it was at risk of excessive speculation. Especially after a technology like DeepSeek went viral, some institutions may have profited by shorting Nvidia&amp;rsquo;s stock. By accurately grasping market timing and predicting the influence of DeepSeek, these institutions successfully made a profit.&lt;/p&gt;
&lt;h3 id=&#34;large-model-chain-of-thought-interaction-from-results-to-process&#34;&gt;&lt;strong&gt;Large Model Chain-of-Thought Interaction: From “Results” to “Process”&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;In traditional AI applications, many practitioners and investors have focused more on the &amp;ldquo;results&amp;rdquo; of AI models – such as generated images and text – direct outputs. However, in discussions related to DeepSeek, an increasing number of people are beginning to realize that the underlying chain-of-thought hidden behind large models is the core content worth paying more attention to. Previously, we could only see the results output by the model, but now we need to understand its logic, algorithms, and how to optimize the model&amp;rsquo;s performance by adjusting these factors.&lt;/p&gt;
&lt;p&gt;This shift in thinking is actually a deep reflection on AI research and application. From simple black-box operations to truly understanding the internal workings of models, many technical personnel and investors have begun to re-examine the future development direction of artificial intelligence. The popularity of DeepSeek perfectly demonstrates this breakthrough application of the chain-of-thought, it encourages people to focus not only on the final output results but also on the entire model construction and optimization process.&lt;/p&gt;
&lt;h3 id=&#34;summary&#34;&gt;&lt;strong&gt;Summary&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;The sudden surge in popularity of DeepSeek, NVIDIA’s stock plummet, and the subsequent short-selling activities by institutions – it all seems like a meticulously designed scheme. Through an in-depth understanding of large model chain-of-thought reasoning, we can see that the application of AI technology is not merely superficial layering of phenomena, but also deep digging and optimization of the model’s internal logic. As the technology progresses, we may see more innovative tools like DeepSeek in the future, driving AI research and applications to a higher level.&lt;/p&gt;
&lt;p&gt;This phenomenon not only shows us the immense potential of AI technology, but also prompts us to start thinking about the commercial games and capital operations behind the technology. The future market trend will continue to be the focus of the battle between technology and capital.&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
