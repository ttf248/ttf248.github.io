<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Stable-Diffusion on Uncle Xiang&#39;s Notebook</title>
        <link>https://ttf248.life/en/tags/stable-diffusion/</link>
        <description>Recent content in Stable-Diffusion on Uncle Xiang&#39;s Notebook</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en</language>
        <lastBuildDate>Sun, 25 May 2025 02:57:45 +0800</lastBuildDate><atom:link href="https://ttf248.life/en/tags/stable-diffusion/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>Stable Diffusion - The ups and downs of installing from scratch</title>
        <link>https://ttf248.life/en/p/stable-diffusion-zero-install-saga/</link>
        <pubDate>Thu, 13 Apr 2023 00:23:54 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/stable-diffusion-zero-install-saga/</guid>
        <description>&lt;p&gt;Domestic resources generally recommend &lt;strong&gt;Chuyou&amp;rsquo;s&lt;/strong&gt; one-click deployment package. Thinking it’s an open-source project based on &lt;code&gt;Python&lt;/code&gt;, I figured the deployment wouldn&amp;rsquo;t be too complex, so I decided to try starting from scratch.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;I messed around with AI image generation, specifically upgraded my graphics card to an entry-level &lt;code&gt;3060 12g&lt;/code&gt;; the &lt;code&gt;960&lt;/code&gt;, which served faithfully for seven years, has now been retired&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The core &lt;code&gt;pytorch CUDA&lt;/code&gt; installation, I had installed it locally before when writing &lt;code&gt;Python&lt;/code&gt; game assistant scripts, but still encountered problems – the &lt;code&gt;CUDA&lt;/code&gt; license could not be activated&lt;/p&gt;
&lt;h2 id=&#34;pending&#34;&gt;Pending
&lt;/h2&gt;&lt;p&gt;Reorganize the article structure, first introducing PyTorch, version compatibility, and how to check versions
How to create a new virtual environment from scratch and deploy PyTorch locally
Translate drafts from scratch, stable diffusion &lt;a class=&#34;link&#34; href=&#34;https://stable-diffusion-art.com/install-windows/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://stable-diffusion-art.com/install-windows/&lt;/a&gt;
Organize reference materials&lt;/p&gt;
&lt;h2 id=&#34;steps&#34;&gt;Steps
&lt;/h2&gt;&lt;p&gt;You might not find a step-by-step installation tutorial by searching in Chinese. If you search in English on &lt;code&gt;Google&lt;/code&gt;, there are many similar tutorials that start from scratch. It briefly introduced the need to install &lt;code&gt;git&lt;/code&gt;, then explained the need to install &lt;code&gt;python&lt;/code&gt;. Then, it just involved downloading the repository and running the script with a double click.&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/AUTOMATIC1111/stable-diffusion-webui&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/AUTOMATIC1111/stable-diffusion-webui&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;For detailed usage and troubleshooting, please refer to the &lt;code&gt;issues&lt;/code&gt; section: &lt;a class=&#34;link&#34; href=&#34;https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I also don&amp;rsquo;t know why no one explained what this repository is for. Actually, it’s not hard to tell from the name – it’s an interface console that makes it more convenient for us to use. In fact, when installing, it will download the official repository content and obtain the actual &lt;code&gt;SD&lt;/code&gt; code.&lt;/p&gt;
&lt;p&gt;The warehouse also includes an installation startup script. It automatically detects whether there is a &lt;code&gt;Python&lt;/code&gt; virtual environment in the current folder. If so, it defaults to using the &lt;code&gt;python&lt;/code&gt; from the current path.&lt;/p&gt;
&lt;p&gt;If you&amp;rsquo;re a complete beginner, it’s recommended that you check out: &lt;a class=&#34;link&#34; href=&#34;https://stable-diffusion-art.com/install-windows/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://stable-diffusion-art.com/install-windows/&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;pytorch&#34;&gt;pytorch
&lt;/h2&gt;&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://pytorch.org/get-started/locally/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://pytorch.org/get-started/locally/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This is what I wanted to talk about today. First, don&amp;rsquo;t listen to their instructions and start the script directly. Python installs dependencies through requirement files, which are minor issues. The core issue is your graphics card driver version, which needs to correspond with PyTorch. Many people have already explained this relationship; you can find it online if you search for it.&lt;/p&gt;
&lt;p&gt;Reference: &lt;a class=&#34;link&#34; href=&#34;https://blog.csdn.net/weixin_40660408/article/details/129896700&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://blog.csdn.net/weixin_40660408/article/details/129896700&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Creating a virtual environment is like having an empty space where you can directly run the script from the official website to install PyTorch&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -c &amp;quot;import torch; print(torch.version.cuda)&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -c &amp;quot;import torch; print(torch.__version__, torch.cuda.is_available())&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The two scripts above can check the CUDA version you need to install and whether the installation was successful&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s not recommended to use fancy operations here. Just copy the logic from the official page and install it directly. Install it directly using pip, and your PyTorch is likely to fail or CUDA might not be activated.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Important: Make sure the folder path doesn&amp;rsquo;t contain any unnecessary characters, otherwise it may prevent PyTorch from working properly&lt;/p&gt;
&lt;p&gt;I installed it many times back and forth, and also tried downloading the official installation file and installing it manually. I was just thinking about upgrading to version 2.0 because the official documentation says that version 2.0 will be faster. But I haven&amp;rsquo;t used it much before, and I don’t know if the Python version or something else is affecting it. In the meantime, I also checked the official manual, which recommends using version 3.8. This created a small conflict because the one-click installation package previously used had version 3.10. Finally, I started from scratch by creating a new folder, creating a virtual environment, and ensuring that torch was installed successfully.&lt;/p&gt;
&lt;p&gt;Then move this installed virtual environment into the web UI folder. After that, when you start the installation script, most of the dependency issues should be resolved.&lt;/p&gt;
&lt;p&gt;After moving, you need to run: python -m pip install &amp;ndash;upgrade &amp;ndash;force-reinstall pip to fix it&lt;/p&gt;
&lt;p&gt;It might seem a bit strange, but I spent quite a while troubleshooting this. It couldn&amp;rsquo;t correctly identify my torch, so to eliminate all possible interference factors, I decided to install it first and then the other dependencies.&lt;/p&gt;
&lt;h2 id=&#34;xformers&#34;&gt;Xformers
&lt;/h2&gt;&lt;p&gt;It is recommended to enable it, as it can accelerate image generation and reduce existing usage. The side effect is that &lt;strong&gt;images generated with the same parameters may not be as stable&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Xformers&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;stable-diffusion-webui:Xformers&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://huggingface.co/docs/diffusers/optimization/xformers&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;huggingface optimization&lt;/a&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Optimization Ratio&lt;/th&gt;
&lt;th&gt;Time taken&lt;/th&gt;
&lt;th&gt;Torch active/reserved&lt;/th&gt;
&lt;th&gt;Sys VRAM&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;100.00%&lt;/td&gt;
&lt;td&gt;2m 57.03s&lt;/td&gt;
&lt;td&gt;7440/10058 MiB&lt;/td&gt;
&lt;td&gt;12288/12288 MiB (100.0%)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;51.02%&lt;/td&gt;
&lt;td&gt;1m 29.21s&lt;/td&gt;
&lt;td&gt;4547/7164 MiB&lt;/td&gt;
&lt;td&gt;9298/12288 MiB (75.67%)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;((masterpiece)),((best quality)),((high detial)),((realistic,))
Industrial age city, deep canyons in the middle,chinese architectural streets,bazaars, Bridges, (rainy days:1.2), (steampunk:0.8), chinese architecture
Negative prompt: nsfw,((cowboy)),(((pubic))), ((((pubic_hair))))sketch, duplicate, ugly, huge eyes, text, logo, monochrome, worst face, (bad and mutated hands:1.3), (worst quality:2.0), (low quality:2.0), (blurry:2.0), horror, geometry, bad_prompt, (bad hands), (missing fingers), multiple limbs, bad anatomy, (interlocked fingers:1.2), Ugly Fingers, (extra digit and hands and fingers and legs and arms:1.4), crown braid, ((2girl)), (deformed fingers:1.2), (long fingers:1.2),succubus wings,horn,succubus horn,succubus hairstyle, (bad-artist-anime), bad-artist, bad hand, borrowed character, text focus, watermark, sample watermark, character watermark, lofter username, photo date watermark, movie poster, magazine cover, journal, cover, cover page, doujin cover, album cover, manga cover, brand name imitation, EasyNegative,Tights, silk stockings,shorts
Steps: 35, Sampler: DPM adaptive, CFG scale: 5.5, Seed: 2223996555, Size: 1088x1088, Model hash: 543bcbc212, Model: base_Anything-V3.0-pruned, Clip skip: 2, ENSD: 31337
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;afterword&#34;&gt;Afterword
&lt;/h2&gt;&lt;p&gt;Why not recommend the one-click deployment package? Because that package contains some settings that were custom-made by the author, and they are not the same as the official original version. If you&amp;rsquo;re a beginner, you may not know why it’s best to start with the parameters provided by the official source first. As you use it for longer, refer to the official manual more often, and you will understand which parameters need adjustment.&lt;/p&gt;
&lt;h2 id=&#34;choosing-a-graphics-card&#34;&gt;Choosing a Graphics Card
&lt;/h2&gt;&lt;p&gt;After the cryptocurrency mining boom, graphics card prices are relatively lower now. For ordinary entry-level players choosing between a 3060 and a 3060 Ti, it&amp;rsquo;s generally recommended to go with the 3060 version with 12GB of VRAM because it can generate images at higher resolutions. Why do you need a higher resolution? Because you can increase the resolution during generation, resulting in clearer and more detailed images. If you’re just generating smaller images, 8GB of VRAM is sufficient.&lt;/p&gt;
&lt;p&gt;Also, the &lt;strong&gt;high-definition enhancement&lt;/strong&gt; option refines details, making the picture more detailed and also requiring more video memory&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s a summary table of the single-precision (FP32), half-precision (FP16), and double-precision (FP64) floating-point compute capabilities specifications for NVIDIA GeForce GTX 970, GeForce RTX 3060 Ti, GeForce RTX 3060, GeForce RTX 3080, and GeForce RTX 3080 Ti&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Graphics Card Model&lt;/th&gt;
&lt;th&gt;Year of Release&lt;/th&gt;
&lt;th&gt;Single-Precision Floating-Point Calculation Capability (TFLOPS)&lt;/th&gt;
&lt;th&gt;Half-Precision Floating-Point Calculation Capability (TFLOPS)&lt;/th&gt;
&lt;th&gt;Double-Precision Floating-Point Calculation Capability (TFLOPS)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;GeForce GTX 970&lt;/td&gt;
&lt;td&gt;2014&lt;/td&gt;
&lt;td&gt;3.49&lt;/td&gt;
&lt;td&gt;87.2&lt;/td&gt;
&lt;td&gt;0.109&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;GeForce RTX 3060 Ti&lt;/td&gt;
&lt;td&gt;2020&lt;/td&gt;
&lt;td&gt;16.2&lt;/td&gt;
&lt;td&gt;32.4&lt;/td&gt;
&lt;td&gt;0.51&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;GeForce RTX 3060&lt;/td&gt;
&lt;td&gt;2021&lt;/td&gt;
&lt;td&gt;12.7&lt;/td&gt;
&lt;td&gt;25.4&lt;/td&gt;
&lt;td&gt;0.39&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;GeForce RTX 3080&lt;/td&gt;
&lt;td&gt;2020&lt;/td&gt;
&lt;td&gt;29.8&lt;/td&gt;
&lt;td&gt;58.9&lt;/td&gt;
&lt;td&gt;0.93&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;GeForce RTX 3080 Ti&lt;/td&gt;
&lt;td&gt;2021&lt;/td&gt;
&lt;td&gt;34.8&lt;/td&gt;
&lt;td&gt;68.7&lt;/td&gt;
&lt;td&gt;1.36&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Extracted, &lt;a class=&#34;link&#34; href=&#34;https://docs.google.com/spreadsheets/d/1Zlv4UFiciSgmJZncCujuXKHwc4BcxbjbSBg71-SdeNk/edit#gid=0&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;various graphics card performance test data&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;update&#34;&gt;Update
&lt;/h2&gt;&lt;p&gt;After six months, I had intended to review the installation steps and explain more basic concepts, but it turns out that for ordinary people using AI image generation, it really just involves adjusting parameters based on images provided by experts or re-rendering existing images in a formatted way&lt;/p&gt;
&lt;p&gt;We tried using AI to generate UI assets for a mini-program, but after all that effort, the results weren&amp;rsquo;t satisfactory. It’s better if I just pull the resource images directly from the official mini-program.&lt;/p&gt;</description>
        </item>
        
    </channel>
</rss>
