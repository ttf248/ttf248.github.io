<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>AI on Uncle Xiang&#39;s Notebook</title>
        <link>https://ttf248.life/en/tags/ai/</link>
        <description>Recent content in AI on Uncle Xiang&#39;s Notebook</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en</language>
        <lastBuildDate>Mon, 02 Jun 2025 19:00:25 +0800</lastBuildDate><atom:link href="https://ttf248.life/en/tags/ai/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>Old problems, the flamboyant beauty of blossoming flowers (a reference to a famous Chinese poem).</title>
        <link>https://ttf248.life/en/p/old-ailment-stunning-flowers/</link>
        <pubDate>Mon, 26 May 2025 23:54:12 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/old-ailment-stunning-flowers/</guid>
        <description>&lt;p&gt;For many years, I’ve focused on backend development, and recently started to explore &lt;code&gt;AI&lt;/code&gt; programming while dipping my toes into some frontend-related content. However, during this period of tinkering, I gradually realized I was falling back into an old habit – being dazzled by shiny new things. I constantly want to use &lt;code&gt;AI&lt;/code&gt; to create a frontend interface, but in reality, these attempts haven’t provided much practical benefit for my current work and have instead wasted my energy.&lt;/p&gt;
&lt;h2 id=&#34;ai-use-cases&#34;&gt;AI Use Cases
&lt;/h2&gt;&lt;p&gt;In small projects, AI tools can truly shine, particularly when writing independent functions with low system coupling and simple business logic. These tasks typically have clear inputs and outputs, and rely on limited context – perfectly suited to the current capabilities of AI-assisted programming.&lt;/p&gt;
&lt;p&gt;However, when facing complex system architectures or deep business logic, AI’s limitations begin to emerge. It may generate code that appears reasonable but is actually detached from the project&amp;rsquo;s real needs, or even introduce potential issues that are difficult to debug. In these scenarios, AI is best suited as an assistive tool rather than a fully autonomous code generator. We need to conduct rigorous review and testing of generated code to ensure it meets actual requirements.&lt;/p&gt;
&lt;h2 id=&#34;errors-and-the-cost-of-learning&#34;&gt;Errors and the Cost of Learning
&lt;/h2&gt;&lt;p&gt;While attempting to generate frontend code using AI, I encountered numerous challenges. As frontend development isn&amp;rsquo;t a domain I’m familiar with, troubleshooting often proved time-consuming and frustrating. Even after adjusting prompts to have the AI rewrite the code, it was difficult to avoid the appearance of some low-level errors. This iterative process not only wasted time but also highlighted that my current focus should be on backend business logic rather than groping around in unfamiliar territory.&lt;/p&gt;
&lt;p&gt;Looking back at the project completed over the weekend, I’m more confident that focusing on backend development and user interaction logic, implementing functionality through a console, is the most efficient approach currently. Perhaps systematically learning frontend knowledge would be a better strategy when I have more time and energy.&lt;/p&gt;
&lt;h2 id=&#34;frontend-learning-plan&#34;&gt;Frontend Learning Plan
&lt;/h2&gt;&lt;p&gt;The frontend technology stack is complex and diverse, so it’s unrealistic to quickly master it. I plan to first choose a framework, such as Vue.js or React.js, and deeply learn its core concepts and usage methods. Only after becoming familiar with the fundamentals will I attempt to use AI to generate frontend code, which can effectively avoid errors and wasted time caused by unfamiliarity.&lt;/p&gt;
&lt;p&gt;In short, the focus for this stage should be on backend development, steadily building my core skills. Once the timing is right, I’ll explore the combination of frontend and AI – potentially yielding greater rewards.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Claude4 released, attempting to develop: Hugo tags, hyperlink translation assistant</title>
        <link>https://ttf248.life/en/p/claude-4-release-hugo-tags-hyperlink-translation-assistant/</link>
        <pubDate>Sat, 24 May 2025 03:05:31 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/claude-4-release-hugo-tags-hyperlink-translation-assistant/</guid>
        <description>&lt;p&gt;This site is developed using Hugo, but I’ve always used Chinese titles, which results in less friendly generated article links. In simpler terms, when shared, they don&amp;rsquo;t look as good because the Chinese characters are escaped into formats like %E4%BD%A0%E5%A5%BD within the links. While you can solve this by setting a slug, it’s tedious to do manually every time.&lt;/p&gt;
&lt;p&gt;Therefore, I decided to try using Claude4 to develop a translation assistant that automatically converts Chinese titles to English slugs and adds hyperlinks within the articles. This would eliminate the need for manual setup.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Claude4 is amazing – its contextual understanding has significantly improved, as has its efficiency in handling complex tasks.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;!-- more --&gt;
&lt;h2 id=&#34;project-address&#34;&gt;Project Address
&lt;/h2&gt;&lt;p&gt;Domestic Project Address: &lt;a class=&#34;link&#34; href=&#34;https://cnb.cool/ttf248/hugo-content-suite&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://cnb.cool/ttf248/hugo-content-suite&lt;/a&gt;
International Project Address: &lt;a class=&#34;link&#34; href=&#34;https://github.com/ttf248/hugo-content-suite&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/ttf248/hugo-content-suite&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;code-implementation&#34;&gt;Code Implementation
&lt;/h2&gt;&lt;p&gt;Let&amp;rsquo;s first discuss the implementation approach: We need to scan all articles, extract tag information and article titles, and then call the local large model (such as gemma-3-12b-it) for translation.&lt;/p&gt;
&lt;p&gt;In actual development, Claude4 showcased several significant advantages compared to previous generation large models. Due to the diverse functional requirements, Claude4 automatically designed an interactive menu, comprehensively considering various usage scenarios. For example, in tag processing, Claude4 not only supports tag statistics and analysis but also includes classification statistics and can even detect &lt;strong&gt;unlabeled articles&lt;/strong&gt;. Furthermore, it provides &lt;strong&gt;preview&lt;/strong&gt; and tag page generation functionalities.&lt;/p&gt;
&lt;p&gt;Whether it&amp;rsquo;s integrating with local large models, adding translation caches, or performing large-scale code refactoring, Claude4 completes everything in one go, with almost no issues. Despite the project’s small scale, it incorporates many minor features. Previously, large models often forgot earlier content during development, but this time the &lt;code&gt;Claude4&lt;/code&gt; performed exceptionally well, &lt;strong&gt;virtually eliminating context loss&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;In short, its intelligence has increased, and we plan to switch to &lt;code&gt;Claude4&lt;/code&gt; for more development work as our primary coding model.&lt;/p&gt;
&lt;h2 id=&#34;translation-cache&#34;&gt;Translation Cache
&lt;/h2&gt;&lt;p&gt;This approach, besides reducing the number of calls to large models, is quite efficient when running a 12b model locally – it doesn’t waste much time. However, if you need to call the large model every time, it will still be somewhat slow. Secondly, to fix the connections within articles, if a full update operation is executed and the article title is very long, there&amp;rsquo;s occasionally a situation where the two translated results differ, causing the link to change – which is quite awkward.&lt;/p&gt;
&lt;h2 id=&#34;feature-optimization&#34;&gt;Feature Optimization
&lt;/h2&gt;&lt;p&gt;The entire project was handed over to &lt;code&gt;Claude4&lt;/code&gt; to analyze the space for optimization and generate the following recommendations:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;Externalize Configuration - Improve maintainability and flexibility
Structured Logging - Facilitates troubleshooting and monitoring
Performance Monitoring - Understand system runtime status
User Experience - Visual feedback such as progress bars
Error Handling - A more robust exception handling mechanism
Code Organization - Clearer module partitioning
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The code was reviewed, and everything appeared to be in order. For example, the configuration files were modified to convert existing configurations from the original code into default configurations. If a corresponding configuration file was not found when reading the configuration file, a default configuration file would automatically generate to avoid user errors. Here&amp;rsquo;s the English translation of the provided text:&lt;/p&gt;
&lt;p&gt;“Requirements: When translating, dynamically calculate the current translation efficiency, estimate remaining time, and output relevant information to the console. Currently, we’re obtaining the character count of the article, the number of characters translated per line, the time taken, and fitting a calculation for the translation time of every 100 characters. Simultaneously, we&amp;rsquo;re calculating the estimated remaining translation time.&lt;/p&gt;
&lt;p&gt;The code is complete, but the results aren’t satisfactory, so I asked AI to provide a new design solution:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Provide multiple efficiency calculation methods: real-time efficiency, average efficiency, and sliding window efficiency&lt;/li&gt;
&lt;li&gt;Improve display methods: progress bars, segmented statistics, and dynamic refresh&lt;/li&gt;
&lt;li&gt;Add more useful metrics: API call counts, success rates, etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;After completing the code, I discovered a new surprise – translation efficiency statistics were flooding the console in real time, but without endless scrolling.”&lt;/p&gt;
&lt;h2 id=&#34;feature-optimization-1&#34;&gt;Feature Optimization
&lt;/h2&gt;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;Translate text to English (total 7163 characters)...
Detected 53 lines needing translation
  [1/53] Stage1/6 [░░░░░░░░░░░░░░░░░░░░░░░░░] 1.9% translating 354 characters...
    ✅ Completed (3.1s) | API call #1
    ✅ Completed (1.5s) | API call #2
    ✅ Completed (0.9s) | API call #3
    ✅ Completed (0.2s) | API call #4
    ✅ Completed (1.0s) | API call #5
    ✅ Completed (1.0s) | API call #6
    ✅ Completed (0.2s) | API call #7
    📊 Progress: Lines 13.2% (7/53) | Characters 12.9% (925/7163) 114.6 characters per second 📊
    ⚡ Efficiency: Real-time 76.4 | Average 117.9 | Recent 109.0 | Stage 113.6 characters per second 📊
    🎯 Success rate: 100.0% (7/7) | Remaining: 46 lines 7 seconds] 9.4% translating 110 characters...
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;feature-optimization-2&#34;&gt;Feature Optimization
&lt;/h2&gt;&lt;p&gt;🎯 Success Rate: 100.0% (7/7) | Remaining: 46 lines 7 7s] 9.4% Translation 110 characters&amp;hellip;
⏱️ Estimated Remaining: 55s | Expected Completion: 00:10:19 8s] 11.3% Translation 114 characters&amp;hellip;
💾 Processing Speed: 3211.3 rows/minute | Total Time: 8s] 13.2% Translation 16 characters&amp;hellip;
[8/53] Stage1/6 [███░░░░░░░░░░░░░░░░░░░░░░] 15.1% Translation 166 characters&amp;hellip;&lt;/p&gt;
&lt;h2 id=&#34;feature-optimization-3&#34;&gt;Feature Optimization
&lt;/h2&gt;&lt;p&gt;Previously, the code used to control the program wasn&amp;rsquo;t extensive, so I was curious about how it was implemented. Examining the code:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-golang&#34;&gt;// Clear screen and redraw (dynamic refresh effect)
if translationCount &amp;gt; 1 {
    fmt.Print(&amp;quot;\033[6A\033[K&amp;quot;) // Move up 6 lines and clear
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;performance-statistics-menu&#34;&gt;Performance Statistics Menu
&lt;/h3&gt;&lt;p&gt;The newly added &lt;strong&gt;Performance Statistics Menu&lt;/strong&gt;, which I myself designed, isn&amp;rsquo;t as well-designed as this one.&lt;/p&gt;
&lt;p&gt;📊 Performance Statistics:
🔄 Translation Count: 360
⚡ Cache Hit Rate: 1.4% (5/365)
⏱️ Average Translation Time: 315.927234ms
📁 File Operations: 73
❌ Error Count: 0&lt;/p&gt;
&lt;h3 id=&#34;progress-bar-display&#34;&gt;Progress Bar Display
&lt;/h3&gt;&lt;p&gt;The newly added &lt;strong&gt;Progress Bar Display&lt;/strong&gt;, providing detailed progress, elapsed time, and estimated remaining time.
Please select function (0-13): 10
🔍 Collecting translation target&amp;hellip;
📄 Cached file loaded, containing 0 translation records
📊 Translation cache statistics:
🏷️ Total tags: 229
📝 Total articles: 131
✅ Cached: 0
🔄 To be translated: 360&lt;/p&gt;
&lt;h3 id=&#34;progress-bar-display-1&#34;&gt;Progress Bar Display
&lt;/h3&gt;&lt;p&gt;Confirm full translation cache generation? (y/n): y
🚀 Generating full translation cache&amp;hellip;
📄 Loaded cache file, containing 0 translation records
🔍 Checking translations in cache&amp;hellip;
🔄 Need to translate 360 new tags
[░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 5/360 (1.4%) - Time taken: 3s - Estimated remaining: 3m8s
💾 Saved cache file, containing 5 translation records
[█░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 10/360 (2. 8%) - Time taken: 6s - Estimated remaining: 3m28s 💾 Cache file saved, containing 10 translation records
[██░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 15/360 (4.2%) - Time taken: 9s - Estimated remaining: 3m30s 💾 Cache file saved, containing 15 translation records&lt;/p&gt;
&lt;h3 id=&#34;progress-bar-display-2&#34;&gt;Progress Bar Display
&lt;/h3&gt;&lt;p&gt;[██████████████████████████████████] 20/360 (5.6%) - Time Elapsed: 13s - Estimated Remaining: 3m36s💾 Cache file saved, containing 20 translation records
[████████████████████████████████░░] 25/360 (6.9%) - Time Elapsed: 16s - Estimated Remaining: 3m33s💾 Cache file saved, containing 25 translation records
[██████████████████████████████████] 30/360 (8. (3%) - Time taken: 19s - Estimated remaining: 3m30s [████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 35/360 (9.7%) - Time taken: 22s - Estimated remaining: 3m25s [████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░]&lt;/p&gt;
&lt;h3 id=&#34;progress-bar-display-3&#34;&gt;Progress Bar Display
&lt;/h3&gt;&lt;hr&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;=== Hugo Blog Management Tool ===

🚀 Core Features
  1. One-Click Processing of All (Complete Blog Handling Process)

📝 Content Management
  2. Generate Tag Pages
  3. Generate Article Slug
  4. Translate Articles into Multi-Language Versions

💾 Cache Management
  5. View Cache Status
  6. Generate Full Translation Cache
  7. Clear Translation Cache

  0. Exit Program
&lt;/code&gt;&lt;/pre&gt;
</description>
        </item>
        <item>
        <title>AI overuse can lead to some lingering effects.</title>
        <link>https://ttf248.life/en/p/ai-overuse-side-effects/</link>
        <pubDate>Wed, 14 May 2025 19:39:50 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/ai-overuse-side-effects/</guid>
        <description>&lt;p&gt;Since we’ve launched “AI Inspiration Collision Forum,” there’s been a lot of random content being recorded and published, with people rushing to use AI to document and release everything. Quiet reflection and independent thought have become increasingly scarce. Moving forward, it would be beneficial to slightly control the output of this section and consolidate it into a monthly magazine format – releasing one article per month would suffice.&lt;/p&gt;
&lt;p&gt;This feels like a kind of aftereffect or side effect; efficiency has increased, but the depth and breadth of thinking have declined.&lt;/p&gt;
&lt;h2 id=&#34;efficiency-boost-undeniable&#34;&gt;Efficiency Boost: Undeniable
&lt;/h2&gt;&lt;p&gt;The “Seven-Second Fish Sightseeing” column used to be poorly maintained, with only a few hot events covered. Due to laziness, I hadn’t searched the internet for relevant materials or compiled records. Now that various AI tools are available, all it takes is outlining the key points, and AI can automatically search for related events, generate articles as needed, simply format them, and publish them.&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s like a blessing for lazy people – efficiency has increased significantly, almost to the point of doubling effort.&lt;/p&gt;
&lt;p&gt;Beyond writing articles, efficiency gains are real when coding. Previously, writing code often required detailed reading of API interface documentation. Now, I can skip this entirely. This is incredibly valuable because familiarizing oneself with APIs is “physical labor,” not “mental labor.” AI handles this part perfectly.&lt;/p&gt;
&lt;h2 id=&#34;spam-content&#34;&gt;Spam Content
&lt;/h2&gt;&lt;p&gt;Many articles have poor quality content, not to say that there&amp;rsquo;s nothing there, but it’s difficult to read, lacking a sense of reality and what people actually experience. This is a style I didn’t enjoy in the past – like chewing wax.&lt;/p&gt;
&lt;p&gt;Alternatively, AI-generated content truly feels like products of a流水线 (liú shuǐ liàn - assembly line) production, lacking soul.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Spam of the New Internet Era&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;forgetting&#34;&gt;Forgetting
&lt;/h2&gt;&lt;p&gt;This type of document is entirely AI-generated, and the reader’s context is unclear. However, over time, my own impressions will become blurred, or even forgotten.&lt;/p&gt;
&lt;p&gt;Similar problems occur when writing code – without reviewing commit records, I can&amp;rsquo;t remember how I originally thought about it or why I wrote it that way. This is particularly noticeable with code generated through repeated communication with AI, where the final code differs significantly from the initial ideas, sometimes to the point of being completely different.&lt;/p&gt;
&lt;h2 id=&#34;search&#34;&gt;Search
&lt;/h2&gt;&lt;p&gt;Recently, the number of times I’ve opened Google and Baidu has noticeably decreased. Many questions are now being answered by AI for searching and interaction, and the results are much better than traditional search engines.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s mourn the &lt;code&gt;bing ai&lt;/code&gt;, which may no longer be active, a pioneering AI tool from a major company that could connect to the internet and search.&lt;/p&gt;
&lt;p&gt;I’m using Google less, and I’ve reduced my visits to &lt;code&gt;stackoverflow&lt;/code&gt; as well. Many questions are now simply asked of AI – this website is gradually being phased out by the times.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion
&lt;/h2&gt;&lt;p&gt;My maintained blog, originally with very little traffic, is now even less expected of; it’s primarily a place for self-reflection and recording.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>No coding, design and develop a self-selected stock module.</title>
        <link>https://ttf248.life/en/p/design-develop-custom-stock-module-no-code/</link>
        <pubDate>Thu, 27 Feb 2025 23:20:39 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/design-develop-custom-stock-module-no-code/</guid>
        <description>&lt;ul&gt;
&lt;li&gt;Last month, we experimented with cursor, but due to the limitations of the free quota, we didn&amp;rsquo;t develop overly complex features; we just did some basic testing. We discovered then that Byte also released similar products, both using the same large models – Claude-3.5 – at their core.
Byte’s product is called Trae, initially launched in the Mac version and finally released its Windows version in February of this year. Big companies are good because you can freely “white嫖” (literally translates to &amp;ldquo;free grab&amp;rdquo;), without having to pay, with unlimited use of Claude-3.5. The performance of this model is quite impressive.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Ultimately, we got stuck on the development of candlestick charts. As I don’t understand React at all, I had to give up. To continue developing, I would need to supplement my knowledge of front-end basics, breaking down the task into smaller, more manageable pieces instead of giving a single large task: developing candlestick charts.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;issues-found&#34;&gt;Issues Found
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;Due to the lack of sufficient training data using foreign AI models, and Vue3 + Element-Plus, we chose React as the frontend framework.&lt;/li&gt;
&lt;li&gt;There may be occasional syntax errors requiring manual fixes.&lt;/li&gt;
&lt;li&gt;Solutions for some complex problems require manual guidance.&lt;/li&gt;
&lt;li&gt;Code structure optimization requires manual instruction.&lt;/li&gt;
&lt;li&gt;The most time-consuming part was packaging the frontend code into a container – this was due to my zero experience with &lt;code&gt;.env.production&lt;/code&gt; and &lt;code&gt;tsconfig.json&lt;/code&gt;, completely lacking understanding of these concepts; I sought help from community members along the way to sort out the logic. There are significant differences between the development (dev) mode and build mode in frontend development, and the checks performed on code. Backend database and service container scripts were completed in a total of five minutes.
&lt;strong&gt;Currently, AI primarily focuses on increasing development efficiency; having a foundation is best, not relying on AI to solve all problems.&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;repository-address&#34;&gt;Repository Address
&lt;/h2&gt;&lt;p&gt;As the title indicates, this time we&amp;rsquo;re going to chat with AI without actually coding and see what we can achieve.  Let’s take a look at the final outcome.&lt;/p&gt;
&lt;p&gt;Repository Address: &lt;a class=&#34;link&#34; href=&#34;https://github.com/ttf248/trae-demo&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/ttf248/trae-demo&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;For detailed usage instructions, please refer to the README.md file in the repository.&lt;/p&gt;
&lt;p&gt;The repository contains numerous submission records, most of which are conversations between me and Trae, as well as my testing of Trae’s functionalities, with notes on whether manual intervention was used to implement each feature.&lt;/p&gt;
&lt;h2 id=&#34;prompt&#34;&gt;Prompt
&lt;/h2&gt;&lt;p&gt;The project involves developing functionality from scratch, based on the following prompt:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;Based on the project prototype diagram, develop features including: stock selection (portfolio management), which requires support for adding, deleting, modifying, and querying contracts. The stock selection interface needs to display basic market data. Support multiple different market switches.

Frontend: React
Backend: Golang Gin GORM
Database: PostgreSQL

The backend needs to support cross-domain requests, while also considering data validation and error handling. If the backend service is unavailable, the frontend should display an alert message.

The backend needs to log requests and responses; the frontend also prints communication logs for troubleshooting purposes.
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;ui-and-interaction-optimization&#34;&gt;UI and Interaction Optimization
&lt;/h2&gt;&lt;p&gt;The design of the front-end interface relies entirely on Grok. We initially created a prototype within Trae, but it lacked aesthetics. Because the model used has strong coding capabilities but weaker other abilities, we need to use Grok to optimize the front-end UI.&lt;/p&gt;
&lt;p&gt;By taking screenshots of the current interface and uploading them to Grok, we can receive numerous optimization suggestions at once. We then manually evaluate these suggestions and copy them into Trae to execute and observe the results of the optimizations.&lt;/p&gt;
&lt;h3 id=&#34;technology-stack&#34;&gt;Technology Stack
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Frontend: React + TypeScript&lt;/li&gt;
&lt;li&gt;Backend: Golang + Gin + GORM&lt;/li&gt;
&lt;li&gt;Database: PostgreSQL 17&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;system-architecture&#34;&gt;System Architecture
&lt;/h2&gt;&lt;h2 id=&#34;backend-architecture&#34;&gt;Backend Architecture
&lt;/h2&gt;&lt;p&gt;The backend utilizes the Gin framework (Go) to implement RESTful APIs, with the following key modules:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Database Module&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Utilizes GORM as an ORM framework&lt;/li&gt;
&lt;li&gt;Supports database connection configuration via environment variables&lt;/li&gt;
&lt;li&gt;Automatically performs database schema migrations&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Routing Module&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;RESTful API design&lt;/li&gt;
&lt;li&gt;A unified error handling mechanism&lt;/li&gt;
&lt;li&gt;Built-in request logging&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Cross-Origin Handling&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Supports cross-origin requests from local development environments&lt;/li&gt;
&lt;li&gt;Configurable CORS policies&lt;/li&gt;
&lt;li&gt;Cookie cross-domain support&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;frontend-architecture&#34;&gt;Frontend Architecture
&lt;/h2&gt;&lt;p&gt;The frontend was built using React + TypeScript, implementing the following features:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Stock list display&lt;/li&gt;
&lt;li&gt;Watchlist management&lt;/li&gt;
&lt;li&gt;Real-time quote data display&lt;/li&gt;
&lt;li&gt;Error handling mechanism&lt;/li&gt;
&lt;/ul&gt;</description>
        </item>
        <item>
        <title>AI Development Over Two Years: Somewhat similar to the state before a Docker release – fragmented and rapidly evolving.</title>
        <link>https://ttf248.life/en/p/ai-development-two-years-like-docker-pre-release/</link>
        <pubDate>Thu, 20 Feb 2025 18:16:37 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/ai-development-two-years-like-docker-pre-release/</guid>
        <description>&lt;p&gt;Artificial Intelligence (AI) has undoubtedly been one of the most hotly debated topics in the technology field in recent years, particularly over the past two years, with AI technology experiencing rapid advancements. Whether it’s deep learning, natural language processing, or computer vision and automated decision systems, the application scenarios for AI are constantly emerging. However, despite continuous technological breakthroughs, AI still faces a bottleneck similar to that of a Docker release – a lack of a killer app to truly ignite the market.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Prompt: Write an article: AI has developed for two years, which is somewhat like the state of Docker’s pre-release, lacking a killer application, based on existing technologies, create a perfect landing scenario, Docker doesn&amp;rsquo;t use too many new technologies, but the overall solution is reasonable, and it changes the workflow of operations and development.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;ai-development-status-the-technology-is-mature-but-applications-still-need-breakthroughs&#34;&gt;AI Development Status: The Technology is Mature, but Applications Still Need Breakthroughs
&lt;/h2&gt;&lt;p&gt;From a technical perspective, AI has made significant progress in the past two years. Whether it’s OpenAI&amp;rsquo;s GPT series models or Google&amp;rsquo;s BERT and DeepMind&amp;rsquo;s Alpha series, AI processing capabilities have far exceeded previous expectations. Particularly in the field of natural language processing, models like GPT-4 not only possess powerful generative abilities but also demonstrate astonishing performance in understanding and reasoning.&lt;/p&gt;
&lt;p&gt;However, despite technological advancements, AI’s implementation in practical applications faces certain challenges. Similar to Docker&amp;rsquo;s state before its release, although AI has enormous potential, there isn&amp;rsquo;t yet a killer application that can be widely adopted and transform industries. Everyone is talking about the future of AI, but it may not always be possible to find an application scenario that can directly bring about revolutionary change.&lt;/p&gt;
&lt;h2 id=&#34;docker-and-ai-parallels-technology-doesnt-have-to-be-innovation-its-about-the-solution&#34;&gt;Docker and AI Parallels: Technology Doesn&amp;rsquo;t Have to Be Innovation, It’s About the Solution
&lt;/h2&gt;&lt;p&gt;Looking back at the history leading up to Docker’s release, we can readily see many similarities between the technological environment at that time and the current state of AI development. Before Docker, container technology wasn’t a new concept; early technologies like LXC (Linux Containers) and virtualization already possessed the basic capabilities for containerization. However, Docker didn&amp;rsquo;t introduce disruptive technology itself. Instead, it presented a simpler, more intuitive, and efficient solution through the clever integration and optimization of existing technologies. This solution didn’t bring about revolutionary technology, but it addressed many pain points in operations and development processes, dramatically simplifying software deployment, scaling, and management workflows.&lt;/p&gt;
&lt;p&gt;Similarly, the AI field is facing a similar situation. Currently, AI technology is no longer a “new thing,” but to truly achieve large-scale applications, it still requires a perfect landing scenario. Like Docker, integrating and optimizing existing technologies to form a reasonable application solution. A killer AI application may not rely on groundbreaking technological breakthroughs, but rather how to integrate existing technologies to solve pain points and needs in actual business operations.&lt;/p&gt;
&lt;h2 id=&#34;how-to-find-ais-docker-moment&#34;&gt;How to Find AI&amp;rsquo;s &amp;ldquo;Docker Moment&amp;rdquo;?
&lt;/h2&gt;&lt;p&gt;To truly enable the widespread adoption of AI technology, we need to focus on several key areas:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Deep Exploration of Real-World Scenarios&lt;/strong&gt;
Currently, many AI applications are still primarily experimental in nature and lack large-scale real-world implementation. Areas such as AI customer service and intelligent recommendations are widely used, but their functionality is still limited and has not yet broken through industry bottlenecks. True breakthroughs may come from industries that have been plagued by traditional methods for a long time, such as healthcare, manufacturing, and logistics. AI can help these companies improve efficiency and reduce costs in complex scenarios through more efficient data processing and predictive analytics.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Productization and Ease of Use&lt;/strong&gt;
Similar to how Docker simplifies containerization processes to improve operations efficiency, the ease of use of AI products is equally crucial. AI’s proliferation isn&amp;rsquo;t just about the spread of technology; it’s also about the proliferation of its products. Integrating AI into daily workflows, allowing users to easily utilize these tools without needing to deeply understand the underlying technology, is a crucial step in AI adoption.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Ecosystem Building &amp;amp; Standardization&lt;/strong&gt;
Any new technology’s widespread application relies on ecosystem building. Docker’s rapid rise is due to its openness and compatibility, enabling developers to seamlessly connect with various cloud platforms, tools, and services. Similarly, AI&amp;rsquo;s future depends on the construction of an ecosystem. The standardization of AI, model sharing, data openness, and technological integration will all influence whether AI can form widespread industry applications.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;conclusion-the-future-of-ai-is-full-of-possibilities-but-requires-more-refined-implementation&#34;&gt;Conclusion: The Future of AI is Full of Possibilities, but Requires More Refined Implementation
&lt;/h2&gt;&lt;p&gt;Despite the significant advancements in AI technology over the past two years, it remains in the stage of “lacking killer applications” at present. Similar to containerization technology before Docker’s release, AI needs a reasonable application scenario to deeply integrate existing technologies with business requirements in order to truly achieve large-scale adoption and widespread use. While technological innovation is important, solutions that can simplify processes and improve efficiency are more likely to drive the popularization and development of the technology.&lt;/p&gt;
&lt;p&gt;In the future, AI may evolve like Docker – not through disruptive technological breakthroughs, but by integrating existing technologies to create a perfect application scenario, ultimately changing the way we work and live.&lt;/p&gt;</description>
        </item>
        <item>
        <title>ollama local deployment of deepseek-R1</title>
        <link>https://ttf248.life/en/p/ollama-local-deployment-deepseek-r1/</link>
        <pubDate>Fri, 07 Feb 2025 22:41:02 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/ollama-local-deployment-deepseek-r1/</guid>
        <description>&lt;p&gt;Ollama is an open-source AI tool designed to enable users to run and deploy large language models (LLMs) locally. Its goal is to provide a convenient and efficient way for developers to use models like GPT on their local machines without relying on cloud services. Ollama supports multiple models and focuses on optimizing performance, allowing even resource-constrained devices to smoothly run these models.&lt;/p&gt;
&lt;p&gt;Through Ollama, users can utilize text-based AI applications and interact with locally deployed models without concerns about data privacy or high API usage fees. You can invoke different models via a command-line interface (CLI) for tasks such as natural language processing and question answering. &amp;gt; ollama is suitable for trying out different models, and after testing the Windows version, it couldn&amp;rsquo;t fully leverage the hardware’s performance – this may be due to the Windows version. When deploying 32b parameter models with low memory and GPU load, the response speed is very slow.&lt;/p&gt;
&lt;h2 id=&#34;hardware-overview&#34;&gt;Hardware Overview
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;Operating System: Windows 11&lt;/li&gt;
&lt;li&gt;CPU: i7-10700K&lt;/li&gt;
&lt;li&gt;Memory: 40GB&lt;/li&gt;
&lt;li&gt;Graphics Card: RTX 3060 12GB&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;environment-setup&#34;&gt;Environment Setup
&lt;/h2&gt;&lt;p&gt;Add the following system environment variables for easier use:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;set OLLAMA_MODELS=E:\ollama&lt;/code&gt;&lt;/strong&gt;
This variable specifies the location where Ollama models are stored. &lt;code&gt;E:\ollama&lt;/code&gt; is a folder path indicating that all local model files will be stored in this directory. Ollama will load and use the language models you download or deploy based on this path. You can store your model files in another location by simply changing this path.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;set OLLAMA_HOST=127.0.0.1:8000&lt;/code&gt;&lt;/strong&gt;
This environment variable sets the host and port for the Ollama service.
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;127.0.0.1&lt;/code&gt; is the localhost address, meaning the Ollama service will only listen for requests from the local machine.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;8000&lt;/code&gt; is the specified port number, indicating that the Ollama service will wait for and process requests on port 8000.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;environment-setup-1&#34;&gt;Environment Setup
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;set OLLAMA_ORIGINS=*&lt;/code&gt;&lt;/strong&gt;
This environment variable controls which origins are allowed to access the Ollama service.
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;*&lt;/code&gt; indicates that all sources (i.e., all domains and IP addresses) are permitted to access Ollama service. This is typically used in development and debugging environments, where production environments usually specify stricter origin control, limiting only specific domains or IPs to access your service to improve security.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;deepseek-r1-model-deployment&#34;&gt;DeepSeek-R1 Model Deployment
&lt;/h2&gt;&lt;p&gt;ollama installation is straightforward, so we won&amp;rsquo;t detail it here.&lt;/p&gt;
&lt;p&gt;Post-installation verification:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;C:\Users\core&amp;gt;ollama -v
ollama version is 0.5.11
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To deploy the model, refer to the official model page and select the appropriate parameter model: &lt;code&gt;ollama run deepseek-r1:14b&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;The 14b parameter version effectively remembers conversation context; smaller parameter versions cannot retain context. The 32b parameter version is very sluggish when deployed locally and hasn&amp;rsquo;t been further tested.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.ollama.com/library/deepseek-r1&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://www.ollama.com/library/deepseek-r1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/SPEvYTmTBxhoEkJqm1yPmw&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://mp.weixin.qq.com/s/SPEvYTmTBxhoEkJqm1yPmw&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://blog.csdn.net/x18990027/article/details/145368094&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://blog.csdn.net/x18990027/article/details/145368094&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>DeepSeek suddenly went viral before the Spring Festival, NVIDIA stock plummeted: The behind institutional operations and the chain of thought for large models.</title>
        <link>https://ttf248.life/en/p/deepseek-explodes-before-chinese-new-year-nvidia-stock-plummets-behind-the-scenes-and-large-language-model-reasoning/</link>
        <pubDate>Fri, 07 Feb 2025 20:36:05 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/deepseek-explodes-before-chinese-new-year-nvidia-stock-plummets-behind-the-scenes-and-large-language-model-reasoning/</guid>
        <description>&lt;p&gt;Prior to the Spring Festival, DeepSeek had become a hot topic, attracting widespread attention on social media in just a few days. This sudden surge in popularity was surprising and triggered a chain reaction in the market. Meanwhile, NVIDIA’s stock price plummeted, raising concerns among many investors, with some institutions conducting large-scale short selling during this period, seemingly pointing to a “carefully orchestrated” situation.&lt;/p&gt;
&lt;h3 id=&#34;deepseeks-sudden-surge-in-popularity-rapidly-becoming-a-focus&#34;&gt;&lt;strong&gt;DeepSeek&amp;rsquo;s Sudden Surge in Popularity: Rapidly Becoming a Focus&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;DeepSeek is an AI-powered tool focused on optimizing deep learning models, particularly in the application of natural language processing (NLP) and image generation. In the days leading up to Chinese New Year, this project suddenly garnered significant attention from investors and tech professionals alike. The performance of its team and the demonstrated technical results sparked strong interest among many. Discussions about DeepSeek dominated all topics within the developer community and on social media platforms.&lt;/p&gt;
&lt;p&gt;However, DeepSeek’s sudden surge in popularity was not accidental. After analysis, many began to suspect that certain entities may have been involved behind it.&lt;/p&gt;
&lt;h3 id=&#34;nvidia-stock-plummets-the-hand-behind-the-short-selling-operation&#34;&gt;&lt;strong&gt;Nvidia Stock Plummets: The Hand Behind the Short Selling Operation&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;Nvidia, a global leader in graphics processing units (GPUs), has long been a key hardware provider for many large models and AI computations. With the rapid development of the AI market, Nvidia’s stock has performed strongly over the long term and even become a favorite among many investors. However, with the explosive popularity of DeepSeek and the market&amp;rsquo;s intense focus on its technology, Nvidia’s stock experienced a dramatic plunge.
The underlying cause of this phenomenon may involve institutional investor short selling strategies. Over the past few years, as AI technology has spread, Nvidia’s stock price has been significantly inflated, leading many investors to believe that it carries risks of excessive speculation. Particularly after a technology like DeepSeek went viral, some institutions may have profited substantially by shorting Nvidia&amp;rsquo;s stock.&lt;/p&gt;
&lt;h3 id=&#34;chain-of-thought-reasoning-with-large-models-from-results-to-process&#34;&gt;&lt;strong&gt;Chain-of-Thought Reasoning with Large Models: From “Results” to “Process”&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;In traditional artificial intelligence applications, many practitioners and investors have focused more on the “results” of AI models – such as generated images or text outputs. In discussions related to DeepSeek, however, an increasing number of people are beginning to realize that the underlying chain-of-thought is the core content worth paying attention to behind large models. Previously, we could only see the results of model output, but now we need to understand the logic, algorithms, and how to optimize model performance by adjusting these factors.&lt;/p&gt;
&lt;p&gt;This shift in thinking represents a deep reflection on AI research and application. From simple black-box operations to a genuine shift towards understanding the internal workings of models, leading many technologists and investors to re-evaluate the future direction of artificial intelligence. The popularity of DeepSeek perfectly represents a breakthrough in this thought chain, prompting people to focus on the entire model construction and optimization process rather than just the final output results.&lt;/p&gt;
&lt;h3 id=&#34;summary&#34;&gt;&lt;strong&gt;Summary&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;The sudden surge in popularity of DeepSeek, NVIDIA’s stock plummet, and the subsequent short-selling activities by institutions – it all seems like a meticulously designed scheme. Through an in-depth understanding of large model chain-of-thought reasoning, we can see that the application of AI technology is not merely superficial layering of phenomena, but also deep digging and optimization of the model’s internal logic. As the technology progresses, we may see more innovative tools like DeepSeek in the future, driving AI research and applications to a higher level.&lt;/p&gt;
&lt;p&gt;This phenomenon not only shows us the immense potential of AI technology, but also prompts us to start thinking about the commercial games and capital operations behind the technology. The future market trend will continue to be the focus of the battle between technology and capital.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Cursor AI Programming IDE Trial</title>
        <link>https://ttf248.life/en/p/cursor-ai-programming-ide-trial/</link>
        <pubDate>Thu, 23 Jan 2025 19:30:13 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/cursor-ai-programming-ide-trial/</guid>
        <description>&lt;p&gt;It seems like another year has passed, and the biggest change at work is a significant increase in the involvement of AI. Previously, switching between different development languages required developers to be familiar with various language-specific API interfaces. Now, these basic code snippets can all be generated by AI, which is a huge blessing for developers.&lt;/p&gt;
&lt;h2 id=&#34;chatgpt&#34;&gt;ChatGPT
&lt;/h2&gt;&lt;p&gt;As early as 23 years ago, I’ve written two simple introductory articles about it. Now it&amp;rsquo;s been 25 years – how to say this? – I haven’t felt a significant improvement. It still needs to develop its own cognition, be able to reasonably break down tasks, and, of course, most importantly, identify whether AI-generated code contains bugs.&lt;/p&gt;
&lt;h2 id=&#34;github-copilot&#34;&gt;Github Copilot
&lt;/h2&gt;&lt;p&gt;It was a long time ago, but I saw some information saying that Singapore deployed the server and it’s used domestically. So there&amp;rsquo;s no need to use a VPN for extended periods anymore. However, when logging in, you still need to connect to a VPN. But this VPN only needs to be used during login, and then you can turn it off.&lt;/p&gt;
&lt;p&gt;I use Github Copilot more frequently for daily tasks. This plugin can be directly used in VS Code and Visual Studio without switching between the two applications. Compared to ChatGPT, Github Copilot offers better support for projects, is more user-friendly in interaction, and allows you to feed parts of your local files to it – &lt;strong&gt;“training” the AI&lt;/strong&gt; – so that the generated code is more aligned with your project.&lt;/p&gt;
&lt;h2 id=&#34;cursor-ai&#34;&gt;Cursor AI
&lt;/h2&gt;&lt;p&gt;Recently I’ve come across a new AI programming IDE, Cursor AI. This IDE is based on Github Copilot, but it&amp;rsquo;s more intelligent and can help you create files directly.
I gave it a quick try and found it to be pretty good, however, its understanding of existing projects isn’t quite there yet. When dealing with large local project files, or major refactoring, optimization, and adjustments, developers still need to &lt;strong&gt;break down tasks&lt;/strong&gt;.
Here&amp;rsquo;s an example: Switching to Cursor’s engineering mode, inputting the following content: “Create a personal resume webpage, supporting multiple different styles switching, and remember to fill in some personal information for data display.”
After several back-and-forths, you can get the following webpage – of course, this webpage is relatively simple, but it&amp;rsquo;s still quite good for beginners. ## Cursor AI
Recently I’ve come across a new AI programming IDE, Cursor AI. This IDE is based on Github Copilot, but it&amp;rsquo;s more intelligent and can help you create files directly.
I gave it a quick try and found it to be pretty good, however, its understanding of existing projects isn’t quite there yet. When dealing with large local project files, or major refactoring, optimization, and adjustments, developers still need to &lt;strong&gt;break down tasks&lt;/strong&gt;.
Here&amp;rsquo;s an example: Switching to Cursor’s engineering mode, inputting the following content: “Create a personal resume webpage, supporting multiple different styles switching, and remember to populate some personal information for data display.”
After several back-and-forths (&lt;strong&gt;pulling&lt;/strong&gt; on it), you can get the following webpage. Of course, this webpage is relatively simple, but it’s still quite good for beginners.&lt;/p&gt;
&lt;h2 id=&#34;cursor-ai-1&#34;&gt;Cursor AI
&lt;/h2&gt;&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://ttf248.life/cursor/index.html&#34; &gt;Resume&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>AI Programming and Task Decomposition</title>
        <link>https://ttf248.life/en/p/ai-programming-and-task-decomposition/</link>
        <pubDate>Fri, 22 Dec 2023 08:44:26 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/ai-programming-and-task-decomposition/</guid>
        <description>&lt;p&gt;Two years ago, I added a copy function to the site, which took me half a day of tinkering. Ultimately, the rendering effect wasn&amp;rsquo;t quite satisfactory, and as a somewhat mediocre frontend developer, I didn’t plan on improving it – if it worked, it was good enough. This year, I used &lt;code&gt;AI&lt;/code&gt; to develop a mini-program, which has made me more familiar with frontend development. I refactored it (the &lt;code&gt;AI&lt;/code&gt; redesigned it).&lt;/p&gt;
&lt;p&gt;Insight: Breaking down tasks really helps; many small tasks are quite natural for the &lt;code&gt;AI&lt;/code&gt; to develop. For the closed mini-program, 75% of the code was provided by &lt;code&gt;AI&lt;/code&gt;, and multiple tasks were broken down and delivered, with manual assembly of project code.&lt;/p&gt;
&lt;h2 id=&#34;previous-context&#34;&gt;Previous Context
&lt;/h2&gt;&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://ttf248.github.io/post/2022/001/%E4%BB%A5%E4%B8%BA%E5%BE%88%E7%AE%80%E5%8D%95%E7%9A%84%E4%BA%8B%E6%83%85%E5%A2%9E%E5%8A%A0%E4%B8%80%E4%B8%AA%E4%BB%A3%E7%A0%81%E5%A4%8D%E5%88%B6%E6%8C%89%E9%92%AE/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Making simple things easier: Adding a code copy button&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;content&#34;&gt;Content
&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;promot&lt;/strong&gt;: Design CSS and JavaScript code to locate webpage elements with the class &amp;ldquo;highlight&amp;rdquo;, and within that element&amp;rsquo;s container, add a copy button to enable copying the content. The first draft of the code was immediately problematic; generate a revised version that is essentially usable, but the copied content was incorrect, containing redundant text; further guidance led to optimizing the core color design, including which effects to incorporate.&lt;/p&gt;
&lt;p&gt;Having learned your specific requirements for button size and color, we can redesign the button styles to accommodate these demands.  Below is an adjusted CSS style sheet incorporating your provided color scheme and optimized for the button&amp;rsquo;s dimensions:&lt;/p&gt;
&lt;h2 id=&#34;content-1&#34;&gt;Content
&lt;/h2&gt;&lt;pre&gt;&lt;code class=&#34;language-css&#34;&gt;.copy-btn {
  position: absolute;
  top: 2px; /* Adjust position */
  right: 2px;
  padding: 3px 6px; /* Reduce size */
  font-size: 12px; /* Decrease font size */
  background-color: #8FD1E1; /* Button background color */
  color: #272822; /* Text color */
  border: 1px solid #272822; /* Border color */
  border-radius: 3px; /* Rounded corners */
  cursor: pointer;
  opacity: 0.7; /* Default semi-transparent */
  transition: opacity 0.3s ease, background-color 0.3s ease; /* Smooth transition effect */
}

.copy-btn:hover {
  opacity: 1; /* Unhide on hover */
  background-color: #A3D4E5; /* Background color on hover */
}

.highlight {
  position: relative;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;content-2&#34;&gt;Content
&lt;/h2&gt;&lt;p&gt;In this style, I’ve reduced the size and font size of the buttons to make them more refined. Additionally, the buttons have a semi-transparent default state that becomes opaque and has a slightly brighter background color when you hover over them, providing visual feedback. This design meets your color requirements while maintaining modern and user-friendly characteristics.&lt;/p&gt;
&lt;p&gt;I’ve also provided a simpler &lt;code&gt;JavaScript&lt;/code&gt; implementation:&lt;/p&gt;
&lt;h2 id=&#34;content-3&#34;&gt;Content
&lt;/h2&gt;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;document.addEventListener(&#39;DOMContentLoaded&#39;, function() {
  // Find all elements with the class &#39;highlight&#39;
  const highlights = document.querySelectorAll(&#39;.highlight&#39;);

  highlights.forEach(function(highlight) {
    // Create a copy button
    const copyBtn = document.createElement(&#39;button&#39;);
    copyBtn.textContent = &#39;Copy&#39;;
    copyBtn.classList.add(&#39;copy-btn&#39;);

    // Append the button to each .highlight element
    highlight.appendChild(copyBtn);
  });
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;content-4&#34;&gt;Content
&lt;/h2&gt;&lt;p&gt;// Add the button to each element with the class &amp;ldquo;highlight&amp;rdquo;
highlight.appendChild(copyBtn);&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// Add click event listener
copyBtn.addEventListener(&#39;click&#39;, function() {
  // Assuming the first &amp;lt;code&amp;gt; element under &amp;quot;.highlight&amp;quot; contains the code to copy
  const code = highlight.querySelector(&#39;code&#39;) ? highlight.querySelector(&#39;code&#39;).textContent : &#39;&#39;;
});
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;## Content

navigator.clipboard.writeText(code).then(() =&amp;gt; {
    // Display copy success message
    copyBtn.textContent = &#39;Copied!&#39;;
    setTimeout(() =&amp;gt; {
      copyBtn.textContent = &#39;Copy&#39;;
    }, 2000);
  }).catch(err =&amp;gt; {
    console.error(&#39;Copy failed: &#39;, err);
  });
});
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;epilogue&#34;&gt;Epilogue
&lt;/h2&gt;&lt;p&gt;While I had hoped to see a copy button added to the top-right corner of the code area, due to my lack of familiarity with frontend development, I was unable to accurately describe the functionality based on keywords and spent several rounds iterating with &lt;code&gt;AI&lt;/code&gt;. It remains to be seen what impact &lt;code&gt;AI&lt;/code&gt; will have on software development over time.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Prompt Engineer</title>
        <link>https://ttf248.life/en/p/prompt-engineer/</link>
        <pubDate>Sun, 26 Mar 2023 20:46:53 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/prompt-engineer/</guid>
        <description>&lt;p&gt;Just as we needed to learn the techniques of searching engines back then, we also need to learn some techniques for communicating with &lt;code&gt;AI&lt;/code&gt;, providing reasonable and sufficient constraints, and efficiently obtaining the desired answers.&lt;/p&gt;
&lt;p&gt;If you look at it from a different angle, current &lt;code&gt;AI&lt;/code&gt; is like a very good student with excellent memory – it has the ability to memorize everything. What we need to do is learn how to communicate with &lt;code&gt;AI&lt;/code&gt; correctly, effectively, and precisely describe our needs to help &lt;code&gt;AI&lt;/code&gt; generate the expected results.&lt;/p&gt;
&lt;p&gt;##科普
The incredibly popular &lt;code&gt;AI&lt;/code&gt; – specifically &lt;code&gt;Generative Pre-Training&lt;/code&gt; – literally translates to generative pre-training. It’s a deep learning model that trains text generation based on internet-available data, used for tasks such as question answering, text summarization generation, machine translation, classification, code generation, and conversational AI. Currently, there have been various versions of models like GPT-1, GPT-2, GPT-3, and GPT-4, each larger and more powerful than the previous one.&lt;/p&gt;
&lt;h2 id=&#34;does-it-really-have-intelligence&#34;&gt;Does it Really Have Intelligence?
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;Similarity is high, accuracy increases accordingly.&lt;/li&gt;
&lt;li&gt;Basic, repetitive tasks, after specific training, no longer require human intervention.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Generative AI is a technology that utilizes existing data such as text, audio, and images to create new content. It can be used for various tasks including text generation, speech synthesis, image generation, and dialogue systems. The logical reasoning of Generative AI depends on its training data and model structure. Generally, Generative AI can follow grammar, logic, and common sense to a certain extent, but it may also produce errors, biases, or unrealistic content. Therefore, the output of Generative AI requires human judgment and verification; it should not be blindly trusted or used.&lt;/p&gt;
&lt;h2 id=&#34;prompt-engineer&#34;&gt;Prompt Engineer
&lt;/h2&gt;&lt;p&gt;Time flows only forward, and people need to learn to adapt to the current. You can think of &lt;code&gt;AI&lt;/code&gt; as not intelligent, lacking logic, and frequently producing unusable code.&lt;/p&gt;
&lt;p&gt;If you shift your perspective, the current &lt;code&gt;AI&lt;/code&gt; is like a child with excellent memory – it has the ability to memorize after reading something repeatedly, much like copying homework. What we need to do is learn how to communicate correctly, effectively, and with &lt;code&gt;AI&lt;/code&gt;, precisely describing our needs to help &lt;code&gt;AI&lt;/code&gt; generate the desired results.&lt;/p&gt;
&lt;h2 id=&#34;dialogue-patterns&#34;&gt;Dialogue Patterns
&lt;/h2&gt;&lt;p&gt;Two years ago, when GitHub Copilot was released, no one could have imagined that two years later, OpenAI would appear out of nowhere, allowing humans to recognize the capabilities of large language models.&lt;/p&gt;
&lt;p&gt;Based on &lt;strong&gt;comment-based programming&lt;/strong&gt; and &lt;strong&gt;conversational programming&lt;/strong&gt;, the interaction logic is completely different. The dialogue pattern is friendly for novice users, and it’s crucial to mention NewBing providing follow-up prompts after each question. Microsoft is attempting to guide users to obtain more content from the AI knowledge base.&lt;/p&gt;
&lt;h2 id=&#34;lets-illustrate-with-an-example&#34;&gt;Let&amp;rsquo;s illustrate with an example
&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Scenario:&lt;/strong&gt; A long high-speed train journey, coding is a good choice. With limited traffic, processing a few images, obtaining image materials, downloading software to process the images is a bit extravagant, and network conditions are not ideal.
&lt;strong&gt;Requirements:&lt;/strong&gt; There’s a batch of image materials with transparent backgrounds that will be used as navigation bars for small programs. The selected state is colored, and the unselected state is black and white. All the materials are in color, and they need to be preprocessed.
&lt;strong&gt;Prompt:&lt;/strong&gt; Write a Python code snippet to read all images from a folder, convert the images to black and white while maintaining the transparent background, save the images, optimize image quality, reduce file size, and simultaneously display the comparison of the file sizes before and after optimization in the console.&lt;/p&gt;
&lt;h2 id=&#34;lets-illustrate-with-an-example-1&#34;&gt;Let&amp;rsquo;s illustrate with an example
&lt;/h2&gt;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import necessary libraries
import os
import PIL
from PIL import Image
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;lets-illustrate-with-an-example-2&#34;&gt;Let&amp;rsquo;s illustrate with an example
&lt;/h2&gt;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Define a function to convert images to black and white, maintain the transparent background, save the image, and return the file size.
def convert_and_save(image_file):
    # Open the image
    image = Image.open(image_file)
    # Get the image mode, if it&#39;s RGBA mode, it means there is a transparent background
    mode = image.mode
    if mode == &amp;quot;RGBA&amp;quot;:
        # Create a white background image with the same size as the image
        background = Image.new(&amp;quot;RGB&amp;quot;, image.size, (255, 255, 255))
        # Paste the original image onto the white background, ignoring transparent pixels
        background.paste(image, mask=image.split()[3])
        # Convert the composite image to grayscale mode
        gray_image = background.convert(&amp;quot;L&amp;quot;)
        # Convert the grayscale image back to RGBA mode to preserve the transparent background
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;lets-illustrate-with-an-example-3&#34;&gt;Let&amp;rsquo;s illustrate with an example
&lt;/h2&gt;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;gray_image = background.convert(&amp;quot;L&amp;quot;)
        # Convert the grayscale image to RGBA mode to preserve the transparent background
        final_image = gray_image.convert(&amp;quot;RGBA&amp;quot;)
    else:
        # If it&#39;s not in RGBA mode, directly convert the image to grayscale mode
        final_image = image.convert(&amp;quot;L&amp;quot;)
    # Get the original image file name and extension
    file_name, file_ext = os.path.splitext(image_file)
    # Define the new image file name, adding _bw suffix to indicate black and white
    new_file_name = file_name + &amp;quot;_bw&amp;quot; + file_ext
    # Save the new image, optimizing quality to reduce file size
    final_image.save(new_file_name, optimize=True)
    # Get the file sizes of the original and new images and return
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;lets-illustrate-with-an-example-4&#34;&gt;Let&amp;rsquo;s illustrate with an example
&lt;/h2&gt;&lt;h1 id=&#34;save-the-new-image-and-optimize-quality-reducing-file-size&#34;&gt;Save the new image and optimize quality, reducing file size
&lt;/h1&gt;&lt;pre&gt;&lt;code&gt;final_image.save(new_file_name, optimize=True)
# Get the file sizes of the original and new images and return them
old_size = os.path.getsize(image_file)
new_size = os.path.getsize(new_file_name)
return old_size, new_size
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Example:&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Define a folder path to store all images:
&lt;code&gt;folder_path = &amp;quot;images&amp;quot;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Iterate through all files in the folder:
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for file in os.listdir(folder_path):
  # Construct the full file path
  file_path = os.path.join(folder_path, file)
  # Check if it&#39;s an image file (based on extension; add or modify extensions as needed)
  if file_path.endswith((&amp;quot;.png&amp;quot;, &amp;quot;.jpg&amp;quot;, &amp;quot;.jpeg&amp;quot;, &amp;quot;.gif&amp;quot;)):
    # Call the function to convert and save the image, and get the file size
    old_size, new_size = convert_and_save(file_path)
    # Display the comparison of file sizes before and after optimization in the console
    print(f&amp;quot;{file}: {old_size} bytes -&amp;gt; {new_size} bytes&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;lets-illustrate-this-with-an-example&#34;&gt;Let&amp;rsquo;s illustrate this with an example:
&lt;/h2&gt;&lt;p&gt;&lt;code&gt;AI&lt;/code&gt; quickly provided the answer, which looked very beautiful and worked well. Experienced program sources would find the following code not generic enough and unable to be deployed to production:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Parameters were fixed&lt;/li&gt;
&lt;li&gt;There were no log files; information was printed to the console&lt;/li&gt;
&lt;li&gt;The image processing task was suitable for a multi-threaded model&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;NewBing&lt;/code&gt; appeared, based on the prompt, selecting a question: &lt;strong&gt;What improvements can be made to this code?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The answer given this time was truly beyond expectations. General optimization measures that most programmers could think of were provided by &lt;code&gt;AI&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;This code may still have some areas for improvement, such as:

You could use the argparse library to allow users to specify the folder path and extension through command-line parameters instead of hardcoding them in the code.
``` You can use the logging library to record the program&#39;s runtime instead of directly printing to the console.
You can use try-except blocks to catch potential exceptions, such as file opening failures or image conversion failures, and handle them appropriately.
You can utilize the multiprocessing library to leverage multi-core processors and accelerate the image conversion and saving processes.

Friendly responses are considered, taking into account differences in the deployment environment and allowing for non-support of multi-processing.

## Let&#39;s illustrate with an example

```shell
Of course, I’m happy to help you modify your code. I added argparse, logging, and try-except statements, but I didn’t add the multiprocessing library because I wasn’t sure if your computer supported multi-core processing. The modified code is as follows:
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;lets-illustrate-with-an-example-5&#34;&gt;Let&amp;rsquo;s illustrate with an example
&lt;/h2&gt;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import necessary libraries
import argparse
import logging
import multiprocessing
import os

from PIL import Image
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;lets-illustrate-with-an-example-6&#34;&gt;Let&amp;rsquo;s illustrate with an example
&lt;/h2&gt;&lt;h1 id=&#34;define-a-function-to-convert-images-to-black-and-white-while-maintaining-the-transparent-background-save-the-image-and-return-the-file-size&#34;&gt;Define a function to convert images to black and white while maintaining the transparent background, save the image, and return the file size
&lt;/h1&gt;&lt;p&gt;def convert_and_save(image_file):
# Open the image
try:
image = Image.open(image_file)
except Exception as e:
logging.error(f&amp;quot;Failed to open {image_file}: {e}&amp;quot;)
return None, None
# Get the image mode; if it&amp;rsquo;s RGBA mode, it means there is a transparent background
mode = image.mode
if mode == &amp;ldquo;RGBA&amp;rdquo;:
# Create a white background image with the same size as the image
background = Image.new(&amp;ldquo;RGB&amp;rdquo;, image.size, (255, 255, 255))
# Paste the original image onto the white background, ignoring transparent pixels
background.paste(image, mask=image.split()[3])&lt;/p&gt;
&lt;h2 id=&#34;lets-illustrate-with-an-example-7&#34;&gt;Let&amp;rsquo;s illustrate with an example
&lt;/h2&gt;&lt;h1 id=&#34;paste-the-original-image-onto-a-white-background-ignoring-transparent-pixels&#34;&gt;Paste the original image onto a white background, ignoring transparent pixels:
&lt;/h1&gt;&lt;pre&gt;&lt;code&gt;    background.paste(image, mask=image.split()[3])
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;convert-the-composite-image-to-grayscale-mode&#34;&gt;Convert the composite image to grayscale mode:
&lt;/h1&gt;&lt;pre&gt;&lt;code&gt;    gray_image = background.convert(&amp;quot;L&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;convert-the-grayscale-image-back-to-rgba-mode-to-preserve-the-transparent-background&#34;&gt;Convert the grayscale image back to RGBA mode to preserve the transparent background:
&lt;/h1&gt;&lt;pre&gt;&lt;code&gt;    final_image = gray_image.convert(&amp;quot;RGBA&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;else:&lt;/p&gt;
&lt;h1 id=&#34;if-its-not-in-rgba-mode-simply-convert-the-image-to-grayscale&#34;&gt;If it&amp;rsquo;s not in RGBA mode, simply convert the image to grayscale:
&lt;/h1&gt;&lt;pre&gt;&lt;code&gt;    final_image = image.convert(&amp;quot;L&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;get-the-original-images-filename-and-extension&#34;&gt;Get the original image&amp;rsquo;s filename and extension:
&lt;/h1&gt;&lt;pre&gt;&lt;code&gt;file_name, file_ext = os.path.splitext(image_file)
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;define-the-new-images-filename-adding-_bw-suffix-to-indicate-black-and-white&#34;&gt;Define the new image&amp;rsquo;s filename, adding &amp;ldquo;_bw&amp;rdquo; suffix to indicate black and white:
&lt;/h1&gt;&lt;pre&gt;&lt;code&gt;new_file_name = file_name + &amp;quot;_bw&amp;quot; + file_ext
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;save-the-new-image-optimizing-quality-to-reduce-file-size&#34;&gt;Save the new image, optimizing quality to reduce file size
&lt;/h1&gt;&lt;h2 id=&#34;lets-illustrate-this-with-an-example-1&#34;&gt;Let&amp;rsquo;s illustrate this with an example
&lt;/h2&gt;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;new_file_name = file_name + &amp;quot;_bw&amp;quot; + file_ext
    # Save the new image and optimize quality to reduce file size
    try:
        final_image.save(new_file_name, optimize=True)
    except Exception as e:
        logging.error(f&amp;quot;Failed to save {new_file_name}: {e}&amp;quot;)
        return None, None
    # Get the file sizes of the original and new images and return them
    old_size = os.path.getsize(image_file)
    new_size = os.path.getsize(new_file_name)
    return file_name, old_size, new_size
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;lets-illustrate-with-an-example-8&#34;&gt;Let&amp;rsquo;s illustrate with an example
&lt;/h2&gt;&lt;h1 id=&#34;define-a-function-to-parse-command-line-arguments-and-return-a-list-of-folder-paths-and-file-extensions&#34;&gt;Define a function to parse command-line arguments and return a list of folder paths and file extensions.
&lt;/h1&gt;&lt;p&gt;def parse_args():
# Create a parser object
parser = argparse.ArgumentParser(description=&amp;ldquo;Convert images to black and white and optimize quality.&amp;rdquo;)
# Add a required positional argument to represent the folder path
parser.add_argument(&amp;ldquo;folder_path&amp;rdquo;, help=&amp;ldquo;The path of the folder that contains the images.&amp;rdquo;)
# Add an optional argument to represent the list of extensions, defaulting to png, jpg, jpeg and gif
parser.add_argument(&amp;quot;-e&amp;quot;, &amp;ldquo;&amp;ndash;extensions&amp;rdquo;, nargs=&amp;quot;+&amp;quot;, default=[&amp;quot;.png&amp;quot;, &amp;ldquo;.jpg&amp;rdquo;, &amp;ldquo;.jpeg&amp;rdquo;, &amp;ldquo;.gif&amp;rdquo;], help=&amp;ldquo;The extensions of the image files.&amp;rdquo;)
# Parse command-line arguments and return the result object&lt;/p&gt;
&lt;h2 id=&#34;lets-illustrate-this-with-an-example-2&#34;&gt;Let&amp;rsquo;s illustrate this with an example
&lt;/h2&gt;&lt;h1 id=&#34;parse-command-line-arguments-and-return-the-result-object&#34;&gt;Parse command-line arguments and return the result object
&lt;/h1&gt;&lt;pre&gt;&lt;code&gt;args = parser.parse_args()
return args.folder_path, args.extensions
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;lets-illustrate-with-an-example-9&#34;&gt;Let&amp;rsquo;s illustrate with an example
&lt;/h2&gt;&lt;h1 id=&#34;define-a-function-to-print-the-comparison-of-file-sizes-before-and-after-optimization&#34;&gt;Define a function to print the comparison of file sizes before and after optimization
&lt;/h1&gt;&lt;p&gt;def print_result(result):
# If the result is not empty, it indicates that the conversion and saving were successful
if result:
# Unpack the result into a tuple of filename and file size
if len(result) == 3:
file, old_size, new_size = result
# Display the comparison of file sizes before and after optimization in the console
logging.info(f&amp;quot;{file}: {old_size} bytes -&amp;gt; {new_size} bytes&amp;quot;)
else:
logging.info(f&amp;quot;{result}&amp;quot;)&lt;/p&gt;
&lt;h2 id=&#34;lets-illustrate-with-an-example-10&#34;&gt;Let&amp;rsquo;s illustrate with an example
&lt;/h2&gt;&lt;h1 id=&#34;configure-the-logger-to-output-logs-to-both-the-console-and-files-setting-the-log-level-to-info&#34;&gt;Configure the logger to output logs to both the console and files, setting the log level to INFO
&lt;/h1&gt;&lt;p&gt;logging.basicConfig(level=logging.INFO,
format=&amp;quot;%(asctime)s %(levelname)s %(message)s&amp;quot;,
handlers=[logging.StreamHandler(),
logging.FileHandler(&amp;ldquo;log.txt&amp;rdquo;)])&lt;/p&gt;
&lt;h1 id=&#34;call-the-function-to-get-the-folder-path-and-extension-list&#34;&gt;Call the function to get the folder path and extension list
&lt;/h1&gt;&lt;p&gt;folder_path, extensions = parse_args()&lt;/p&gt;
&lt;h2 id=&#34;lets-illustrate-with-an-example-11&#34;&gt;Let&amp;rsquo;s illustrate with an example
&lt;/h2&gt;&lt;h1 id=&#34;calling-a-function-to-get-the-folder-path-and-extension-list&#34;&gt;Calling a function to get the folder path and extension list
&lt;/h1&gt;&lt;p&gt;folder_path, extensions = parse_args()&lt;/p&gt;
&lt;p&gt;if &lt;strong&gt;name&lt;/strong&gt; == &amp;ldquo;&lt;strong&gt;main&lt;/strong&gt;&amp;rdquo;:
# Windows needs this function because Windows lacks the fork() function (this is not entirely accurate).
# Therefore, on Windows, fork() is simulated by creating a new process. The code running in this new process, which is what runs in the child process on Linux,
# Because this code must run in unrelated processes, it must be passed to them before execution.
# The way to pass it is to serialize it first and then send it through a pipe from the original process to the new process.
# Furthermore, the new process is notified that it must execute the code passed through the pipe by passing the &amp;ndash;multiprocessing-fork command line parameter to it.&lt;/p&gt;
&lt;h2 id=&#34;lets-illustrate-with-an-example-12&#34;&gt;Let&amp;rsquo;s illustrate with an example
&lt;/h2&gt;&lt;h1 id=&#34;create-a-process-pool-automatically-assigning-processes-based-on-the-number-of-computer-cores&#34;&gt;Create a process pool, automatically assigning processes based on the number of computer cores
&lt;/h1&gt;&lt;pre&gt;&lt;code&gt;pool = multiprocessing.Pool()
# Create an empty list to store the result objects of asynchronous tasks
results = []
# Iterate through all files in the folder
for file in os.listdir(folder_path):
    # Construct the full file path
    file_path = os.path.join(folder_path, file)
    # Check if it&#39;s an image file, judging by the extension (you can modify the list of extensions as needed)
    if any(file_path.endswith(ext) for ext in extensions):
        # Call the function to convert and save the image, getting the file size, using asynchronous mode without blocking the main process
        result = pool.apply_async(convert_and_save, args=(file_path,), callback=print_result)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;lets-illustrate-with-an-example-13&#34;&gt;Let&amp;rsquo;s illustrate with an example
&lt;/h2&gt;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;result = pool.apply_async(convert_and_save, args=(file_path,), callback=print_result)
            # Add the result object to the list
            results.append((file, result))
    # Close the process pool, no longer accepting new tasks
    pool.close()
    # Wait for all tasks to complete
    pool.join()
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;epilogue&#34;&gt;Epilogue
&lt;/h2&gt;&lt;p&gt;Due to local development being on a &lt;code&gt;windows&lt;/code&gt; system, the first answer given by &lt;code&gt;AI&lt;/code&gt; did not include the &lt;code&gt;main&lt;/code&gt; function and also lacked &lt;code&gt;multiprocessing.freeze_support&lt;/code&gt;. The code was fixed after following up and encountering an error.
Just as learning the techniques of search engines required skill, we also need to learn how to communicate with &lt;code&gt;AI&lt;/code&gt;, providing reasonable and sufficient constraints to efficiently obtain the desired answers.
Note: &lt;strong&gt;If you are a programming beginner, if you still don&amp;rsquo;t understand certain parts of the code based on the given comments, please continue to ask related questions.&lt;/strong&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>AI-assisted programming, the evolution of productivity</title>
        <link>https://ttf248.life/en/p/ai-assisted-programming-productivity-evolution/</link>
        <pubDate>Tue, 28 Feb 2023 17:05:17 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/ai-assisted-programming-productivity-evolution/</guid>
        <description>&lt;p&gt;“GitHub Copilot” was released less than two years ago, and “ChatGPT” has appeared. I don’t fully understand the underlying principles; I&amp;rsquo;ve been using them for a while. The two tools’ levels of assistance are completely different, but they have both achieved a significant increase in productivity.&lt;/p&gt;
&lt;p&gt;For things that are too complex, AI still can’t do it – after all, they lack logic, or they rely on patterns or fixed frameworks; the training data is sufficient, and AI’s effect is nine out of ten.&lt;/p&gt;
&lt;h2 id=&#34;github-copilot&#34;&gt;GitHub Copilot
&lt;/h2&gt;&lt;p&gt;When publishing, I looked at the official introduction’s “demo” and found it wasn&amp;rsquo;t very smart. After trying it out, it wasn’t great, so I gave up on it.&lt;/p&gt;
&lt;p&gt;Before 2022, when I had some free time, I switched to the new version to try it out, and the effect was already quite good. The only issue was that the speed in China was a bit slow, which was probably due to network reasons. For languages like &lt;code&gt;Python&lt;/code&gt;, the code generated based on comments was pretty good, and much faster than researching and writing code myself.&lt;/p&gt;
&lt;h2 id=&#34;chatgpt&#34;&gt;ChatGPT
&lt;/h2&gt;&lt;p&gt;&lt;code&gt;Github Copilot&lt;/code&gt; still focused on developers, i.e., from a development perspective when designing the product. You need to understand how programming works and guide AI assistance through comments gradually. &lt;code&gt;ChatGPT&lt;/code&gt; has moved to the next level, &lt;strong&gt;allowing non-experts to quickly get started&lt;/strong&gt;. With its conversational chat interface, you can simply pose questions, describe requirements, and ask for clarification if you don&amp;rsquo;t understand something. Here’s a translation of the provided text into English:&lt;/p&gt;
&lt;p&gt;“Let&amp;rsquo;s take an example to understand the concept of tokenization, but don&amp;rsquo;t know how to code it. We want to calculate the similarity between two strings:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;Python tokenize and calculate the similarity between &amp;quot;Weitian Yuntong&amp;quot;, &amp;quot;LOGORY&amp;quot; and &amp;quot;Hefei Weitian Yuntong Information Technology Co., Ltd.&amp;quot;;
Before comparison, standardize the strings by converting English to lowercase;
If the second string contains the first string&#39;s starting character, truncate the second string to a length equal to the first string’s length.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Hello, this is Bing. 😊&lt;/p&gt;
&lt;p&gt;According to online information 123, Python tokenization can be implemented using the jieba library. The jieba library supports three tokenization modes: precise mode, full-potential mode, and search engine mode. You can choose an appropriate mode based on your needs.” To compare the similarity of two strings, you first need to tokenize them and then calculate metrics such as word vectors or edit distance. You can use libraries like NumPy or SciPy for performing mathematical operations.&lt;/p&gt;
&lt;h2 id=&#34;chatgpt-1&#34;&gt;ChatGPT
&lt;/h2&gt;&lt;p&gt;To standardize string processing, converting to lowercase in English can be achieved using Python&amp;rsquo;s built-in &lt;code&gt;lower()&lt;/code&gt; function.
To extract the second string, you can use Python’s slicing operator [ ] to achieve this.
Below is a possible code example:&lt;/p&gt;
&lt;h2 id=&#34;chatgpt-2&#34;&gt;ChatGPT
&lt;/h2&gt;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import jieba
import numpy as np
from scipy.spatial.distance import cosine

# Define two strings
str1 = &amp;quot;維天運通 LOGORY&amp;quot;
str2 = &amp;quot;合肥維天運通信息科技股份有限公司&amp;quot;

# Normalize strings, convert English to lowercase
str1 = str1.lower()
str2 = str2.lower()

# Find the position of the first character of str1 in str2
index = str2.find(str1[0])

# If found, slice str2
if index != -1:
    str2 = str2[index:index+len(str1)]

# Use jieba library to tokenize strings
words1 = list(jieba.cut(str1))
words2 = list(jieba.cut(str2))

# Calculate word vectors (using frequency as weights)
vector1 = np.zeros(len(words1))
vector2 = np.zeros(len(words1))
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;chatgpt-3&#34;&gt;ChatGPT
&lt;/h2&gt;&lt;h1 id=&#34;calculate-the-word-vectors-of-two-strings-with-frequency-as-weight&#34;&gt;Calculate the word vectors of two strings (with frequency as weight)
&lt;/h1&gt;&lt;p&gt;vector1 = np.zeros(len(words1))
vector2 = np.zeros(len(words1))&lt;/p&gt;
&lt;p&gt;for i, word in enumerate(words1):
vector1[i] += 1  # Count the number of times each word appears in the first string
if word in words2:
vector2[i] += 1  # Count the number of times each word appears in the second string&lt;/p&gt;
&lt;h1 id=&#34;calculate-the-similarity-between-the-two-strings-using-cosine-distance-as-an-index&#34;&gt;Calculate the similarity between the two strings (using cosine distance as an index)
&lt;/h1&gt;&lt;p&gt;similarity = 1 - cosine(vector1, vector2)&lt;/p&gt;
&lt;p&gt;print(&amp;ldquo;The similarity between the two strings is:&amp;rdquo;, similarity)&lt;/p&gt;
&lt;h2 id=&#34;chatgpt-4&#34;&gt;ChatGPT
&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Going Further – Consult the Execution Results. Congratulations, you’ve identified an issue! The result was hallucinated; it didn&amp;rsquo;t actually run the code, but will provide a good-looking response.&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&#34;example&#34;&gt;Example
&lt;/h3&gt;&lt;p&gt;Question: SQL statement to retrieve records from the product code table where the product code is within the range of 80000-89999.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;Hello, this is Bing. 😊

Based on search result 12, you can use the SQL BETWEEN operator to select values within a given range. The BETWEEN operator includes both the start and end values.

You need to first specify the table name and column name that you want to query, and then use the BETWEEN operator and the range values in the WHERE clause.

For example, if you want to retrieve records from the product code table where the product code is between 80000 and 89999, you can use the following statement:

SELECT * FROM product_code
WHERE code BETWEEN 80000 AND 89999;
Here, * represents selecting all columns 3.
``` If you only want to select specific columns, such as product name and price, you can separate them with commas like this:

SELECT name, price FROM product_code
WHERE code BETWEEN 80000 AND 89999;

Hopefully, this helps.

Please note that product codes are strings, and your code is incorrect.

### Example

```markdown
Sorry, I didn’t realize the product code was a string. If that’s the case, you need to add single quotes around both ends of the range values, as follows:

SELECT * FROM product_code
WHERE code BETWEEN &#39;80000&#39; AND &#39;89999&#39;;
Please forgive my mistake. 🙏
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Congratulations, it recognizes errors!&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
