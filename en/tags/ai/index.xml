<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>AI on Uncle Xiang&#39;s Notebook</title>
        <link>https://ttf248.life/en/tags/ai/</link>
        <description>Recent content in AI on Uncle Xiang&#39;s Notebook</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en</language>
        <lastBuildDate>Mon, 02 Jun 2025 20:54:02 +0800</lastBuildDate><atom:link href="https://ttf248.life/en/tags/ai/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>Old problems, the flamboyant beauty of blossoming flowers. (This captures the essence and poetic nature of the original.)

Alternatively, a more literal translation could be: “Old ailments, beautiful eyes like blooming flowers.” However, the first option is generally preferred for its aesthetic quality.</title>
        <link>https://ttf248.life/en/p/old-ailment-stunning-flowers/</link>
        <pubDate>Mon, 26 May 2025 23:54:12 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/old-ailment-stunning-flowers/</guid>
        <description>&lt;p&gt;For many years, I’ve focused on backend development, and recently started to explore &lt;code&gt;AI&lt;/code&gt; programming while dipping my toes into some frontend-related content. However, during this period of tinkering, I gradually realized I was falling back into an old habit – being dazzled by shiny new things. I constantly try to use &lt;code&gt;AI&lt;/code&gt; to create a frontend interface, but in reality, these attempts haven’t provided much practical benefit for my current work and are actually wasting my energy.&lt;/p&gt;
&lt;h2 id=&#34;ai-use-cases&#34;&gt;AI Use Cases
&lt;/h2&gt;&lt;p&gt;In small projects, AI tools can truly shine, particularly when writing independent functions with low coupling to the system and simple business logic. These tasks typically have clear inputs and outputs, and rely less on context – making them well-suited for the current capabilities of AI-assisted programming.&lt;/p&gt;
&lt;p&gt;However, when facing complex system architectures or deep business logic, AI’s limitations begin to emerge. It may generate code that appears reasonable but is actually detached from the project&amp;rsquo;s real needs, or even introduce potential issues that are difficult to debug. In these scenarios, AI is best suited as an assistive tool rather than a fully autonomous code generator. We need to conduct rigorous review and testing of the generated code to ensure it meets actual requirements.&lt;/p&gt;
&lt;h2 id=&#34;errors-and-the-cost-of-learning&#34;&gt;Errors and the Cost of Learning
&lt;/h2&gt;&lt;p&gt;While attempting to generate frontend code using AI, I encountered numerous challenges. As frontend development isn&amp;rsquo;t a domain I’m familiar with, troubleshooting often proved time-consuming and frustrating. Even after adjusting prompts to have the AI rewrite the code, it was difficult to avoid the appearance of some low-level errors. This iterative process not only wasted time but also highlighted that my current focus should be on backend business logic rather than groping around in unfamiliar territory.&lt;/p&gt;
&lt;p&gt;Looking back at the project completed over the weekend, I’m even more convinced that focusing on backend development and user interaction logic, implementing functionality through a console, is the most efficient approach currently. Perhaps systematically learning frontend knowledge would be a better strategy when I have more time and energy.&lt;/p&gt;
&lt;h2 id=&#34;frontend-learning-plan&#34;&gt;Frontend Learning Plan
&lt;/h2&gt;&lt;p&gt;The frontend technology stack is complex and diverse, so it’s unrealistic to quickly master it. I plan to first choose a framework, such as Vue.js or React.js, and deeply learn its core concepts and usage methods. Only after becoming familiar with the fundamentals will I attempt to use AI to generate frontend code, which can effectively avoid errors and wasted time caused by unfamiliarity.&lt;/p&gt;
&lt;p&gt;In short, the focus for this stage should be on backend development, steadily building my core skills. When the timing is right, I’ll then explore the combination of frontend and AI – potentially yielding greater rewards.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Claude4 released, attempting to develop: Hugo tags, hyperlink translation assistant</title>
        <link>https://ttf248.life/en/p/claude-4-release-hugo-tags-hyperlink-translation-assistant/</link>
        <pubDate>Sat, 24 May 2025 03:05:31 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/claude-4-release-hugo-tags-hyperlink-translation-assistant/</guid>
        <description>&lt;p&gt;This site is developed using Hugo, but I’ve always used Chinese titles, which results in less friendly generated article links. In simpler terms, when shared, they don&amp;rsquo;t look as good because the Chinese characters are escaped into formats like %E4%BD%A0%E5%A5%BD within the links. While you can solve this by setting a slug, it’s tedious to do manually every time.&lt;/p&gt;
&lt;p&gt;Therefore, I decided to try using Claude4 to develop a translation assistant that automatically converts Chinese titles to English slugs and adds hyperlinks within the articles. This would eliminate the need for manual setup.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Claude4 is amazing – its contextual understanding has significantly improved, as has its efficiency in handling complex tasks.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;!-- more --&gt;
&lt;h2 id=&#34;project-address&#34;&gt;Project Address
&lt;/h2&gt;&lt;p&gt;Domestic Project Address: &lt;a class=&#34;link&#34; href=&#34;https://cnb.cool/ttf248/hugo-content-suite&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://cnb.cool/ttf248/hugo-content-suite&lt;/a&gt;
International Project Address: &lt;a class=&#34;link&#34; href=&#34;https://github.com/ttf248/hugo-content-suite&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/ttf248/hugo-content-suite&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;code-implementation&#34;&gt;Code Implementation
&lt;/h2&gt;&lt;p&gt;Let&amp;rsquo;s first discuss the implementation approach: We need to scan all articles, extract tag information and article titles, and then call on our local large model (such as gemma-3-12b-it) for translation.&lt;/p&gt;
&lt;p&gt;In actual development, Claude4 showcased several significant advantages compared to previous generation large models. Due to the diverse functional requirements, Claude4 automatically designed an interactive menu, comprehensively considering various usage scenarios. For example, in tag processing, Claude4 not only supports tag statistics and analysis but also includes classification statistics and can even detect &lt;strong&gt;unlabeled articles&lt;/strong&gt;. Furthermore, it provides &lt;strong&gt;preview&lt;/strong&gt; and tag page generation functionalities.&lt;/p&gt;
&lt;p&gt;Whether it&amp;rsquo;s integrating with local large models, adding translation caches, or performing large-scale code refactoring, Claude4 completes everything in one go, with almost no issues. Despite the relatively small project size, it includes many minor features. In previous development processes, large models often forgot earlier content, but Claude4 performed exceptionally well this time, &lt;strong&gt;virtually without any context forgetting issues.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In short, intelligence has increased, and we plan to switch to Claude4 for more development work as our primary coding model.&lt;/p&gt;
&lt;h2 id=&#34;translation-cache&#34;&gt;Translation Cache
&lt;/h2&gt;&lt;p&gt;This approach, besides reducing the number of calls to large models, performs quite efficiently when running a 12b model locally – it doesn’t waste much time. However, if you need to call the large model every time, it will still be somewhat slow. Furthermore, to fix the connections within articles, if a full update operation is executed and the article title is very long, there&amp;rsquo;s occasionally a situation where the two translated results differ, causing the link to change – which is quite awkward.&lt;/p&gt;
&lt;h2 id=&#34;feature-optimization&#34;&gt;Feature Optimization
&lt;/h2&gt;&lt;p&gt;The entire project was handed over to &lt;code&gt;Claude4&lt;/code&gt; to analyze the space for optimization and obtain the following suggestions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Externalize Configuration - Improve maintainability and flexibility&lt;/li&gt;
&lt;li&gt;Structured Logging - Facilitate troubleshooting and monitoring&lt;/li&gt;
&lt;li&gt;Performance Monitoring - Understand system running status&lt;/li&gt;
&lt;li&gt;User Experience - Visual feedback such as progress bars&lt;/li&gt;
&lt;li&gt;Error Handling - A more robust exception handling mechanism&lt;/li&gt;
&lt;li&gt;Code Organization - Clearer module division&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The code was reviewed, and there were no major issues, such as the configuration files. The original code&amp;rsquo;s configurations were converted to default settings, and if a corresponding configuration file wasn’t found during reading, a default one would be automatically generated to prevent user errors.
Requirement: When translating text into English, dynamically calculate the current translation efficiency, estimate the remaining time, and output relevant information to the console: Now it has obtained the character count of the article, the character count for each line being translated, the translation time, and fits the calculation of the translation time for every 100 characters. Simultaneously, it calculates the estimated remaining translation time.&lt;/p&gt;
&lt;p&gt;The code was completed, but the effect wasn&amp;rsquo;t very satisfactory, so I asked AI to provide a new design solution:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Provide multiple efficiency calculation methods: Real-time efficiency, average efficiency, sliding window efficiency&lt;/li&gt;
&lt;li&gt;Improve display method: Progress bars, segmented statistics, dynamic refresh&lt;/li&gt;
&lt;li&gt;Add more useful metrics: API call counts, success rates, etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;After the code was completed, a new surprise was discovered – translation efficiency statistics flooded in in real time, but without endless scrolling.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;Translating text to English (total 7163 characters)...
Detected 53 lines to translate
  [1/53] Stage1/6 [░░░░░░░░░░░░░░░░░░░░░░░░░] 1.9% Translating 354 characters...
    ✅ Completed (3.1s) | API Call #1
    ✅ Completed (1.5s) | API Call #2
    ✅ Completed (0.9s) | API Call #3
    ✅ Completed (0.2s) | API Call #4
    ✅ Completed (1.0s) | API Call #5
    ✅ Completed (1.0s) | API Call #6
    ✅ Completed (0.2s) | API Call #7
    📊 Progress: 13.2% (7/53) | 12.9% (925/7163) 114.6 characters/second 📊
    ⚡ Efficiency: Real-time 76.4 | Average 117.9 | Recent 109.0 | Stage 113.6 characters/second 📊
    🎯 Success Rate: 100.0% (7/7) | Remaining: 46 lines 7 seconds] 9.4% Translating 110 characters...
    ⏱️ Estimated remaining: 55s | Predicted completion time: 00:10:19 8s] 11.3% Translating 114 characters...
    💾 Processing speed: 3211.3 lines/minute | Total time: 8s] 13.2% Translating 16 characters...
  [8/53] Stage1/6 [███░░░░░░░░░░░░░░░░░░░░░░] 15.1% Translating 166 characters...
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Previously, the control program wasn&amp;rsquo;t written with many features, so I was curious about how it was implemented and looked at the code:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-golang&#34;&gt;// Clear screen and redisplay (dynamic refresh effect)
if translationCount &amp;gt; 1 {
    fmt.Print(&amp;quot;\033[6A\033[K&amp;quot;) // Move up 6 lines and clear
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;performance-statistics-menu&#34;&gt;Performance Statistics Menu
&lt;/h3&gt;&lt;p&gt;The newly added &lt;strong&gt;Performance Statistics Menu&lt;/strong&gt;, which I myself designed, isn&amp;rsquo;t as well-designed as this one.&lt;/p&gt;
&lt;p&gt;📊 Performance Statistics:
🔄 Translation Count: 360
⚡ Cache Hit Rate: 1.4% (5/365)
⏱️ Average Translation Time: 315.927234ms
📁 File Operations: 73
❌ Error Count: 0&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Data Mining&lt;/li&gt;
&lt;li&gt;Deep Learning&lt;/li&gt;
&lt;li&gt;Neural Network&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;progress-bar-display&#34;&gt;Progress Bar Display
&lt;/h3&gt;&lt;p&gt;New &lt;strong&gt;Progress Bar Display&lt;/strong&gt;, detailed progress, elapsed time, estimated remaining time.
Please select function (0-13): 10
🔍 Collecting translation target&amp;hellip;
📄 Loaded cache file, containing 0 translation records
📊 Translation Cache Statistics:
🏷️ Total tags: 229
📝 Total articles: 131
✅ Cached: 0
🔄 To be translated: 360
Confirm to generate full translation cache? (y/n): y
🚀 Generating full translation cache&amp;hellip;
📄 Loaded cache file, containing 0 translation records
🔍 Checking translations in cache&amp;hellip;
🔄 Need to translate 360 new tags
[░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 5/360 (1.4%) - Time taken: 3s - Estimated remaining: 3m8s
💾 Saved cache file, containing 5 translation records
[█░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 10/360 (2.8%) - Time taken: 6s - Estimated remaining: 3m28s
💾 Saved cache file, containing 10 translation records
[██░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 15/360 (4.2%) - Time taken: 9s - Estimated remaining: 3m30s
💾 Saved cache file, containing 15 translation records
[██░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 20/360 (5.6%) - Time taken: 13s - Estimated remaining: 3m36s
💾 Saved cache file, containing 20 translation records
[███░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 25/360 (6.9%) - Time taken: 16s - Estimated remaining: 3m33s
💾 Saved cache file, containing 25 translation records
[████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 30/360 (8.3%) - Time taken: 19s - Estimated remaining: 3m30s
💾 Saved cache file, containing 30 translation records
[████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 35/360 (9.7%) - Time taken: 22s - Estimated remaining: 3m25s
💾 Saved cache file, containing 35 translation records&lt;/p&gt;
&lt;hr&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;=== Hugo Blog Management Tool ===

🚀 Core Features
  1. One-click full blog processing (complete blog workflow)

📝 Content Management
  2. Generate tag pages
  3. Generate article slugs
  4. Translate articles into multi-language versions

💾 Cache Management
  5. View cache status
  6. Generate full translation cache
  7. Clear translation cache

  0. Exit the program
&lt;/code&gt;&lt;/pre&gt;
</description>
        </item>
        <item>
        <title>AI overuse can lead to some lingering effects.</title>
        <link>https://ttf248.life/en/p/ai-overuse-side-effects/</link>
        <pubDate>Wed, 14 May 2025 19:39:50 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/ai-overuse-side-effects/</guid>
        <description>&lt;p&gt;Since we’ve established “AI Inspiration Collision Forum,” there’s been a lot of random content being created, with people experimenting with AI to record and publish things, but thoughtful reflection is becoming increasingly scarce. Moving forward, it would be beneficial to slightly control the output of this section and consolidate it into a monthly magazine format – releasing one article per month would suffice.&lt;/p&gt;
&lt;p&gt;This feels like a sort of aftereffect or side effect; efficiency has increased, but the depth and breadth of thought have declined.&lt;/p&gt;
&lt;h2 id=&#34;efficiency-boost-undeniable&#34;&gt;Efficiency Boost: Undeniable
&lt;/h2&gt;&lt;p&gt;The “Seven-Second Fish Sightseeing” column used to be poorly maintained, with only a few hot events covered. Due to laziness, I hadn’t searched the internet for relevant materials or compiled records. Now that various AI tools are available, all it takes is outlining the key points, and AI can automatically search for related events, generate articles as needed, simply format them, and publish them.&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s like a blessing for lazy people – efficiency has increased significantly, almost to the point of doubling efforts.&lt;/p&gt;
&lt;p&gt;Beyond writing articles, efficiency gains are real when coding. Previously, writing code often required detailed reading of API interface documentation. Now, I can skip this entirely. This is incredibly valuable because familiarizing oneself with APIs is “physical labor,” not “mental labor.” AI handles this part perfectly.&lt;/p&gt;
&lt;h2 id=&#34;spam-content&#34;&gt;Spam Content
&lt;/h2&gt;&lt;p&gt;Many articles have poor quality content, not to say that there’s nothing there; it just doesn&amp;rsquo;t read well, lacking a sense of reality and what people actually experience. It’s a style I didn’t enjoy before – like chewing wax.&lt;/p&gt;
&lt;p&gt;From another perspective, AI-generated content really does feel like products of a流水线 (liú shuǐ liàn - assembly line) production, lacking soul.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;New Era Internet Spam&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;forgetting&#34;&gt;Forgetting
&lt;/h2&gt;&lt;p&gt;This type of document is entirely AI-generated, and the reader’s context is unclear. However, over time, my own impressions will become blurred, or even forgotten.&lt;/p&gt;
&lt;p&gt;Similar issues occur when writing code – without reviewing commit records, I can&amp;rsquo;t remember how I originally thought about it, or why I wrote it that way. This is particularly evident with code generated through repeated communication with AI, where the final code differs significantly from the initial ideas, sometimes drastically so.&lt;/p&gt;
&lt;h2 id=&#34;search&#34;&gt;Search
&lt;/h2&gt;&lt;p&gt;Recently, the number of times I’ve opened Google and Baidu has noticeably decreased. Many questions are now being answered by AI for searching and interaction, and the results are much better than traditional search engines.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s mourn the &lt;code&gt;bing ai&lt;/code&gt;, which may no longer be active, a pioneering AI tool from a major company that could connect to the internet and search.&lt;/p&gt;
&lt;p&gt;I’m using Google less, and the number of times I visit &lt;code&gt;stackoverflow&lt;/code&gt; has decreased as well. Many questions are now simply asked of AI – this website is gradually being phased out by the times.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion
&lt;/h2&gt;&lt;p&gt;My maintained blog, originally with very little traffic, is now even less expected of; it’s primarily a place for self-reflection and recording.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>No coding, design and develop a self-selected stock module.</title>
        <link>https://ttf248.life/en/p/design-develop-custom-stock-module-no-code/</link>
        <pubDate>Thu, 27 Feb 2025 23:20:39 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/design-develop-custom-stock-module-no-code/</guid>
        <description>&lt;ul&gt;
&lt;li&gt;Last month, we experimented with cursor, but due to the limitations of the free quota, we didn&amp;rsquo;t develop overly complex features; we just did some basic testing. We discovered then that Byte also released similar products, both using the same large models – Claude-3.5 – at their core.
Byte’s product is called Trae, initially launched in the Mac version and finally released its Windows version in February of this year. Big companies are good because you can freely “white嫖” (literally translates to &amp;ldquo;free-eat&amp;rdquo;), without having to pay, with unlimited use of Claude-3.5 – this model performs quite well.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Ultimately, we got stuck on the development of candlestick charts. As I don’t understand React at all, I had to give up. To continue developing, I would need to supplement my knowledge of front-end basics, breaking down the task into smaller, more manageable pieces instead of directly giving me a large task: developing candlestick charts.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;issues-found&#34;&gt;Issues Found
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;Due to the lack of training data caused by using foreign AI models and Vue3 + Element-Plus, React was chosen as the frontend framework.&lt;/li&gt;
&lt;li&gt;Occasional syntax errors may exist and require manual fixes.&lt;/li&gt;
&lt;li&gt;Solutions for some complex problems require manual guidance.&lt;/li&gt;
&lt;li&gt;Code structure optimization requires manual instruction.&lt;/li&gt;
&lt;li&gt;The most time-consuming part was packaging the frontend code into a container, due to my zero foundation in &lt;code&gt;.env.production&lt;/code&gt; and &lt;code&gt;tsconfig.json&lt;/code&gt;, I had no concept of these either; I only straightened out the corresponding logic by asking for help from bean sprouts halfway through. There are significant differences between the dev mode and build mode of frontend development, and the checks performed on the code. Backend database and service container scripts were completed in a total of five minutes.
&lt;strong&gt;Currently, AI mainly improves the efficiency of development; having a foundation is best, not that AI will solve all problems for you.&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;repository-address&#34;&gt;Repository Address
&lt;/h2&gt;&lt;p&gt;As the title indicates, this time we’re going to try a chat-based approach with AI instead of writing code, aiming to design and develop a self-trading module. Let&amp;rsquo;s see what results we can achieve.&lt;/p&gt;
&lt;p&gt;Repository address: &lt;a class=&#34;link&#34; href=&#34;https://github.com/ttf248/trae-demo&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/ttf248/trae-demo&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;For detailed usage instructions, please refer to the README.md file in the repository.&lt;/p&gt;
&lt;p&gt;The repository contains numerous submission records, most of which are dialogues between me and Trae, along with my testing of Trae’s functionalities, and notes on whether manual intervention was required to implement each feature.&lt;/p&gt;
&lt;h2 id=&#34;prompt&#34;&gt;Prompt
&lt;/h2&gt;&lt;p&gt;The project is to develop functionality from scratch, based on the following project prototype diagram:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;Based on the project prototype diagram, develop features: stock selection (watchlist), which needs to support adding, deleting, modifying, and querying contracts. The watchlist interface should display basic market data. Support multiple different market switches.

Frontend: react
Backend: golang gin gorm
Database: PostgreSQL

The backend needs to support cross-origin requests, while also considering data validation and error handling. If the backend service is unavailable, the frontend should display an alert message.

The backend needs to display request and response logs; the frontend also prints communication logs for troubleshooting purposes.
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;ui-and-interaction-optimization&#34;&gt;UI and Interaction Optimization
&lt;/h2&gt;&lt;p&gt;The design of the front-end interface relies entirely on Grok. We initially created a prototype within Trae, but it lacked aesthetics. Because the model used has strong coding capabilities but weaker other abilities, we need to use Grok to optimize the front-end UI.&lt;/p&gt;
&lt;p&gt;By taking screenshots of the current interface and uploading them to Grok, we can receive numerous optimization suggestions at once. We then manually evaluate these suggestions and copy them into Trae to execute and observe the results of the optimizations.&lt;/p&gt;
&lt;h3 id=&#34;technology-stack&#34;&gt;Technology Stack
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Frontend: React + TypeScript&lt;/li&gt;
&lt;li&gt;Backend: Golang + Gin + GORM&lt;/li&gt;
&lt;li&gt;Database: PostgreSQL 17&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;system-architecture&#34;&gt;System Architecture
&lt;/h2&gt;&lt;h2 id=&#34;backend-architecture&#34;&gt;Backend Architecture
&lt;/h2&gt;&lt;p&gt;The backend utilizes the Gin framework (Go) to implement RESTful APIs, with the following key modules:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Database Module&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Utilizes GORM as an ORM framework&lt;/li&gt;
&lt;li&gt;Supports database connection configuration via environment variables&lt;/li&gt;
&lt;li&gt;Automatically performs database schema migrations&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Routing Module&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;RESTful API design&lt;/li&gt;
&lt;li&gt;A unified error handling mechanism&lt;/li&gt;
&lt;li&gt;Built-in request logging&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Cross-Origin Handling&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Supports cross-origin requests from local development environments&lt;/li&gt;
&lt;li&gt;Configurable CORS policies&lt;/li&gt;
&lt;li&gt;Supports cookie-based cross-origin access&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;frontend-architecture&#34;&gt;Frontend Architecture
&lt;/h2&gt;&lt;p&gt;The frontend was built using React + TypeScript, implementing the following features:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Stock list display&lt;/li&gt;
&lt;li&gt;Watchlist management&lt;/li&gt;
&lt;li&gt;Real-time quote data display&lt;/li&gt;
&lt;li&gt;Error handling mechanism&lt;/li&gt;
&lt;/ul&gt;</description>
        </item>
        <item>
        <title>AI Development Over Two Years: Somewhat similar to the state before a Docker release – fragmented and rapidly evolving.</title>
        <link>https://ttf248.life/en/p/ai-development-two-years-like-docker-pre-release/</link>
        <pubDate>Thu, 20 Feb 2025 18:16:37 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/ai-development-two-years-like-docker-pre-release/</guid>
        <description>&lt;p&gt;Artificial Intelligence (AI) has undoubtedly been one of the most hotly debated topics in the technology field in recent years, particularly over the past two years, with AI technology experiencing rapid advancements. Whether it’s deep learning, natural language processing, or computer vision and automated decision systems, the application scenarios for AI are constantly emerging. However, despite continuous technological breakthroughs, AI still faces a bottleneck similar to that of a Docker release – a lack of a killer app to truly ignite the market.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Prompt: Write an article: AI has developed for two years, which is somewhat like the state of Docker’s pre-release, lacking a killer application, based on existing technologies, create a perfect landing scenario, Docker did not use too many new technologies, but the overall solution is reasonable, and it changes the workflow of operations and development.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;ai-development-status-the-technology-is-mature-but-applications-still-need-breakthroughs&#34;&gt;AI Development Status: The Technology is Mature, but Applications Still Need Breakthroughs
&lt;/h2&gt;&lt;p&gt;From a technical perspective, AI has made significant progress in the past two years. Whether it’s OpenAI&amp;rsquo;s GPT series models or Google’s BERT and DeepMind’s Alpha series, AI processing capabilities have far exceeded previous expectations. Particularly in the field of natural language processing, models like GPT-4 not only possess powerful generative abilities but also demonstrate astonishing performance in understanding and reasoning.&lt;/p&gt;
&lt;p&gt;However, despite technological advancements, AI&amp;rsquo;s implementation in practical applications faces certain challenges. Similar to Docker’s state before its release, although AI has enormous potential, there hasn&amp;rsquo;t yet been a truly widespread and industry-changing killer app. People are talking about the prospects of AI, but they may not be able to find an application that can directly bring about revolutionary changes. Many AI applications remain in their initial stages of experimentation, and most require further integration and optimization.&lt;/p&gt;
&lt;h2 id=&#34;docker-and-ai-parallels-technology-doesnt-have-to-be-innovation-its-about-the-solution&#34;&gt;Docker and AI Parallels: Technology Doesn&amp;rsquo;t Have to Be Innovation, It’s About the Solution
&lt;/h2&gt;&lt;p&gt;Looking back at Docker’s release history, we can readily see many similarities between the technological environment at the time and the current state of AI development. Before Docker, container technology wasn’t a new concept; early technologies like LXC (Linux Containers) and virtualization already possessed the basic capabilities for containerization. However, Docker didn&amp;rsquo;t introduce disruptive technology itself. Instead, it presented a simpler, more intuitive, and efficient solution through the clever integration and optimization of existing technologies. This solution didn’t rely on revolutionary technological breakthroughs, but rather addressed many pain points in software deployment, scaling, and management processes, significantly simplifying the workflow.&lt;/p&gt;
&lt;p&gt;Similarly, the AI field is facing a similar situation. While current AI technology isn&amp;rsquo;t “new,” achieving truly large-scale applications still requires a perfect implementation scenario – just like Docker, integrating and optimizing existing technologies to form a reasonable application solution. Killer AI applications may not rely on entirely new technological breakthroughs, but rather on how to integrate existing technologies to solve practical business pain points and needs.&lt;/p&gt;
&lt;h2 id=&#34;how-to-find-ais-docker-moment&#34;&gt;How to Find AI&amp;rsquo;s &amp;ldquo;Docker Moment&amp;rdquo;?
&lt;/h2&gt;&lt;p&gt;To truly enable the widespread adoption of AI technology, we need to focus on several key areas:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Deep Exploration of Practical Scenarios&lt;/strong&gt;
Currently, many AI applications are still largely experimental and lack large-scale real-world implementation.  For example, while AI customer service and intelligent recommendations are widely used, their functionality is often limited and hasn&amp;rsquo;t yet broken through industry bottlenecks. True breakthroughs may come from industries that have been struggling with traditional methods for a long time, such as healthcare, manufacturing, and logistics. AI can help these companies improve efficiency and reduce costs in complex scenarios through more efficient data processing and predictive analytics.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Productization and Ease of Use&lt;/strong&gt;
Just like Docker simplifies containerization processes to boost operational efficiency, the ease of use of AI products is equally crucial. The widespread adoption of AI isn&amp;rsquo;t just about technological proliferation; it’s also about product proliferation. Integrating AI into daily workflows, allowing users to easily utilize these tools without needing to deeply understand the technology, is a key step in AI implementation.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Ecosystem Building and Standardization&lt;/strong&gt;
Any new technology’s widespread adoption relies on ecosystem building. Docker&amp;rsquo;s rapid rise was due to its openness and compatibility, enabling developers to seamlessly integrate with various cloud platforms, tools, and services. Similarly, the future of AI depends on the construction of an ecosystem.  Standardization of AI, model sharing, data openness, and technological interoperability will all influence whether AI can form widespread industry applications.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;conclusion-the-future-of-ai-is-full-of-possibilities-but-requires-more-refined-implementation&#34;&gt;Conclusion: The Future of AI is Full of Possibilities, but Requires More Refined Implementation
&lt;/h2&gt;&lt;p&gt;Despite the significant advancements in AI technology over the past two years, it remains in the stage of “lacking killer applications” at present. Similar to containerization technology before Docker’s release, AI needs a reasonable application scenario that deeply integrates existing technologies with business requirements to truly achieve large-scale adoption and widespread use. While technological innovation is important, solutions that simplify processes and improve efficiency are more likely to drive the popularization and development of the technology.&lt;/p&gt;
&lt;p&gt;In the future, AI may evolve like Docker – not through disruptive technological breakthroughs, but by integrating existing technologies to create a perfect application scenario, ultimately changing the way we work and live.&lt;/p&gt;</description>
        </item>
        <item>
        <title>ollama local deployment of deepseek-R1</title>
        <link>https://ttf248.life/en/p/ollama-local-deployment-deepseek-r1/</link>
        <pubDate>Fri, 07 Feb 2025 22:41:02 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/ollama-local-deployment-deepseek-r1/</guid>
        <description>&lt;p&gt;Ollama is an open-source AI tool designed to enable users to run and deploy large language models (LLMs) locally. Its goal is to provide a convenient and efficient way for developers to use models like GPT on their local machines without relying on cloud services. Ollama supports multiple models and focuses on optimizing performance, allowing even resource-constrained devices to smoothly run these models.&lt;/p&gt;
&lt;p&gt;Through Ollama, users can utilize text-based AI applications and interact with locally deployed models without worrying about data privacy or high API usage fees. You can invoke different models via a command-line interface (CLI) for tasks such as natural language processing and question answering.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Ollama is suitable for experimenting with various models; after testing the Windows version, it couldn&amp;rsquo;t fully leverage the hardware’s performance, possibly due to the Windows version. When deploying 32b parameter models, with low memory and GPU load, the response speed is slow.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;hardware-overview&#34;&gt;Hardware Overview
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;Operating System: Windows 11&lt;/li&gt;
&lt;li&gt;CPU: i7-10700K&lt;/li&gt;
&lt;li&gt;Memory: 40GB&lt;/li&gt;
&lt;li&gt;Graphics Card: RTX 3060 12GB&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;environment-setup&#34;&gt;Environment Setup
&lt;/h2&gt;&lt;p&gt;Add the system environment variable to facilitate subsequent use:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;set OLLAMA_MODELS=E:\ollama&lt;/code&gt;&lt;/strong&gt;
This variable specifies the location where Ollama models are stored. &lt;code&gt;E:\ollama&lt;/code&gt; is a folder path indicating that all local model files will be stored in this directory. Ollama will load and use the language models you download or deploy based on this path. You can store model files in other locations by simply changing this path.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;set OLLAMA_HOST=127.0.0.1:8000&lt;/code&gt;&lt;/strong&gt;
This environment variable sets the host and port for the Ollama service.
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;127.0.0.1&lt;/code&gt; is the localhost address, meaning the Ollama service will only listen for requests from the local machine.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;8000&lt;/code&gt; is the specified port number, indicating that the Ollama service will wait for and process requests on port 8000. You can change the port number if needed, but make sure it&amp;rsquo;s not already in use by another application.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;set OLLAMA_ORIGINS=*&lt;/code&gt;&lt;/strong&gt;
This environment variable controls which origins are allowed to access the Ollama service.
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;*&lt;/code&gt; indicates that all origins (i.e., all domains and IP addresses) can access the Ollama service. This is typically used in development and debugging environments, and in production environments, it&amp;rsquo;s usually necessary to specify stricter origin control, limiting only specific domains or IPs to access your service for enhanced security.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;deepseek-r1-model-deployment&#34;&gt;DeepSeek-R1 Model Deployment
&lt;/h2&gt;&lt;p&gt;ollama installation is straightforward, so we won&amp;rsquo;t detail it here.&lt;/p&gt;
&lt;p&gt;Post-installation verification:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;C:\Users\core&amp;gt;ollama -v
ollama version is 0.5.11
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To deploy the model, refer to the official model page and select the appropriate parameter model: &lt;code&gt;ollama run deepseek-r1:14b&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;The 14b parameter version effectively remembers conversation context; smaller parameter versions cannot retain context. The 32b parameter version is very sluggish when deployed locally and hasn&amp;rsquo;t been further tested.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.ollama.com/library/deepseek-r1&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://www.ollama.com/library/deepseek-r1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/SPEvYTmTBxhoEkJqm1yPmw&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://mp.weixin.qq.com/s/SPEvYTmTBxhoEkJqm1yPmw&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://blog.csdn.net/x18990027/article/details/145368094&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://blog.csdn.net/x18990027/article/details/145368094&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
        </item>
        <item>
        <title>DeepSeek suddenly went viral before the Spring Festival, NVIDIA stock plummeted: The underlying institutional operations and the chain of thought for large models.</title>
        <link>https://ttf248.life/en/p/deepseek-explodes-before-chinese-new-year-nvidia-stock-plummets-behind-the-scenes-and-large-language-model-reasoning/</link>
        <pubDate>Fri, 07 Feb 2025 20:36:05 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/deepseek-explodes-before-chinese-new-year-nvidia-stock-plummets-behind-the-scenes-and-large-language-model-reasoning/</guid>
        <description>&lt;p&gt;Prior to the Spring Festival, DeepSeek had become a hot topic, attracting widespread attention on social media in just a few days. This sudden surge in popularity was surprising and triggered a chain reaction in the market. Meanwhile, NVIDIA’s stock price plummeted, raising concerns among many investors, with some institutions conducting large-scale short selling during this period, seemingly pointing to a “carefully orchestrated” situation.&lt;/p&gt;
&lt;h3 id=&#34;deepseeks-sudden-surge-in-popularity-rapidly-becoming-a-focus&#34;&gt;&lt;strong&gt;DeepSeek&amp;rsquo;s Sudden Surge in Popularity: Rapidly Becoming a Focus&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;DeepSeek is an AI-powered tool focused on optimizing deep learning models, particularly in the application of natural language processing (NLP) and image generation. In the days leading up to Chinese New Year, this project suddenly attracted significant attention from investors and tech professionals alike. The performance of its team and the demonstrated technical results sparked strong interest among many. Discussions about DeepSeek dominated virtually every topic within the developer community and on social media platforms.&lt;/p&gt;
&lt;p&gt;However, DeepSeek’s sudden surge in popularity was not accidental. After analysis, many began to suspect that certain entities were involved behind it. Notably, after its rise to prominence, NVIDIA&amp;rsquo;s stock experienced a significant decline, suggesting that some factors were driving this change.&lt;/p&gt;
&lt;h3 id=&#34;nvidia-stock-plummets-the-hand-behind-the-short-selling-operation&#34;&gt;&lt;strong&gt;Nvidia Stock Plummets: The Hand Behind the Short Selling Operation&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;Nvidia, a global leader in graphics processing units (GPUs), has long been a key hardware provider for many large models and AI computations. With the rapid development of the AI market, Nvidia’s stock has performed strongly over the long term and even become a favorite among many investors. However, with the explosive popularity of DeepSeek and the market&amp;rsquo;s intense focus on its technology, Nvidia’s stock experienced a dramatic plunge.&lt;/p&gt;
&lt;p&gt;The reason behind this phenomenon may involve institutional investor short selling strategies. Over the past few years, as AI technology has spread, Nvidia’s stock price has been significantly inflated, leading many investors to believe that it was at risk of excessive speculation. Especially after a technology like DeepSeek went viral, some institutions may have profited by shorting Nvidia&amp;rsquo;s stock. By accurately grasping market timing and predicting the influence of DeepSeek, these institutions successfully made a profit.&lt;/p&gt;
&lt;h3 id=&#34;large-model-chain-of-thought-interaction-from-results-to-process&#34;&gt;&lt;strong&gt;Large Model Chain-of-Thought Interaction: From “Results” to “Process”&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;In traditional AI applications, many practitioners and investors have focused more on the &amp;ldquo;results&amp;rdquo; of AI models – such as generated images and text – direct outputs. However, in discussions related to DeepSeek, an increasing number of people are beginning to realize that the underlying chain-of-thought hidden behind large models is the core content worth paying more attention to. Previously, we could only see the results output by the model, but now we need to understand its logic, algorithms, and how to optimize the model&amp;rsquo;s performance by adjusting these factors.&lt;/p&gt;
&lt;p&gt;This shift in thinking is actually a deep reflection on AI research and application. From simple black-box operations to truly understanding the internal workings of models, many technical personnel and investors have begun to re-examine the future development direction of artificial intelligence. The popularity of DeepSeek perfectly demonstrates this breakthrough application of the chain-of-thought, it encourages people to focus not only on the final output results but also on the entire model construction and optimization process.&lt;/p&gt;
&lt;h3 id=&#34;summary&#34;&gt;&lt;strong&gt;Summary&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;The sudden surge in popularity of DeepSeek, NVIDIA’s stock plummet, and the subsequent short-selling activities by institutions – it all seems like a meticulously designed scheme. Through an in-depth understanding of large model chain-of-thought reasoning, we can see that the application of AI technology is not merely superficial layering of phenomena, but also deep digging and optimization of the model’s internal logic. As the technology progresses, we may see more innovative tools like DeepSeek in the future, driving AI research and applications to a higher level.&lt;/p&gt;
&lt;p&gt;This phenomenon not only shows us the immense potential of AI technology, but also prompts us to start thinking about the commercial games and capital operations behind the technology. The future market trend will continue to be the focus of the battle between technology and capital.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Cursor AI Programming IDE Trial</title>
        <link>https://ttf248.life/en/p/cursor-ai-programming-ide-trial/</link>
        <pubDate>Thu, 23 Jan 2025 19:30:13 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/cursor-ai-programming-ide-trial/</guid>
        <description>&lt;p&gt;It seems like another year has passed, and the biggest change at work is a significant increase in AI participation. Previously, switching between different development languages required developers to be familiar with various language-specific API interfaces. Now, these basic code snippets can all be generated by AI, which is a huge blessing for developers.&lt;/p&gt;
&lt;h2 id=&#34;chatgpt&#34;&gt;ChatGPT
&lt;/h2&gt;&lt;p&gt;As early as 2023, I’ve written two simple introductory articles about it. Now it&amp;rsquo;s been 25 years – how to put this… I haven’t felt a significant improvement. It still needs to develop its own cognition, be able to reasonably break down tasks, and, of course, most importantly, identify whether AI-generated code contains bugs.&lt;/p&gt;
&lt;h2 id=&#34;github-copilot&#34;&gt;Github Copilot
&lt;/h2&gt;&lt;p&gt;It was a long time ago, but I saw some information saying that Singapore deployed the server and it’s available for use in China. No longer need to maintain a VPN connection for extended periods. Of course, when logging in, you still need to connect to a VPN, but this VPN only needs to be used during login, and then you can turn it off.&lt;/p&gt;
&lt;p&gt;In daily use, Github Copilot is also heavily relied upon. This plugin can be directly used in VS Code and Visual Studio without switching between the two applications. Compared to ChatGPT, Github Copilot provides better support for projects, is more user-friendly in interaction, and allows you to feed partial local files to it – &lt;strong&gt;“training” the AI&lt;/strong&gt; – so that the generated code is more aligned with your project.&lt;/p&gt;
&lt;h2 id=&#34;cursor-ai&#34;&gt;Cursor AI
&lt;/h2&gt;&lt;p&gt;Recently I’ve seen a new AI programming IDE, Cursor AI. This IDE is based on Github Copilot, but this IDE is more intelligent and can help you create files directly.
I tried it briefly and found it to be pretty good, but its understanding of existing projects isn&amp;rsquo;t quite there yet. When working with large local project files, or for major refactoring, optimization, and adjustments, developers still need to &lt;strong&gt;break down tasks&lt;/strong&gt;.
Here’s an example: Switching to Cursor’s engineering mode, inputting the following content: “Create a personal resume webpage, supporting multiple different styles switching, and remember to fill in some personal information for data display.”
After several back-and-forths (&lt;strong&gt;pulling&lt;/strong&gt;), you can get the following webpage. Of course, this webpage is relatively simple, but it’s still pretty good for beginners.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Currently, registered users can enjoy a free trial of 150 advanced APIs, while paying users are limited to 5000 advanced APIs.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;cursor-ai-1&#34;&gt;Cursor AI
&lt;/h2&gt;&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://ttf248.life/cursor/index.html&#34; &gt;Resume&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>AI Programming and Task Decomposition</title>
        <link>https://ttf248.life/en/p/ai-programming-and-task-decomposition/</link>
        <pubDate>Fri, 22 Dec 2023 08:44:26 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/ai-programming-and-task-decomposition/</guid>
        <description>&lt;p&gt;Two years ago, I added a copy function to the site, which took me half a day of tinkering. Ultimately, the rendering effect wasn&amp;rsquo;t quite satisfactory, and as a somewhat mediocre frontend developer, I didn’t plan on improving it – if it worked, it was good enough. This year, I used &lt;code&gt;AI&lt;/code&gt; to develop a mini-program, which has also made me more familiar with frontend development. I refactored it (the &lt;code&gt;AI&lt;/code&gt; redesigned it).&lt;/p&gt;
&lt;p&gt;Realization: Breaking down tasks really helps; many small tasks are quite natural for the &lt;code&gt;AI&lt;/code&gt; to develop. For the closed mini-program, 75% of the code was provided by &lt;code&gt;AI&lt;/code&gt;, broken down into multiple tasks delivered, and manually assembled the project code.&lt;/p&gt;
&lt;h2 id=&#34;previous-context&#34;&gt;Previous Context
&lt;/h2&gt;&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://ttf248.github.io/post/2022/001/%E4%BB%A5%E4%B8%BA%E5%BE%88%E7%AE%80%E5%8D%95%E7%9A%84%E4%BA%8B%E6%83%85%E5%A2%9E%E5%8A%A0%E4%B8%80%E4%B8%AA%E4%BB%A3%E7%A0%81%E5%A4%8D%E5%88%B6%E6%8C%89%E9%92%AE/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Making simple things easier: Adding a code copy button&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;main-text&#34;&gt;Main Text
&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;promot&lt;/strong&gt;: Design CSS and JavaScript code to locate webpage elements with the class &amp;ldquo;highlight&amp;rdquo;, and within their container, add a copy button to enable copying of the content.&lt;/p&gt;
&lt;p&gt;The first draft of the code was immediately problematic; I generated a revised version that is largely functional, but the copied content is incorrect, containing redundant text. Further guidance led to optimization, specifying core color design and effects.&lt;/p&gt;
&lt;p&gt;Having learned your specific requirements for button size and color, we redesigned the button styles to accommodate these demands.  Here’s the adjusted CSS style sheet, incorporating your provided color scheme and optimizing the button size:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-css&#34;&gt;.copy-btn {
  position: absolute;
  top: 2px; /* Adjust position */
  right: 2px;
  padding: 3px 6px; /* Reduce size */
  font-size: 12px; /* Decrease font size */
  background-color: #8FD1E1; /* Button background color */
  color: #272822; /* Text color */
  border: 1px solid #272822; /* Border color */
  border-radius: 3px; /* Rounded corners */
  cursor: pointer;
  opacity: 0.7; /* Default semi-transparent */
  transition: opacity 0.3s ease, background-color 0.3s ease; /* Smooth transition effect */
}

.copy-btn:hover {
  opacity: 1; /* Unhide on hover */
  background-color: #A3D4E5; /* Background color on hover */
}

.highlight {
  position: relative;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this style sheet, I reduced the button&amp;rsquo;s size and font size to create a more refined look. The button’s default state is semi-transparent, becoming fully opaque when hovered over, with a slightly brighter background color for visual feedback. This design meets your color requirements while maintaining modern and user-friendly characteristics.&lt;/p&gt;
&lt;p&gt;Here’s a simpler &lt;code&gt;JavaScript&lt;/code&gt; implementation:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;document.addEventListener(&#39;DOMContentLoaded&#39;, function() {
  // Find all .highlight elements
  const highlights = document.querySelectorAll(&#39;.highlight&#39;);

  highlights.forEach(function(highlight) {
    // Create copy button
    const copyBtn = document.createElement(&#39;button&#39;);
    copyBtn.textContent = &#39;Copy&#39;;
    copyBtn.classList.add(&#39;copy-btn&#39;);

    // Add the button to each .highlight element
    highlight.appendChild(copyBtn);

    // Add click event listener
    copyBtn.addEventListener(&#39;click&#39;, function() {
      // Assume the first &amp;lt;code&amp;gt; element within .highlight contains the code to copy
      const code = highlight.querySelector(&#39;code&#39;) ? highlight.querySelector(&#39;code&#39;).textContent : &#39;&#39;;

      navigator.clipboard.writeText(code).then(() =&amp;gt; {
        // Display success message
        copyBtn.textContent = &#39;Copied!&#39;;
        setTimeout(() =&amp;gt; {
          copyBtn.textContent = &#39;Copy&#39;;
        }, 2000);
      }).catch(err =&amp;gt; {
        console.error(&#39;Copy failed: &#39;, err);
      });
    });
  });
});
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;epilogue&#34;&gt;Epilogue
&lt;/h2&gt;&lt;p&gt;While I had hoped to see a copy button added in the top-right corner of the code area, due to my lack of familiarity with frontend development, I was unable to accurately describe the functionality based on keywords and spent several rounds iterating with &lt;code&gt;AI&lt;/code&gt; without achieving the desired results.&lt;/p&gt;
&lt;p&gt;It remains to be seen what impact &lt;code&gt;AI&lt;/code&gt; will have on software development as time goes on.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Prompt Engineer</title>
        <link>https://ttf248.life/en/p/prompt-engineer/</link>
        <pubDate>Sun, 26 Mar 2023 20:46:53 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/prompt-engineer/</guid>
        <description>&lt;p&gt;Just as we needed to learn the techniques of searching engines back then, we also need to learn some techniques for communicating with &lt;code&gt;AI&lt;/code&gt;, providing reasonable and sufficient constraints, and efficiently obtaining the answers we need.&lt;/p&gt;
&lt;p&gt;If you look at it from a different angle, current &lt;code&gt;AI&lt;/code&gt; is like a very good student with excellent memory – it has the ability to memorize everything. What we need to do is learn how to communicate with &lt;code&gt;AI&lt;/code&gt; correctly, effectively, and precisely describe our needs to help &lt;code&gt;AI&lt;/code&gt; generate the expected results.&lt;/p&gt;
&lt;p&gt;##科普
The incredibly popular &lt;code&gt;AI&lt;/code&gt; – specifically &lt;code&gt;Generative Pre-Training&lt;/code&gt; – literally translates to generative pre-training. It’s a deep learning model that trains text generation based on internet-available data, used for tasks like question answering, text summarization generation, machine translation, classification, code generation, and conversational AI. Currently, there have been various versions of models such as GPT-1, GPT-2, GPT-3, and GPT-4, each larger and more powerful than the previous one.&lt;/p&gt;
&lt;h2 id=&#34;does-it-really-have-intelligence&#34;&gt;Does it Really Have Intelligence?
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;Similarity is high, accuracy increases accordingly.&lt;/li&gt;
&lt;li&gt;Basic, repetitive tasks, after specific training, no longer require human intervention.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Generative AI is a technology that utilizes existing data such as text, audio, and images to create new content. It can be used for various tasks including text generation, speech synthesis, image generation, and dialogue systems. The logical reasoning of Generative AI depends on its training data and model structure. Generally, Generative AI can follow grammar, logic, and common sense to a certain extent, but it may also produce errors, biases, or unrealistic content. Therefore, the output of Generative AI requires human judgment and verification; it should not be blindly trusted or used.&lt;/p&gt;
&lt;h2 id=&#34;prompt-engineer&#34;&gt;Prompt Engineer
&lt;/h2&gt;&lt;p&gt;Time flows only forward, and people need to learn to adapt to the current. You can think of &lt;code&gt;AI&lt;/code&gt; as not intelligent, lacking logic, and often producing unusable code.&lt;/p&gt;
&lt;p&gt;If you shift your perspective, the current &lt;code&gt;AI&lt;/code&gt; is like a child with excellent memory – it has the ability to memorize after reading something repeatedly, much like copying homework. What we need to do is learn how to communicate correctly, effectively, and with &lt;code&gt;AI&lt;/code&gt;, precisely describing our needs to help &lt;code&gt;AI&lt;/code&gt; generate the desired results.&lt;/p&gt;
&lt;h2 id=&#34;dialogue-patterns&#34;&gt;Dialogue Patterns
&lt;/h2&gt;&lt;p&gt;Two years ago, when GitHub Copilot was released, no one could have imagined that two years later, OpenAI would appear out of nowhere, allowing humans to recognize the capabilities of large language models.&lt;/p&gt;
&lt;p&gt;Based on &lt;strong&gt;comment-based programming&lt;/strong&gt; and &lt;strong&gt;conversational programming&lt;/strong&gt;, the interaction logic is completely different. The dialogue pattern is friendly for novice users, and it’s necessary to mention NewBing providing follow-up prompts after each question. Microsoft is trying to guide users to obtain more content from the AI knowledge base.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Example Scenario
&lt;strong&gt;Scene:&lt;/strong&gt; A long high-speed train journey, coding is a good choice. With limited traffic, processing a few images, obtaining image materials, downloading software to process images is a bit extravagant, and network conditions don’t allow it.
&lt;strong&gt;Requirement:&lt;/strong&gt; There are a batch of picture materials with transparent background images used as navigation bars for small programs. The selected state is colored, and the unselected state is black and white. The materials are all in color, and they need to be preprocessed.
&lt;strong&gt;Prompt:&lt;/strong&gt; Write a python code snippet to read all pictures from a folder, convert the pictures into black and white, maintain the transparent background, save the pictures, optimize the image quality, reduce file size, and display it in the console, comparing the sizes before and after optimization.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import necessary libraries
import os
from PIL import Image

# Define a function to convert images to black and white, maintain transparency, save images, and return file sizes
def convert_and_save(image_file):
    # Open the image
    image = Image.open(image_file)
    # Get the image mode, if it&#39;s RGBA mode, indicating a transparent background
    mode = image.mode
    if mode == &amp;quot;RGBA&amp;quot;:
        # Create a white background image with the same size as the original image
        background = Image.new(&amp;quot;RGB&amp;quot;, image.size, (255, 255, 255))
        # Paste the original image onto the white background, ignoring transparent pixels
        background.paste(image, mask=image.split()[3])
        # Convert the composite image to grayscale mode
        gray_image = background.convert(&amp;quot;L&amp;quot;)
        # Convert the grayscale image back to RGBA mode to preserve transparency
        final_image = gray_image.convert(&amp;quot;RGBA&amp;quot;)
    else:
        # If it&#39;s not RGBA mode, directly convert the image to grayscale mode
        final_image = image.convert(&amp;quot;L&amp;quot;)
    # Get the original image file name and extension
    file_name, file_ext = os.path.splitext(image_file)
    # Define the new image file name with _bw suffix for black and white
    new_file_name = file_name + &amp;quot;_bw&amp;quot; + file_ext
    # Save the new image and optimize quality, reduce file size
    final_image.save(new_file_name, optimize=True)
    # Get the original and new image file sizes and return
    old_size = os.path.getsize(image_file)
    new_size = os.path.getsize(new_file_name)
    return old_size, new_size

# Define a folder path to store all images
folder_path = &amp;quot;images&amp;quot;
# Iterate through all files in the folder
for file in os.listdir(folder_path):
    # Concatenate the full file path
    file_path = os.path.join(folder_path, file)
    # Check if the file is an image file based on extension (can add or modify extensions as needed)
    if file_path.endswith((&amp;quot;.png&amp;quot;, &amp;quot;.jpg&amp;quot;, &amp;quot;.jpeg&amp;quot;, &amp;quot;.gif&amp;quot;)):
        # Call the function to convert and save the image, and get the file size
        old_size, new_size = convert_and_save(file_path)
        # Display the comparison of file sizes before and after optimization in the console
        print(f&amp;quot;{file}: {old_size} bytes -&amp;gt; {new_size} bytes&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;AI&lt;/code&gt; quickly provided an answer, looked very beautiful, and worked well, experienced programmers would find that the code below was not very generic, and couldn’t be put into production:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Fixed parameters&lt;/li&gt;
&lt;li&gt;No log files, all information is printed to the console&lt;/li&gt;
&lt;li&gt;The task of image processing is suitable for a multi-threaded model
&lt;code&gt;NewBing&lt;/code&gt; appeared, based on the prompt, chose a question: &lt;strong&gt;What can be improved in this code?&lt;/strong&gt;
The answer given this time was far beyond expectations, and most programmers would think of optimization measures, &lt;code&gt;AI&lt;/code&gt; gave corresponding suggestions.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;This code may still have some improvements, such as:

You can use the argparse library to allow users to specify the folder path and extension through command-line parameters instead of hardcoding them in the code.
You can use the logging library to record the running status of the program instead of printing it directly to the console.
You can use try-except statements to catch potential exceptions, such as file opening failure, image conversion failure, etc., and handle them

## Let&#39;s illustrate with an example

```python
# Import necessary libraries
import argparse
import logging
import multiprocessing
import os

from PIL import Image


# Define a function to convert and save the image, maintaining transparent background and optimizing file size
def convert_and_save(image_file):
    # Open the image
    try:
        image = Image.open(image_file)
    except Exception as e:
        logging.error(f&amp;quot;Failed to open {image_file}: {e}&amp;quot;)
        return None, None
    # Get the image mode; if it&#39;s RGBA, it has a transparent background
    mode = image.mode
    if mode == &amp;quot;RGBA&amp;quot;:
        # Create a white background image with the same size
        background = Image.new(&amp;quot;RGB&amp;quot;, image.size, (255, 255, 255))
        # Paste the original image onto the white background, ignoring transparent pixels
        background.paste(image, mask=image.split()[3])
        # Convert the combined image to grayscale mode
        gray_image = background.convert(&amp;quot;L&amp;quot;)
        # Convert the grayscale image back to RGBA mode to preserve the transparent background
        final_image = gray_image.convert(&amp;quot;RGBA&amp;quot;)
    else:
        # If it&#39;s not RGBA, directly convert the image to grayscale mode
        final_image = image.convert(&amp;quot;L&amp;quot;)
    # Get the original image file name and extension
    file_name, file_ext = os.path.splitext(image_file)
    # Define the new image file name with _bw suffix for black and white
    new_file_name = file_name + &amp;quot;_bw&amp;quot; + file_ext
    # Save the new image, optimizing quality to reduce file size
    try:
        final_image.save(new_file_name, optimize=True)
    except Exception as e:
        logging.error(f&amp;quot;Failed to save {new_file_name}: {e}&amp;quot;)
        return None, None
    # Get the original image and new image file sizes, and return
    old_size = os.path.getsize(image_file)
    new_size = os.path.getsize(new_file_name)
    return file_name, old_size, new_size

# Define a function to parse command-line arguments and return the folder path and extension list
def parse_args():
    # Create an argument parser object
    parser = argparse.ArgumentParser(description=&amp;quot;Convert images to black and white and optimize quality.&amp;quot;)
    # Add a required positional argument, representing the folder path
    parser.add_argument(&amp;quot;folder_path&amp;quot;, help=&amp;quot;The path of the folder that contains the images.&amp;quot;)
    # Add an optional argument, representing the extension list, defaulting to png, jpg, jpeg, and gif
    parser.add_argument(&amp;quot;-e&amp;quot;, &amp;quot;--extensions&amp;quot;, nargs=&amp;quot;+&amp;quot;, default=[&amp;quot;.png&amp;quot;, &amp;quot;.jpg&amp;quot;, &amp;quot;.jpeg&amp;quot;, &amp;quot;.gif&amp;quot;], help=&amp;quot;The extensions of the image files.&amp;quot;)
    # Parse command-line arguments and return the result object
    args = parser.parse_args()
    return args.folder_path, args.extensions

# Define a function to print the comparison of file sizes before and after optimization
def print_result(result):
    # If the result is not empty, indicating successful conversion and saving
    if result:
        # Unpack the result into a tuple of filename and file size tuple
        if len(result) == 3:
            file, old_size, new_size = result
            # Display the comparison of file sizes before and after optimization in the console
            logging.info(f&amp;quot;{file}: {old_size} bytes -&amp;gt; {new_size} bytes&amp;quot;)
        else:
            # Log the result if it&#39;s not a tuple of 3 elements
            logging.info(f&amp;quot;{result}&amp;quot;)

# Configure the logger, outputting logs to the console and files, setting the log level to INFO
logging.basicConfig(level=logging.INFO,
                    format=&amp;quot;%(asctime)s %(levelname)s %(message)s&amp;quot;,
                    handlers=[logging.StreamHandler(),
                              logging.FileHandler(&amp;quot;log.txt&amp;quot;)])

# Call the function to get the folder path and extension list
folder_path, extensions = parse_args()

if __name__ == &amp;quot;__main__&amp;quot;:
    # Windows needs this function because Windows lacks the fork() function (not entirely accurate).
    # Therefore, on Windows, forking is simulated by creating a new process, and the code - Additionally, this new process is instructed to run the code passed through a pipe by passing the `--multiprocessing-fork` command-line argument to it.
- If you examine the implementation of the `freeze_support()` function, its task is to check which process it&#39;s running in and whether it should execute the code passed through a pipe.
- `multiprocessing.freeze_support()`

- A process pool is created, automatically allocating processes based on the number of cores on the computer.
- An empty list, `results`, is created to store the result objects of asynchronous tasks.
- The script iterates through all files in the folder:
- The full file path is constructed using `os.path.join()`.
- It checks if the file ends with any of the extensions specified in the `extensions` list (you can modify this list as needed).
- If it&#39;s an image file, the `convert_and_save` function is called to convert and save the image asynchronously, without blocking the main process. The file size is also obtained.  The result object is then added to the `results` list using `pool.apply_async()` with the callback function `print_result`.
- The process pool is closed to stop accepting new tasks.
- `pool.join()` is called to wait for all tasks in the pool to complete.

## Epilogue
Due to local development being on a `windows` system, the first answer given by `AI` did not include the `main` function and also lacked `multiprocessing.freeze_support`. The code was fixed after following up and encountering an error. 
Just as learning the techniques of search engines required skill, we also need to learn how to communicate with `AI`, providing reasonable and sufficient constraints to efficiently obtain the desired answers.
Note: **If you are a programming beginner, if you still don&#39;t understand certain parts of the code based on the given comments, please continue to ask related questions.**&lt;/code&gt;&lt;/pre&gt;
</description>
        </item>
        <item>
        <title>AI-assisted programming, the evolution of productivity</title>
        <link>https://ttf248.life/en/p/ai-assisted-programming-productivity-evolution/</link>
        <pubDate>Tue, 28 Feb 2023 17:05:17 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/ai-assisted-programming-productivity-evolution/</guid>
        <description>&lt;p&gt;“GitHub Copilot” was released less than two years ago, and “ChatGPT” has appeared. I don’t fully understand the underlying principles; I&amp;rsquo;ve been using them for a while. The two tools’ levels of assistance are completely different, but they have both achieved a significant increase in productivity.&lt;/p&gt;
&lt;p&gt;For things that are too complex, AI still can’t do it, because they lack logic, or they rely on patterns or fixed frameworks – the training data is sufficient, and AI’s effect is nine out of ten.&lt;/p&gt;
&lt;h2 id=&#34;github-copilot&#34;&gt;GitHub Copilot
&lt;/h2&gt;&lt;p&gt;When publishing, I looked at the official introduction’s “demo” and found it wasn&amp;rsquo;t very smart. After trying it out, it wasn’t great, so I gave up on it.&lt;/p&gt;
&lt;p&gt;Before 2022, when I had some free time, I switched to the new version to try it out, and the effect was already quite good. The only issue was that the speed in China was a bit slow, likely due to network reasons. For languages like &lt;code&gt;Python&lt;/code&gt;, the code generated based on comments was pretty good, and much faster than researching and writing code myself.&lt;/p&gt;
&lt;h2 id=&#34;chatgpt&#34;&gt;ChatGPT
&lt;/h2&gt;&lt;p&gt;&lt;code&gt;Github Copilot&lt;/code&gt; still focuses on developers, i.e., from a development perspective when designing the product, requiring you to understand how to program and guide AI assistance through comments.
&lt;code&gt;ChatGPT&lt;/code&gt; has moved to the next level, &lt;strong&gt;allowing non-experts to quickly get started&lt;/strong&gt;.  A conversational chat interface where you can pose questions, describe requirements, and ask for clarification if needed.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s take an example: You know about tokenization but don’t know how to code, wanting to calculate the similarity of strings:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;Python 分词计算 維天運通 LOGORY 和 合肥維天運通信息科技股份有限公司 两个字符串的相似度；
比较前，统一处理字符串，英文转换为小写；
第二个字符中如果能找到第一个字符的首字符，将第二个字符串截位，截位的长度为第一个字符串的长度
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Hello, this is Bing. 😊&lt;/p&gt;
&lt;p&gt;According to online information 123, Python tokenization can be implemented using the &lt;code&gt;jieba&lt;/code&gt; library. The &lt;code&gt;jieba&lt;/code&gt; library supports three segmentation modes: precise mode, full mode, and search engine mode. You can choose an appropriate mode based on your needs.&lt;/p&gt;
&lt;p&gt;To compare two strings, you first need to tokenize them and then calculate their word vectors or edit distance metrics. You can use libraries like NumPy or SciPy for mathematical operations.&lt;/p&gt;
&lt;p&gt;To standardize string processing, converting English to lowercase, you can use Python&amp;rsquo;s built-in &lt;code&gt;lower()&lt;/code&gt; function.&lt;/p&gt;
&lt;p&gt;To slice the second string, you can use Python’s slicing operator [].&lt;/p&gt;
&lt;p&gt;Here is a possible code example:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import jieba
import numpy as np
from scipy.spatial.distance import cosine

# Define two strings
str1 = &amp;quot;維天運通 LOGORY&amp;quot;
str2 = &amp;quot;合肥維天運通信息科技股份有限公司&amp;quot;

# Standardize the string, convert English to lowercase
str1 = str1.lower()
str2 = str2.lower()

# Find the position of the first character of the first string in the second string
index = str2.find(str1[0])

# If found, slice the second string
if index != -1:
    str2 = str2[index:index+len(str1)]

# Tokenize the two strings using the jieba library
words1 = list(jieba.cut(str1))
words2 = list(jieba.cut(str2))

# Calculate the word vectors of the two strings (using frequency as weights)
vector1 = np.zeros(len(words1))
vector2 = np.zeros(len(words1))

for i, word in enumerate(words1):
    vector1[i] += 1 # Count the number of times each word appears in the first string
    if word in words2:
        vector2[i] += 1 # Count the number of times each word appears in the second string

# Calculate the similarity between the two strings (using cosine distance as an index)
similarity = 1 - cosine(vector1, vector2)

print(&amp;quot;The similarity between the two strings is:&amp;quot;, similarity)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Furthermore, consult the results of running and congratulations, you have discovered a problem. The result is fabricated, it does not actually run the code, but will give you a good result.&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&#34;example&#34;&gt;Example
&lt;/h3&gt;&lt;p&gt;Question: SQL statement to retrieve records from the product code table where the product code is within the range 80000-89999.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;Hello, this is Bing. 😊

According to search result 12, you can use the SQL BETWEEN operator to select values within a given range. The BETWEEN operator includes both the start and end values.

You need to first specify the table name and column name that you want to query, and then use the BETWEEN operator and the range value in the WHERE clause.

For example, if you want to retrieve records from the product code table where the product code is between 80000 and 89999, you can use the following statement:

SELECT * FROM product_code
WHERE code BETWEEN 80000 AND 89999;
Here, &#39;*&#39; represents selecting all columns [3]. If you only want to select specific columns, such as name and price, you can separate them with commas, as shown below:

SELECT name, price FROM product_code
WHERE code BETWEEN 80000 AND 89999;

I hope this helps.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Continue questioning, negating it: The product code is a string, your code is wrong&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;Sorry, I didn&#39;t notice that the product code is a string. If that’s the case, you need to add single quotes around both ends of the range values, as follows:

SELECT * FROM product_code
WHERE code BETWEEN &#39;80000&#39; AND &#39;89999&#39;;

Please forgive my mistake. 🙏

Congratulations, it will recognize mistakes.
&lt;/code&gt;&lt;/pre&gt;
</description>
        </item>
        
    </channel>
</rss>
