<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>AI on Uncle Xiang&#39;s Notebook</title>
        <link>https://ttf248.life/en/tags/ai/</link>
        <description>Recent content in AI on Uncle Xiang&#39;s Notebook</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en</language>
        <lastBuildDate>Mon, 02 Jun 2025 07:41:32 +0800</lastBuildDate><atom:link href="https://ttf248.life/en/tags/ai/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>Old problems, the flamboyant beauty of blossoming flowers (a reference to a famous Chinese poem).</title>
        <link>https://ttf248.life/en/p/old-ailment-stunning-flowers/</link>
        <pubDate>Mon, 26 May 2025 23:54:12 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/old-ailment-stunning-flowers/</guid>
        <description>&lt;p&gt;For many years, I‚Äôve focused on backend development, and recently started to explore &lt;code&gt;AI&lt;/code&gt; programming while dipping my toes into some frontend-related content. However, during this period of tinkering, I gradually realized I was falling back into an old habit ‚Äì being dazzled by shiny new things. I constantly try to use &lt;code&gt;AI&lt;/code&gt; to create a frontend interface, but in reality, these attempts haven‚Äôt provided much practical benefit for my current work and are actually diverting my attention.&lt;/p&gt;
&lt;h2 id=&#34;ai-use-cases&#34;&gt;AI Use Cases
&lt;/h2&gt;&lt;p&gt;In small projects, AI tools can truly shine, particularly when writing independent functions with low coupling to the system and simple business logic. These tasks typically have clear inputs and outputs, and rely less on context ‚Äì making them well-suited for the current capabilities of AI-assisted programming.&lt;/p&gt;
&lt;p&gt;However, when facing complex system architectures or deep business logic, the limitations of AI become increasingly apparent. It may generate code that appears reasonable but is actually detached from the project‚Äôs real needs, or even introduce potential issues that are difficult to debug. In these scenarios, AI is best suited as an assistive tool rather than a fully autonomous code generator. We need to conduct rigorous review and testing of any generated code to ensure it meets actual requirements.&lt;/p&gt;
&lt;h2 id=&#34;errors-and-the-cost-of-learning&#34;&gt;Errors and the Cost of Learning
&lt;/h2&gt;&lt;p&gt;While attempting to generate frontend code using AI, I encountered numerous challenges. As frontend development isn&amp;rsquo;t my area of expertise, troubleshooting often proved time-consuming and frustrating. Even after adjusting prompts to have the AI rewrite the code, it was difficult to avoid the appearance of some low-level errors. This iterative process not only wasted time but also highlighted that my current focus should be on backend business logic rather than exploring unfamiliar domains.&lt;/p&gt;
&lt;p&gt;Looking back at the project completed over the weekend, I‚Äôm more confident that focusing on backend development and user interaction logic, implementing functionality through a console, is the most efficient approach currently. Perhaps systematically learning frontend knowledge would be a better strategy when I have more time and energy.&lt;/p&gt;
&lt;h2 id=&#34;frontend-learning-plan&#34;&gt;Frontend Learning Plan
&lt;/h2&gt;&lt;p&gt;The frontend technology stack is complex and diverse, so it‚Äôs unrealistic to quickly master it. I plan to first choose a framework, such as Vue.js or React.js, and deeply learn its core concepts and usage methods. Only after becoming familiar with the fundamentals will I attempt to use AI to generate frontend code, which can effectively avoid errors and wasted time caused by unfamiliarity.&lt;/p&gt;
&lt;p&gt;In short, the focus for this stage should be on backend development, steadily building up my core skills. Once the timing is right, I‚Äôll explore the combination of frontend and AI ‚Äì potentially yielding greater rewards.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Claude4 released, attempting to develop: Hugo tags, hyperlink translation assistant</title>
        <link>https://ttf248.life/en/p/claude-4-release-and-experimentation-hugo-tags-hyperlink-translation-assistant/</link>
        <pubDate>Sat, 24 May 2025 03:05:31 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/claude-4-release-and-experimentation-hugo-tags-hyperlink-translation-assistant/</guid>
        <description>&lt;p&gt;This site is developed using Hugo, but I‚Äôve always used Chinese titles, which results in less friendly generated article links. In simpler terms, when shared, they don&amp;rsquo;t look as good because the Chinese characters are escaped into formats like %E4%BD%A0%E5%A5%BD within the links. While you can solve this by setting a slug, it‚Äôs tedious to do manually every time.&lt;/p&gt;
&lt;p&gt;Therefore, I decided to try using Claude4 to develop a translation assistant that automatically converts Chinese titles to English slugs and adds hyperlinks within the articles. This would eliminate the need for manual setup.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Claude4 is amazing ‚Äì its contextual understanding has significantly improved, as has its efficiency in handling complex tasks.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;!-- more --&gt;
&lt;h2 id=&#34;project-address&#34;&gt;Project Address
&lt;/h2&gt;&lt;p&gt;Domestic Project Address: &lt;a class=&#34;link&#34; href=&#34;https://cnb.cool/ttf248/hugo-content-suite&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://cnb.cool/ttf248/hugo-content-suite&lt;/a&gt;
International Project Address: &lt;a class=&#34;link&#34; href=&#34;https://github.com/ttf248/hugo-content-suite&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/ttf248/hugo-content-suite&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;code-implementation&#34;&gt;Code Implementation
&lt;/h2&gt;&lt;p&gt;Let&amp;rsquo;s first discuss the implementation approach: We need to scan all articles, extract tag information and article titles, and then call the local large model (such as gemma-3-12b-it) for translation.&lt;/p&gt;
&lt;p&gt;In actual development, Claude4 showcased several significant advantages compared to previous generation large models. Due to the diverse functional requirements, Claude4 automatically designed an interactive menu, comprehensively considering various usage scenarios. For example, in tag processing, Claude4 not only supports tag statistics and analysis but also includes classification statistics, and can even detect &lt;strong&gt;unlabeled articles&lt;/strong&gt;. Furthermore, it provides &lt;strong&gt;preview&lt;/strong&gt; and tag page generation functionalities.&lt;/p&gt;
&lt;p&gt;Whether it‚Äôs integrating with the local large model, adding translation caches, or performing large-scale code refactoring, Claude4 completes everything in one go, with almost no issues. Despite the project‚Äôs small scale, it incorporates many minor features. In previous development cycles with large models, there was often a tendency to forget earlier content; however, Claude 4 performed exceptionally well, &lt;strong&gt;virtually eliminating context loss&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;In short, its intelligence has increased, and we plan to switch to Claude 4 for more development work as our primary coding model.&lt;/p&gt;
&lt;h2 id=&#34;translation-cache&#34;&gt;Translation Cache
&lt;/h2&gt;&lt;p&gt;This approach, besides reducing the number of calls to large models, is quite efficient when running a 12b model locally ‚Äì it doesn‚Äôt waste much time. However, if you need to call the large model every time, it will still be somewhat slow. Secondly, to fix the connections within articles, if a full update operation is executed and the article title is very long, there&amp;rsquo;s occasionally a situation where the two translated results differ, causing the link to change ‚Äì which is quite awkward.&lt;/p&gt;
&lt;h2 id=&#34;feature-optimization&#34;&gt;Feature Optimization
&lt;/h2&gt;&lt;p&gt;The entire project was handed over to &lt;code&gt;Claude4&lt;/code&gt; to analyze the space for optimization and generate the following recommendations:&lt;/p&gt;
&lt;p&gt;Reviewed the code, which had no issues whatsoever. For example, the configuration files were reviewed, and the original code‚Äôs configurations were converted into default configurations. When reading the configuration file, if a corresponding configuration file did not exist, a default configuration file would be automatically generated to avoid user operational errors. Here&amp;rsquo;s the English translation of the provided text:&lt;/p&gt;
&lt;p&gt;‚ÄúRequirements: When translating, dynamically calculate the current translation efficiency, estimate remaining time, and output relevant information to the console. Currently, we‚Äôre obtaining the character count of the article, the number of characters translated per line, the time taken, and fitting a calculation for the translation time of every 100 characters. Simultaneously, we&amp;rsquo;re calculating the estimated remaining translation time.&lt;/p&gt;
&lt;p&gt;The code is complete, but the effect isn‚Äôt satisfactory, so I ask AI to provide a new design solution:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Provide multiple efficiency calculation methods: real-time efficiency, average efficiency, and sliding window efficiency&lt;/li&gt;
&lt;li&gt;Improve display format: progress bar, segmented statistics, dynamic refresh&lt;/li&gt;
&lt;li&gt;Add more useful metrics: API call counts, success rates.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;After completing the code, we discovered a new surprise ‚Äì translation efficiency statistics were flooding the console in real time, but without endless scrolling.‚Äù Here&amp;rsquo;s the English translation of the provided text:&lt;/p&gt;
&lt;p&gt;‚ÄúRequirements: When translating, dynamically calculate the current translation efficiency, estimate remaining time, and output relevant information to the console. Currently, we‚Äôre obtaining the character count of the article, the number of characters translated per line, the time taken, and fitting a calculation for the translation time of every 100 characters. Simultaneously, we&amp;rsquo;re calculating the estimated remaining translation time.&lt;/p&gt;
&lt;p&gt;The code is complete, but the results aren‚Äôt satisfactory, so I‚Äôm asking AI to provide a new design solution:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Provide multiple efficiency calculation methods: real-time efficiency, average efficiency, and sliding window efficiency&lt;/li&gt;
&lt;li&gt;Improve display format: progress bars, segmented statistics, and dynamic refresh&lt;/li&gt;
&lt;li&gt;Add more useful metrics: API call counts, success rates, etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;After completing the code, we discovered a new surprise ‚Äì translation efficiency statistics were flooding the console in real time, but without requiring endless scrolling.‚Äù&lt;/p&gt;
&lt;h3 id=&#34;performance-statistics-menu&#34;&gt;Performance Statistics Menu
&lt;/h3&gt;&lt;p&gt;The newly added &lt;strong&gt;Performance Statistics Menu&lt;/strong&gt;, which I myself designed, isn&amp;rsquo;t as well-designed as this one.&lt;/p&gt;
&lt;p&gt;üìä Performance Statistics:
üîÑ Translation Count: 360
‚ö° Cache Hit Rate: 1.4% (5/365)
‚è±Ô∏è Average Translation Time: 315.927234ms
üìÅ File Operations: 73
‚ùå Error Count: 0&lt;/p&gt;
&lt;h3 id=&#34;progress-bar-display&#34;&gt;Progress Bar Display
&lt;/h3&gt;&lt;p&gt;The newly added &lt;strong&gt;Progress Bar Display&lt;/strong&gt;, detailed progress, elapsed time, and estimated remaining time.
Please select function (0-13): 10
üîç Collecting translation target&amp;hellip;
üìÑ Cached file loaded, containing 0 translation records
üìä Translation cache statistics:
üè∑Ô∏è Total tags: 229
üìù Total articles: 131
‚úÖ Cached: 0
üîÑ To be translated: 360&lt;/p&gt;
&lt;h3 id=&#34;progress-bar-display-1&#34;&gt;Progress Bar Display
&lt;/h3&gt;&lt;p&gt;Confirm full translation cache generation? (y/n): y
üöÄ Generating full translation cache&amp;hellip;
üìÑ Loaded cache file, containing 0 translation records
üîç Checking translations in cache&amp;hellip;
üîÑ Need to translate 360 new tags
[‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 5/360 (1.4%) - Time taken: 3s - Estimated remaining: 3m8s
üíæ Saved cache file, containing 5 translation records
[‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 10/360 (2. 8%) - Time taken: 6s - Estimated remaining: 3m28s üíæ Cache file saved, containing 10 translation records
[‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 15/360 (4.2%) - Time taken: 9s - Estimated remaining: 3m30s üíæ Cache file saved, containing 15 translation records&lt;/p&gt;
&lt;h3 id=&#34;progress-bar-display-2&#34;&gt;Progress Bar Display
&lt;/h3&gt;&lt;p&gt;[‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 20/360 (5.6%) - Time Elapsed: 13s - Estimated Remaining: 3m36süíæ Cache file saved, containing 20 translation records
[‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë] 25/360 (6.9%) - Time Elapsed: 16s - Estimated Remaining: 3m33süíæ Cache file saved, containing 25 translation records
[‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 30/360 (8. (3%) - Time taken: 19s - Estimated remaining: 3m30s [‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 35/360 (9.7%) - Time taken: 22s - Estimated remaining: 3m25s [‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë]&lt;/p&gt;
&lt;h3 id=&#34;progress-bar&#34;&gt;Progress Bar
&lt;/h3&gt;&lt;hr&gt;
</description>
        </item>
        <item>
        <title>AI overuse can lead to some lingering effects.</title>
        <link>https://ttf248.life/en/p/ai-overuse-side-effects/</link>
        <pubDate>Wed, 14 May 2025 19:39:50 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/ai-overuse-side-effects/</guid>
        <description>&lt;p&gt;Since we‚Äôve established ‚ÄúAI Inspiration Collision Forum,‚Äù there‚Äôs been a lot of noise ‚Äì many things, both good and bad ‚Äì being tried out using AI for recording and publishing, and less time spent on quiet self-reflection. It feels like this is becoming increasingly rare. Moving forward, it would be beneficial to slightly control the output of this section and consolidate it into a monthly magazine format, releasing one article per month.&lt;/p&gt;
&lt;p&gt;This is almost like a post-traumatic effect ‚Äì or a side effect ‚Äì where efficiency has increased, but the depth and breadth of thought have declined.&lt;/p&gt;
&lt;h2 id=&#34;efficiency-boost-undeniable&#34;&gt;Efficiency Boost: Undeniable
&lt;/h2&gt;&lt;p&gt;The ‚ÄúSeven-Second Fish Sightseeing‚Äù column used to be poorly maintained, with only a few hot events covered. Due to laziness, I hadn‚Äôt searched the internet for relevant materials or compiled notes. Now that various AI tools are available, all it takes is outlining the key points, and AI can automatically search for related events, generate articles as needed, simply format them, and publish them.&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s like a blessing for lazy people ‚Äì efficiency has increased significantly, almost to the point of doubling effort.&lt;/p&gt;
&lt;p&gt;Beyond writing articles, efficiency gains are real when coding. Previously, writing code often required detailed reading of API interface documentation. Now, I can skip this entirely. This is incredibly valuable because familiarizing oneself with APIs is ‚Äúphysical labor,‚Äù not ‚Äúmental labor.‚Äù AI handles this part perfectly.&lt;/p&gt;
&lt;h2 id=&#34;spam-content&#34;&gt;Spam Content
&lt;/h2&gt;&lt;p&gt;Many articles have poor quality content, not to say that there‚Äôs nothing there; it just doesn&amp;rsquo;t read well, lacking a sense of reality and what people actually experience. This is a style I previously disliked ‚Äì like chewing wax.&lt;/p&gt;
&lt;p&gt;From another perspective, AI-generated content really does feel like products of aÊµÅÊ∞¥Á∫ø (li√∫ shu«ê li√†n - assembly line) production, lacking soul.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Spam of the New Internet&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;forgetting&#34;&gt;Forgetting
&lt;/h2&gt;&lt;p&gt;This type of document is entirely AI-generated, and the reader‚Äôs context is unclear. However, over time, my own impressions will become blurred, or even forgotten.&lt;/p&gt;
&lt;p&gt;Similar issues occur when writing code ‚Äì without reviewing commit records, I can&amp;rsquo;t remember how I originally thought about it, or why I wrote it that way. This is particularly evident with code generated through repeated communication with AI, where the final code differs significantly from the initial ideas, sometimes drastically so.&lt;/p&gt;
&lt;h2 id=&#34;search&#34;&gt;Search
&lt;/h2&gt;&lt;p&gt;Recently, the number of times I‚Äôve opened Google and Baidu has noticeably decreased. Many questions are now being answered by AI for searching and interaction, and the results are much better than traditional search engines.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s mourn the &lt;code&gt;bing ai&lt;/code&gt;, which may no longer be active, a pioneering AI tool from a major company that could connect to the internet and search.&lt;/p&gt;
&lt;p&gt;I‚Äôm using Google less, and I‚Äôve reduced my visits to &lt;code&gt;stackoverflow&lt;/code&gt; as well ‚Äì many questions are now simply asked of AI, and this website is gradually being phased out by the times.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion
&lt;/h2&gt;&lt;p&gt;My maintained blog, originally with very little traffic, is now even less expected of; it‚Äôs primarily a place for self-reflection and recording.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>No coding, design and develop a self-selected stock module.</title>
        <link>https://ttf248.life/en/p/no-code-design-develop-custom-stock-module/</link>
        <pubDate>Thu, 27 Feb 2025 23:20:39 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/no-code-design-develop-custom-stock-module/</guid>
        <description>&lt;ul&gt;
&lt;li&gt;Last month, we experimented with cursor, but due to the limitations of the free quota, we didn&amp;rsquo;t develop overly complex features; we just did some basic testing. We discovered then that Byte also released similar products, both using the same large models ‚Äì Claude-3.5 ‚Äì at their core.
Byte‚Äôs product is called Trae, initially launched in the Mac version and finally released its Windows version in February of this year. Big companies are good because you can freely ‚ÄúwhiteÂ´ñ‚Äù (literally translates to &amp;ldquo;free grab&amp;rdquo;) without paying; you get unlimited use of Claude-3.5. The performance of this model is quite impressive.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Ultimately, we got stuck on the development of candlestick charts. As I don‚Äôt understand React at all, I had to give up. To continue developing, I would need to supplement my knowledge of front-end basics, breaking down the task into smaller, more manageable pieces instead of directly giving me a large task: developing candlestick charts.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;issues-found&#34;&gt;Issues Found
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;Due to the limited training data caused by using foreign AI models and Vue3 + Element-Plus, React was chosen as the frontend framework.&lt;/li&gt;
&lt;li&gt;There may be occasional syntax errors requiring manual fixes.&lt;/li&gt;
&lt;li&gt;Solutions for some complex problems require manual guidance.&lt;/li&gt;
&lt;li&gt;Code structure optimization requires manual instruction.&lt;/li&gt;
&lt;li&gt;The most time-consuming part was packaging the frontend code into a container, due to my zero experience with &lt;code&gt;.env.production&lt;/code&gt; and &lt;code&gt;tsconfig.json&lt;/code&gt;, I had no concept of these either; I sought help from community members along the way to sort out the corresponding logic. There are significant differences between the development and build modes in frontend development, and the checks performed on the code. Backend database and service container scripts were completed in a total of five minutes.
&lt;strong&gt;Currently, AI primarily improves development efficiency. Having a foundation is best; it won‚Äôt solve all problems for you.&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;repository-address&#34;&gt;Repository Address
&lt;/h2&gt;&lt;p&gt;As the title indicates, this time we&amp;rsquo;re going to chat with AI without actually coding and see what we can achieve.  Let‚Äôs take a look at the final outcome.&lt;/p&gt;
&lt;p&gt;Repository Address: &lt;a class=&#34;link&#34; href=&#34;https://github.com/ttf248/trae-demo&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/ttf248/trae-demo&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;For detailed usage instructions, please refer to the README.md file in the repository.&lt;/p&gt;
&lt;p&gt;The repository contains numerous submission records, most of which are dialogues between me and Trae, as well as my testing of Trae‚Äôs functionalities, with notes on whether manual intervention was required to implement each feature.&lt;/p&gt;
&lt;h2 id=&#34;prompt&#34;&gt;Prompt
&lt;/h2&gt;&lt;p&gt;Project is created from scratch, below is the project prompt:&lt;/p&gt;
&lt;h2 id=&#34;ui-and-interaction-optimization&#34;&gt;UI and Interaction Optimization
&lt;/h2&gt;&lt;p&gt;The design of the front-end interface relies entirely on Grok. We initially created a prototype within Trae, but it lacked aesthetics. Because the model used has strong coding capabilities but weaker other abilities, we need to use Grok to optimize the front-end UI.&lt;/p&gt;
&lt;p&gt;By taking screenshots of the current interface and uploading them to Grok, we can obtain numerous optimization suggestions at once. We then manually evaluate these suggestions and copy them into Trae to execute and observe the results of the optimizations.&lt;/p&gt;
&lt;h3 id=&#34;technology-stack&#34;&gt;Technology Stack
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Frontend: React + TypeScript&lt;/li&gt;
&lt;li&gt;Backend: Golang + Gin + GORM&lt;/li&gt;
&lt;li&gt;Database: PostgreSQL 17&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;system-architecture&#34;&gt;System Architecture
&lt;/h2&gt;&lt;h2 id=&#34;backend-architecture&#34;&gt;Backend Architecture
&lt;/h2&gt;&lt;p&gt;The backend utilizes the Gin framework (Go) to implement RESTful APIs, with the following key modules:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Database Module&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Utilizes GORM as an ORM framework&lt;/li&gt;
&lt;li&gt;Supports database connection configuration via environment variables&lt;/li&gt;
&lt;li&gt;Automatically performs database schema migrations&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Routing Module&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;RESTful API design&lt;/li&gt;
&lt;li&gt;A unified error handling mechanism&lt;/li&gt;
&lt;li&gt;Built-in request logging&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Cross-Origin Handling&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Supports cross-origin requests from local development environments&lt;/li&gt;
&lt;li&gt;Configurable CORS policies&lt;/li&gt;
&lt;li&gt;Supports cookie-based cross-origin access&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;frontend-architecture&#34;&gt;Frontend Architecture
&lt;/h2&gt;&lt;p&gt;The frontend was built using React + TypeScript, implementing the following features:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Stock list display&lt;/li&gt;
&lt;li&gt;Watchlist management&lt;/li&gt;
&lt;li&gt;Real-time quote data display&lt;/li&gt;
&lt;li&gt;Error handling mechanism&lt;/li&gt;
&lt;/ul&gt;</description>
        </item>
        <item>
        <title>AI Development Over Two Years: Somewhat similar to the state before a Docker release ‚Äì chaotic and rapidly evolving.</title>
        <link>https://ttf248.life/en/p/ai-development-two-years-docker-pre-release/</link>
        <pubDate>Thu, 20 Feb 2025 18:16:37 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/ai-development-two-years-docker-pre-release/</guid>
        <description>&lt;p&gt;Artificial Intelligence (AI) has undoubtedly been one of the most hotly debated topics in the technology field in recent years, particularly over the past two years, with AI technology experiencing rapid advancements. Whether it‚Äôs deep learning, natural language processing, or computer vision and automated decision systems, the application scenarios for AI are constantly emerging. However, despite continuous technological breakthroughs, AI still faces a bottleneck similar to that of a Docker release ‚Äì a lack of a killer app to truly ignite the market.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Prompt: Write an article: AI has developed for two years, which is somewhat similar to the state of a Docker release before publishing, lacking a killer application, and using existing technologies to create a perfect landing scenario. Docker did not use too many new technologies, but the overall solution is reasonable and changes the workflow of operations and development.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;ai-development-status-the-technology-is-mature-but-applications-still-need-breakthroughs&#34;&gt;AI Development Status: The Technology is Mature, but Applications Still Need Breakthroughs
&lt;/h2&gt;&lt;p&gt;From a technical perspective, AI has made significant progress in the past two years. Whether it&amp;rsquo;s OpenAI‚Äôs GPT series models or Google‚Äôs BERT and DeepMind‚Äôs Alpha series, AI processing capabilities have far exceeded previous expectations. Particularly in the field of natural language processing, models like GPT-4 not only possess powerful generative abilities but also demonstrate astonishing performance in understanding and reasoning.&lt;/p&gt;
&lt;p&gt;However, despite technological advancements, AI&amp;rsquo;s implementation in practical applications faces certain challenges. Similar to Docker‚Äôs state before its release, although AI has enormous potential, there hasn‚Äôt yet been a truly widespread and industry-changing killer app. People are talking about the future of AI, but they may not be able to find a directly revolutionary application scenario.&lt;/p&gt;
&lt;h2 id=&#34;docker-and-ai-parallels-technology-doesnt-have-to-be-innovation-its-about-the-solution&#34;&gt;Docker and AI Parallels: Technology Doesn&amp;rsquo;t Have to Be Innovation, It‚Äôs About the Solution
&lt;/h2&gt;&lt;p&gt;Looking back at the history leading up to Docker‚Äôs release, we can readily see many similarities between the technological environment at that time and the current state of AI development. Before Docker, container technology wasn‚Äôt a new concept; early technologies like LXC (Linux Containers) and virtualization already possessed the basic capabilities for containerization. However, Docker didn&amp;rsquo;t introduce disruptive technology itself. Instead, it presented a simpler, more intuitive, and efficient solution through the clever integration and optimization of existing technologies. This solution didn‚Äôt bring about revolutionary technology, but it addressed many pain points in operations and development processes, significantly simplifying software deployment, scaling, and management workflows.&lt;/p&gt;
&lt;p&gt;Similarly, the AI field is facing a similar situation. Currently, AI technology is no longer a ‚Äúnew thing,‚Äù but to truly achieve large-scale applications, it still requires a perfect landing scenario. Like Docker, integrating and optimizing existing technologies to form a reasonable application solution. A killer AI application may not rely on groundbreaking technological breakthroughs, but rather how to integrate existing technologies to solve pain points and needs in actual business operations.&lt;/p&gt;
&lt;h2 id=&#34;how-to-find-ais-docker-moment&#34;&gt;How to Find AI&amp;rsquo;s &amp;ldquo;Docker Moment&amp;rdquo;?
&lt;/h2&gt;&lt;p&gt;To truly enable the widespread adoption of AI technology, we need to focus on several key areas:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Deep Exploration of Real-World Scenarios&lt;/strong&gt;
Currently, many AI applications are still primarily experimental in nature and lack large-scale real-world implementation. Areas such as AI customer service and intelligent recommendations are widely used, but their functionality is still limited and has not yet broken through industry bottlenecks. True breakthroughs may come from industries that have been plagued by traditional methods for a long time, such as healthcare, manufacturing, and logistics. AI can help these companies improve efficiency and reduce costs in complex scenarios through more efficient data processing and predictive analytics.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Productization and Ease of Use&lt;/strong&gt;
Similar to how Docker simplifies containerization processes to increase operational efficiency, the ease of use of AI products is equally important. AI‚Äôs proliferation isn&amp;rsquo;t just about the spread of technology; it‚Äôs also about the proliferation of its products. Integrating AI into daily workflows, allowing users to easily utilize these tools without needing to deeply understand the underlying technology, is a crucial step in AI adoption.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Ecosystem Building &amp;amp; Standardization&lt;/strong&gt;
Any new technology‚Äôs widespread application relies on ecosystem building. Docker‚Äôs rapid rise is due to its openness and compatibility, enabling developers to seamlessly integrate with various cloud platforms, tools, and services. Similarly, the future of AI depends on the construction of an ecosystem. The standardization of AI, model sharing, data openness, and technological interoperability will all influence whether AI can form widespread industry applications.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;conclusion-the-future-of-ai-is-full-of-possibilities-but-requires-more-refined-implementation&#34;&gt;Conclusion: The Future of AI is Full of Possibilities, but Requires More Refined Implementation
&lt;/h2&gt;&lt;p&gt;Despite the significant advancements in AI technology over the past two years, it remains in the stage of ‚Äúlacking killer applications‚Äù at present. Similar to containerization technology before Docker‚Äôs release, AI needs a reasonable application scenario that deeply integrates existing technologies with business requirements to truly achieve large-scale adoption and widespread use. While technological innovation is undoubtedly important, solutions that simplify processes and improve efficiency are more likely to drive the popularization and development of the technology.&lt;/p&gt;
&lt;p&gt;In the future, AI may evolve like Docker ‚Äì not through disruptive technological breakthroughs, but by integrating existing technologies to create a perfect application scenario, ultimately changing the way we work and live.&lt;/p&gt;</description>
        </item>
        <item>
        <title>ollama local deployment of deepseek-R1</title>
        <link>https://ttf248.life/en/p/ollama-local-deployment-deepseek-r1/</link>
        <pubDate>Fri, 07 Feb 2025 22:41:02 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/ollama-local-deployment-deepseek-r1/</guid>
        <description>&lt;p&gt;Ollama is an open-source AI tool designed to enable users to run and deploy large language models (LLMs) locally. Its goal is to provide a convenient and efficient way for developers to use models like GPT on their local machines without relying on cloud services. Ollama supports multiple models and focuses on optimizing performance, allowing even resource-constrained devices to smoothly run these models.&lt;/p&gt;
&lt;p&gt;Through Ollama, users can utilize text-based AI applications and interact with locally deployed models without concerns about data privacy or high API usage fees. You can invoke different models via a command-line interface (CLI) for tasks such as natural language processing and question answering. &amp;gt; ollama is suitable for trying out different models, and after testing the Windows version, it couldn&amp;rsquo;t fully leverage the hardware‚Äôs performance ‚Äì this may be due to the Windows version. When deploying 32b parameter models with low memory and GPU load, the response speed is very slow.&lt;/p&gt;
&lt;h2 id=&#34;hardware-overview&#34;&gt;Hardware Overview
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;Operating System: Windows 11&lt;/li&gt;
&lt;li&gt;CPU: i7-10700K&lt;/li&gt;
&lt;li&gt;Memory: 40GB&lt;/li&gt;
&lt;li&gt;Graphics Card: RTX 3060 12GB&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;environment-setup&#34;&gt;Environment Setup
&lt;/h2&gt;&lt;p&gt;Add the following system environment variables for easier use:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;set OLLAMA_MODELS=E:\ollama&lt;/code&gt;&lt;/strong&gt;
This variable specifies the location where Ollama models are stored. &lt;code&gt;E:\ollama&lt;/code&gt; is a folder path indicating that all local model files will be stored in this directory. Ollama will load and use the language models you download or deploy based on this path. You can store your model files in another location by simply changing this path.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;set OLLAMA_HOST=127.0.0.1:8000&lt;/code&gt;&lt;/strong&gt;
This environment variable sets the host and port for the Ollama service.
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;127.0.0.1&lt;/code&gt; is the localhost address, meaning the Ollama service will only listen for requests from the local machine.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;8000&lt;/code&gt; is the specified port number, indicating that the Ollama service will wait for and process requests on port 8000.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;environment-setup-1&#34;&gt;Environment Setup
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;set OLLAMA_ORIGINS=*&lt;/code&gt;&lt;/strong&gt;
This environment variable controls which origins are allowed to access the Ollama service.
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;*&lt;/code&gt; indicates that any origin (i.e., all domains and IP addresses) can access the Ollama service. This is typically used in development and debugging environments, where production environments usually specify stricter origin control, limiting only specific domains or IPs to access your service to improve security.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;deepseek-r1-model-deployment&#34;&gt;deepseek-R1 Model Deployment
&lt;/h2&gt;&lt;p&gt;ollama installation is straightforward, so we won&amp;rsquo;t detail it here.&lt;/p&gt;
&lt;p&gt;Post-installation verification:&lt;/p&gt;
&lt;p&gt;Deploy the model, referring to the official model page and selecting the appropriate parameter version: &lt;code&gt;ollama run deepseek-r1:14b&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;The 14b parameter version effectively remembers conversation context; smaller parameter versions cannot retain context. The 32b parameter version is very sluggish when deployed locally and hasn&amp;rsquo;t been further tested.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.ollama.com/library/deepseek-r1&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://www.ollama.com/library/deepseek-r1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/SPEvYTmTBxhoEkJqm1yPmw&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://mp.weixin.qq.com/s/SPEvYTmTBxhoEkJqm1yPmw&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://blog.csdn.net/x18990027/article/details/145368094&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://blog.csdn.net/x18990027/article/details/145368094&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>DeepSeek suddenly went viral before the Spring Festival, NVIDIA stock plummeted: The behind institutional operations and the chain of thought for large models.</title>
        <link>https://ttf248.life/en/p/deepseek-chinese-new-year-nvidia-stock-drop-institutional-operations-large-language-model-chain/</link>
        <pubDate>Fri, 07 Feb 2025 20:36:05 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/deepseek-chinese-new-year-nvidia-stock-drop-institutional-operations-large-language-model-chain/</guid>
        <description>&lt;p&gt;Prior to the Spring Festival, DeepSeek had become a hot topic, attracting widespread attention on social media in just a few days. This sudden surge in popularity was astonishing and triggered a chain reaction in the market. Meanwhile, NVIDIA‚Äôs stock price plummeted, raising concerns among many investors, with some institutions conducting large-scale short selling during this period, seemingly pointing to a ‚Äúcarefully orchestrated‚Äù situation.&lt;/p&gt;
&lt;h3 id=&#34;deepseeks-sudden-surge-in-popularity-rapidly-becoming-a-focus&#34;&gt;&lt;strong&gt;DeepSeek&amp;rsquo;s Sudden Surge in Popularity: Rapidly Becoming a Focus&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;DeepSeek is an AI-powered tool focused on optimizing deep learning models, particularly in the applications of natural language processing (NLP) and image generation. In the days leading up to Chinese New Year, this project suddenly garnered significant attention from investors and tech professionals alike. The performance of its team and the demonstrated technical results sparked strong interest among many. Discussions about DeepSeek dominated all topics within the developer community and on social media platforms.&lt;/p&gt;
&lt;p&gt;However, DeepSeek‚Äôs sudden surge in popularity was not accidental. After analysis, many began to suspect that certain entities may have been involved behind it.&lt;/p&gt;
&lt;h3 id=&#34;nvidia-stock-plummets-the-hand-behind-the-short-selling-operation&#34;&gt;&lt;strong&gt;Nvidia Stock Plummets: The Hand Behind the Short Selling Operation&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;Nvidia, a global leader in graphics processing units (GPUs), has long been a key hardware provider for many large models and AI computations. With the rapid development of the AI market, Nvidia‚Äôs stock has performed strongly over the long term and even become a favorite among many investors. However, with the explosive popularity of DeepSeek and the market&amp;rsquo;s intense focus on its technology, Nvidia‚Äôs stock experienced a dramatic plunge.
The underlying cause of this phenomenon may involve institutional investor short selling strategies. Over the past few years, as AI technology has proliferated, Nvidia‚Äôs stock price has been significantly inflated, leading many investors to believe that it carries risks of excessive speculation. Particularly after a technology like DeepSeek went viral, some institutions may have profited substantially by shorting Nvidia&amp;rsquo;s stock.&lt;/p&gt;
&lt;h3 id=&#34;chain-of-thought-reasoning-with-large-models-from-results-to-process&#34;&gt;&lt;strong&gt;Chain-of-Thought Reasoning with Large Models: From ‚ÄúResults‚Äù to ‚ÄúProcess‚Äù&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;In traditional artificial intelligence applications, many practitioners and investors have focused more on the ‚Äúresults‚Äù of AI models ‚Äì such as generated images or text ‚Äì rather than the underlying reasoning process. In discussions related to DeepSeek, an increasing number of people are beginning to realize that the chain-of-thought hidden behind large models is the core content worth paying attention to. Previously, we could only see the model‚Äôs output results, but now we need to understand the logic, algorithms, and how to optimize the model&amp;rsquo;s performance by adjusting these factors.&lt;/p&gt;
&lt;p&gt;This shift in thinking represents a deep reflection on AI research and application. From simple black-box operations to a genuine shift towards understanding the internal workings of models, leading many technologists and investors to re-evaluate the future direction of artificial intelligence. The popularity of DeepSeek perfectly represents a breakthrough in this thought chain; it prompted people to focus on the entire model construction and optimization process, rather than just the final output results.&lt;/p&gt;
&lt;h3 id=&#34;summary&#34;&gt;&lt;strong&gt;Summary&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;The sudden surge in popularity of DeepSeek, NVIDIA‚Äôs stock plummet, and the subsequent short-selling activities by institutions ‚Äì it all seems like a meticulously designed scheme. Through an in-depth understanding of large model chain-of-thought reasoning, we can see that the application of AI technology is not merely superficial layering of phenomena, but also deep digging and optimization of the model‚Äôs internal logic. As the technology progresses, we may likely see more innovative tools like DeepSeek in the future, driving AI research and applications to a higher level.&lt;/p&gt;
&lt;p&gt;This phenomenon not only shows us the immense potential of AI technology, but also prompts us to start thinking about the commercial games and capital operations behind the technology. The future market trajectory will continue to be a focus of the ongoing battle between technology and capital.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Cursor AI Programming IDE Trial</title>
        <link>https://ttf248.life/en/p/cursor-ai-programming-ide-trial/</link>
        <pubDate>Thu, 23 Jan 2025 19:30:13 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/cursor-ai-programming-ide-trial/</guid>
        <description>&lt;p&gt;It seems like another year has passed, and the biggest change at work is a significantly increased involvement of AI. Previously, switching between different development languages required developers to be familiar with various language-specific API interfaces. Now, these basic code snippets can all be generated by AI, which is a huge blessing for developers.&lt;/p&gt;
&lt;h2 id=&#34;chatgpt&#34;&gt;ChatGPT
&lt;/h2&gt;&lt;p&gt;As early as 2023, I‚Äôve written two simple introductory articles about it. Now it&amp;rsquo;s been 25 years ‚Äì how to put this‚Ä¶ I haven‚Äôt felt a significant improvement. It still needs to develop its own cognition, be able to reasonably break down tasks, and, of course, most importantly, identify whether AI-generated code contains bugs.&lt;/p&gt;
&lt;h2 id=&#34;github-copilot&#34;&gt;Github Copilot
&lt;/h2&gt;&lt;p&gt;It was a long time ago, but I saw some information saying that Singapore deployed the server and it‚Äôs available for use in China. No longer need to maintain a VPN for extended periods. Of course, you still need to connect to a VPN when logging in, but this VPN only needs to be used during login, and then you can turn it off.&lt;/p&gt;
&lt;p&gt;In daily use, Github Copilot is also heavily relied upon. This plugin can be directly used in VS Code and Visual Studio without switching between the two applications. Compared to ChatGPT, Github Copilot provides better support for projects, a more user-friendly interaction, and allows you to feed partial local files to it ‚Äì &lt;strong&gt;‚Äútraining‚Äù the AI&lt;/strong&gt; ‚Äì so that the generated code is more aligned with your project.&lt;/p&gt;
&lt;h2 id=&#34;cursor-ai&#34;&gt;Cursor AI
&lt;/h2&gt;&lt;p&gt;Recently I‚Äôve come across a new AI programming IDE, Cursor AI. This IDE is based on Github Copilot, but it&amp;rsquo;s more intelligent and can help you create files directly.
I gave it a quick try and found it to be pretty good, however, its understanding of existing projects isn‚Äôt quite there yet. When dealing with large local project files, or major refactoring, optimization, and adjustments, developers still need to &lt;strong&gt;break down tasks&lt;/strong&gt;.
Here&amp;rsquo;s an example: Switching to Cursor‚Äôs engineering mode, inputting the following content: ‚ÄúCreate a personal resume webpage, supporting multiple different styles switching, and remember to populate some personal information for data display.‚Äù
After several back-and-forths (&lt;strong&gt;pulling&lt;/strong&gt;), you can obtain the following webpage. Of course, this webpage is relatively simple, but it‚Äôs still quite good for beginners. ## Cursor AI
Recently I‚Äôve come across a new AI programming IDE, Cursor AI. This IDE is based on Github Copilot, but it&amp;rsquo;s more intelligent and can help you create files directly.
I gave it a quick try and found it to be pretty good, however, its understanding of existing projects isn‚Äôt quite there yet. When dealing with large local project files, or major refactoring, optimization, and adjustments, developers still need to &lt;strong&gt;break down tasks&lt;/strong&gt;.
Here&amp;rsquo;s an example: Switching to Cursor‚Äôs engineering mode, inputting the following content: ‚ÄúCreate a personal resume webpage, supporting multiple different styles switching, and remember to fill in some personal information for data display.‚Äù
After several back-and-forths, you can obtain the following webpage. Of course, this webpage is relatively simple, but it&amp;rsquo;s still quite good for beginners.&lt;/p&gt;
&lt;h2 id=&#34;cursor-ai-1&#34;&gt;Cursor AI
&lt;/h2&gt;&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://ttf248.life/cursor/index.html&#34; &gt;Resume&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>AI Programming and Task Decomposition</title>
        <link>https://ttf248.life/en/p/ai-programming-and-task-decomposition/</link>
        <pubDate>Fri, 22 Dec 2023 08:44:26 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/ai-programming-and-task-decomposition/</guid>
        <description>&lt;p&gt;Two years ago, I added a copy function to the site, which took me half a day of tinkering. Ultimately, the rendering effect wasn&amp;rsquo;t quite satisfactory, and as a somewhat mediocre frontend developer, I didn‚Äôt plan on improving it ‚Äì if it worked, it was good enough. This year, I used &lt;code&gt;AI&lt;/code&gt; to develop a mini-program, which has made me more familiar with frontend development. I refactored it (the &lt;code&gt;AI&lt;/code&gt; redesigned it).&lt;/p&gt;
&lt;p&gt;Realization: Breaking down tasks really helps; many small tasks are quite natural for the &lt;code&gt;AI&lt;/code&gt; to develop. For the closed mini-program, 75% of the code was provided by &lt;code&gt;AI&lt;/code&gt;, broken down into multiple tasks delivered, and manually assembled the project code.&lt;/p&gt;
&lt;h2 id=&#34;previous-context&#34;&gt;Previous Context
&lt;/h2&gt;&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://ttf248.github.io/post/2022/001/%E4%BB%A5%E4%B8%BA%E5%BE%88%E7%AE%80%E5%8D%95%E7%9A%84%E4%BA%8B%E6%83%85%E5%A2%9E%E5%8A%A0%E4%B8%80%E4%B8%AA%E4%BB%A3%E7%A0%81%E5%A4%8D%E5%88%B6%E6%8C%89%E9%92%AE/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Making simple things easier: Adding a code copy button&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;main-text&#34;&gt;Main Text
&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;promot&lt;/strong&gt;: Design CSS and JavaScript code to locate webpage elements with the class &amp;ldquo;highlight&amp;rdquo;, and add a copy button within its container, enabling copy-to-clipboard functionality.&lt;/p&gt;
&lt;p&gt;The first draft of the code was immediately problematic; I regenerated it, making it largely usable, but the copied content was incorrect, containing redundant text. Further guidance led to optimizing the core color design and specifying desired effects.&lt;/p&gt;
&lt;p&gt;Having learned your specific requirements for button size and color, we redesigned the button styles to accommodate these demands.  Below is an adjusted CSS style that incorporates your provided color scheme and optimizes the button&amp;rsquo;s dimensions:&lt;/p&gt;
&lt;p&gt;Within this style, I reduced the button‚Äôs size and font size to create a more refined look. ‚ÄúSimultaneously, the button‚Äôs default state is semi-transparent. When hovering over the button, it becomes opaque and the background color slightly brightens to provide visual feedback. This design meets your color requirements while maintaining a modern and user-friendly aesthetic.&lt;/p&gt;
&lt;p&gt;A simpler &lt;code&gt;JavaScript&lt;/code&gt; implementation has also been provided:‚Äù&lt;/p&gt;
&lt;h2 id=&#34;epilogue&#34;&gt;Epilogue
&lt;/h2&gt;&lt;p&gt;While I had hoped to see a copy button added to the top-right corner of the code area, due to my lack of familiarity with front-end development and an inability to accurately describe the functionality based on keywords and ‚ÄúAI,‚Äù I spent several rounds chasing after it without achieving the desired outcome.&lt;/p&gt;
&lt;p&gt;It remains to be seen what impact ‚ÄúAI‚Äù will have on software development as time goes on.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Prompt Engineer</title>
        <link>https://ttf248.life/en/p/prompt-engineer/</link>
        <pubDate>Sun, 26 Mar 2023 20:46:53 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/prompt-engineer/</guid>
        <description>&lt;p&gt;Just as we needed to learn the techniques of searching engines back then, we also need to learn some techniques for communicating with &lt;code&gt;AI&lt;/code&gt;, providing reasonable and sufficient constraints, and efficiently obtaining the answers we need.&lt;/p&gt;
&lt;p&gt;If you look at it from a different angle, current &lt;code&gt;AI&lt;/code&gt; is like a very good student with excellent memory ‚Äì it has the ability to memorize everything. What we need to do is learn how to communicate with &lt;code&gt;AI&lt;/code&gt; correctly, effectively, and precisely describe our needs to help &lt;code&gt;AI&lt;/code&gt; generate the expected results.&lt;/p&gt;
&lt;p&gt;##ÁßëÊôÆ
The incredibly popular &lt;code&gt;AI&lt;/code&gt; ‚Äì specifically &lt;code&gt;Generative Pre-Training&lt;/code&gt; ‚Äì literally translates to generative pre-training. It‚Äôs a deep learning model that trains text generation based on internet-available data, used for tasks like question answering, text summarization generation, machine translation, classification, code generation, and conversational AI. Currently, there have been various versions of models such as GPT-1, GPT-2, GPT-3, and GPT-4, each larger and more powerful than the previous one.&lt;/p&gt;
&lt;h2 id=&#34;does-it-really-have-intelligence&#34;&gt;Does it Really Have Intelligence?
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;Similarity is high, accuracy increases accordingly.&lt;/li&gt;
&lt;li&gt;Basic, repetitive tasks, after specific training, no longer require human intervention.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Generative AI is a technology that utilizes existing data such as text, audio, and images to create new content. It can be used for various tasks including text generation, speech synthesis, image generation, and dialogue systems. The logical reasoning of Generative AI depends on its training data and model structure. Generally, Generative AI can follow grammar, logic, and common sense to a certain extent, but it may also produce errors, biases, or unrealistic content. Therefore, the output of Generative AI requires human judgment and verification; it should not be blindly trusted or used.&lt;/p&gt;
&lt;h2 id=&#34;prompt-engineer&#34;&gt;Prompt Engineer
&lt;/h2&gt;&lt;p&gt;Time flows only forward, and people need to learn to adapt to the current. You can think of &lt;code&gt;AI&lt;/code&gt; as not intelligent, lacking logic, and often producing unusable code.&lt;/p&gt;
&lt;p&gt;If you shift your perspective, the current &lt;code&gt;AI&lt;/code&gt; is like a child with excellent memory ‚Äì it has the ability to memorize after reading something repeatedly, much like copying homework. What we need to do is learn how to communicate correctly, effectively, and with &lt;code&gt;AI&lt;/code&gt;, precisely describing our needs to help &lt;code&gt;AI&lt;/code&gt; generate the desired results.&lt;/p&gt;
&lt;h2 id=&#34;dialogue-patterns&#34;&gt;Dialogue Patterns
&lt;/h2&gt;&lt;p&gt;Two years ago, when GitHub Copilot was released, no one could have imagined that two years later, OpenAI would appear out of nowhere, allowing humans to recognize the capabilities of large language models.&lt;/p&gt;
&lt;p&gt;Based on &lt;strong&gt;comment-based programming&lt;/strong&gt; and &lt;strong&gt;conversational programming&lt;/strong&gt;, the interaction logic is completely different. The dialogue pattern offers a user-friendly experience for novice users, and it‚Äôs essential to mention NewBing providing follow-up prompts after each question. Microsoft is attempting to guide users to obtain more content from the AI knowledge base.&lt;/p&gt;
&lt;h2 id=&#34;lets-illustrate-with-an-example&#34;&gt;Let&amp;rsquo;s illustrate with an example
&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Scenario:&lt;/strong&gt; A long high-speed train journey, and coding is a good choice. With limited traffic, processing a few images, obtaining image materials, downloading software to process the images is a bit extravagant, and network conditions aren‚Äôt ideal.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Requirements:&lt;/strong&gt; There‚Äôs a batch of image materials with transparent backgrounds that need to be used as navigation bars for small programs. The selected state is colored, and the unselected state is black and white. The materials are all in color, and they need to be preprocessed. &lt;strong&gt;Prompt:&lt;/strong&gt; Write a Python code to read all images from a folder, convert them to black and white while maintaining the transparent background, optimize image quality, reduce file size, and display the before-and-after file size comparison in the console.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;AI&lt;/code&gt; quickly provided an answer that looked very beautiful and worked well; an experienced programming source would find the code less generic and unable to be deployed into production:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Parameters were fixed.&lt;/li&gt;
&lt;li&gt;There were no log files, all information was printed to the console.&lt;/li&gt;
&lt;li&gt;The image processing task was suitable for a multi-threaded model.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;NewBing&lt;/code&gt; appeared, based on the prompt, choosing a question: &lt;strong&gt;What improvements can be made to this code?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This time, the answer was honest and completely exceeded expectations; typical optimization measures that programmers would think of were all provided by &lt;code&gt;AI&lt;/code&gt;, and corresponding suggestions were given.&lt;/p&gt;
&lt;p&gt;A friendly response was given, considering there are differences in the deployment environment, it allowed for not supporting multi-processes. ## Let&amp;rsquo;s illustrate with an example&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Scenario:&lt;/strong&gt; A long high-speed train journey, coding is a good choice. With limited traffic, processing a few images, obtaining image materials, downloading software to process the images is a bit extravagant, and network conditions aren‚Äôt ideal.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Requirements:&lt;/strong&gt; There‚Äôs a batch of image materials with transparent backgrounds intended for use as navigation bars in small programs. The selected state is colored, and the unselected state is black and white. The materials are all in color, and they need to be preprocessed.&lt;/p&gt;
&lt;h2 id=&#34;epilogue&#34;&gt;Epilogue
&lt;/h2&gt;&lt;p&gt;Due to local development being on a &lt;code&gt;windows&lt;/code&gt; system, the first answer given by &lt;code&gt;AI&lt;/code&gt; did not include the &lt;code&gt;main&lt;/code&gt; function and lacked &lt;code&gt;multiprocessing.freeze_support&lt;/code&gt;. The code was fixed after following up and encountering an error.
Just as learning search engine techniques required effort, we also need to learn how to communicate with &lt;code&gt;AI&lt;/code&gt;, providing reasonable and sufficient constraints to efficiently obtain the desired answers.
Note: &lt;strong&gt;If you are a programming beginner, if you still don&amp;rsquo;t understand certain parts of the code based on the given comments, please continue to ask related questions.&lt;/strong&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>AI-assisted programming, the evolution of productivity</title>
        <link>https://ttf248.life/en/p/ai-assisted-programming-productivity-evolution/</link>
        <pubDate>Tue, 28 Feb 2023 17:05:17 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/ai-assisted-programming-productivity-evolution/</guid>
        <description>&lt;p&gt;‚ÄúGitHub Copilot‚Äù was released less than two years ago, and ‚ÄúChatGPT‚Äù appeared shortly after. I don‚Äôt fully understand the underlying principles, but I&amp;rsquo;ve used them for a while. The two tools differ completely in their level of assistance, but they have both significantly increased productivity.&lt;/p&gt;
&lt;p&gt;For things that are too complex, AI still can‚Äôt do it ‚Äì after all, they lack logic, or operate on established patterns or fixed frameworks. If the training data is sufficient, AI can achieve nine out of ten results.&lt;/p&gt;
&lt;h2 id=&#34;github-copilot&#34;&gt;GitHub Copilot
&lt;/h2&gt;&lt;p&gt;When released, the demo on the official website wasn&amp;rsquo;t very smart, so I tried it and didn‚Äôt find it useful, so I gave up.&lt;/p&gt;
&lt;p&gt;Before 2022, when I had some free time, I switched to the new version to try it out, and the effect was already quite good; however, the speed in China was a bit slow, which was probably due to network issues. For languages like &lt;code&gt;Python&lt;/code&gt;, the code generated based on comments was pretty good, and much faster than researching and writing code myself.&lt;/p&gt;
&lt;h2 id=&#34;chatgpt&#34;&gt;ChatGPT
&lt;/h2&gt;&lt;p&gt;&lt;code&gt;Github Copilot&lt;/code&gt; still focuses on developers, i.e., from a development perspective when designing the product. You need to understand how to program and gradually guide AI assistance through comments.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;ChatGPT&lt;/code&gt; has moved to the next level, &lt;strong&gt;allowing non-programmers to quickly get started&lt;/strong&gt;. The conversational chat interface design allows you to simply ask questions, describe requirements, and ask for clarification if you don&amp;rsquo;t understand.&lt;/p&gt;
&lt;p&gt;Let‚Äôs take a simple example: You know about tokenization but don‚Äôt know how to code it; you want to calculate the similarity of strings:&lt;/p&gt;
&lt;p&gt;Hello, this is Bing. üòä&lt;/p&gt;
&lt;p&gt;Based on online information 123, Python tokenization can be implemented using the jieba library. The jieba library supports three tokenization modes: precise mode, full mode, and search engine mode. You can choose the appropriate mode based on your needs. To compare the similarity of two strings, you first need to tokenize them and then calculate their word vectors or edit distance metrics. You can use libraries like NumPy or SciPy for mathematical operations.&lt;/p&gt;
&lt;p&gt;To standardize string processing, convert English strings to lowercase using Python&amp;rsquo;s built-in &lt;code&gt;lower()&lt;/code&gt; function.&lt;/p&gt;
&lt;p&gt;To slice the second string, you can use Python‚Äôs slicing operator [].&lt;/p&gt;
&lt;p&gt;Here is a possible code example:&lt;/p&gt;
&lt;h2 id=&#34;chatgpt-1&#34;&gt;ChatGPT
&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Going Further ‚Äì Consult the Execution Results. Congratulations, you‚Äôve identified an issue! The result was hallucinated; it did not actually run the code, but will provide a good-looking response.&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&#34;example&#34;&gt;Example
&lt;/h3&gt;&lt;p&gt;Question: SQL statement to retrieve records from the product code table where the product code falls within the range 80000-89999.
Follow-up question: Negate it: The product code is a string, your code is wrong.
Congratulations, it will misidentify.&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
