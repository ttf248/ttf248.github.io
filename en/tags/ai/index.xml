<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Ai on Uncle Xiang&#39;s Notebook</title>
        <link>https://ttf248.life/en/tags/ai/</link>
        <description>Recent content in Ai on Uncle Xiang&#39;s Notebook</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en</language>
        <lastBuildDate>Mon, 26 May 2025 00:49:57 +0800</lastBuildDate><atom:link href="https://ttf248.life/en/tags/ai/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>Claude4 released, attempting development: Hugo tag, hyperlink translation assistant</title>
        <link>https://ttf248.life/en/p/claude-4-release-and-experimentation-hugo-tags-hyperlink-translation-assistant/</link>
        <pubDate>Sat, 24 May 2025 03:05:31 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/claude-4-release-and-experimentation-hugo-tags-hyperlink-translation-assistant/</guid>
        <description>&lt;p&gt;This site is built with Hugo, but I&amp;rsquo;ve been using Chinese titles, which results in unfriendly URLs. To put it simply, when shared, they don&amp;rsquo;t look good because Chinese characters are encoded as things like %E4%BD%A0%E5%A5%BD. While slugs can fix this, manually setting them each time is too much trouble.&lt;/p&gt;
&lt;p&gt;So, today I tried using Claude4 to develop a translation assistant that automatically converts Chinese titles into English slugs and adds hyperlinks within articles. This would eliminate the need for manual setup.&lt;/p&gt;
&lt;p&gt;Claude 4 is excellent; its ability to understand context and handle complex tasks has been significantly improved&lt;/p&gt;
&lt;p&gt;Project Address&lt;/p&gt;
&lt;p&gt;Domestic project address: [https://cnb.cool/ttf248/hugo-content-suite]
Foreign project address: [https://github.com/ttf248/hugo-content-suite]&lt;/p&gt;
&lt;p&gt;Code Implementation&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s the translation: First, let me outline the approach: We need to scan all articles, extract tag information and article titles, then call a local large language model (like gemma-3-12b-it) for translation&lt;/p&gt;
&lt;p&gt;In practical development, &lt;code&gt;Claude4&lt;/code&gt; demonstrates several significant improvements compared to previous generations of large language models. Due to extensive functional requirements, &lt;code&gt;Claude4&lt;/code&gt; automatically designs interactive menus and comprehensively considers various use cases. For example, in tag processing, &lt;code&gt;Claude4&lt;/code&gt; not only supports tag statistics and analysis but also includes classification statistics and can even detect &lt;strong&gt;untagged articles&lt;/strong&gt;. Furthermore, it provides &lt;strong&gt;preview&lt;/strong&gt; and tag page generation functionalities.&lt;/p&gt;
&lt;p&gt;Whether it&amp;rsquo;s integrating with local large language models, adding translation caching, or conducting extensive code refactoring, &lt;code&gt;Claude4&lt;/code&gt; completed everything in one go with virtually no issues. Despite the projectâ€™s modest size and numerous small features, large language models often forget previous context during development. This time, however, &lt;code&gt;Claude4&lt;/code&gt; performed exceptionally well, &lt;strong&gt;showing almost no contextual forgetting.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In short, intelligence has improved, and we&amp;rsquo;re preparing to switch to &lt;code&gt;Claude4&lt;/code&gt; for more development work as our primary coding model&lt;/p&gt;
&lt;p&gt;Translation Cache&lt;/p&gt;
&lt;p&gt;This approach, aside from reducing large model calls, offers good efficiency when running a 12b model locally without causing delays. However, repeatedly calling the large model can still be slow. Secondly, to maintain article links, full updates sometimes produce inconsistent results due to long titles, leading to link changes â€“ which is quite awkward.&lt;/p&gt;
&lt;p&gt;Feature Optimization&lt;/p&gt;
&lt;p&gt;The entire project was given to Claude4 for analysis, resulting in these optimization suggestions:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;é…ç½®å¤–ç½®åŒ– - æé«˜å¯ç»´æŠ¤æ€§å’Œçµæ´»æ€§
ç»“æ„åŒ–æ—¥å¿— - ä¾¿äºé—®é¢˜æ’æŸ¥å’Œç›‘æ§
æ€§èƒ½ç›‘æ§ - äº†è§£ç³»ç»Ÿè¿è¡ŒçŠ¶å†µ
ç”¨æˆ·ä½“éªŒ - è¿›åº¦æ¡ç­‰è§†è§‰åé¦ˆ
é”™è¯¯å¤„ç† - æ›´å®Œå–„çš„å¼‚å¸¸å¤„ç†æœºåˆ¶
ä»£ç ç»„ç»‡ - æ›´æ¸…æ™°çš„æ¨¡å—åˆ’åˆ†
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The code review was completed and everything looks good. For example, the configuration file now automatically generates a default if one doesn&amp;rsquo;t exist when reading it, converting previous configurations to defaults â€“ preventing user errors.&lt;/p&gt;
&lt;h3 id=&#34;heading&#34;&gt;
&lt;/h3&gt;&lt;p&gt;Requirements: While translating the main text, dynamically calculate the current translation efficiency, estimate the remaining time, and output relevant information to the console. This includes tracking the number of characters obtained from the article, the number of characters translated per line, the elapsed time, a fitted calculation for the translation time per 100 characters, and an estimated remaining translation time for the entire article.&lt;/p&gt;
&lt;p&gt;The code is done, but I&amp;rsquo;m not very satisfied with the results, so I asked AI to provide new design suggestions&lt;/p&gt;
&lt;p&gt;Provides multiple efficiency calculation methods: real-time efficiency, average efficiency, sliding window efficiency
Improved display methods: progress bar, segmented statistics, dynamic refresh
Add more useful metrics: API call count, success rate, etc&lt;/p&gt;
&lt;p&gt;After completing the code, I discovered a new surprise â€“ real-time statistics on translation efficiency are displayed without mindless scrolling&lt;/p&gt;
&lt;p&gt;Translating body text to English..
Detected 53 lines needing translation
Translating 354 characters..
Completed (3.1s) | API call #1
Completed (1.5s) | API call #2
Completed (0.9s) | API call #3
Completed (0.2s) | API call #4
Completed
Completed
Completed (0.2s) | API call #7
Progress: 13.2% (7/53) | Characters 12.9% (925/7163) 114.6 characters/second
Efficiency: Real-time 76.4 | Average 117.9 | Recent 109.0 | Stage 113.6 characters/second ğŸ“Š
Success Rate: 100.0% (7/7) | Remaining: 46 lines, 7 seconds] 9.4% Translation of 110 charactersâ€¦
Estimated remaining: 55s | Estimated completion: 10:19 8s | 11.3% | Translating 114 charactersâ€¦
Processing speed: 3211.3 lines/minute | Total time: 8s] 13.2% Translated 16 charactersâ€¦
Stage 1/6 [â–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 15.1% Translating 166 charactersâ€¦&lt;/p&gt;
&lt;p&gt;I haven&amp;rsquo;t controlled many programs before, but I&amp;rsquo;m curious about how it&amp;rsquo;s implemented, so I looked at the code&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-golang&#34;&gt;// æ¸…å±å¹¶é‡æ–°æ˜¾ç¤º (åŠ¨æ€åˆ·æ–°æ•ˆæœ)
if translationCount &amp;gt; 1 {
 Â  fmt.Print(&amp;quot;\033[6A\033[K&amp;quot;) // ä¸Šç§»6è¡Œå¹¶æ¸…é™¤
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Performance Statistics Menu&lt;/p&gt;
&lt;p&gt;The new &lt;strong&gt;performance statistics menu&lt;/strong&gt;, which I designed myself, might not even be this comprehensive&lt;/p&gt;
&lt;p&gt;Performance Statistics:
Translation count: 360
Cache hit rate: 1.4% (5/365)
Average translation time: 315.927234ms
File Operations: 73
Incorrect attempts: 0&lt;/p&gt;
&lt;p&gt;Progress bar display&lt;/p&gt;
&lt;p&gt;New &lt;strong&gt;progress bar display&lt;/strong&gt;, showing detailed progress, elapsed time, and estimated remaining time&lt;/p&gt;
&lt;p&gt;Please select function (0-13): 10
Collecting translation target..
Cached file loaded, containing 0 translation records&lt;/p&gt;
&lt;p&gt;Translation cache statistics:
Total labels: 229
Total articles: 131
Cached: 0 items
360 items need translating&lt;/p&gt;
&lt;p&gt;Confirm generating full translation cache? (y/n): y
Generating full translation cache..
Cached file loaded, containing 0 translation records
Checking cached translations..
Need to translate 360 new tags
5/360 (1.4%) - Time taken: 3s - Estimated remaining: 3m8sğŸ’¾ Saved cache file, containing 5 translation records
10/360 (2.8%) - Time taken: 6s - Estimated remaining time: 3m28sğŸ’¾ Saved cache file, containing 10 translation records
15/360 (4.2%) - Time taken: 9s - Estimated remaining: 3m30sğŸ’¾ Saved cache file, containing 15 translation records
20/360 (5.6%) - Time taken: 13s - Estimated remaining time: 3m36sğŸ’¾ Saved cache file, containing 20 translation records
25/360 (6.9%) - Time taken: 16s - Estimated remaining time: 3m33sğŸ’¾ Saved cache file, containing 25 translation records
30/360 (8.3%) - Time elapsed: 19s - Estimated remaining: 3m30sğŸ’¾ Saved cache file, containing 30 translation records
Saved cache file, containing 35 translation records&lt;/p&gt;
&lt;hr&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;=== Hugo åšå®¢ç®¡ç†å·¥å…· ===

ğŸš€ æ ¸å¿ƒåŠŸèƒ½
  1. ä¸€é”®å¤„ç†å…¨éƒ¨ (å®Œæ•´åšå®¢å¤„ç†æµç¨‹)

ğŸ“ å†…å®¹ç®¡ç†
  2. ç”Ÿæˆæ ‡ç­¾é¡µé¢
  3. ç”Ÿæˆæ–‡ç« Slug
  4. ç¿»è¯‘æ–‡ç« ä¸ºå¤šè¯­è¨€ç‰ˆæœ¬

ğŸ’¾ ç¼“å­˜ç®¡ç†
  5. æŸ¥çœ‹ç¼“å­˜çŠ¶æ€
  6. ç”Ÿæˆå…¨é‡ç¿»è¯‘ç¼“å­˜
  7. æ¸…ç©ºç¿»è¯‘ç¼“å­˜

  0. é€€å‡ºç¨‹åº
&lt;/code&gt;&lt;/pre&gt;</description>
        </item>
        <item>
        <title>Too much AI, a bit of a side effect</title>
        <link>https://ttf248.life/en/p/ai-overuse-side-effects/</link>
        <pubDate>Wed, 14 May 2025 19:39:50 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/ai-overuse-side-effects/</guid>
        <description>&lt;p&gt;Since the new &lt;code&gt;AI Inspiration Hub&lt;/code&gt; was established, a lot of trivial things have been tried to be recorded and published using AI. As a result, there&amp;rsquo;s less time for quiet reflection and contemplation. It would be good to slightly control the output of this column in the future and integrate it into a monthly publication format, releasing one article per month.&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s like a kind of aftereffect, or perhaps a side effect â€“ efficiency is improved, but the depth and breadth of thinking have declined&lt;/p&gt;
&lt;h2 id=&#34;efficiency-improvement-undeniable&#34;&gt;Efficiency improvement: undeniable
&lt;/h2&gt;&lt;p&gt;The &amp;ldquo;Seven Seconds of Fish&amp;rdquo; column hasn&amp;rsquo;t been well-maintained previously. Due to laziness, I didnâ€™t search for information or compile records on some trending events. Now that various AI tools are available, outlining is sufficient; the AI can automatically connect to the internet, search for related event records, and generate articles as needed. A simple formatting adjustment, and it&amp;rsquo;s ready for publication.&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s like a godsend for lazy people, it significantly improves efficiency, and you could even say it achieves results with half the effort&lt;/p&gt;
&lt;p&gt;Throwing aside the need to write articles, when writing code, the efficiency boost is real. Many coding tasks used to require detailed reading of API interface documentation, but now you can skip them directly â€“ and this skipping is quite necessary. Familiarity with APIs is a &lt;code&gt;physical labor&lt;/code&gt;, not a &lt;code&gt;mental labor&lt;/code&gt;; AI handling this part is just right.&lt;/p&gt;
&lt;h2 id=&#34;garbage-content&#34;&gt;Garbage content
&lt;/h2&gt;&lt;p&gt;Many of the submissions are low in quality; they aren&amp;rsquo;t entirely without merit, but they lack a sense of life and warmth. Itâ€™s a style I used to dislike â€“ dry and lifeless.&lt;/p&gt;
&lt;p&gt;To put it another way, AI-generated content does feel a bit like something produced on an assembly line â€“ lacking soul&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Internet junk of the new era&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;forgetfulness&#34;&gt;Forgetfulness
&lt;/h2&gt;&lt;p&gt;This type of article is all AI-generated, and I donâ€™t know the readers&amp;rsquo; situation, but over time, my own impressions will become blurred, or even forgotten&lt;/p&gt;
&lt;p&gt;Similar issues arise when writing code as well. Without checking the commit history, you have no idea what you were thinking at the time and why you wrote it that way. This is especially true for code generated after repeated communication with yourself and AI â€“ the final version can be very different from the initial idea, even completely different.&lt;/p&gt;
&lt;h2 id=&#34;search&#34;&gt;Search
&lt;/h2&gt;&lt;p&gt;I&amp;rsquo;ve noticed a significant decrease in how often I open Google and Baidu recently. Many questions are now searched for using AI, and the interaction and search results are much better than traditional search engines.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s pay our respects to &lt;code&gt;bing ai&lt;/code&gt;, a tool from a major company that was among the first released with internet search capabilities, whether itâ€™s still alive or not&lt;/p&gt;
&lt;p&gt;People are using Google less, and visiting &lt;code&gt;stackoverflow&lt;/code&gt; less frequently too. Many questions can now be directly asked to AI, so this website is gradually being phased out by the times.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion
&lt;/h2&gt;&lt;p&gt;The blog Iâ€™m still maintaining didn&amp;rsquo;t have much traffic to begin with, and now even less so; itâ€™s more of a place for keeping records, writing for myself&lt;/p&gt;</description>
        </item>
        <item>
        <title>Design and develop a customizable stock selection module without coding</title>
        <link>https://ttf248.life/en/p/no-code-design-develop-custom-stock-module/</link>
        <pubDate>Thu, 27 Feb 2025 23:20:39 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/no-code-design-develop-custom-stock-module/</guid>
        <description>&lt;p&gt;Last month we tested Cursor, but due to limitations on the free tier, we didn&amp;rsquo;t do any complex feature development, just some simple testing. At that time, we also discovered that ByteDance released a similar product, and both used the same underlying large language model â€“ Claude-3.5.&lt;/p&gt;
&lt;p&gt;Byte&amp;rsquo;s product is called Trae, first launched for Mac, and finally released a Windows version this February. Products from big companies are just good â€“ you can use them for free, without paying, with unlimited access to Claude-3.5. This model has a very nice effect.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Ultimately, I got stuck on the development of the K-line chart. Due to my basic lack of knowledge in React, I had to give up. To continue developing it, I need to supplement some fundamental front-end knowledge and break down the tasks more finely, rather than giving a large task: develop the K-line chart.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;issues-found&#34;&gt;Issues found
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;Due to insufficient training data for Vue3 + Element-Plus due to the use of foreign AI models, we chose React as the front-end framework&lt;/li&gt;
&lt;li&gt;There may be occasional grammatical errors that need manual correction&lt;/li&gt;
&lt;li&gt;Solutions to some complex problems require human guidance&lt;/li&gt;
&lt;li&gt;Code structure optimization requires human guidance&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The most time-consuming part was packaging the frontend code into a container. As the author has no foundation, I had no concept of &lt;code&gt;.env.production&lt;/code&gt; or &lt;code&gt;tsconfig.json&lt;/code&gt;. I only managed to understand the corresponding logic with help from Bean. There&amp;rsquo;s a big difference in how the frontend development (dev) mode and build mode check the code. The container scripts for the backend database and services took about five minutes altogether.&lt;/p&gt;
&lt;p&gt;Currently, AI primarily improves development efficiency; having a foundation is best, as AI won&amp;rsquo;t solve all your problems&lt;/p&gt;
&lt;h2 id=&#34;warehouse-address&#34;&gt;Warehouse address
&lt;/h2&gt;&lt;p&gt;As the title says, this time we&amp;rsquo;re avoiding writing code and directly engaging with AI to design and develop a customizable stock selection module. Letâ€™s see what kind of results we can achieve in the end.&lt;/p&gt;
&lt;p&gt;Repository address: [https://github.com/ttf248/trae-demo]&lt;/p&gt;
&lt;p&gt;Detailed usage instructions can be found in the README.md file in the repository&lt;/p&gt;
&lt;p&gt;The warehouse contains many submission records, mostly conversations between me and Trae, as well as my tests of some of Trae&amp;rsquo;s functions, with notes indicating whether manual intervention was required to achieve the corresponding functionality&lt;/p&gt;
&lt;h2 id=&#34;prompt&#34;&gt;Prompt
&lt;/h2&gt;&lt;p&gt;The project was created from scratch, here&amp;rsquo;s the prompt for the project:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;åŸºäºé¡¹ç›®åŸå‹å›¾ï¼Œå¼€å‘åŠŸèƒ½ï¼šè‡ªé€‰è‚¡ï¼Œéœ€è¦æ”¯æŒåˆçº¦çš„æ–°å¢ã€åˆ é™¤ã€ä¿®æ”¹ã€æŸ¥è¯¢ã€‚è‡ªé€‰è‚¡ç•Œé¢éœ€è¦å±•ç¤ºåŸºç¡€çš„è¡Œæƒ…æ•°æ®ã€‚æ”¯æŒå¤šä¸ªä¸åŒçš„å¸‚åœºåˆ‡æ¢ã€‚

å‰ç«¯ï¼šreact
åç«¯ï¼šgolang gin gorm
æ•°æ®åº“ï¼šPostgreSQL

æœåŠ¡ç«¯éœ€è¦æ”¯æŒè·¨åŸŸè¯·æ±‚ï¼ŒåŒæ—¶éœ€è¦è€ƒè™‘æ•°æ®çš„æ ¡éªŒå’Œé”™è¯¯å¤„ç†ï¼Œå¦‚æœåç«¯æœåŠ¡ä¸å¯ç”¨ï¼Œå‰ç«¯éœ€è¦å‘Šè­¦æç¤ºã€‚

åç«¯éœ€è¦å±•ç¤ºè¯·æ±‚å’Œåº”ç­”çš„æ—¥å¿—ï¼›å‰ç«¯ä¹Ÿæ‰“å°é€šè®¯çš„æ—¥å¿—ï¼Œæ–¹ä¾¿æ’æŸ¥é—®é¢˜ã€‚
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;ui-and-interaction-optimization&#34;&gt;UI and interaction optimization
&lt;/h2&gt;&lt;p&gt;The design of the front-end interface is completely dependent on Grok. We first created a prototype in Trae, but it lacked aesthetic appeal. Due to the model&amp;rsquo;s strong coding capabilities but relatively weak other abilities, we need to use Grok to optimize the front-end UI.&lt;/p&gt;
&lt;p&gt;By taking a screenshot of the current interface, uploading it to Grok, and having it help us optimize the UI, we can potentially receive many optimization suggestions at once. We will manually evaluate them and then copy them into Trae for execution, observing the effects of the optimization.&lt;/p&gt;
&lt;h3 id=&#34;technology-stack&#34;&gt;Technology stack
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Frontend: React + TypeScript&lt;/li&gt;
&lt;li&gt;Backend: Golang + Gin + GORM&lt;/li&gt;
&lt;li&gt;Database: PostgreSQL 17&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;system-architecture&#34;&gt;System architecture
&lt;/h2&gt;&lt;h2 id=&#34;backend-architecture&#34;&gt;Backend architecture
&lt;/h2&gt;&lt;p&gt;The backend is implemented using the Go Gin framework for a RESTful API, with main modules including:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Database module&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Use GORM as an ORM framework&lt;/li&gt;
&lt;li&gt;Support configuring database connections through environment variables&lt;/li&gt;
&lt;li&gt;Automatically migrate database tables&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Routing module&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;RESTful API Design&lt;/li&gt;
&lt;li&gt;Unified error handling mechanism&lt;/li&gt;
&lt;li&gt;Built-in request logging&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Cross-domain handling&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Support cross-domain in local development environments&lt;/li&gt;
&lt;li&gt;Configurable CORS policy&lt;/li&gt;
&lt;li&gt;Support cross-domain cookies&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;frontend-architecture&#34;&gt;Frontend Architecture
&lt;/h2&gt;&lt;p&gt;Built with React + TypeScript, it achieves:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Stock list display&lt;/li&gt;
&lt;li&gt;Self-selected stock management&lt;/li&gt;
&lt;li&gt;Market Data Display&lt;/li&gt;
&lt;li&gt;Error prompt mechanism&lt;/li&gt;
&lt;/ul&gt;</description>
        </item>
        <item>
        <title>Two years of AI development: somewhat similar to the state before Docker was released</title>
        <link>https://ttf248.life/en/p/ai-development-two-years-docker-pre-release/</link>
        <pubDate>Thu, 20 Feb 2025 18:16:37 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/ai-development-two-years-docker-pre-release/</guid>
        <description>&lt;p&gt;Artificial intelligence (AI) has undoubtedly been one of the most discussed topics in the technology field in recent years, especially with the rapid advancements in AI technology over the past two years. From deep learning and natural language processing to computer vision and automated decision-making systems, applications of AI are constantly emerging. However, despite continuous technological breakthroughs, AI still faces a bottleneck similar to that before Docker&amp;rsquo;s release â€“ a lack of a killer application to truly ignite the market.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The development of AI over the past two years is somewhat similar to the state before Docker was released â€“ it lacks a killer application. Based on existing technologies, we need to create a perfect practical scenario. Docker didn&amp;rsquo;t rely heavily on new technologies, but the entire solution was very reasonable and changed the workflows for operations and development.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;the-current-state-of-ai-development-technology-is-mature-but-application-still-needs-breakthroughs&#34;&gt;The current state of AI development: technology is mature, but application still needs breakthroughs
&lt;/h2&gt;&lt;p&gt;From a technical perspective, AI has made considerable progress in the past two years. Whether it&amp;rsquo;s OpenAI&amp;rsquo;s GPT series models or Google&amp;rsquo;s BERT and DeepMind&amp;rsquo;s Alpha series, AI&amp;rsquo;s processing capabilities have far exceeded previous expectations. Especially in the field of natural language processing, models like GPT-4 not only possess powerful generation capabilities but also demonstrate astonishing performance in understanding and reasoning.&lt;/p&gt;
&lt;p&gt;However, despite the rapid advancement of technology, the practical application of AI faces certain challenges. Similar to the state before Docker was released, although AI has great potential, a truly widespread and industry-transforming killer application hasn&amp;rsquo;t yet emerged. People are talking about the prospects of AI, but may not be able to find an application scenario that can directly bring revolutionary change. Many AI applications are still in the initial trial stage, and most require further integration and optimization.&lt;/p&gt;
&lt;h2 id=&#34;the-similarity-between-docker-and-ai-technology-isnt-necessarily-innovation-solutions-are-key&#34;&gt;The similarity between Docker and AI: Technology isn&amp;rsquo;t necessarily innovation, solutions are key
&lt;/h2&gt;&lt;p&gt;Looking back at the history before Docker&amp;rsquo;s release, we can easily see that the technological environment and the current state of AI development share many similarities. Before Docker was released, container technology wasn&amp;rsquo;t a new concept; early LXC (Linux Containers) and virtualization technologies already possessed basic containerization capabilities. However, Docker cleverly integrated and optimized existing technologies to propose a simpler, more intuitive, and efficient solution. This approach didnâ€™t introduce any groundbreaking technologies but solved many pain points in operations and development processes, greatly simplifying the deployment, scaling, and management of software.&lt;/p&gt;
&lt;p&gt;Similarly, the AI field faces similar circumstances. While current AI technology is no longer a &amp;ldquo;novelty,&amp;rdquo; achieving truly large-scale applications still requires a perfect landing scenario â€“ like Docker â€“ to integrate and optimize existing technologies into a reasonable application plan. The killer application of AI may not depend on entirely new technological breakthroughs, but rather on how to solve real business pain points and needs by integrating existing technologies.&lt;/p&gt;
&lt;h2 id=&#34;how-to-find-ais-docker-moment&#34;&gt;How to find AI&amp;rsquo;s &amp;ldquo;Docker moment&amp;rdquo;?
&lt;/h2&gt;&lt;p&gt;To truly achieve widespread application of AI technology, efforts need to be made in several areas:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;In-depth exploration of real-world scenarios
Currently, many AI applications are still experimental in nature and lack large-scale practical implementation. For example, while AI customer service and intelligent recommendation fields have widespread applications, their functions still have many limitations and haven&amp;rsquo;t yet overcome industry bottlenecks. Real breakthroughs may come from industries long troubled by traditional methods, such as healthcare, manufacturing, and logistics, where AI can help businesses improve efficiency and reduce costs through more efficient data processing and predictive analysis in these complex scenarios.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Productization and Usability
Just as Docker improves operational efficiency by simplifying the containerization process, the usability of AI products is equally crucial. The popularization of AI isn&amp;rsquo;t just about the spread of technology; itâ€™s about the popularization of its productization. Integrating AI into daily workflows and allowing users to easily use these tools without needing a deep understanding of the underlying technology is an important step in implementing AI.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Ecological Construction and Standardization
The widespread adoption of any new technology is inseparable from ecosystem development. Docker&amp;rsquo;s rapid rise is precisely due to its openness and compatibility, allowing developers to easily connect with various cloud platforms, tools, and services. Similarly, the future of AI depends on building an ecosystem. The standardization of AI, model sharing, data openness, and technical integrability will all influence whether AI can form widespread industry applications.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;conclusion-the-future-of-ai-is-full-of-possibilities-but-a-more-complete-landing-plan-is-still-needed&#34;&gt;Conclusion: The future of AI is full of possibilities, but a more complete landing plan is still needed
&lt;/h2&gt;&lt;p&gt;Although AI technology has made significant progress in the past two years, it is still in a stage of â€œlacking a killer application.â€ Similar to containerization technology before Dockerâ€™s release, AI needs a reasonable application scenario that deeply integrates existing technologies with business needs in order to truly achieve large-scale application and popularization. While technological innovation is important, solutions that simplify processes and improve efficiency are better at driving the adoption and development of technology.&lt;/p&gt;
&lt;p&gt;In the future, AI may be like Dockerâ€”not through revolutionary technological breakthroughs, but by integrating existing technologies to create a perfect application scenario, ultimately changing the way we work and live&lt;/p&gt;</description>
        </item>
        <item>
        <title>Deploy DeepSeek-R1 locally</title>
        <link>https://ttf248.life/en/p/ollama-local-deployment-deepseek-r1/</link>
        <pubDate>Fri, 07 Feb 2025 22:41:02 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/ollama-local-deployment-deepseek-r1/</guid>
        <description>&lt;p&gt;Ollama is an open-source AI tool designed to enable users to run and deploy large language models (LLMs) locally. Its goal is to provide a convenient and efficient way for developers to use models like GPT on their local machines without relying on cloud services. Ollama supports various models and focuses on optimizing performance, ensuring that even resource-constrained devices can run these models smoothly.&lt;/p&gt;
&lt;p&gt;With Ollama, users can use text-based AI applications and interact with locally deployed models without worrying about data privacy or high API usage fees. You can call different models through the command-line interface (CLI) to perform tasks such as natural language processing and question answering.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Ollama is suitable for trying out different models. The Windows version, after testing, doesn&amp;rsquo;t fully utilize the hardware&amp;rsquo;s performance; this may be due to the Windows version itself. The Linux version might be better. When deploying a 32b parameter model, with low memory and GPU load, the response speed is very slow.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;hardware-overview&#34;&gt;Hardware Overview
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;Operating system: win11&lt;/li&gt;
&lt;li&gt;CPUï¼ši7-10700K&lt;/li&gt;
&lt;li&gt;Memory: 40GB&lt;/li&gt;
&lt;li&gt;Graphics card: RTX 3060 12GB&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;environmental-preparation&#34;&gt;Environmental preparation
&lt;/h2&gt;&lt;p&gt;Add a new system environment variable for convenient use later&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;set OLLAMA_MODELS=E:\ollama&lt;/code&gt;&lt;/strong&gt;&lt;br&gt;
This variable specifies the storage path for the Ollama model. &lt;code&gt;E:\ollama&lt;/code&gt; is a folder path indicating that all local model files are stored in this directory. Ollama will load and use your downloaded or deployed language models based on this path. You can store the model files in other locations, just change this path.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;set OLLAMA_HOST=127.0.0.1:8000&lt;/code&gt;&lt;/strong&gt;&lt;br&gt;
This environment variable sets the host and port for the Ollama service&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;127.0.0.1&lt;/code&gt; is the local address (localhost), meaning that the Ollama service will only listen for requests from the local machine&lt;/li&gt;
&lt;li&gt;The port number 8000 is the designated port, indicating that the Ollama service will listen for and process requests on port 8000. You can change the port number as needed, but make sure it is not occupied by other applications.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;set OLLAMA_ORIGINS=*&lt;/code&gt;&lt;/strong&gt;&lt;br&gt;
This environment variable controls which sources of requests are allowed to access the Ollama service&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;code&gt;*&lt;/code&gt; indicates that any source (i.e., all domains and IP addresses) is allowed to access the Ollama service. This is typically used in development and debugging environments; in production, you would usually specify stricter source control, limiting access only to specific domains or IPs to improve security.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;deepseek-r1-model-deployment&#34;&gt;DeepSeek-R1 Model Deployment
&lt;/h2&gt;&lt;p&gt;Ollama installation is straightforward and will not be elaborated on here&lt;/p&gt;
&lt;p&gt;Post-installation verification&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;C:\Users\core&amp;gt;ollama -v
ollama version is 0.5.11
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Model deployment, refer to the official website model page and select the corresponding parameters for the model: &lt;code&gt;ollama run deepseek-r1:14b&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;The 14B parameter model can effectively remember conversation context, while smaller parameter versions cannot. The 32B parameter version is very slow when deployed locally, so I didn&amp;rsquo;t conduct further testing.&lt;/p&gt;
&lt;h2 id=&#34;reference-materials&#34;&gt;Reference materials
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.ollama.com/library/deepseek-r1&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://www.ollama.com/library/deepseek-r1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/SPEvYTmTBxhoEkJqm1yPmw&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://mp.weixin.qq.com/s/SPEvYTmTBxhoEkJqm1yPmw&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://blog.csdn.net/x18990027/article/details/145368094&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://blog.csdn.net/x18990027/article/details/145368094&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
        </item>
        <item>
        <title>DeepSeek suddenly surged before the Spring Festival, Nvidia stock plummeted: institutional operations and large model thinking chains</title>
        <link>https://ttf248.life/en/p/deepseek-chinese-new-year-nvidia-stock-drop-institutional-operations-large-language-model-chain/</link>
        <pubDate>Fri, 07 Feb 2025 20:36:05 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/deepseek-chinese-new-year-nvidia-stock-drop-institutional-operations-large-language-model-chain/</guid>
        <description>&lt;p&gt;Ahead of the Spring Festival, DeepSeek briefly became a hot topic, garnering widespread attention on social media in just a few days. This sudden surge in popularity was not only surprising but also triggered a chain reaction in the market. At the same time, Nvidia&amp;rsquo;s stock experienced a sharp decline, raising concerns about its prospects among many investors. Some institutions conducted large-scale short selling during this period, suggesting everything pointed towards a &amp;ldquo;well-planned&amp;rdquo; scenario.&lt;/p&gt;
&lt;h3 id=&#34;deepseeks-rapid-rise-to-prominence-quickly-becoming-the-focus-of-attention&#34;&gt;DeepSeek&amp;rsquo;s rapid rise to prominence: quickly becoming the focus of attention
&lt;/h3&gt;&lt;p&gt;DeepSeek is an AI-based tool focused on optimizing deep learning models, particularly in the fields of natural language processing (NLP) and image generation. In the days leading up to the Spring Festival, this project suddenly garnered significant attention from a large number of investors and technical professionals. The performance of its team and the technological achievements demonstrated have sparked strong interest among many people. Discussions about DeepSeek dominate all topics in the tech circle, whether on developer communities or social media platforms.&lt;/p&gt;
&lt;p&gt;However, DeepSeek&amp;rsquo;s sudden surge in popularity is not accidental. After analysis, many people suspect that some institutions may be involved behind the scenes. Especially after its rise, Nvidiaâ€™s stock price experienced a noticeable decline, and there are clearly factors driving this change.&lt;/p&gt;
&lt;h3 id=&#34;nvidia-stock-plummets-the-force-behind-the-short-selling-operation&#34;&gt;Nvidia stock plummets: The force behind the short-selling operation
&lt;/h3&gt;&lt;p&gt;Nvidia, a global leader in graphics processing units (GPUs), has long been a key hardware provider for many large language models and AI computing. With the rapid development of the AI market, Nvidia&amp;rsquo;s stock has consistently performed strongly and even become a favored investment target for many investors. However, with the rise of DeepSeek and the marketâ€™s high attention to its technology, Nvidia&amp;rsquo;s stock experienced a sharp decline.&lt;/p&gt;
&lt;p&gt;Behind this phenomenon, there may be involved the short-selling strategies of institutional investors. In recent years, with the popularization of AI technology, Nvidia&amp;rsquo;s stock price has been highly driven up, and many investors began to believe that its stock price was overhyped. Especially after DeepSeekâ€™s explosive success, some institutions may have gained substantial profits by shorting Nvidia&amp;rsquo;s stock. By seizing precise market timing and predicting the influence of DeepSeek, these institutions successfully profited from it.&lt;/p&gt;
&lt;h3 id=&#34;exploring-large-model-thinking-chains-from-results-to-process&#34;&gt;Exploring Large Model Thinking Chains: From &amp;ldquo;Results&amp;rdquo; to &amp;ldquo;Process&amp;rdquo;
&lt;/h3&gt;&lt;p&gt;In traditional AI applications, many practitioners and investors focus more on the &amp;ldquo;results&amp;rdquo; of AI modelsâ€”such as generated images, text, and other direct outputs. However, in discussions related to DeepSeek, an increasing number of people are realizing that the thinking chain hidden behind large models is the core content that deserves more attention. In the past, we could only see the results of model output, but now, we need to understand the underlying logic, algorithms, and how to optimize model performance by adjusting these factors.&lt;/p&gt;
&lt;p&gt;This shift in thinking is, in essence, a deep reflection on AI research and application. Moving from simple black-box operations to truly understanding the internal workings of models has led many technical personnel and investors to begin reassessing the future direction of artificial intelligence. DeepSeek&amp;rsquo;s popularity exemplifies this breakthrough application, prompting people to focus on the entire model construction and optimization process, rather than just the final output.&lt;/p&gt;
&lt;h3 id=&#34;summary&#34;&gt;Summary
&lt;/h3&gt;&lt;p&gt;DeepSeek&amp;rsquo;s sudden surge in popularity, Nvidiaâ€™s stock plunge, and the short-selling operations of institutions behind the market â€“ all this seems to be part of a carefully designed scheme. Through an in-depth understanding of large language model thinking chains, we can see that the application of AI technology is not merely a superficial accumulation of features but rather a deep exploration and optimization of the model&amp;rsquo;s internal logic. As technology advances, we may witness more innovative tools like DeepSeek, driving AI research and applications to higher levels.&lt;/p&gt;
&lt;p&gt;This phenomenon not only shows us the immense potential of AI technology but also prompts us to begin thinking about the commercial competition and capital operations behind the technology. The future trend of the market will be a continued focus of the interplay between technology and capital.&lt;/p&gt;</description>
        </item>
        <item>
        <title>Cursor AI programming IDE trial</title>
        <link>https://ttf248.life/en/p/cursor-ai-programming-ide-trial/</link>
        <pubDate>Thu, 23 Jan 2025 19:30:13 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/cursor-ai-programming-ide-trial/</guid>
        <description>&lt;p&gt;Another year has gone by. The biggest change at work is the noticeably increased involvement of AI. Previously, switching between different development languages required developers to be familiar with various languages&amp;rsquo; distinct APIs. Now, these basic codes can all be generated through AI, which is a great boon for developers.&lt;/p&gt;
&lt;h2 id=&#34;chatgpt&#34;&gt;ChatGPT
&lt;/h2&gt;&lt;p&gt;Back in 2023, I revised two simple introductory articles. Now it&amp;rsquo;s 2025, and honestly, I havenâ€™t perceived any significant progress. It still requires developing your own understanding, being able to reasonably break down tasks, and most importantly, identifying whether there are bugs in the AI-generated code.&lt;/p&gt;
&lt;h2 id=&#34;github-copilot&#34;&gt;Github copilot
&lt;/h2&gt;&lt;p&gt;I forgot what day it is, but I saw in the documentation that Singapore deployed servers. For domestic use, we no longer need to constantly use a VPN. Of course, you still need a VPN to log in, but this VPN only needs to be used once for login and can then be turned off.&lt;/p&gt;
&lt;p&gt;I use Github Copilot more often on a daily basis; this plugin can be used directly in VS Code and Visual Studio, without switching between the two programs. Compared to ChatGPT, Github Copilot offers better project support and a friendlier interaction. You can choose parts of local files to &amp;ldquo;train&amp;rdquo; the AI, so the generated code is more aligned with your project.&lt;/p&gt;
&lt;h2 id=&#34;cursor-ai&#34;&gt;Cursor AI
&lt;/h2&gt;&lt;p&gt;I recently saw a new AI programming IDE, Cursor AI. This IDE is also based on GitHub Copilot, but this IDE is even more intelligent and can help you create files directly.&lt;/p&gt;
&lt;p&gt;I tried it briefly, and it feels pretty good, but my understanding of the existing project is still not enough. When there are many local project files, large refactoring and optimization adjustments still require &lt;strong&gt;developers to break down tasks&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s an example: Switch to cursoâ€™s engineering mode, enter the following content: Create a personal resume webpage that supports switching between multiple different styles. Remember to fill in some personal information for data display.&lt;/p&gt;
&lt;p&gt;After a few back-and-forths, you can obtain the following webpage. Of course, this webpage is still relatively simple, but it&amp;rsquo;s quite good for beginners.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Currently, registered users can enjoy 150 free trials of the advanced API, while paying users are limited to 5000 calls&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Resume&lt;/p&gt;</description>
        </item>
        <item>
        <title>AI programming and task decomposition</title>
        <link>https://ttf248.life/en/p/ai-programming-and-task-decomposition/</link>
        <pubDate>Fri, 22 Dec 2023 08:44:26 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/ai-programming-and-task-decomposition/</guid>
        <description>&lt;p&gt;Two years ago, I added a copy function for the code area on the site. It took almost half a day of effort, and the final rendering effect was not ideal. As a novice front-end developer, I didn&amp;rsquo;t bother to improve it; as long as it worked, that was fine. This year, using &lt;code&gt;AI&lt;/code&gt; to develop a mini program, Iâ€™m also more familiar with front-end development, so I rebuilt (redesigned by AI) it.&lt;/p&gt;
&lt;p&gt;Understanding the Dao: The task has been broken down into many small tasks, and AI development is still smooth. For a previously defunct mini-program, 75% of the code was provided by AI, and it was delivered through multiple tasks, with manual assembly of project code.&lt;/p&gt;
&lt;h2 id=&#34;previous-text&#34;&gt;Previous text
&lt;/h2&gt;&lt;p&gt;Adding a code copy button to what seems like a simple thing&lt;/p&gt;
&lt;h2 id=&#34;main-body&#34;&gt;Main body
&lt;/h2&gt;&lt;p&gt;Design CSS and JavaScript code to locate webpage elements with the class &amp;ldquo;highlight&amp;rdquo;. Within the container of these elements, add a copy button to implement the functionality of copying the code.&lt;/p&gt;
&lt;p&gt;The first draft of the code looks problematic at a glance. A second draft was generated, which is basically usable, but contains incorrect copied content and redundant text. Further optimization is needed, with manual specification of core color design and included effects.&lt;/p&gt;
&lt;p&gt;We understand you have specific requirements for the button&amp;rsquo;s size and color. We can redesign the button style to accommodate these requirements. Here is an adjusted CSS style, which includes the color scheme you provided and optimizes the buttonâ€™s dimensions.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-css&#34;&gt;.copy-btn {
  position: absolute;
  top: 2px; /* è°ƒæ•´ä½ç½® */
  right: 2px;
  padding: 3px 6px; /* ç¼©å°å°ºå¯¸ */
  font-size: 12px; /* å‡å°å­—ä½“å¤§å° */
  background-color: #8FD1E1; /* æŒ‰é’®èƒŒæ™¯è‰² */
  color: #272822; /* æ–‡å­—é¢œè‰² */
  border: 1px solid #272822; /* è¾¹æ¡†é¢œè‰² */
  border-radius: 3px; /* åœ†è§’ */
  cursor: pointer;
  opacity: 0.7; /* é»˜è®¤åŠé€æ˜ */
  transition: opacity 0.3s ease, background-color 0.3s ease; /* å¹³æ»‘è¿‡æ¸¡æ•ˆæœ */
}

.copy-btn:hover {
  opacity: 1; /* é¼ æ ‡æ‚¬åœæ—¶ä¸é€æ˜ */
  background-color: #A3D4E5; /* é¼ æ ‡æ‚¬åœæ—¶çš„èƒŒæ™¯è‰² */
}

.highlight {
  position: relative;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this style, I have reduced the size of the buttons and font size to make them more refined. At the same time, the default state of the button is semi-transparent, and when the mouse hovers over it, it becomes opaque, and the background color will become slightly brighter to provide visual feedback. This design both meets your color requirements and maintains a modern and user-friendly feel.&lt;/p&gt;
&lt;p&gt;A simpler JavaScript was provided accordingly&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-JavaScript&#34;&gt;document.addEventListener(&#39;DOMContentLoaded&#39;, function() {
  // æŸ¥æ‰¾æ‰€æœ‰ .highlight å…ƒç´ 
  const highlights = document.querySelectorAll(&#39;.highlight&#39;);

  highlights.forEach(function(highlight) {
    // åˆ›å»ºå¤åˆ¶æŒ‰é’®
    const copyBtn = document.createElement(&#39;button&#39;);
    copyBtn.textContent = &#39;å¤åˆ¶&#39;;
    copyBtn.classList.add(&#39;copy-btn&#39;);

    // å°†æŒ‰é’®æ·»åŠ åˆ°æ¯ä¸ª .highlight å…ƒç´ ä¸­
    highlight.appendChild(copyBtn);

    // æ·»åŠ ç‚¹å‡»äº‹ä»¶ç›‘å¬å™¨
    copyBtn.addEventListener(&#39;click&#39;, function() {
      // å‡è®¾ .highlight ä¸‹çš„ç¬¬ä¸€ä¸ª &amp;lt;code&amp;gt; å…ƒç´ åŒ…å«è¦å¤åˆ¶çš„ä»£ç 
      const code = highlight.querySelector(&#39;code&#39;) ? highlight.querySelector(&#39;code&#39;).textContent : &#39;&#39;;

      navigator.clipboard.writeText(code).then(() =&amp;gt; {
        // æ˜¾ç¤ºå¤åˆ¶æˆåŠŸä¿¡æ¯
        copyBtn.textContent = &#39;å·²å¤åˆ¶!&#39;;
        setTimeout(() =&amp;gt; {
          copyBtn.textContent = &#39;å¤åˆ¶&#39;;
        }, 2000);
      }).catch(err =&amp;gt; {
        console.error(&#39;å¤åˆ¶å¤±è´¥: &#39;, err);
      });
    });
  });
});
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;afterword&#34;&gt;Afterword
&lt;/h2&gt;&lt;p&gt;Although I would prefer a copy button to be added in the top area of the code region, due to my lack of understanding of frontend development, I couldn&amp;rsquo;t accurately describe the function based on keywords. After several rounds of troubleshooting with &lt;code&gt;AI&lt;/code&gt;, it still didnâ€™t reach the expected result.&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s just that I don&amp;rsquo;t know how much impact AI will have on software development as time goes on&lt;/p&gt;</description>
        </item>
        <item>
        <title>Prompt Engineer</title>
        <link>https://ttf248.life/en/p/prompt-engineer/</link>
        <pubDate>Sun, 26 Mar 2023 20:46:53 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/prompt-engineer/</guid>
        <description>&lt;p&gt;Just as we learned techniques for using search engines back in the day, we also need to learn some techniques for communicating with AI â€“ providing reasonable and sufficient constraints to efficiently obtain the answers we need&lt;/p&gt;
&lt;p&gt;If you look at it from a different angle, the current AI is like a child with excellent memory; it has the ability to remember everything and can copy homework. What we need to do is learn how to communicate with AI correctly and effectively, describe our needs precisely, and help AI generate the expected results.&lt;/p&gt;
&lt;h2 id=&#34;popular-science&#34;&gt;Popular science
&lt;/h2&gt;&lt;p&gt;The &lt;code&gt;AI&lt;/code&gt; that bursts forth like a shooting star, more accurately known as &lt;code&gt;Generative Pre-Training&lt;/code&gt;, translates literally to generative pre-training. It is a text generation deep learning model trained on internet-available data and used for tasks such as question answering, text summarization, machine translation, classification, code generation, and conversational AI. There are currently different versions of the model, including GPT-1, GPT-2, GPT-3, and GPT-4, each version being larger and more powerful than its predecessor.&lt;/p&gt;
&lt;h2 id=&#34;does-it-really-have-intelligence&#34;&gt;Does it really have intelligence?
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;The higher the similarity, the higher the accuracy&lt;/li&gt;
&lt;li&gt;Basic, repetitive tasks no longer require human intervention after specific training&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Generative AI is a technology that utilizes existing data such as text, audio, and images to create new content. It can be used for various tasks including text generation, speech synthesis, image generation, and dialogue systems. The logic of generative AI depends on its training data and model structure. Generally speaking, generative AI can follow grammar, logic, and common sense to a certain extent, but it may also produce errors, biases, or untrue content. Therefore, the output of generative AI requires human judgment and verification and should not be blindly trusted or used.&lt;/p&gt;
&lt;h2 id=&#34;prompt-engineer&#34;&gt;Prompt Engineer
&lt;/h2&gt;&lt;p&gt;The river of time doesn&amp;rsquo;t flow backward; people need to learn to adapt to the trend. You can think of &lt;code&gt;AI&lt;/code&gt; as not intelligent, lacking logic, and often producing unusable code.&lt;/p&gt;
&lt;p&gt;If you look at it from a different angle, the current AI is like a child with excellent memory; it has the ability to remember everything and can copy homework. What we need to do is learn how to communicate with AI correctly and effectively, describe our needs precisely, and help AI generate the expected results.&lt;/p&gt;
&lt;h2 id=&#34;dialogue-mode&#34;&gt;Dialogue mode
&lt;/h2&gt;&lt;p&gt;Two years after &lt;code&gt;GitHub Copilot&lt;/code&gt; was released, no one could have predicted that two years later, &lt;code&gt;OpenAI&lt;/code&gt; would emerge, allowing humans to recognize the capabilities of large language models&lt;/p&gt;
&lt;p&gt;Based on &lt;strong&gt;annotation-based programming&lt;/strong&gt; and &lt;strong&gt;dialogue-based programming&lt;/strong&gt;, the interactive logic is completely different. Dialogue&amp;rsquo;s pattern provides a user-friendly interaction experience for novice users. It must be mentioned that &lt;code&gt;NewBing&lt;/code&gt; offers follow-up prompts after each question, with Microsoft attempting to guide users to obtain more content from the &lt;code&gt;AI&lt;/code&gt; knowledge base.&lt;/p&gt;
&lt;h2 id=&#34;for-example&#34;&gt;For example
&lt;/h2&gt;&lt;p&gt;The high-speed train journey is long, and coding is a good option. With limited data, processing several images and obtaining image resources is fine, but downloading software to process them would be a bit extravagant, and the network conditions don&amp;rsquo;t allow it.&lt;/p&gt;
&lt;p&gt;We have a batch of image assets with transparent backgrounds to be used as navigation bar icons for a mini-program. The selected state should be in color, and the unselected state should be black and white. The original assets are all in color and need some preprocessing.&lt;/p&gt;
&lt;p&gt;Write a Python code to read all images from a folder, convert them to black and white, maintain the transparent background, optimize the image quality when saving to reduce file size, and display the comparison of the file sizes before and after optimization in the console&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# å¯¼å…¥å¿…è¦çš„åº“
import os
import PIL
from PIL import Image

# å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œå°†å›¾ç‰‡è½¬æ¢æˆé»‘ç™½ï¼Œç»´æŒé€æ˜èƒŒæ™¯ï¼Œä¿å­˜å›¾ç‰‡ï¼Œå¹¶è¿”å›æ–‡ä»¶ä½“ç§¯
def convert_and_save(image_file):
    # æ‰“å¼€å›¾ç‰‡
    image = Image.open(image_file)
    # è·å–å›¾ç‰‡çš„æ¨¡å¼ï¼Œå¦‚æœæ˜¯ RGBA æ¨¡å¼ï¼Œè¯´æ˜æœ‰é€æ˜èƒŒæ™¯
    mode = image.mode
    if mode == &amp;quot;RGBA&amp;quot;:
        # åˆ›å»ºä¸€ä¸ªå’Œå›¾ç‰‡å¤§å°ä¸€è‡´çš„ç™½è‰²èƒŒæ™¯å›¾ç‰‡
        background = Image.new(&amp;quot;RGB&amp;quot;, image.size, (255, 255, 255))
        # å°†åŸå›¾ç‰‡ç²˜è´´åˆ°ç™½è‰²èƒŒæ™¯ä¸Šï¼Œå¿½ç•¥é€æ˜åƒç´ 
        background.paste(image, mask=image.split()[3])
        # å°†åˆæˆçš„å›¾ç‰‡è½¬æ¢æˆç°åº¦æ¨¡å¼
        gray_image = background.convert(&amp;quot;L&amp;quot;)
        # å°†ç°åº¦å›¾ç‰‡å†è½¬æ¢æˆ RGBA æ¨¡å¼ï¼Œä»¥ä¾¿ä¿ç•™é€æ˜èƒŒæ™¯
        final_image = gray_image.convert(&amp;quot;RGBA&amp;quot;)
    else:
        # å¦‚æœä¸æ˜¯ RGBA æ¨¡å¼ï¼Œç›´æ¥å°†å›¾ç‰‡è½¬æ¢æˆç°åº¦æ¨¡å¼
        final_image = image.convert(&amp;quot;L&amp;quot;)
    # è·å–åŸå›¾ç‰‡çš„æ–‡ä»¶åå’Œæ‰©å±•å
    file_name, file_ext = os.path.splitext(image_file)
    # å®šä¹‰æ–°å›¾ç‰‡çš„æ–‡ä»¶åï¼Œæ·»åŠ  _bw åç¼€è¡¨ç¤ºé»‘ç™½
    new_file_name = file_name + &amp;quot;_bw&amp;quot; + file_ext
    # ä¿å­˜æ–°å›¾ç‰‡ï¼Œå¹¶ä¼˜åŒ–è´¨é‡ï¼Œå‡å°‘æ–‡ä»¶ä½“ç§¯
    final_image.save(new_file_name, optimize=True)
    # è·å–åŸå›¾ç‰‡å’Œæ–°å›¾ç‰‡çš„æ–‡ä»¶ä½“ç§¯ï¼Œå¹¶è¿”å›
    old_size = os.path.getsize(image_file)
    new_size = os.path.getsize(new_file_name)
    return old_size, new_size

# å®šä¹‰ä¸€ä¸ªæ–‡ä»¶å¤¹è·¯å¾„ï¼Œå­˜æ”¾æ‰€æœ‰çš„å›¾ç‰‡
folder_path = &amp;quot;images&amp;quot;
# éå†æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰æ–‡ä»¶
for file in os.listdir(folder_path):
    # æ‹¼æ¥å®Œæ•´çš„æ–‡ä»¶è·¯å¾„
    file_path = os.path.join(folder_path, file)
    # åˆ¤æ–­æ˜¯å¦æ˜¯å›¾ç‰‡æ–‡ä»¶ï¼Œæ ¹æ®æ‰©å±•ååˆ¤æ–­ï¼Œå¯ä»¥æ ¹æ®éœ€è¦æ·»åŠ æˆ–ä¿®æ”¹æ‰©å±•å
    if file_path.endswith((&amp;quot;.png&amp;quot;, &amp;quot;.jpg&amp;quot;, &amp;quot;.jpeg&amp;quot;, &amp;quot;.gif&amp;quot;)):
        # è°ƒç”¨å‡½æ•°ï¼Œè½¬æ¢å¹¶ä¿å­˜å›¾ç‰‡ï¼Œå¹¶è·å–æ–‡ä»¶ä½“ç§¯
        old_size, new_size = convert_and_save(file_path)
        # åœ¨æ§åˆ¶å°å±•ç¤ºä¼˜åŒ–å‰åçš„æ–‡ä»¶ä½“ç§¯å¤§å°å¯¹æ¯”
        print(f&amp;quot;{file}: {old_size} bytes -&amp;gt; {new_size} bytes&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;AI quickly provided an answer, it looks good and works fine, but experienced programmers will find the code below not very general and unsuitable for production environments&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Parameters are fixed&lt;/li&gt;
&lt;li&gt;There are no log files; the information is printed to the console&lt;/li&gt;
&lt;li&gt;Image processing tasks are suitable for multithreaded models&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;NewBing is here, following prompts, and selected a question: &lt;strong&gt;Are there any areas for improvement in this code?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Honestly, this answer completely exceeded expectations. The AI provided corresponding suggestions for optimizations that a typical programmer would think of.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;è¿™æ®µä»£ç å¯èƒ½è¿˜æœ‰ä¸€äº›å¯ä»¥æ”¹è¿›çš„åœ°æ–¹ï¼Œæ¯”å¦‚ï¼š

å¯ä»¥ä½¿ç”¨ argparse åº“ï¼Œè®©ç”¨æˆ·å¯ä»¥é€šè¿‡å‘½ä»¤è¡Œå‚æ•°æŒ‡å®šæ–‡ä»¶å¤¹è·¯å¾„å’Œæ‰©å±•åï¼Œè€Œä¸æ˜¯å†™æ­»åœ¨ä»£ç é‡Œã€‚
å¯ä»¥ä½¿ç”¨ logging åº“ï¼Œè®°å½•ç¨‹åºçš„è¿è¡Œæƒ…å†µï¼Œè€Œä¸æ˜¯ç›´æ¥æ‰“å°åˆ°æ§åˆ¶å°ã€‚
å¯ä»¥ä½¿ç”¨ try-except è¯­å¥ï¼Œæ•è·å¯èƒ½å‡ºç°çš„å¼‚å¸¸ï¼Œæ¯”å¦‚æ–‡ä»¶æ‰“å¼€å¤±è´¥ï¼Œå›¾ç‰‡è½¬æ¢å¤±è´¥ç­‰ï¼Œå¹¶åšç›¸åº”çš„å¤„ç†ã€‚
å¯ä»¥ä½¿ç”¨ multiprocessing åº“ï¼Œåˆ©ç”¨å¤šæ ¸å¤„ç†å™¨ï¼ŒåŠ é€Ÿå›¾ç‰‡çš„è½¬æ¢å’Œä¿å­˜è¿‡ç¨‹ã€‚
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A very friendly response, considering the differences in deployment environments, supporting single-process execution is permitted&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;å½“ç„¶ï¼Œæˆ‘å¾ˆä¹æ„å¸®ä½ ä¿®æ”¹ä¸€ä¸‹ä»£ç ã€‚æˆ‘å°è¯•æ·»åŠ äº† argparse, logging å’Œ try-except è¯­å¥ï¼Œä½†æ²¡æœ‰æ·»åŠ  multiprocessing åº“ï¼Œå› ä¸ºæˆ‘ä¸ç¡®å®šä½ çš„ç”µè„‘æ˜¯å¦æ”¯æŒå¤šæ ¸å¤„ç†ã€‚ä¿®æ”¹åçš„ä»£ç å¦‚ä¸‹ï¼š
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# å¯¼å…¥å¿…è¦çš„åº“
import argparse
import logging
import multiprocessing
import os

from PIL import Image


# å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œå°†å›¾ç‰‡è½¬æ¢æˆé»‘ç™½ï¼Œç»´æŒé€æ˜èƒŒæ™¯ï¼Œä¿å­˜å›¾ç‰‡ï¼Œå¹¶è¿”å›æ–‡ä»¶ä½“ç§¯
def convert_and_save(image_file):
    # æ‰“å¼€å›¾ç‰‡
    try:
        image = Image.open(image_file)
    except Exception as e:
        logging.error(f&amp;quot;Failed to open {image_file}: {e}&amp;quot;)
        return None, None
    # è·å–å›¾ç‰‡çš„æ¨¡å¼ï¼Œå¦‚æœæ˜¯ RGBA æ¨¡å¼ï¼Œè¯´æ˜æœ‰é€æ˜èƒŒæ™¯
    mode = image.mode
    if mode == &amp;quot;RGBA&amp;quot;:
        # åˆ›å»ºä¸€ä¸ªå’Œå›¾ç‰‡å¤§å°ä¸€è‡´çš„ç™½è‰²èƒŒæ™¯å›¾ç‰‡
        background = Image.new(&amp;quot;RGB&amp;quot;, image.size, (255, 255, 255))
        # å°†åŸå›¾ç‰‡ç²˜è´´åˆ°ç™½è‰²èƒŒæ™¯ä¸Šï¼Œå¿½ç•¥é€æ˜åƒç´ 
        background.paste(image, mask=image.split()[3])
        # å°†åˆæˆçš„å›¾ç‰‡è½¬æ¢æˆç°åº¦æ¨¡å¼
        gray_image = background.convert(&amp;quot;L&amp;quot;)
        # å°†ç°åº¦å›¾ç‰‡å†è½¬æ¢æˆ RGBA æ¨¡å¼ï¼Œä»¥ä¾¿ä¿ç•™é€æ˜èƒŒæ™¯
        final_image = gray_image.convert(&amp;quot;RGBA&amp;quot;)
    else:
        # å¦‚æœä¸æ˜¯ RGBA æ¨¡å¼ï¼Œç›´æ¥å°†å›¾ç‰‡è½¬æ¢æˆç°åº¦æ¨¡å¼
        final_image = image.convert(&amp;quot;L&amp;quot;)
    # è·å–åŸå›¾ç‰‡çš„æ–‡ä»¶åå’Œæ‰©å±•å
    file_name, file_ext = os.path.splitext(image_file)
    # å®šä¹‰æ–°å›¾ç‰‡çš„æ–‡ä»¶åï¼Œæ·»åŠ  _bw åç¼€è¡¨ç¤ºé»‘ç™½
    new_file_name = file_name + &amp;quot;_bw&amp;quot; + file_ext
    # ä¿å­˜æ–°å›¾ç‰‡ï¼Œå¹¶ä¼˜åŒ–è´¨é‡ï¼Œå‡å°‘æ–‡ä»¶ä½“ç§¯
    try:
        final_image.save(new_file_name, optimize=True)
    except Exception as e:
        logging.error(f&amp;quot;Failed to save {new_file_name}: {e}&amp;quot;)
        return None, None
    # è·å–åŸå›¾ç‰‡å’Œæ–°å›¾ç‰‡çš„æ–‡ä»¶ä½“ç§¯ï¼Œå¹¶è¿”å›
    old_size = os.path.getsize(image_file)
    new_size = os.path.getsize(new_file_name)
    return file_name, old_size, new_size

# å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œè§£æå‘½ä»¤è¡Œå‚æ•°ï¼Œå¹¶è¿”å›æ–‡ä»¶å¤¹è·¯å¾„å’Œæ‰©å±•ååˆ—è¡¨
def parse_args():
    # åˆ›å»ºä¸€ä¸ªè§£æå™¨å¯¹è±¡
    parser = argparse.ArgumentParser(description=&amp;quot;Convert images to black and white and optimize quality.&amp;quot;)
    # æ·»åŠ ä¸€ä¸ªå¿…é€‰çš„ä½ç½®å‚æ•°ï¼Œè¡¨ç¤ºæ–‡ä»¶å¤¹è·¯å¾„
    parser.add_argument(&amp;quot;folder_path&amp;quot;, help=&amp;quot;The path of the folder that contains the images.&amp;quot;)
    # æ·»åŠ ä¸€ä¸ªå¯é€‰çš„å‚æ•°ï¼Œè¡¨ç¤ºæ‰©å±•ååˆ—è¡¨ï¼Œé»˜è®¤ä¸º png, jpg, jpeg å’Œ gif
    parser.add_argument(&amp;quot;-e&amp;quot;, &amp;quot;--extensions&amp;quot;, nargs=&amp;quot;+&amp;quot;, default=[&amp;quot;.png&amp;quot;, &amp;quot;.jpg&amp;quot;, &amp;quot;.jpeg&amp;quot;, &amp;quot;.gif&amp;quot;], help=&amp;quot;The extensions of the image files.&amp;quot;)
    # è§£æå‘½ä»¤è¡Œå‚æ•°ï¼Œå¹¶è¿”å›ç»“æœå¯¹è±¡
    args = parser.parse_args()
    return args.folder_path, args.extensions

# å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œæ‰“å°ä¼˜åŒ–å‰åçš„æ–‡ä»¶ä½“ç§¯å¤§å°å¯¹æ¯”
def print_result(result):
    # å¦‚æœç»“æœä¸ä¸ºç©ºï¼Œè¯´æ˜è½¬æ¢å’Œä¿å­˜æˆåŠŸ
    if result:
        # è§£åŒ…ç»“æœä¸ºæ–‡ä»¶åå’Œæ–‡ä»¶ä½“ç§¯å…ƒç»„
        if len(result) == 3:
            file, old_size, new_size = result
            # åœ¨æ§åˆ¶å°å±•ç¤ºä¼˜åŒ–å‰åçš„æ–‡ä»¶ä½“ç§¯å¤§å°å¯¹æ¯”
            logging.info(f&amp;quot;{file}: {old_size} bytes -&amp;gt; {new_size} bytes&amp;quot;)
        else:
            logging.info(f&amp;quot;{result}&amp;quot;)

# é…ç½®æ—¥å¿—è®°å½•å™¨ï¼Œå°†æ—¥å¿—è¾“å‡ºåˆ°æ§åˆ¶å°å’Œæ–‡ä»¶ä¸­ï¼Œè®¾ç½®æ—¥å¿—ç­‰çº§ä¸º INFO
logging.basicConfig(level=logging.INFO,
                    format=&amp;quot;%(asctime)s %(levelname)s %(message)s&amp;quot;,
                    handlers=[logging.StreamHandler(),
                              logging.FileHandler(&amp;quot;log.txt&amp;quot;)])

# è°ƒç”¨å‡½æ•°ï¼Œè·å–æ–‡ä»¶å¤¹è·¯å¾„å’Œæ‰©å±•ååˆ—è¡¨
folder_path, extensions = parse_args()

if __name__ == &amp;quot;__main__&amp;quot;:
    # Windows éœ€è¦è¿™ä¸ªå‡½æ•°çš„åŸå› æ˜¯ Windows ç¼ºä¹ fork() å‡½æ•°ï¼ˆè¿™ä¸å®Œå…¨æ­£ç¡®ï¼‰ã€‚
    # å› æ­¤ï¼Œåœ¨ Windows ä¸Šï¼Œfork() æ˜¯é€šè¿‡åˆ›å»ºä¸€ä¸ªæ–°çš„è¿›ç¨‹æ¥æ¨¡æ‹Ÿçš„ï¼Œè¿™ä¸ªæ–°çš„è¿›ç¨‹ä¸­è¿è¡Œçš„ä»£ç ï¼Œåœ¨ Linux ä¸Šæ˜¯åœ¨å­è¿›ç¨‹ä¸­è¿è¡Œçš„ã€‚
    # ç”±äºè¿™äº›ä»£ç è¦åœ¨æŠ€æœ¯ä¸Šæ— å…³çš„è¿›ç¨‹ä¸­è¿è¡Œï¼Œæ‰€ä»¥å®ƒå¿…é¡»åœ¨è¿è¡Œä¹‹å‰è¢«ä¼ é€’åˆ°é‚£é‡Œã€‚
    # ä¼ é€’çš„æ–¹å¼æ˜¯å…ˆå°†å®ƒä»¬åºåˆ—åŒ–ï¼Œç„¶åé€šè¿‡ç®¡é“ä»åŸå§‹è¿›ç¨‹å‘é€åˆ°æ–°çš„è¿›ç¨‹ã€‚
    # å¦å¤–ï¼Œè¿™ä¸ªæ–°çš„è¿›ç¨‹è¢«é€šçŸ¥å®ƒå¿…é¡»è¿è¡Œé€šè¿‡ç®¡é“ä¼ é€’çš„ä»£ç ï¼Œé€šè¿‡å‘å®ƒä¼ é€’ --multiprocessing-fork å‘½ä»¤è¡Œå‚æ•°ã€‚
    # å¦‚æœä½ çœ‹ä¸€ä¸‹ freeze_support() å‡½æ•°çš„å®ç°ï¼Œå®ƒçš„ä»»åŠ¡æ˜¯æ£€æŸ¥å®ƒè¿è¡Œåœ¨å“ªä¸ªè¿›ç¨‹ä¸­ï¼Œæ˜¯å¦åº”è¯¥è¿è¡Œé€šè¿‡ç®¡é“ä¼ é€’çš„ä»£ç 
    multiprocessing.freeze_support()

    # åˆ›å»ºä¸€ä¸ªè¿›ç¨‹æ± ï¼Œæ ¹æ®ç”µè„‘çš„æ ¸å¿ƒæ•°è‡ªåŠ¨åˆ†é…è¿›ç¨‹
    pool = multiprocessing.Pool()
    # åˆ›å»ºä¸€ä¸ªç©ºåˆ—è¡¨ï¼Œç”¨äºå­˜æ”¾å¼‚æ­¥ä»»åŠ¡çš„ç»“æœå¯¹è±¡
    results = []
    # éå†æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰æ–‡ä»¶
    for file in os.listdir(folder_path):
        # æ‹¼æ¥å®Œæ•´çš„æ–‡ä»¶è·¯å¾„
        file_path = os.path.join(folder_path, file)
        # åˆ¤æ–­æ˜¯å¦æ˜¯å›¾ç‰‡æ–‡ä»¶ï¼Œæ ¹æ®æ‰©å±•ååˆ¤æ–­ï¼Œå¯ä»¥æ ¹æ®éœ€è¦ä¿®æ”¹æ‰©å±•ååˆ—è¡¨
        if any(file_path.endswith(ext) for ext in extensions):
            # è°ƒç”¨å‡½æ•°ï¼Œè½¬æ¢å¹¶ä¿å­˜å›¾ç‰‡ï¼Œå¹¶è·å–æ–‡ä»¶ä½“ç§¯ï¼Œä½¿ç”¨å¼‚æ­¥æ–¹å¼ï¼Œä¸é˜»å¡ä¸»è¿›ç¨‹
            result = pool.apply_async(convert_and_save, args=(file_path,), callback=print_result)
            # å°†ç»“æœå¯¹è±¡æ·»åŠ åˆ°åˆ—è¡¨ä¸­
            results.append((file, result))
    # å…³é—­è¿›ç¨‹æ± ï¼Œä¸å†æ¥å—æ–°çš„ä»»åŠ¡
    pool.close()
    # ç­‰å¾…æ‰€æœ‰çš„ä»»åŠ¡å®Œæˆ
    pool.join()
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;afterword&#34;&gt;Afterword
&lt;/h2&gt;&lt;p&gt;Because local development is on a &lt;code&gt;Windows&lt;/code&gt; system, the AI&amp;rsquo;s initial answer didn&amp;rsquo;t include a &lt;code&gt;main&lt;/code&gt; function or &lt;code&gt;multiprocessing.freeze_support&lt;/code&gt;. The code was fixed by tracing the error messages that occurred through follow-up questions.&lt;/p&gt;
&lt;p&gt;Just as we learned techniques for using search engines back in the day, we also need to learn some techniques for communicating with AI â€“ providing reasonable and sufficient constraints to efficiently obtain the answers we need&lt;/p&gt;
&lt;p&gt;Note: &lt;strong&gt;If you are a programming novice and have any questions about the code based on the comments, feel free to ask for clarification.&lt;/strong&gt;&lt;/p&gt;</description>
        </item>
        <item>
        <title>AI-assisted programming, an evolution of productivity</title>
        <link>https://ttf248.life/en/p/ai-assisted-programming-productivity-evolution/</link>
        <pubDate>Tue, 28 Feb 2023 17:05:17 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/ai-assisted-programming-productivity-evolution/</guid>
        <description>&lt;p&gt;GitHub Copilot was released less than two years ago, and then ChatGPT appeared. I don&amp;rsquo;t fully understand the underlying principles, but Iâ€™ve used both for a while. The two tools offer completely different levels of assistance, but they both significantly improve productivity.&lt;/p&gt;
&lt;p&gt;Very complex things, AI still can&amp;rsquo;t do. After all, they lack logic; things that are routine or have a fixed paradigm, if the training data is sufficient, AI can achieve about 90% effectiveness.&lt;/p&gt;
&lt;h2 id=&#34;github-copilot&#34;&gt;github copilot
&lt;/h2&gt;&lt;p&gt;When it was released, looking at the demo introduction on the official website, it didn&amp;rsquo;t seem very smart, and after trying it out, it wasn&amp;rsquo;t very usable, so I gave up&lt;/p&gt;
&lt;p&gt;Before 2022, when I had nothing else to do, I switched to the new version to try it out, and the effect is already pretty good. The only problem is that the speed in China is a bit slow, probably due to network issues. Code written based on comments using languages like &lt;code&gt;Python&lt;/code&gt; is quite decent; it&amp;rsquo;s much faster than looking up information myself and writing code from scratch.&lt;/p&gt;
&lt;h2 id=&#34;chatgpt&#34;&gt;ChatGPT
&lt;/h2&gt;&lt;p&gt;GitHub Copilot is still designed from a programmer&amp;rsquo;s, or developer&amp;rsquo;s, perspective. You need to understand how to program and gradually guide the AI with comments to assist in programming.&lt;/p&gt;
&lt;p&gt;ChatGPT has reached the next level, &lt;strong&gt;making it easy for even non-experts to get started&lt;/strong&gt;. With its conversational and interactive design, you can freely ask questions, describe your needs, and continue asking if there&amp;rsquo;s anything you don&amp;rsquo;t understand.&lt;/p&gt;
&lt;p&gt;For example, you understand the concept of tokenization but don&amp;rsquo;t know how to code it, and want to calculate string similarity:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;Python åˆ†è¯è®¡ç®— ç¶­å¤©é‹é€š LOGORY å’Œ åˆè‚¥ç¶­å¤©é‹é€šä¿¡æ¯ç§‘æŠ€è‚¡ä»½æœ‰é™å…¬å¸ ä¸¤ä¸ªå­—ç¬¦ä¸²çš„ç›¸ä¼¼åº¦ï¼›
æ¯”è¾ƒå‰ï¼Œç»Ÿä¸€å¤„ç†å­—ç¬¦ä¸²ï¼Œè‹±æ–‡è½¬æ¢ä¸ºå°å†™ï¼›
ç¬¬äºŒä¸ªå­—ç¬¦ä¸­å¦‚æœèƒ½æ‰¾åˆ°ç¬¬ä¸€ä¸ªå­—ç¬¦çš„é¦–å­—ç¬¦ï¼Œå°†ç¬¬äºŒä¸ªå­—ç¬¦ä¸²æˆªä½ï¼Œæˆªä½çš„é•¿åº¦ä¸ºç¬¬ä¸€ä¸ªå­—ç¬¦ä¸²çš„é•¿åº¦
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Hello, this is Bing. ğŸ˜Š&lt;/p&gt;
&lt;p&gt;According to online information 123, Python tokenization can be implemented using the jieba library. The jieba library supports three segmentation modes: precise mode, full mode, and search engine mode. You can choose the appropriate mode based on your needs.&lt;/p&gt;
&lt;p&gt;To compare the similarity of two strings, you first need to tokenize them and then calculate indicators such as word vectors or edit distance. You can use libraries like NumPy or SciPy for mathematical operations.&lt;/p&gt;
&lt;p&gt;To uniformly handle strings, converting them to lowercase can be achieved using Python&amp;rsquo;s built-in function lower()&lt;/p&gt;
&lt;p&gt;To extract the second string, you can use Python&amp;rsquo;s slicing operator []&lt;/p&gt;
&lt;p&gt;Here is a possible code example:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import jieba
import numpy as np
from scipy.spatial.distance import cosine

# å®šä¹‰ä¸¤ä¸ªå­—ç¬¦ä¸²
str1 = &amp;quot;ç¶­å¤©é‹é€š LOGORY&amp;quot;
str2 = &amp;quot;åˆè‚¥ç¶­å¤©é‹é€šä¿¡æ¯ç§‘æŠ€è‚¡ä»½æœ‰é™å…¬å¸&amp;quot;

# ç»Ÿä¸€å¤„ç†å­—ç¬¦ä¸²ï¼Œè‹±æ–‡è½¬æ¢ä¸ºå°å†™
str1 = str1.lower()
str2 = str2.lower()

# æ‰¾åˆ°ç¬¬äºŒä¸ªå­—ç¬¦ä¸²ä¸­ç¬¬ä¸€ä¸ªå­—ç¬¦çš„ä½ç½®
index = str2.find(str1[0])

# å¦‚æœæ‰¾åˆ°äº†ï¼Œæˆªå–ç¬¬äºŒä¸ªå­—ç¬¦ä¸²
if index != -1:
    str2 = str2[index:index+len(str1)]

# ä½¿ç”¨ jieba åº“è¿›è¡Œåˆ†è¯
words1 = list(jieba.cut(str1))
words2 = list(jieba.cut(str2))

# è®¡ç®—ä¸¤ä¸ªå­—ç¬¦ä¸²çš„è¯å‘é‡ï¼ˆä»¥è¯é¢‘ä½œä¸ºæƒé‡ï¼‰
vector1 = np.zeros(len(words1))
vector2 = np.zeros(len(words1))

for i, word in enumerate(words1):
    vector1[i] += 1 # ç»Ÿè®¡ç¬¬ä¸€ä¸ªå­—ç¬¦ä¸²ä¸­æ¯ä¸ªè¯å‡ºç°çš„æ¬¡æ•°
    if word in words2:
        vector2[i] += 1 # ç»Ÿè®¡ç¬¬äºŒä¸ªå­—ç¬¦ä¸²ä¸­æ¯ä¸ªè¯å‡ºç°çš„æ¬¡æ•°

# è®¡ç®—ä¸¤ä¸ªå­—ç¬¦ä¸²çš„ç›¸ä¼¼åº¦ï¼ˆä»¥ä½™å¼¦è·ç¦»ä½œä¸ºæŒ‡æ ‡ï¼‰
similarity = 1 - cosine(vector1, vector2)

print(&amp;quot;ä¸¤ä¸ªå­—ç¬¦ä¸²çš„ç›¸ä¼¼åº¦æ˜¯ï¼š&amp;quot;, similarity)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To take it a step further, I consulted the results of the run and congratulated you on discovering a problem â€“ the results were fabricated. It didn&amp;rsquo;t actually execute any code but gave a decent result.&lt;/p&gt;
&lt;h3 id=&#34;chestnuts&#34;&gt;Chestnuts
&lt;/h3&gt;&lt;p&gt;Question: SQL statement, retrieve records from the product code table where the product codes are within the range of 80000-89999&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;ä½ å¥½ï¼Œè¿™æ˜¯ Bingã€‚ğŸ˜Š

æ ¹æ®æœç´¢ç»“æœ12ï¼Œä½ å¯ä»¥ä½¿ç”¨ SQL BETWEEN è¿ç®—ç¬¦æ¥é€‰æ‹©ç»™å®šåŒºé—´å†…çš„å€¼ã€‚BETWEEN è¿ç®—ç¬¦åŒ…å«äº†å¼€å§‹å’Œç»“æŸçš„å€¼ã€‚

ä½ éœ€è¦å…ˆæŒ‡å®šè¦æŸ¥è¯¢çš„è¡¨åå’Œåˆ—åï¼Œç„¶ååœ¨ WHERE å­å¥ä¸­ä½¿ç”¨ BETWEEN è¿ç®—ç¬¦å’ŒåŒºé—´å€¼ã€‚

ä¾‹å¦‚ï¼Œå¦‚æœä½ è¦æŸ¥è¯¢äº§å“ä»£ç è¡¨ä¸­ï¼Œäº§å“ä»£ç åœ¨ 80000 åˆ° 89999 ä¹‹é—´çš„è®°å½•ï¼Œä½ å¯ä»¥ä½¿ç”¨ä»¥ä¸‹è¯­å¥ï¼š

SELECT * FROM product_code
WHERE code BETWEEN 80000 AND 89999;
è¿™é‡Œï¼Œ* è¡¨ç¤ºé€‰æ‹©æ‰€æœ‰åˆ—3ã€‚å¦‚æœä½ åªæƒ³é€‰æ‹©ç‰¹å®šçš„åˆ—ï¼Œæ¯”å¦‚äº§å“åç§°å’Œä»·æ ¼ï¼Œä½ å¯ä»¥ç”¨é€—å·åˆ†éš”å®ƒä»¬ï¼Œå¦‚ä¸‹ï¼š

SELECT name, price FROM product_code
WHERE code BETWEEN 80000 AND 89999;
å¸Œæœ›è¿™èƒ½å¸®åŠ©åˆ°ä½ ã€‚
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Keep asking, negate it: The product code is a string, your code is wrong&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;æŠ±æ­‰ï¼Œæˆ‘æ²¡æœ‰æ³¨æ„åˆ°äº§å“ä»£ç æ˜¯å­—ç¬¦ä¸²ã€‚å¦‚æœæ˜¯è¿™æ ·çš„è¯ï¼Œä½ éœ€è¦åœ¨åŒºé—´å€¼ä¸¤è¾¹åŠ ä¸Šå•å¼•å·ï¼Œå¦‚ä¸‹ï¼š

SELECT * FROM product_code
WHERE code BETWEEN &#39;80000&#39; AND &#39;89999&#39;;
è¯·åŸè°…æˆ‘çš„é”™è¯¯ã€‚ğŸ™
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Congratulations, it will make a mistake&lt;/p&gt;</description>
        </item>
        
    </channel>
</rss>
