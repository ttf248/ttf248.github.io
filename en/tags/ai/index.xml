<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Ai on Uncle Xiang&#39;s Notebook</title>
        <link>https://ttf248.life/en/tags/ai/</link>
        <description>Recent content in Ai on Uncle Xiang&#39;s Notebook</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en</language>
        <lastBuildDate>Thu, 06 Nov 2025 01:40:10 +0800</lastBuildDate><atom:link href="https://ttf248.life/en/tags/ai/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>Command-line AI Coding Interaction</title>
        <link>https://ttf248.life/en/p/command-line-ai-coding-interaction/</link>
        <pubDate>Wed, 05 Nov 2025 14:12:46 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/command-line-ai-coding-interaction/</guid>
        <description>&lt;ul&gt;
&lt;li&gt;The convention is to open Trae and prepare to start coding, and a notification arrives: the Claude model has been shut down and cannot be used; it’s highly likely that it won&amp;rsquo;t be recovered. The official provided a compensation plan, increasing usage by 300 (as of January).&lt;/li&gt;
&lt;li&gt;Checking it out, as expected, Anthropic is following US regulations to prohibit domestic companies from continuing to use the Claude series models. I joined the trae Discord community and saw many people complaining about the shutdown of the Claude model – after all, most people came here for Claude. The signs had already appeared before the Claude 4.5 model was synchronized on Trae; it hadn&amp;rsquo;t launched.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;attempt&#34;&gt;Attempt
&lt;/h2&gt;&lt;p&gt;With a last-ditch effort, I experimented with other models that are still supported, including OpenAI’s gpt-3.5-turbo, gpt-4, and Google’s Gemini Pro.&lt;/p&gt;
&lt;p&gt;How to put it… the results weren&amp;rsquo;t very ideal. I don’t know how the Trae offshore team developed them; according to logic, there shouldn’t be such a huge difference. The prompts used for testing were the same as those I practiced with on my previous hand-built project: Xiaolan Shu (Little Blue Book), which I had written about in articles before.&lt;/p&gt;
&lt;p&gt;Adding to that, I was dissatisfied with the Trae IDE itself, so I emailed the Trae team requesting a refund.&lt;/p&gt;
&lt;h2 id=&#34;changes&#34;&gt;Changes
&lt;/h2&gt;&lt;p&gt;It wasn’t misremembered – Google released the first terminal-based AI programming, which was more general-purpose than daily smart suggestions in IDEs and allowed developers to continue using their original development environments.&lt;/p&gt;
&lt;p&gt;OpenAI and Anthropic both released Claude Code and Codex; these tools and models aren&amp;rsquo;t completely tied together, and you can integrate other models by modifying configuration files.&lt;/p&gt;
&lt;p&gt;In the Discord community, someone mentioned minimax m2 and glm4 – both are small blue book projects domestically. I tried the former and it was pretty good.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Installation requires a VPN connection. To switch between different models, I recommend: &lt;a class=&#34;link&#34; href=&#34;https://github.com/farion1231/cc-switch&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/farion1231/cc-switch&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;claude-code&#34;&gt;claude code
&lt;/h2&gt;&lt;p&gt;Depends on node js, command: npm install -g @anthropic-ai/claude-code&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;╭─── Claude Code v2.0.33 ────────────────────────────────────────────────────────────────────────────────╮
│                                    │ Tips for getting started                                          │
│            Welcome back!           │ Run /init to create a CLAUDE.md file with instructions for Claude │
│                                    │ ───────────────────────────────────────────────────────────────── │
│               ▐▛███▜▌              │ Recent activity                                                   │
│              ▝▜█████▛▘             │ No recent activity                                                │
│                ▘▘ ▝▝               │                                                                   │
│                                    │                                                                   │
│   minimax-m2 · API Usage Billing   │                                                                   │
│          F:\dev\notebook           │                                                                   │
╰────────────────────────────────────────────────────────────────────────────────────────────────────────╯

─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
&amp;gt; Try &amp;quot;create a util logging.py that...&amp;quot;
─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
  ! for bash mode       double tap esc to clear input      ctrl + _ to undo
  / for commands        alt + m to auto-accept edits       alt + v to paste images
  @ for file paths      ctrl + o for verbose output
  # to memorize         ctrl + t to show todos
                        tab to toggle thinking
                        backslash (\) + return (⏎) for 
                        newline
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;codex&#34;&gt;codex
&lt;/h2&gt;&lt;p&gt;Not yet experienced, reference materials: &lt;a class=&#34;link&#34; href=&#34;https://platform.minimaxi.com/docs/guides/text-ai-coding-tools#%E5%9C%A8-codex-cli-%E4%B8%AD%E4%BD%BF%E7%94%A8-minimax-m2&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://platform.minimaxi.com/docs/guides/text-ai-coding-tools#%E5%9C%A8-codex-cli-%E4%B8%AD%E4%BD%BF%E7%94%A8-minimax-m2&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>All the gifts of AI have already been marked up with prices in the shadows.</title>
        <link>https://ttf248.life/en/p/all-the-gifts-of-ai-have-already-been-marked-up-with-prices-in-the-shadows/</link>
        <pubDate>Fri, 10 Oct 2025 19:09:36 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/all-the-gifts-of-ai-have-already-been-marked-up-with-prices-in-the-shadows/</guid>
        <description>&lt;p&gt;The frequency of article releases has noticeably increased with the use of AI, and I intend to differentiate this in the tags within the text as well. The author’s column will also note the names of large models. However, the issue persists: articles generated by AI have significantly reduced my level of involvement, with many articles being forgotten after a month or so. This occurs similarly when writing code – instead of analyzing problems based on existing knowledge, I instinctively turn to AI for analysis and troubleshooting, leading to a clear increase in “laziness.”&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Generative AI may boost work efficiency, but its &amp;ldquo;gift&amp;rdquo; comes at a huge cost. A research study at Peking University analyzed 410,000 papers and longitudinal experiments, finding that AI accelerates knowledge production but leads to severe homogenization; a Harvard study showed that AI causes “credential bias,” with junior positions decreasing by 7.7%, exacerbating the Matthew effect. On a personal level, the creativity boost brought about by AI is a fleeting &amp;ldquo;illusion&amp;rdquo; – it disappears when deactivated, but ideological homogenization persists, forming “creative scars.”&lt;/p&gt;
&lt;h2 id=&#34;current-situation&#34;&gt;Current Situation
&lt;/h2&gt;&lt;p&gt;Generative AI is not only reshaping industries across the board but fundamentally altering how humans write, think, and reason. &lt;strong&gt;Following the release of ChatGPT3.5, an optimistic expectation spread widely: AI would bring about “work leveling.”&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In 2023, two MIT economics PhDs published empirical research on this claim in the &lt;em&gt;Science&lt;/em&gt; journal, providing evidence to support it: that generative AI significantly boosts the performance of low-performing employees, potentially bridging the gap with high-performing ones and reducing inequality.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;Science&lt;/em&gt; journal editors summarized this as, “Weaker participants benefited most from ChatGPT, a finding with important implications for policies aimed at reducing productivity inequalities through AI.”&lt;/p&gt;
&lt;p&gt;However, &lt;strong&gt;two years later, reality seems to have not fully followed this ideal path.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In 2025, two Harvard economics PhDs, analyzing recruitment and employment data covering over 6.2 million employees and more than 1.5 billion instances between 2015 and 2025, revealed a stark truth: &lt;strong&gt;Generative AI is reshaping the labor market in a “credential-biased” way.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The data showed that between 2015 and 2022, the employment growth curves for junior and senior positions were roughly consistent, but starting in 2023, they began to diverge: Senior positions continued to rise, while junior positions started to decline.&lt;/p&gt;
&lt;p&gt;For companies deeply embracing AI, the number of their junior-level positions decreased by approximately 7.7% over six quarters, while senior positions remained largely unaffected and even saw slight growth. This phenomenon was primarily due to a reduction in hiring rather than mass layoffs.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;AI has not brought about equitable leveling but has instead exacerbated the Matthew effect – “the rich get richer.”&lt;/strong&gt;  Cheng Travel CEO Liang Jianzhang commented on the paper: &amp;ldquo;AI will replace basic intellectual labor, exacerbating the difficulties faced by young people in education, marriage and early career stages.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;The structural changes in the labor market are just the tip of the iceberg. &lt;strong&gt;A deeper question then arises: as AI is integrated into our workflows, what impact is it having on human creativity itself? Is the efficiency boost brought about by AI truly internalized individual capabilities? Is it shaping – or even “homogenizing” – our thoughts in ways we haven’t yet perceived? Once individuals become overly reliant on AI, is their independent, original thinking ability enhanced, or is it subtly being weakened?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Recently, Professor Li Guiquan&amp;rsquo;s research group at Peking University published a paper in the social science top journal &lt;em&gt;Technology in Society&lt;/em&gt;, addressing these key issues head-on.&lt;/p&gt;
&lt;p&gt;The core of the research comprised two parts. The first was a large-scale natural experiment that analyzed over 41,000 academic papers across all 21 disciplines before and after the release of ChatGPT3.5, dissecting AI’s true impact on global knowledge production. The second was a longitudinal behavioral experiment conducted over several months, exploring AI&amp;rsquo;s long-term causal effects on individual cognitive abilities in a laboratory setting.&lt;/p&gt;
&lt;p&gt;Combining breakpoint regression design and machine learning techniques, the research team revealed generative AI’s long-term and genuine impacts on both individual creativity and group homogeneity.&lt;/p&gt;
&lt;p&gt;This journal is JCR 1-star, with an impact factor of 12.5, ranking 271 out of 271 journals in the socialscience, Interdisciplinary category.&lt;/p&gt;
&lt;h2 id=&#34;410000-papers-collective-unconscious&#34;&gt;410,000 Papers&amp;rsquo; &amp;ldquo;Collective Unconscious&amp;rdquo;
&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;The most terrifying thing isn’t the noise, but the chorus of voices.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;410000-papers-collective-unconscious-1&#34;&gt;410,000 Papers’ “Collective Unconscious”
&lt;/h2&gt;&lt;p&gt;The study was a large-scale natural experiment.
The research team extracted academic outputs spanning all 21 disciplines – physics, life sciences and biomedical sciences, applied sciences, social sciences, arts and humanities – from the authoritative Web of Science Core Collection database. Through random sampling of over 17,000 scholars, the team ultimately collected all 419,344 papers published before and after the release of ChatGPT-3.5, constructing a massive dataset to analyze the true impact of AI on global knowledge production.
&lt;img src=&#34;https://ttf248.life/p/all-the-gifts-of-ai-have-already-been-marked-up-with-prices-in-the-shadows/20251010191749.png&#34;
	width=&#34;1024&#34;
	height=&#34;493&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;Large Dataset&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;207&#34;
		data-flex-basis=&#34;498px&#34;
	
&gt;
Illustration of homogeneity and creativity in academic papers before and after the release of Generative AI.&lt;/p&gt;
&lt;p&gt;As shown in the figure above, prior to 2022, global academic output (red/blue lines) exhibited steady growth alongside homogeneity (gray line). However, following the release of ChatGPT3.5, both curves experienced a sharp increase in slope.&lt;/p&gt;
&lt;p&gt;In other words, after GPT3.5’s release, academia not only accelerated knowledge production (creativity) at an unprecedented rate but also intensified the homogenization of its content at an even faster pace, clearly demonstrating the “double-edged sword” effect of generative AI on knowledge production.&lt;/p&gt;
&lt;p&gt;To demonstrate that the observed changes were caused by AI and not due to chance, the research team employed a causal inference method called “Regression Discontinuity Design” (RDD).&lt;/p&gt;
&lt;h3 id=&#34;how-to-do&#34;&gt;How to Do
&lt;/h3&gt;&lt;p&gt;They viewed the release of ChatGPT-3.5 in December 2022 as a natural “time breakpoint.” Whether a paper was published before or after that date posed numerous uncontrollable factors for individual scholars (such as review cycles), effectively creating a randomized “experimental group” (with the opportunity to use AI) and a “control group” (unable to use AI).&lt;/p&gt;
&lt;h3 id=&#34;why-its-reliable&#34;&gt;Why it’s Reliable
&lt;/h3&gt;&lt;p&gt;This “pseudo-randomness” allows researchers to effectively isolate other long-term confounding factors and precisely identify the causal effects brought about by AI. To ensure the rigor of this method, the team also conducted a series of specialized statistical tests, confirming that scholars did not engage in strategic behaviors such as “delaying publication” or “early release” before or after the “breakpoint,” thereby guaranteeing the reliability of the research results.&lt;/p&gt;
&lt;h3 id=&#34;how-to-quantify-creativity-and-homogeneity-metrics&#34;&gt;How to Quantify “Creativity” and “Homogeneity” Metrics?
&lt;/h3&gt;&lt;p&gt;Following the confirmation of causality, the research team conducted a quantitative analysis of these 40+ thousand papers across two dimensions: “creativity” and “homogeneity.”&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Creativity:&lt;/strong&gt; Evaluated based on the number of paper publications and the quality of those publications (JCR Quartiles).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Number: The total number of papers published by an author.&lt;/li&gt;
&lt;li&gt;Quality: The JCR Quartile score of the journal in which the paper was published. This is a prestigious journal ranking system, with Q1 representing the top 25% of journals in a field and Q4 representing the bottom 25%.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Homogeneity:&lt;/strong&gt; Evaluated through content similarity and language style similarity.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Content Similarity: Utilizing an SBERT deep learning model to convert paper abstracts into numerical “vectors,” then calculating the “cosine similarity” between these vectors to determine the degree of similarity in their core meaning.&lt;/li&gt;
&lt;li&gt;Language Style Similarity: Employing a character-level matching algorithm to scan and calculate repeated phrases and sentence structures between paper abstracts, thereby measuring the similarity of writing styles.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;a-double-edged-sword-more-efficient-yet-more-monotonous&#34;&gt;A Double-Edged Sword: More Efficient, Yet More Monotonous
&lt;/h3&gt;&lt;p&gt;As shown, the analysis results clearly reveal a “double-edged sword” effect.&lt;/p&gt;
&lt;p&gt;On one hand, the emergence of AI has become a powerful “accelerator” for academic output: the average annual publication volume per scholar increased by 0.9 papers, and the quality of published journals averaged an increase of 6%. This effect is particularly prominent in fields such as technology and physical sciences.&lt;/p&gt;
&lt;p&gt;However, on the other hand, the gains in efficiency are coming at the cost of diversity in thought and expression. Data show that the average annual similarity of writing styles in papers increased surprisingly by 79%, while the thematic content of papers also showed a significant convergence, with the most serious phenomenon being homogenization in physics, arts, and humanities.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ttf248.life/p/all-the-gifts-of-ai-have-already-been-marked-up-with-prices-in-the-shadows/20251010192022.png&#34;
	width=&#34;1024&#34;
	height=&#34;688&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;Breakdown Regression Results Chart&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;148&#34;
		data-flex-basis=&#34;357px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;This large-scale natural experiment conducted by Peking University researchers provides us with real-world macro evidence: Generative AI is indeed a powerful “accelerator” for academic output, helping scholars to produce and publish faster in better journals. However, this increase in efficiency comes at the cost of diversity in thought and expression.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Global knowledge production seems to be becoming more efficient and more “monotonous” in this “great exchange.”&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Meanwhile, research two also left a deeper question: what does this macro trend mean for individuals who are immersed in it? Does the creativity boost brought by AI represent genuine personal cognitive growth?&lt;/p&gt;
&lt;p&gt;To answer this question, the research team conducted a longitudinal behavioral experiment tracking over several months in a controlled laboratory environment in research two to explore the long-term causal effects of AI on individual cognitive abilities.&lt;/p&gt;
&lt;h2 id=&#34;scars-of-creativity-left-by-ai&#34;&gt;Scars of Creativity Left by AI
&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;Once ideas submit to habit, they lose the possibility of creation.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;ais-creative-scars&#34;&gt;AI&amp;rsquo;s Creative Scars
&lt;/h2&gt;&lt;p&gt;In fact, there have already been numerous laboratories using small-sample empirical studies from different angles to confirm the trends revealed by macro data. For example, research at Cornell University found that AI writing assistants sacrifice cultural uniqueness and cause users’ expressions to tend towards “Western paradigms”; research at Santa Clara University also showed that individuals who used ChatGPT were more similar in their creativity semantically.
Notably, a research team from MIT directly observed the brains of individuals using electroencephalography (EEG) technology, finding that the brain activity level of the group of students who used ChatGPT was significantly lower than that of the group that relied solely on their own thinking or used search engines.
These studies point to one conclusion: AI is sacrificing cognitive input and diversity to enhance efficiency.
However, most research focuses on the immediate impact of using AI, with little exploration of whether the effects of AI “leaving the field” can be sustained and whether its long-term negative impacts will diminish.
This study by Peking University made a new attempt in this area.
It not only observed the immediate effect of AI in a seven-day experiment, but also systematically tested the long-term consequences of AI dependence through two independent tracking tests after the end of the experiment – on day 30 and day 60. This allowed us to truly see whether what AI brought was a transferable “ability” or a fleeting, uninternalizable “illusion.”
Specifically, the Peking University research team randomly divided 61 college students into two groups: “AI experimental group” (able to use ChatGPT-4) and “pure brainpower control group.”
The experiment design consisted of three key stages: first, all participants completed a creativity baseline test on day one without using AI; then, from days two to six, the “AI experimental group” completed daily creativity tasks with AI assistance, while the “pure brainpower control group” completed the tasks without assistance; finally, and most importantly, on day seven, day 30, and day 60, all participants had to complete the final tracking test without any AI assistance.
&lt;img src=&#34;https://ttf248.life/p/all-the-gifts-of-ai-have-already-been-marked-up-with-prices-in-the-shadows/20251010192137.png&#34;
	width=&#34;1024&#34;
	height=&#34;656&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;Experimental Design Illustration&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;156&#34;
		data-flex-basis=&#34;374px&#34;
	
&gt;
To comprehensively evaluate “creativity,” the study used a composite task mode covering multiple dimensions. These tasks included:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Divergent Thinking Test:&lt;/strong&gt; The classic “Alternative Uses Task” (AUT), requiring participants to come up with as many novel uses as possible for everyday items (such as “a pen”).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Creative Problem Solving:&lt;/strong&gt; More realistic business scenario questions, such as asking participants to design innovative features for a “smart bicycle.”&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Convergent Thinking Test:&lt;/strong&gt; The “Remote Association Test” (RAT) added during the tracking phase, requiring participants to find a word that can connect three unrelated words simultaneously.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Insight Question:&lt;/strong&gt; The classic “Candle Problem,” requiring participants to fix a candle on the wall with a box of nails, a candle, and a box of matches, without letting the wax drip onto the table.
To ensure the scientificity of the assessment, the study used the “gold standard” in the field – expert consensus evaluation (CAT). Multiple expert judges independently scored thousands of creative outputs (including divergent thinking tasks and complex problem solutions) on multiple dimensions such as novelty, practicality, and flexibility in a “blind” condition where they were unaware of the grouping situation and research purpose. High data consistency (rating agreement ICCs &amp;gt; 0.90) ensured the scientific and fairness of the assessment results.
The homogeneity measurement method used in Study II adopted the same technical methods as Study I to ensure consistency between the two studies’ evaluation standards.
The experimental results clearly revealed a stark asymmetry:&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Creativity Enhancement is Transient and Unsustainable:&lt;/strong&gt; During the AI usage phase (days 2-6), the “AI experimental group”’s creativity indicators were indeed far beyond that of the “pure brainpower group.” However, once the AI was removed, this advantage instantly disappeared. Starting from day seven until day 60, there was no significant difference in creativity performance between the two groups. More alarmingly, in the 60-day convergent thinking test, the participants in the experimental group’s performance was even significantly worse than that of the control group who had never used AI, what AI brought was not a transferable “ability,” but rather an uninternalizable “illusion.”&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;The Homogenization of Thoughts is Long-lasting and Leaves &amp;ldquo;Creative Scars&amp;rdquo;:&lt;/strong&gt; In contrast to the fleeting creativity enhancement, the homogenization of thought demonstrated surprising “stickiness.” Even after two months of not using AI, the output content from the “AI experiment group” – regardless of whether it was measured in terms of semantics or language style – still exhibited significantly higher similarity compared to the control group.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This longitudinal study provided direct causal evidence confirming the long-term impact of AI on individual creativity. The potential brought by AI may only be a “creative illusion” that cannot be internalized, while the resulting tendency towards homogenization of thought could become an enduring “creative scar,” persistently embedded in our cognitive and expressive habits.&lt;/p&gt;
&lt;h2 id=&#34;if-the-world-had-no-new-creativity&#34;&gt;If the world had no new creativity
&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;It is the best of times, it is the worst of times.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;if-the-world-lacked-new-creativity&#34;&gt;If the World Lacked New Creativity
&lt;/h2&gt;&lt;p&gt;This research from Tsinghua University, concluding that we shouldn’t abandon AI entirely just because of our own frustration, instead aims to remind us that we must consciously understand and address the long-term impact of prolonged reliance on AI on individual thinking and cognitive habits.&lt;/p&gt;
&lt;p&gt;The “homogenization” trend revealed in the study is rooted in profound principles of cognitive science: AI outputs easily trigger a powerful “anchoring effect” in users. When AI quickly generates an apparently “decent” answer or framework, our minds become anchored to this initial solution, making it difficult for subsequent thought and creativity to significantly deviate, ultimately leading to the convergence of ideas at the group level.&lt;/p&gt;
&lt;p&gt;In July of this year, when Huang Renjun made a calm assessment during an interview with CNN: &amp;ldquo;If the world lacks new creativity, then the productivity gains brought about by AI will translate into unemployment.”&lt;/p&gt;
&lt;p&gt;As generative AI is increasingly used, the internet’s information and human knowledge base are becoming more homogenized at an unprecedented speed. Tsinghua University&amp;rsquo;s research cold-heartedly confirms that this trend exists. If society can continuously generate new ideas, AI will create more diverse employment opportunities; if it only repeats old tasks, AI can complete them in seconds.&lt;/p&gt;
&lt;p&gt;AI amplifies creativity but also accelerates the expulsion of “those with dry ideas.”&lt;/p&gt;
&lt;h2 id=&#34;in-the-age-of-ai-how-to-maintain-sharp-thinking&#34;&gt;In the Age of AI, How to Maintain Sharp Thinking
&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;AI alleviates our workload, but we need to establish a thinking system capable of deep thought, one that can interact with AI, articulate the problems we want AI to solve, and also engage in reasoning about those problems. Simultaneously, we must evaluate whether AI has answered correctly – we need dialectical thinking. —Huang Renfu&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;in-the-age-of-ai-how-to-maintain-sharp-thinking-1&#34;&gt;In the Age of AI, How to Maintain Sharp Thinking
&lt;/h2&gt;&lt;p&gt;As individuals navigating the age of AI, how should we position ourselves? How can we enjoy the convenience of AI while avoiding creative barrenness? Combining insights from research, here are some specific action recommendations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Treat AI as a “Thinking Drill”:&lt;/strong&gt; Consider it an tireless companion that provides unlimited perspectives. Use it for brainstorming, generating multiple possibilities, and challenging your ingrained assumptions. However, the final filtering, deepening, decision-making, and accountability for the results must always be yours.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Deliberately Practice &amp;ldquo;Cognitive Friction&amp;rdquo;:&lt;/strong&gt; The most effective way to combat “anchoring bias” is to actively create “cognitive friction.” Don’t readily accept the AI&amp;rsquo;s first answer. Intentionally challenge it, find its logical flaws, and question aspects it hasn’t considered. This practice of critical thinking is key to maintaining our independent thinking abilities.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Establish &amp;ldquo;AI-Free Time&amp;rdquo;:&lt;/strong&gt; Just as we need regular exercise to prevent muscle atrophy, we also need to regularly allow our brains to exercise without AI assistance. Regularly designate a weekly period of “AI-free time” for thinking, planning, and creating using the most basic tools – paper and pen or a blank document. This deliberate &amp;ldquo;cognitive detox&amp;rdquo; ensures that our core creative and reasoning abilities won’t deteriorate in comfort.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>ByteDance AI’s SOLO encoding new paradigm</title>
        <link>https://ttf248.life/en/p/bytedance-ais-solo-encoding-new-paradigm/</link>
        <pubDate>Thu, 14 Aug 2025 21:49:06 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/bytedance-ais-solo-encoding-new-paradigm/</guid>
        <description>&lt;p&gt;Previously used products, during code development, were largely satisfactory, but Byte’s SOLO introduced significant differences. Initially, participation in beta testing was through invitation codes, and now it involves submitting an email address for review. After approval, the product can be used. I’ve forgotten when I submitted the application, but today I received a notification from Trae indicating that my application had been approved.&lt;/p&gt;
&lt;h2 id=&#34;bytesolos-advantages&#34;&gt;ByteSolo’s Advantages
&lt;/h2&gt;&lt;p&gt;Following a standard project development process, it incorporates UI design, requirements analysis, functional design, technical solution implementation, and finally code development. In other words, the overall interaction logic is more friendly for beginners. For complex projects, the traditional approach remains more effective – AI as an auxiliary tool to modify and optimize existing code.&lt;/p&gt;
&lt;h2 id=&#34;hong-kong--us-stocks-app-development&#34;&gt;Hong Kong &amp;amp; US Stocks App Development
&lt;/h2&gt;&lt;p&gt;That’s right, the title is exactly what I wrote in the prompt statement, with no additional descriptions. In SOLO mode, first a requirements document and a technical architecture document were outputted. If the user confirms they are okay with it, code development will proceed.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/ttf248/ai-coding-trae-solo/blob/master/.trae/documents/%E6%B8%AF%E7%BE%8E%E8%82%A1APP%E4%BA%A7%E5%93%81%E9%9C%80%E6%B1%82%E6%96%87%E6%A1%A3.md&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Requirements Document&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/ttf248/ai-coding-trae-solo/blob/master/.trae/documents/%E6%B8%AF%E7%BE%8E%E8%82%A1APP%E6%8A%80%E6%9C%AF%E6%9E%B6%E6%9E%84%E6%96%87%E6%A1%A3.md&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Technical Architecture Document&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As I said, while the design isn&amp;rsquo;t particularly perfect, it can be used as a draft.  Due to my lack of UI design experience and not having tested the functionality of importing Figma design drafts, it’s perfectly acceptable as a first draft.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;During the code construction process, some documents were also discovered. In SOLO mode, the AI attempts to handle any existing issues on its own, such as needing to increase the field design in my local database, it first generates the corresponding script and then writes a js script to execute the database script.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;There were also some highlights that I didn&amp;rsquo;t find while researching materials. The task decomposition feature was taken from a previous comic reading project. Byte is not short of money; one request can execute a lot of tasks, but this scenario doesn’t occur frequently, new projects will consume a large number of tokens. It’s only when users develop dependencies that they become profitable here.
&lt;img src=&#34;https://ttf248.life/p/bytedance-ais-solo-encoding-new-paradigm/20250814215722.png&#34;
	width=&#34;571&#34;
	height=&#34;236&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;Task Decomposition Function&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;241&#34;
		data-flex-basis=&#34;580px&#34;
	
&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Daily Musings</title>
        <link>https://ttf248.life/en/p/daily-musings/</link>
        <pubDate>Fri, 01 Aug 2025 18:57:38 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/daily-musings/</guid>
        <description>&lt;p&gt;Using AI too much, we start thinking of AI for everything – often, learning new developments is more reliable with search engines plus official project documentation.&lt;/p&gt;
&lt;p&gt;Hong Kong stocks entered on the dip, encountered a pullback, and then just randomly traded, resulting in basic losses.&lt;/p&gt;
&lt;h2 id=&#34;strategy-trading&#34;&gt;Strategy Trading
&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;Not necessarily to actually practice and make money, but rather to improve my own abilities through learning. I don&amp;rsquo;t believe in these trading indicators; I mostly trust the national fortune (luck) and investing in broad market indices via a buy-and-hold strategy.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;algorithmic-trading&#34;&gt;Algorithmic Trading
&lt;/h2&gt;&lt;p&gt;The inspiration from last month’s “AI-less project” didn&amp;rsquo;t prove useful, so I started trying to implement it with AI – and that’s where the problems began. The correct approach should have been to first gather information and see what existing projects were doing. Previously, I hadn’t done any algorithmic trading; I wasn’t familiar with indicators, market data handling, etc.&lt;/p&gt;
&lt;p&gt;The initial plan was unreliable, entirely based on my own speculation (“YY”) through communication with AI. I learned about the backtesting framework this way, and looked at GitHub – the project was quite active.&lt;/p&gt;
&lt;p&gt;Over-reliance on AI led to wanting to use it for everything, even requesting an AI-generated learning resource, a vibrant project, and official documentation that’s well-maintained. Prioritizing official documentation or searching for high-quality blogs is better; the learning plans provided by AI were too weak and didn&amp;rsquo;t keep up with current code versions.&lt;/p&gt;
&lt;p&gt;Project Structure Adjustments:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Data Download:&lt;/strong&gt; I chose Yahoo Finance – this provides daily candlestick data for price changes (price fluctuations).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Backtesting Official Guide:&lt;/strong&gt; Learning the basics of using the official backtesting guide.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;TA-Lib Installation and Usage:&lt;/strong&gt; Installing and using TA-Lib, calculating common indicators, and displaying the data through backtesting.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Implementing Alipay’s Fixed Investment Logic:&lt;/strong&gt; This strategy is better suited for long-term ETF investments.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;hong-kong-stock-trading&#34;&gt;Hong Kong Stock Trading
&lt;/h2&gt;&lt;p&gt;Entering Hong Kong stock trading was roughly two months ago, a simple recap.&lt;/p&gt;
&lt;p&gt;The timing and motivation for buying Meituan were straightforward – I saw the net inflow of domestic funds and didn’t delve deeply into Meituan&amp;rsquo;s recent situation; coincidentally, I caught the food delivery war and became a shareholder in Meituan. Buying Xiaomi was also at a high price point, and it was appropriate to reduce my position and exit some shares when an opportunity arose. The four factories were implemented, presenting an opportunity, but one that required persistence – about three years have passed.&lt;/p&gt;
&lt;p&gt;The timing of entry was when Hong Kong’s new consumer and technology stocks were already expensive, coupled with the concept of virtual currencies; when technology stocks retraced, it was a recurring issue – the frequency of adding to positions is not suitable for long-term investment. Today, I thought about betting on a rebound, buying a hand full, but the next day it fell and I couldn’t bear to sell it. When encountering sustained declines, one easily gets trapped.&lt;/p&gt;
&lt;p&gt;As a long-term investor, my trading frequency was too high – roughly twice a week would be sufficient.
The Shanghai Composite Index is 3600; domestic large-cap stocks haven&amp;rsquo;t been much invested in through fixed deposits; this missed an opportunity as well.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Recent Usage Experiences of Large Models</title>
        <link>https://ttf248.life/en/p/recent-usage-experiences-of-large-models/</link>
        <pubDate>Wed, 09 Jul 2025 21:09:01 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/recent-usage-experiences-of-large-models/</guid>
        <description>&lt;p&gt;Currently, no large model stands out as particularly superior; each company has its own strengths and preferred use cases.&lt;/p&gt;
&lt;h2 id=&#34;technical-documentation&#34;&gt;Technical Documentation
&lt;/h2&gt;&lt;p&gt;For feeding code or asking IT technical questions: ChatGPT and Gemini&lt;/p&gt;
&lt;h2 id=&#34;write-code&#34;&gt;Write Code
&lt;/h2&gt;&lt;p&gt;Gather requirements and request code modifications: Claude&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Daily Musings</title>
        <link>https://ttf248.life/en/p/daily-musings/</link>
        <pubDate>Thu, 19 Jun 2025 19:07:33 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/daily-musings/</guid>
        <description>&lt;p&gt;AI has been integrated into daily development workflows, and recent investments have shifted focus from external funds to internal stocks and ETFs.&lt;/p&gt;
&lt;h2 id=&#34;open-source-projects&#34;&gt;Open Source Projects
&lt;/h2&gt;&lt;h3 id=&#34;project-log&#34;&gt;Project Log
&lt;/h3&gt;&lt;p&gt;Last week, I was bored and tried to obtain GitHub badges, starting to use the Issue module. Previously, when writing code, I always wanted to find a place to record the content of each AI modification, but creating a separate document to record it felt disorganized. Now that we have the Issue module, tagging them to distinguish between bugs, features, enhancements, etc., has made the records clear and efficient. Even if I might not need them in the future, recording them is still a form of accumulation.
&lt;a class=&#34;link&#34; href=&#34;https://github.com/ttf248/comic-reader/issues?q=is%3Aissue&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;View Issue List&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;release-notes&#34;&gt;Release Notes
&lt;/h3&gt;&lt;p&gt;Release record, find the most recent related commits. Since all commits are AI-generated, copy all recent commit records from the web interface and give them to AI to organize – this is a good release record.
&lt;a class=&#34;link&#34; href=&#34;https://github.com/ttf248/comic-reader/releases/tag/v1.9.0&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/ttf248/comic-reader/releases/tag/v1.9.0&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;engagement&#34;&gt;Engagement
&lt;/h3&gt;&lt;p&gt;That time it was featured on GitHub’s personal page, it passively increased the motivation to write code – after all, the data visualization made things quite fascinating. Even simple positive feedback could provide the impetus to keep going.&lt;/p&gt;
&lt;h3 id=&#34;trae&#34;&gt;Trae
&lt;/h3&gt;&lt;p&gt;Paid for a monthly trial, honestly, it’s better in VSCode using Claude4. The practical effect is also better. Sometimes, even with the same problem, Trae gives better answers. Should I buy an annual subscription later? Given my current chaotic usage frequency, Trae might not be enough; don&amp;rsquo;t think about it too much, we’ll see when we run out. Byte should still have other paid plans and purchase more call volumes.
&lt;del&gt;Simple small issues can be solved using Microsoft, GitHub Copilot, and various models can also be called.&lt;/del&gt;
The plan is to fail – GitHub Copilot is now starting to limit the number of calls &lt;a class=&#34;link&#34; href=&#34;https://docs.github.com/zh/copilot/about-github-copilot/plans-for-github-copilot#models&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;618 opened restrictions&lt;/a&gt;
How to check &lt;a class=&#34;link&#34; href=&#34;https://docs.github.com/zh/copilot/managing-copilot/understanding-and-managing-copilot-usage/monitoring-your-copilot-usage-and-entitlements#downloading-a-monthly-usage-report&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;current usage&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;investment&#34;&gt;Investment
&lt;/h2&gt;&lt;p&gt;Ultimately, I couldn’t resist. Since the Hong Kong Connect channel was opened, I hadn&amp;rsquo;t traded in the Hong Kong market, and I bought some Xiaomi stock thinking about their upcoming new car launch. It rose a bit, so I sold it, and when it fell, I bought it back again, repeating this process several times before the new car was released, making a small profit on the stock.&lt;/p&gt;
&lt;p&gt;At this time, I didn’t know what to do but look at the cross-border capital flows of Hong Kong Connect, watching Meituan&amp;rsquo;s net inflow of funds, and followed suit by buying it in, successfully becoming a shareholder, forgetting to look at the overall capital flow, only a part of the domestic purchases, and there were still many foreign investors in the Hong Kong market. This time it was just an opportunity to practice with blue-chip stocks, slowly holding on to the final gains.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Controlling positions and losses are &lt;strong&gt;contrary to human nature&lt;/strong&gt;, slow down, slow down, don’t rush. If Xiaomi&amp;rsquo;s new car doesn’t perform well, should I sell? That’s a question. My investment cognition is still insufficient, and I need to read more books and learn more. The Fed isn’t planning on cutting interest rates, and the Hong Kong market plunged once, if I missed the entry time? That wouldn&amp;rsquo;t be right, what if the news came out that interest rates were being cut and the Hong Kong market soared? This is investment, testing human nature.
&lt;code&gt;A phrase I often repeat: buying is for national fortune, but I don’t really believe in national fortune.&lt;/code&gt;
Aside from the belief in national fortune, &lt;strong&gt;another one is attention&lt;/strong&gt;. Since you&amp;rsquo;re going to do long-term investing, frequent monitoring of the market is pointless. Ten minutes in the morning and ten minutes at the close are enough. What is the final expected return rate? There’s still no clear stop-loss position.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;When the market falls sharply, Tencent is a hot commodity, with capital flocking to it.
| 00700 | Tencent Holdings | 498.600 | 80.08 billion |&lt;/p&gt;
&lt;h2 id=&#34;investment-1&#34;&gt;Investment
&lt;/h2&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Code&lt;/th&gt;
&lt;th&gt;Name&lt;/th&gt;
&lt;th&gt;Latest Price&lt;/th&gt;
&lt;th&gt;Turnover&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;03690&lt;/td&gt;
&lt;td&gt;Meituan-W&lt;/td&gt;
&lt;td&gt;128.100&lt;/td&gt;
&lt;td&gt;6.881 Billion&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;investment-2&#34;&gt;Investment
&lt;/h2&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Code&lt;/th&gt;
&lt;th&gt;Name&lt;/th&gt;
&lt;th&gt;Latest Price&lt;/th&gt;
&lt;th&gt;Volume&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;09992&lt;/td&gt;
&lt;td&gt;Bubble Mate&lt;/td&gt;
&lt;td&gt;247.200&lt;/td&gt;
&lt;td&gt;5.703 Billion&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;investment-3&#34;&gt;Investment
&lt;/h2&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Code&lt;/th&gt;
&lt;th&gt;Name&lt;/th&gt;
&lt;th&gt;Latest Price&lt;/th&gt;
&lt;th&gt;Volume&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;09988&lt;/td&gt;
&lt;td&gt;Alibaba-W&lt;/td&gt;
&lt;td&gt;109.800&lt;/td&gt;
&lt;td&gt;532.2 Billion&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;investment-4&#34;&gt;Investment
&lt;/h2&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Code&lt;/th&gt;
&lt;th&gt;Name&lt;/th&gt;
&lt;th&gt;Latest Price&lt;/th&gt;
&lt;th&gt;Turnover&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;01810&lt;/td&gt;
&lt;td&gt;Xiaomi Group-W&lt;/td&gt;
&lt;td&gt;53.050&lt;/td&gt;
&lt;td&gt;41.05 Billion&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
</description>
        </item>
        <item>
        <title>Blog Translation Project Musings: Cultural Transmission, AI Programming</title>
        <link>https://ttf248.life/en/p/blog-translation-project-musings-cultural-transmission-ai-programming/</link>
        <pubDate>Mon, 02 Jun 2025 21:41:00 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/blog-translation-project-musings-cultural-transmission-ai-programming/</guid>
        <description>&lt;p&gt;Cultural Propagation: Its ideological influence, subtle and pervasive.
AI Programming: Not performing software design, resulting in a lot of rework.&lt;/p&gt;
&lt;h2 id=&#34;cultural-propagation&#34;&gt;Cultural Propagation
&lt;/h2&gt;&lt;p&gt;Initially, the project only supported English, Japanese, and Korean. Thinking it was just AI translation, we wondered if supporting more languages wouldn’t be a good idea. So, French, Russian, and Hindi were added. At this point, no problems were detected; when the program executed translations, formatting issues arose due to historical code problems, requiring re-translation of archived articles.&lt;/p&gt;
&lt;p&gt;Statistical timing reminders indicated that it would take nearly 20 hours to complete all translations, given that it was a large local model with 12b parameters. Considering reducing translation time, we deleted French, Russian, and Hindi. This is when things started to feel wrong – why did I instinctively choose Korean and Japanese among the initially supported languages?&lt;/p&gt;
&lt;p&gt;According to global population distribution, these two languages had relatively small audiences, especially Korean, which had approximately 80 million users worldwide. Japanese was slightly more prevalent, with around 1.2 billion people. In contrast, French, Russian, and Hindi had user populations exceeding 100 million.&lt;/p&gt;
&lt;p&gt;At this point, we realized that the popularity of Korean and Japanese wasn’t due to the sheer number of language speakers, but rather the influence of cultural propagation. Korean and Japanese cultures have a wide-reaching impact globally, particularly in Asia. K-pop, anime, and television dramas attracted a large fanbase, leading these fans to naturally develop an interest in the corresponding languages.&lt;/p&gt;
&lt;p&gt;Looking back at our growth history, I frequently watched Japanese anime and manga as a child, and later watched many Korean films and TV series. This led me to instinctively choose these familiar languages when setting up the project’s initial language settings.&lt;/p&gt;
&lt;h3 id=&#34;software-design-and-ai-programming&#34;&gt;Software Design and AI Programming
&lt;/h3&gt;&lt;p&gt;The initial translation assistant was initially just a simple tool, but after experiencing Claude4’s coding capabilities, it gradually expanded its functionality, adding modules for article translation and tag translation. As the features increased, so did the code complexity. Although AI refactored the codebase to appear more organized, in expanding new functionalities or fixing bugs, AI-generated code often suffered from repetition issues.&lt;/p&gt;
&lt;p&gt;AI lacks a holistic understanding of overall structure and design principles when generating code. It typically modifies and extends existing code rather than effectively reusing existing modules, leading to redundant code. Manual cleanup of duplicate code is then required, which inadvertently increases development costs.&lt;/p&gt;
&lt;p&gt;Furthermore, while AI-generated code is syntactically correct, it often suffers from problems in logic and design. For example, a slight adjustment to the prompt in another project resulted in completely different webpage structures lacking consistency. This reflects a lack of proper initial design, with new features added arbitrarily and haphazardly, leading to a chaotic codebase.&lt;/p&gt;
&lt;p&gt;This also reminds us that core software engineering experience remains indispensable. A rational design not only reduces rework but also enhances code maintainability and extensibility. While AI is a powerful tool, it cannot replace human deep understanding and planning of systems.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Blog Translation Project Musings: Historical Conversations</title>
        <link>https://ttf248.life/en/p/blog-translation-project-musings-historical-conversations/</link>
        <pubDate>Mon, 02 Jun 2025 21:16:24 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/blog-translation-project-musings-historical-conversations/</guid>
        <description>&lt;p&gt;The initial design of the blog translation project was overly complex – first parsing Markdown format, then using placeholders to protect the content, and finally sending it to a large model for translation. This was entirely unnecessary; large models inherently possess the ability to recognize Markdown syntax and can directly process the original content while maintaining formatting during translation.&lt;/p&gt;
&lt;p&gt;Our work shifted from debugging code to debugging the &lt;strong&gt;prompting&lt;/strong&gt; of the model.
Model: &lt;code&gt;google/gemma-3-4b&lt;/code&gt;
Hardware: &lt;code&gt;Nvidia 3060 12GB&lt;/code&gt;
Indeed, we chose a non-thinking model – thinking models were inefficient when executing translation tasks. We compared the performance of 4b and 12b parameters, and for translation purposes, gemma3’s 4b parameter was sufficient; there was no significant advantage in terms of 12b parameters.
12b parameter speed: &lt;strong&gt;11.32 tok/sec&lt;/strong&gt; , 4b parameter speed: &lt;strong&gt;75.21 tok/sec&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;background-introduction&#34;&gt;Background Introduction
&lt;/h2&gt;&lt;p&gt;Despite adding various constraints within the &lt;strong&gt;system&lt;/strong&gt;, the output translation results still presented some issues, such as: lack of formatting protection, inclusion of extraneous explanatory content. When defining roles, it was already stated to protect Markdown format and only output translation results; ultimately, the translation remained unstable.&lt;/p&gt;
&lt;p&gt;At this point, I remembered encountering a comic translation project previously, which also leveraged the capabilities of large models. Its translation effect seemed better than mine. Upon reviewing the code and comparing the request data, the comic translation project would include a set of context with each request, in addition to the current translation content, it would also include previous translation content.&lt;/p&gt;
&lt;p&gt;What were the benefits? Not only did this improve the coherence between preceding and following translations, but it also ensured the stability of the output format.&lt;/p&gt;
&lt;h2 id=&#34;the-importance-of-historical-conversations&#34;&gt;The Importance of Historical Conversations
&lt;/h2&gt;&lt;p&gt;As large AI models (such as the GPT series, Claude, Gemini, etc.) become more prevalent, an increasing number of businesses and developers are accessing these models via APIs to build intelligent customer service, content generation, code assistant, and other applications. However, many people encounter a common issue during initial access: &lt;strong&gt;model outputs are disjointed, lack contextual understanding, and even answer the wrong questions.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;A key reason for this phenomenon is – &lt;strong&gt;not including historical conversation content in API requests.&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;what-is-a-history-dialogue&#34;&gt;What is a History Dialogue?
&lt;/h2&gt;&lt;p&gt;A history dialogue refers to the exchange records between a model and a user within a single conversation session. In most large language model APIs (such as OpenAI’s Chat Completions API), developers need to construct the complete &lt;code&gt;messages&lt;/code&gt; array themselves, passing the historical dialogue in turn as &lt;code&gt;user&lt;/code&gt; and &lt;code&gt;assistant&lt;/code&gt; message format.&lt;/p&gt;
&lt;h3 id=&#34;example&#34;&gt;Example
&lt;/h3&gt;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{
  &amp;quot;model&amp;quot;: &amp;quot;gpt-4&amp;quot;,
  &amp;quot;messages&amp;quot;: [
    {&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;Write me a resignation letter&amp;quot;},
    {&amp;quot;role&amp;quot;: &amp;quot;assistant&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;Okay, what do you want to write about as the reason for your resignation?&amp;quot;},
    {&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;I want to pursue personal career development&amp;quot;}
  ]
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you only send the last sentence:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;I want to pursue personal career development&amp;quot;}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The model won&amp;rsquo;t know you are writing a resignation letter, and its output quality will be very poor because it doesn’t understand the context.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;why-historical-dialogue-is-so-important&#34;&gt;Why Historical Dialogue is So Important?
&lt;/h2&gt;&lt;h3 id=&#34;1-build-context-enhance-coherence&#34;&gt;1. &lt;strong&gt;Build Context, Enhance Coherence&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;AI models are inherently “context-driven.” They cannot remember anything that has happened “previously,” unless you explicitly tell it. By passing in the dialogue history, the model can better understand your intent and topic context, resulting in outputs more aligned with expectations.&lt;/p&gt;
&lt;h3 id=&#34;2-reduce-misunderstanding-rate&#34;&gt;2. &lt;strong&gt;Reduce Misunderstanding Rate&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;If you want the model to complete a multi-turn instruction, such as writing, summarizing, or debugging code, historical context allows the model to gradually accumulate understanding and avoid going off-topic or losing focus midway through.&lt;/p&gt;
&lt;h3 id=&#34;3-simulating-realistic-human-dialogue-behavior&#34;&gt;3. &lt;strong&gt;Simulating Realistic Human Dialogue Behavior&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;In practical applications such as customer service systems, educational assistants, and health consultations, user questions often unfold gradually rather than being expressed clearly in a single instance. Preserving dialogue history allows the AI to behave more like a “memoryful assistant.”&lt;/p&gt;
&lt;h2 id=&#34;how-to-correctly-add-historical-conversations-in-an-api&#34;&gt;How to Correctly Add Historical Conversations in an API?
&lt;/h2&gt;&lt;p&gt;Using OpenAI&amp;rsquo;s API as an example, we recommend following the structure below:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;messages = [
    {&amp;quot;role&amp;quot;: &amp;quot;system&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;You are a professional legal assistant&amp;quot;},
    {&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;What are the essential conditions for a contract?&amp;quot;},
    {&amp;quot;role&amp;quot;: &amp;quot;assistant&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;Contract validity requires fulfilling several conditions: ...&amp;quot;},
    {&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;Does an oral agreement count?&amp;quot;}
]

response = openai.ChatCompletion.create(
    model=&amp;quot;gpt-4&amp;quot;,
    messages=messages
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Use the &lt;code&gt;system&lt;/code&gt; message to set the model&amp;rsquo;s behavior and identity.&lt;/li&gt;
&lt;li&gt;Only retain recent key conversations, not necessarily the entire history (to avoid exceeding token limits).&lt;/li&gt;
&lt;li&gt;In long sessions, truncate early content and maintain core information summaries to control token consumption.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;practical-recommendations&#34;&gt;Practical Recommendations
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Dialogue State Management&lt;/strong&gt;: The backend needs to design caching mechanisms to record each user’s conversation history (e.g., Redis, database).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Limit Length&lt;/strong&gt;: OpenAI GPT-4 has a context length of 128k tokens, Claude 3 can reach 200k~1M, and requires reasonable truncation.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dynamic Summarization of History&lt;/strong&gt;: When the historical content is too long, use a model to first summarize the old conversations before adding them to the dialogue context.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;summary&#34;&gt;Summary
&lt;/h2&gt;&lt;p&gt;AI large model capabilities are powerful, but developers need to “feed” it sufficient contextual information. &lt;strong&gt;By adding historical conversations within API requests, not only can the quality and coherence of the model’s output be significantly improved, but users can also experience a more natural and realistic conversation.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Regardless of whether you&amp;rsquo;re building AI customer service, writing assistants, coding helpers, or educational applications, this is an optimization technique that cannot be ignored.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Old problems, the flamboyant beauty of blossoming flowers. (This captures the essence and poetic nature of the original.)

Alternatively, a more literal translation could be: “Old ailments, beautiful eyes like blooming flowers.” However, the first option is generally preferred for its aesthetic quality.</title>
        <link>https://ttf248.life/en/p/old-ailment-stunning-flowers/</link>
        <pubDate>Mon, 26 May 2025 23:54:12 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/old-ailment-stunning-flowers/</guid>
        <description>&lt;p&gt;For many years, I’ve focused on backend development, and recently started to explore &lt;code&gt;AI&lt;/code&gt; programming while dipping my toes into some frontend-related content. However, during this period of tinkering, I gradually realized I was falling back into an old habit – being dazzled by shiny new things. I constantly try to use &lt;code&gt;AI&lt;/code&gt; to create a frontend interface, but in reality, these attempts haven’t provided much practical benefit for my current work and are actually wasting my energy.&lt;/p&gt;
&lt;h2 id=&#34;ai-use-cases&#34;&gt;AI Use Cases
&lt;/h2&gt;&lt;p&gt;In small projects, AI tools can truly shine, particularly when writing independent functions with low coupling to the system and simple business logic. These tasks typically have clear inputs and outputs, and rely less on context – making them well-suited for the current capabilities of AI-assisted programming.&lt;/p&gt;
&lt;p&gt;However, when facing complex system architectures or deep business logic, AI’s limitations begin to emerge. It may generate code that appears reasonable but is actually detached from the project&amp;rsquo;s real needs, or even introduce potential issues that are difficult to debug. In these scenarios, AI is best suited as an assistive tool rather than a fully autonomous code generator. We need to conduct rigorous review and testing of the generated code to ensure it meets actual requirements.&lt;/p&gt;
&lt;h2 id=&#34;errors-and-the-cost-of-learning&#34;&gt;Errors and the Cost of Learning
&lt;/h2&gt;&lt;p&gt;While attempting to generate frontend code using AI, I encountered numerous challenges. As frontend development isn&amp;rsquo;t a domain I’m familiar with, troubleshooting often proved time-consuming and frustrating. Even after adjusting prompts to have the AI rewrite the code, it was difficult to avoid the appearance of some low-level errors. This iterative process not only wasted time but also highlighted that my current focus should be on backend business logic rather than groping around in unfamiliar territory.&lt;/p&gt;
&lt;p&gt;Looking back at the project completed over the weekend, I’m even more convinced that focusing on backend development and user interaction logic, implementing functionality through a console, is the most efficient approach currently. Perhaps systematically learning frontend knowledge would be a better strategy when I have more time and energy.&lt;/p&gt;
&lt;h2 id=&#34;frontend-learning-plan&#34;&gt;Frontend Learning Plan
&lt;/h2&gt;&lt;p&gt;The frontend technology stack is complex and diverse, so it’s unrealistic to quickly master it. I plan to first choose a framework, such as Vue.js or React.js, and deeply learn its core concepts and usage methods. Only after becoming familiar with the fundamentals will I attempt to use AI to generate frontend code, which can effectively avoid errors and wasted time caused by unfamiliarity.&lt;/p&gt;
&lt;p&gt;In short, the focus for this stage should be on backend development, steadily building my core skills. When the timing is right, I’ll then explore the combination of frontend and AI – potentially yielding greater rewards.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Claude4 released, attempting to develop: Hugo tags, hyperlink translation assistant</title>
        <link>https://ttf248.life/en/p/claude-4-release-hugo-tags-hyperlink-translation-assistant/</link>
        <pubDate>Sat, 24 May 2025 03:05:31 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/claude-4-release-hugo-tags-hyperlink-translation-assistant/</guid>
        <description>&lt;p&gt;This site is developed using Hugo, but I’ve always used Chinese titles, which results in less friendly generated article links. In simpler terms, when shared, they don&amp;rsquo;t look as good because the Chinese characters are escaped into formats like %E4%BD%A0%E5%A5%BD within the links. While you can solve this by setting a slug, it’s tedious to do manually every time.&lt;/p&gt;
&lt;p&gt;Therefore, I decided to try using Claude4 to develop a translation assistant that automatically converts Chinese titles to English slugs and adds hyperlinks within the articles. This would eliminate the need for manual setup.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Claude4 is amazing – its contextual understanding has significantly improved, as has its efficiency in handling complex tasks.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;!-- more --&gt;
&lt;h2 id=&#34;project-address&#34;&gt;Project Address
&lt;/h2&gt;&lt;p&gt;Domestic Project Address: &lt;a class=&#34;link&#34; href=&#34;https://cnb.cool/ttf248/hugo-content-suite&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://cnb.cool/ttf248/hugo-content-suite&lt;/a&gt;
International Project Address: &lt;a class=&#34;link&#34; href=&#34;https://github.com/ttf248/hugo-content-suite&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/ttf248/hugo-content-suite&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;code-implementation&#34;&gt;Code Implementation
&lt;/h2&gt;&lt;p&gt;Let&amp;rsquo;s first discuss the implementation approach: We need to scan all articles, extract tag information and article titles, and then call on our local large model (such as gemma-3-12b-it) for translation.&lt;/p&gt;
&lt;p&gt;In actual development, Claude4 showcased several significant advantages compared to previous generation large models. Due to the diverse functional requirements, Claude4 automatically designed an interactive menu, comprehensively considering various usage scenarios. For example, in tag processing, Claude4 not only supports tag statistics and analysis but also includes classification statistics and can even detect &lt;strong&gt;unlabeled articles&lt;/strong&gt;. Furthermore, it provides &lt;strong&gt;preview&lt;/strong&gt; and tag page generation functionalities.&lt;/p&gt;
&lt;p&gt;Whether it&amp;rsquo;s integrating with local large models, adding translation caches, or performing large-scale code refactoring, Claude4 completes everything in one go, with almost no issues. Despite the relatively small project size, it includes many minor features. In previous development processes, large models often forgot earlier content, but Claude4 performed exceptionally well this time, &lt;strong&gt;virtually without any context forgetting issues.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In short, intelligence has increased, and we plan to switch to Claude4 for more development work as our primary coding model.&lt;/p&gt;
&lt;h2 id=&#34;translation-cache&#34;&gt;Translation Cache
&lt;/h2&gt;&lt;p&gt;This approach, besides reducing the number of calls to large models, performs quite efficiently when running a 12b model locally – it doesn’t waste much time. However, if you need to call the large model every time, it will still be somewhat slow. Furthermore, to fix the connections within articles, if a full update operation is executed and the article title is very long, there&amp;rsquo;s occasionally a situation where the two translated results differ, causing the link to change – which is quite awkward.&lt;/p&gt;
&lt;h2 id=&#34;feature-optimization&#34;&gt;Feature Optimization
&lt;/h2&gt;&lt;p&gt;The entire project was handed over to &lt;code&gt;Claude4&lt;/code&gt; to analyze the space for optimization and obtain the following suggestions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Externalize Configuration - Improve maintainability and flexibility&lt;/li&gt;
&lt;li&gt;Structured Logging - Facilitate troubleshooting and monitoring&lt;/li&gt;
&lt;li&gt;Performance Monitoring - Understand system running status&lt;/li&gt;
&lt;li&gt;User Experience - Visual feedback such as progress bars&lt;/li&gt;
&lt;li&gt;Error Handling - A more robust exception handling mechanism&lt;/li&gt;
&lt;li&gt;Code Organization - Clearer module division&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The code was reviewed, and there were no major issues, such as the configuration files. The original code&amp;rsquo;s configurations were converted to default settings, and if a corresponding configuration file wasn’t found during reading, a default one would be automatically generated to prevent user errors.
Requirement: When translating text into English, dynamically calculate the current translation efficiency, estimate the remaining time, and output relevant information to the console: Now it has obtained the character count of the article, the character count for each line being translated, the translation time, and fits the calculation of the translation time for every 100 characters. Simultaneously, it calculates the estimated remaining translation time.&lt;/p&gt;
&lt;p&gt;The code was completed, but the effect wasn&amp;rsquo;t very satisfactory, so I asked AI to provide a new design solution:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Provide multiple efficiency calculation methods: Real-time efficiency, average efficiency, sliding window efficiency&lt;/li&gt;
&lt;li&gt;Improve display method: Progress bars, segmented statistics, dynamic refresh&lt;/li&gt;
&lt;li&gt;Add more useful metrics: API call counts, success rates, etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;After the code was completed, a new surprise was discovered – translation efficiency statistics flooded in in real time, but without endless scrolling.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;Translating text to English (total 7163 characters)...
Detected 53 lines to translate
  [1/53] Stage1/6 [░░░░░░░░░░░░░░░░░░░░░░░░░] 1.9% Translating 354 characters...
    ✅ Completed (3.1s) | API Call #1
    ✅ Completed (1.5s) | API Call #2
    ✅ Completed (0.9s) | API Call #3
    ✅ Completed (0.2s) | API Call #4
    ✅ Completed (1.0s) | API Call #5
    ✅ Completed (1.0s) | API Call #6
    ✅ Completed (0.2s) | API Call #7
    📊 Progress: 13.2% (7/53) | 12.9% (925/7163) 114.6 characters/second 📊
    ⚡ Efficiency: Real-time 76.4 | Average 117.9 | Recent 109.0 | Stage 113.6 characters/second 📊
    🎯 Success Rate: 100.0% (7/7) | Remaining: 46 lines 7 seconds] 9.4% Translating 110 characters...
    ⏱️ Estimated remaining: 55s | Predicted completion time: 00:10:19 8s] 11.3% Translating 114 characters...
    💾 Processing speed: 3211.3 lines/minute | Total time: 8s] 13.2% Translating 16 characters...
  [8/53] Stage1/6 [███░░░░░░░░░░░░░░░░░░░░░░] 15.1% Translating 166 characters...
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Previously, the control program wasn&amp;rsquo;t written with many features, so I was curious about how it was implemented and looked at the code:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-golang&#34;&gt;// Clear screen and redisplay (dynamic refresh effect)
if translationCount &amp;gt; 1 {
    fmt.Print(&amp;quot;\033[6A\033[K&amp;quot;) // Move up 6 lines and clear
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;performance-statistics-menu&#34;&gt;Performance Statistics Menu
&lt;/h3&gt;&lt;p&gt;The newly added &lt;strong&gt;Performance Statistics Menu&lt;/strong&gt;, which I myself designed, isn&amp;rsquo;t as well-designed as this one.&lt;/p&gt;
&lt;p&gt;📊 Performance Statistics:
🔄 Translation Count: 360
⚡ Cache Hit Rate: 1.4% (5/365)
⏱️ Average Translation Time: 315.927234ms
📁 File Operations: 73
❌ Error Count: 0&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Data Mining&lt;/li&gt;
&lt;li&gt;Deep Learning&lt;/li&gt;
&lt;li&gt;Neural Network&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;progress-bar-display&#34;&gt;Progress Bar Display
&lt;/h3&gt;&lt;p&gt;New &lt;strong&gt;Progress Bar Display&lt;/strong&gt;, detailed progress, elapsed time, estimated remaining time.
Please select function (0-13): 10
🔍 Collecting translation target&amp;hellip;
📄 Loaded cache file, containing 0 translation records
📊 Translation Cache Statistics:
🏷️ Total tags: 229
📝 Total articles: 131
✅ Cached: 0
🔄 To be translated: 360
Confirm to generate full translation cache? (y/n): y
🚀 Generating full translation cache&amp;hellip;
📄 Loaded cache file, containing 0 translation records
🔍 Checking translations in cache&amp;hellip;
🔄 Need to translate 360 new tags
[░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 5/360 (1.4%) - Time taken: 3s - Estimated remaining: 3m8s
💾 Saved cache file, containing 5 translation records
[█░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 10/360 (2.8%) - Time taken: 6s - Estimated remaining: 3m28s
💾 Saved cache file, containing 10 translation records
[██░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 15/360 (4.2%) - Time taken: 9s - Estimated remaining: 3m30s
💾 Saved cache file, containing 15 translation records
[██░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 20/360 (5.6%) - Time taken: 13s - Estimated remaining: 3m36s
💾 Saved cache file, containing 20 translation records
[███░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 25/360 (6.9%) - Time taken: 16s - Estimated remaining: 3m33s
💾 Saved cache file, containing 25 translation records
[████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 30/360 (8.3%) - Time taken: 19s - Estimated remaining: 3m30s
💾 Saved cache file, containing 30 translation records
[████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 35/360 (9.7%) - Time taken: 22s - Estimated remaining: 3m25s
💾 Saved cache file, containing 35 translation records&lt;/p&gt;
&lt;hr&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;=== Hugo Blog Management Tool ===

🚀 Core Features
  1. One-click full blog processing (complete blog workflow)

📝 Content Management
  2. Generate tag pages
  3. Generate article slugs
  4. Translate articles into multi-language versions

💾 Cache Management
  5. View cache status
  6. Generate full translation cache
  7. Clear translation cache

  0. Exit the program
&lt;/code&gt;&lt;/pre&gt;
</description>
        </item>
        <item>
        <title>AI overuse can lead to some lingering effects.</title>
        <link>https://ttf248.life/en/p/ai-overuse-side-effects/</link>
        <pubDate>Wed, 14 May 2025 19:39:50 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/ai-overuse-side-effects/</guid>
        <description>&lt;p&gt;Since we’ve established “AI Inspiration Collision Forum,” there’s been a lot of random content being created, with people experimenting with AI to record and publish things, but thoughtful reflection is becoming increasingly scarce. Moving forward, it would be beneficial to slightly control the output of this section and consolidate it into a monthly magazine format – releasing one article per month would suffice.&lt;/p&gt;
&lt;p&gt;This feels like a sort of aftereffect or side effect; efficiency has increased, but the depth and breadth of thought have declined.&lt;/p&gt;
&lt;h2 id=&#34;efficiency-boost-undeniable&#34;&gt;Efficiency Boost: Undeniable
&lt;/h2&gt;&lt;p&gt;The “Seven-Second Fish Sightseeing” column used to be poorly maintained, with only a few hot events covered. Due to laziness, I hadn’t searched the internet for relevant materials or compiled records. Now that various AI tools are available, all it takes is outlining the key points, and AI can automatically search for related events, generate articles as needed, simply format them, and publish them.&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s like a blessing for lazy people – efficiency has increased significantly, almost to the point of doubling efforts.&lt;/p&gt;
&lt;p&gt;Beyond writing articles, efficiency gains are real when coding. Previously, writing code often required detailed reading of API interface documentation. Now, I can skip this entirely. This is incredibly valuable because familiarizing oneself with APIs is “physical labor,” not “mental labor.” AI handles this part perfectly.&lt;/p&gt;
&lt;h2 id=&#34;spam-content&#34;&gt;Spam Content
&lt;/h2&gt;&lt;p&gt;Many articles have poor quality content, not to say that there’s nothing there; it just doesn&amp;rsquo;t read well, lacking a sense of reality and what people actually experience. It’s a style I didn’t enjoy before – like chewing wax.&lt;/p&gt;
&lt;p&gt;From another perspective, AI-generated content really does feel like products of a流水线 (liú shuǐ liàn - assembly line) production, lacking soul.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;New Era Internet Spam&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;forgetting&#34;&gt;Forgetting
&lt;/h2&gt;&lt;p&gt;This type of document is entirely AI-generated, and the reader’s context is unclear. However, over time, my own impressions will become blurred, or even forgotten.&lt;/p&gt;
&lt;p&gt;Similar issues occur when writing code – without reviewing commit records, I can&amp;rsquo;t remember how I originally thought about it, or why I wrote it that way. This is particularly evident with code generated through repeated communication with AI, where the final code differs significantly from the initial ideas, sometimes drastically so.&lt;/p&gt;
&lt;h2 id=&#34;search&#34;&gt;Search
&lt;/h2&gt;&lt;p&gt;Recently, the number of times I’ve opened Google and Baidu has noticeably decreased. Many questions are now being answered by AI for searching and interaction, and the results are much better than traditional search engines.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s mourn the &lt;code&gt;bing ai&lt;/code&gt;, which may no longer be active, a pioneering AI tool from a major company that could connect to the internet and search.&lt;/p&gt;
&lt;p&gt;I’m using Google less, and the number of times I visit &lt;code&gt;stackoverflow&lt;/code&gt; has decreased as well. Many questions are now simply asked of AI – this website is gradually being phased out by the times.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion
&lt;/h2&gt;&lt;p&gt;My maintained blog, originally with very little traffic, is now even less expected of; it’s primarily a place for self-reflection and recording.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>No coding, design and develop a self-selected stock module.</title>
        <link>https://ttf248.life/en/p/design-develop-custom-stock-module-no-code/</link>
        <pubDate>Thu, 27 Feb 2025 23:20:39 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/design-develop-custom-stock-module-no-code/</guid>
        <description>&lt;ul&gt;
&lt;li&gt;Last month, we experimented with cursor, but due to the limitations of the free quota, we didn&amp;rsquo;t develop overly complex features; we just did some basic testing. We discovered then that Byte also released similar products, both using the same large models – Claude-3.5 – at their core.
Byte’s product is called Trae, initially launched in the Mac version and finally released its Windows version in February of this year. Big companies are good because you can freely “white嫖” (literally translates to &amp;ldquo;free-eat&amp;rdquo;), without having to pay, with unlimited use of Claude-3.5 – this model performs quite well.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Ultimately, we got stuck on the development of candlestick charts. As I don’t understand React at all, I had to give up. To continue developing, I would need to supplement my knowledge of front-end basics, breaking down the task into smaller, more manageable pieces instead of directly giving me a large task: developing candlestick charts.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;issues-found&#34;&gt;Issues Found
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;Due to the lack of training data caused by using foreign AI models and Vue3 + Element-Plus, React was chosen as the frontend framework.&lt;/li&gt;
&lt;li&gt;Occasional syntax errors may exist and require manual fixes.&lt;/li&gt;
&lt;li&gt;Solutions for some complex problems require manual guidance.&lt;/li&gt;
&lt;li&gt;Code structure optimization requires manual instruction.&lt;/li&gt;
&lt;li&gt;The most time-consuming part was packaging the frontend code into a container, due to my zero foundation in &lt;code&gt;.env.production&lt;/code&gt; and &lt;code&gt;tsconfig.json&lt;/code&gt;, I had no concept of these either; I only straightened out the corresponding logic by asking for help from bean sprouts halfway through. There are significant differences between the dev mode and build mode of frontend development, and the checks performed on the code. Backend database and service container scripts were completed in a total of five minutes.
&lt;strong&gt;Currently, AI mainly improves the efficiency of development; having a foundation is best, not that AI will solve all problems for you.&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;repository-address&#34;&gt;Repository Address
&lt;/h2&gt;&lt;p&gt;As the title indicates, this time we’re going to try a chat-based approach with AI instead of writing code, aiming to design and develop a self-trading module. Let&amp;rsquo;s see what results we can achieve.&lt;/p&gt;
&lt;p&gt;Repository address: &lt;a class=&#34;link&#34; href=&#34;https://github.com/ttf248/trae-demo&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/ttf248/trae-demo&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;For detailed usage instructions, please refer to the README.md file in the repository.&lt;/p&gt;
&lt;p&gt;The repository contains numerous submission records, most of which are dialogues between me and Trae, along with my testing of Trae’s functionalities, and notes on whether manual intervention was required to implement each feature.&lt;/p&gt;
&lt;h2 id=&#34;prompt&#34;&gt;Prompt
&lt;/h2&gt;&lt;p&gt;The project is to develop functionality from scratch, based on the following project prototype diagram:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;Based on the project prototype diagram, develop features: stock selection (watchlist), which needs to support adding, deleting, modifying, and querying contracts. The watchlist interface should display basic market data. Support multiple different market switches.

Frontend: react
Backend: golang gin gorm
Database: PostgreSQL

The backend needs to support cross-origin requests, while also considering data validation and error handling. If the backend service is unavailable, the frontend should display an alert message.

The backend needs to display request and response logs; the frontend also prints communication logs for troubleshooting purposes.
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;ui-and-interaction-optimization&#34;&gt;UI and Interaction Optimization
&lt;/h2&gt;&lt;p&gt;The design of the front-end interface relies entirely on Grok. We initially created a prototype within Trae, but it lacked aesthetics. Because the model used has strong coding capabilities but weaker other abilities, we need to use Grok to optimize the front-end UI.&lt;/p&gt;
&lt;p&gt;By taking screenshots of the current interface and uploading them to Grok, we can receive numerous optimization suggestions at once. We then manually evaluate these suggestions and copy them into Trae to execute and observe the results of the optimizations.&lt;/p&gt;
&lt;h3 id=&#34;technology-stack&#34;&gt;Technology Stack
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Frontend: React + TypeScript&lt;/li&gt;
&lt;li&gt;Backend: Golang + Gin + GORM&lt;/li&gt;
&lt;li&gt;Database: PostgreSQL 17&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;system-architecture&#34;&gt;System Architecture
&lt;/h2&gt;&lt;h2 id=&#34;backend-architecture&#34;&gt;Backend Architecture
&lt;/h2&gt;&lt;p&gt;The backend utilizes the Gin framework (Go) to implement RESTful APIs, with the following key modules:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Database Module&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Utilizes GORM as an ORM framework&lt;/li&gt;
&lt;li&gt;Supports database connection configuration via environment variables&lt;/li&gt;
&lt;li&gt;Automatically performs database schema migrations&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Routing Module&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;RESTful API design&lt;/li&gt;
&lt;li&gt;A unified error handling mechanism&lt;/li&gt;
&lt;li&gt;Built-in request logging&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Cross-Origin Handling&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Supports cross-origin requests from local development environments&lt;/li&gt;
&lt;li&gt;Configurable CORS policies&lt;/li&gt;
&lt;li&gt;Supports cookie-based cross-origin access&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;frontend-architecture&#34;&gt;Frontend Architecture
&lt;/h2&gt;&lt;p&gt;The frontend was built using React + TypeScript, implementing the following features:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Stock list display&lt;/li&gt;
&lt;li&gt;Watchlist management&lt;/li&gt;
&lt;li&gt;Real-time quote data display&lt;/li&gt;
&lt;li&gt;Error handling mechanism&lt;/li&gt;
&lt;/ul&gt;</description>
        </item>
        <item>
        <title>AI Development Over Two Years: Somewhat similar to the state before a Docker release – fragmented and rapidly evolving.</title>
        <link>https://ttf248.life/en/p/ai-development-two-years-like-docker-pre-release/</link>
        <pubDate>Thu, 20 Feb 2025 18:16:37 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/ai-development-two-years-like-docker-pre-release/</guid>
        <description>&lt;p&gt;Artificial Intelligence (AI) has undoubtedly been one of the most hotly debated topics in the technology field in recent years, particularly over the past two years, with AI technology experiencing rapid advancements. Whether it’s deep learning, natural language processing, or computer vision and automated decision systems, the application scenarios for AI are constantly emerging. However, despite continuous technological breakthroughs, AI still faces a bottleneck similar to that of a Docker release – a lack of a killer app to truly ignite the market.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Prompt: Write an article: AI has developed for two years, which is somewhat like the state of Docker’s pre-release, lacking a killer application, based on existing technologies, create a perfect landing scenario, Docker did not use too many new technologies, but the overall solution is reasonable, and it changes the workflow of operations and development.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;ai-development-status-the-technology-is-mature-but-applications-still-need-breakthroughs&#34;&gt;AI Development Status: The Technology is Mature, but Applications Still Need Breakthroughs
&lt;/h2&gt;&lt;p&gt;From a technical perspective, AI has made significant progress in the past two years. Whether it’s OpenAI&amp;rsquo;s GPT series models or Google’s BERT and DeepMind’s Alpha series, AI processing capabilities have far exceeded previous expectations. Particularly in the field of natural language processing, models like GPT-4 not only possess powerful generative abilities but also demonstrate astonishing performance in understanding and reasoning.&lt;/p&gt;
&lt;p&gt;However, despite technological advancements, AI&amp;rsquo;s implementation in practical applications faces certain challenges. Similar to Docker’s state before its release, although AI has enormous potential, there hasn&amp;rsquo;t yet been a truly widespread and industry-changing killer app. People are talking about the prospects of AI, but they may not be able to find an application that can directly bring about revolutionary changes. Many AI applications remain in their initial stages of experimentation, and most require further integration and optimization.&lt;/p&gt;
&lt;h2 id=&#34;docker-and-ai-parallels-technology-doesnt-have-to-be-innovation-its-about-the-solution&#34;&gt;Docker and AI Parallels: Technology Doesn&amp;rsquo;t Have to Be Innovation, It’s About the Solution
&lt;/h2&gt;&lt;p&gt;Looking back at Docker’s release history, we can readily see many similarities between the technological environment at the time and the current state of AI development. Before Docker, container technology wasn’t a new concept; early technologies like LXC (Linux Containers) and virtualization already possessed the basic capabilities for containerization. However, Docker didn&amp;rsquo;t introduce disruptive technology itself. Instead, it presented a simpler, more intuitive, and efficient solution through the clever integration and optimization of existing technologies. This solution didn’t rely on revolutionary technological breakthroughs, but rather addressed many pain points in software deployment, scaling, and management processes, significantly simplifying the workflow.&lt;/p&gt;
&lt;p&gt;Similarly, the AI field is facing a similar situation. While current AI technology isn&amp;rsquo;t “new,” achieving truly large-scale applications still requires a perfect implementation scenario – just like Docker, integrating and optimizing existing technologies to form a reasonable application solution. Killer AI applications may not rely on entirely new technological breakthroughs, but rather on how to integrate existing technologies to solve practical business pain points and needs.&lt;/p&gt;
&lt;h2 id=&#34;how-to-find-ais-docker-moment&#34;&gt;How to Find AI&amp;rsquo;s &amp;ldquo;Docker Moment&amp;rdquo;?
&lt;/h2&gt;&lt;p&gt;To truly enable the widespread adoption of AI technology, we need to focus on several key areas:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Deep Exploration of Practical Scenarios&lt;/strong&gt;
Currently, many AI applications are still largely experimental and lack large-scale real-world implementation.  For example, while AI customer service and intelligent recommendations are widely used, their functionality is often limited and hasn&amp;rsquo;t yet broken through industry bottlenecks. True breakthroughs may come from industries that have been struggling with traditional methods for a long time, such as healthcare, manufacturing, and logistics. AI can help these companies improve efficiency and reduce costs in complex scenarios through more efficient data processing and predictive analytics.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Productization and Ease of Use&lt;/strong&gt;
Just like Docker simplifies containerization processes to boost operational efficiency, the ease of use of AI products is equally crucial. The widespread adoption of AI isn&amp;rsquo;t just about technological proliferation; it’s also about product proliferation. Integrating AI into daily workflows, allowing users to easily utilize these tools without needing to deeply understand the technology, is a key step in AI implementation.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Ecosystem Building and Standardization&lt;/strong&gt;
Any new technology’s widespread adoption relies on ecosystem building. Docker&amp;rsquo;s rapid rise was due to its openness and compatibility, enabling developers to seamlessly integrate with various cloud platforms, tools, and services. Similarly, the future of AI depends on the construction of an ecosystem.  Standardization of AI, model sharing, data openness, and technological interoperability will all influence whether AI can form widespread industry applications.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;conclusion-the-future-of-ai-is-full-of-possibilities-but-requires-more-refined-implementation&#34;&gt;Conclusion: The Future of AI is Full of Possibilities, but Requires More Refined Implementation
&lt;/h2&gt;&lt;p&gt;Despite the significant advancements in AI technology over the past two years, it remains in the stage of “lacking killer applications” at present. Similar to containerization technology before Docker’s release, AI needs a reasonable application scenario that deeply integrates existing technologies with business requirements to truly achieve large-scale adoption and widespread use. While technological innovation is important, solutions that simplify processes and improve efficiency are more likely to drive the popularization and development of the technology.&lt;/p&gt;
&lt;p&gt;In the future, AI may evolve like Docker – not through disruptive technological breakthroughs, but by integrating existing technologies to create a perfect application scenario, ultimately changing the way we work and live.&lt;/p&gt;</description>
        </item>
        <item>
        <title>ollama local deployment of deepseek-R1</title>
        <link>https://ttf248.life/en/p/ollama-local-deployment-deepseek-r1/</link>
        <pubDate>Fri, 07 Feb 2025 22:41:02 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/ollama-local-deployment-deepseek-r1/</guid>
        <description>&lt;p&gt;Ollama is an open-source AI tool designed to enable users to run and deploy large language models (LLMs) locally. Its goal is to provide a convenient and efficient way for developers to use models like GPT on their local machines without relying on cloud services. Ollama supports multiple models and focuses on optimizing performance, allowing even resource-constrained devices to smoothly run these models.&lt;/p&gt;
&lt;p&gt;Through Ollama, users can utilize text-based AI applications and interact with locally deployed models without worrying about data privacy or high API usage fees. You can invoke different models via a command-line interface (CLI) for tasks such as natural language processing and question answering.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Ollama is suitable for experimenting with various models; after testing the Windows version, it couldn&amp;rsquo;t fully leverage the hardware’s performance, possibly due to the Windows version. When deploying 32b parameter models, with low memory and GPU load, the response speed is slow.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;hardware-overview&#34;&gt;Hardware Overview
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;Operating System: Windows 11&lt;/li&gt;
&lt;li&gt;CPU: i7-10700K&lt;/li&gt;
&lt;li&gt;Memory: 40GB&lt;/li&gt;
&lt;li&gt;Graphics Card: RTX 3060 12GB&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;environment-setup&#34;&gt;Environment Setup
&lt;/h2&gt;&lt;p&gt;Add the system environment variable to facilitate subsequent use:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;set OLLAMA_MODELS=E:\ollama&lt;/code&gt;&lt;/strong&gt;
This variable specifies the location where Ollama models are stored. &lt;code&gt;E:\ollama&lt;/code&gt; is a folder path indicating that all local model files will be stored in this directory. Ollama will load and use the language models you download or deploy based on this path. You can store model files in other locations by simply changing this path.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;set OLLAMA_HOST=127.0.0.1:8000&lt;/code&gt;&lt;/strong&gt;
This environment variable sets the host and port for the Ollama service.
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;127.0.0.1&lt;/code&gt; is the localhost address, meaning the Ollama service will only listen for requests from the local machine.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;8000&lt;/code&gt; is the specified port number, indicating that the Ollama service will wait for and process requests on port 8000. You can change the port number if needed, but make sure it&amp;rsquo;s not already in use by another application.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;set OLLAMA_ORIGINS=*&lt;/code&gt;&lt;/strong&gt;
This environment variable controls which origins are allowed to access the Ollama service.
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;*&lt;/code&gt; indicates that all origins (i.e., all domains and IP addresses) can access the Ollama service. This is typically used in development and debugging environments, and in production environments, it&amp;rsquo;s usually necessary to specify stricter origin control, limiting only specific domains or IPs to access your service for enhanced security.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;deepseek-r1-model-deployment&#34;&gt;DeepSeek-R1 Model Deployment
&lt;/h2&gt;&lt;p&gt;ollama installation is straightforward, so we won&amp;rsquo;t detail it here.&lt;/p&gt;
&lt;p&gt;Post-installation verification:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;C:\Users\core&amp;gt;ollama -v
ollama version is 0.5.11
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To deploy the model, refer to the official model page and select the appropriate parameter model: &lt;code&gt;ollama run deepseek-r1:14b&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;The 14b parameter version effectively remembers conversation context; smaller parameter versions cannot retain context. The 32b parameter version is very sluggish when deployed locally and hasn&amp;rsquo;t been further tested.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.ollama.com/library/deepseek-r1&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://www.ollama.com/library/deepseek-r1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/SPEvYTmTBxhoEkJqm1yPmw&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://mp.weixin.qq.com/s/SPEvYTmTBxhoEkJqm1yPmw&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://blog.csdn.net/x18990027/article/details/145368094&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://blog.csdn.net/x18990027/article/details/145368094&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
        </item>
        <item>
        <title>Cursor AI Programming IDE Trial</title>
        <link>https://ttf248.life/en/p/cursor-ai-programming-ide-trial/</link>
        <pubDate>Thu, 23 Jan 2025 19:30:13 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/cursor-ai-programming-ide-trial/</guid>
        <description>&lt;p&gt;It seems like another year has passed, and the biggest change at work is a significant increase in AI participation. Previously, switching between different development languages required developers to be familiar with various language-specific API interfaces. Now, these basic code snippets can all be generated by AI, which is a huge blessing for developers.&lt;/p&gt;
&lt;h2 id=&#34;chatgpt&#34;&gt;ChatGPT
&lt;/h2&gt;&lt;p&gt;As early as 2023, I’ve written two simple introductory articles about it. Now it&amp;rsquo;s been 25 years – how to put this… I haven’t felt a significant improvement. It still needs to develop its own cognition, be able to reasonably break down tasks, and, of course, most importantly, identify whether AI-generated code contains bugs.&lt;/p&gt;
&lt;h2 id=&#34;github-copilot&#34;&gt;Github Copilot
&lt;/h2&gt;&lt;p&gt;It was a long time ago, but I saw some information saying that Singapore deployed the server and it’s available for use in China. No longer need to maintain a VPN connection for extended periods. Of course, when logging in, you still need to connect to a VPN, but this VPN only needs to be used during login, and then you can turn it off.&lt;/p&gt;
&lt;p&gt;In daily use, Github Copilot is also heavily relied upon. This plugin can be directly used in VS Code and Visual Studio without switching between the two applications. Compared to ChatGPT, Github Copilot provides better support for projects, is more user-friendly in interaction, and allows you to feed partial local files to it – &lt;strong&gt;“training” the AI&lt;/strong&gt; – so that the generated code is more aligned with your project.&lt;/p&gt;
&lt;h2 id=&#34;cursor-ai&#34;&gt;Cursor AI
&lt;/h2&gt;&lt;p&gt;Recently I’ve seen a new AI programming IDE, Cursor AI. This IDE is based on Github Copilot, but this IDE is more intelligent and can help you create files directly.
I tried it briefly and found it to be pretty good, but its understanding of existing projects isn&amp;rsquo;t quite there yet. When working with large local project files, or for major refactoring, optimization, and adjustments, developers still need to &lt;strong&gt;break down tasks&lt;/strong&gt;.
Here’s an example: Switching to Cursor’s engineering mode, inputting the following content: “Create a personal resume webpage, supporting multiple different styles switching, and remember to fill in some personal information for data display.”
After several back-and-forths (&lt;strong&gt;pulling&lt;/strong&gt;), you can get the following webpage. Of course, this webpage is relatively simple, but it’s still pretty good for beginners.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Currently, registered users can enjoy a free trial of 150 advanced APIs, while paying users are limited to 5000 advanced APIs.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;cursor-ai-1&#34;&gt;Cursor AI
&lt;/h2&gt;&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://ttf248.life/cursor/index.html&#34; &gt;Resume&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>AI Programming and Task Decomposition</title>
        <link>https://ttf248.life/en/p/ai-programming-and-task-decomposition/</link>
        <pubDate>Fri, 22 Dec 2023 08:44:26 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/ai-programming-and-task-decomposition/</guid>
        <description>&lt;p&gt;Two years ago, I added a copy function to the site, which took me half a day of tinkering. Ultimately, the rendering effect wasn&amp;rsquo;t quite satisfactory, and as a somewhat mediocre frontend developer, I didn’t plan on improving it – if it worked, it was good enough. This year, I used &lt;code&gt;AI&lt;/code&gt; to develop a mini-program, which has also made me more familiar with frontend development. I refactored it (the &lt;code&gt;AI&lt;/code&gt; redesigned it).&lt;/p&gt;
&lt;p&gt;Realization: Breaking down tasks really helps; many small tasks are quite natural for the &lt;code&gt;AI&lt;/code&gt; to develop. For the closed mini-program, 75% of the code was provided by &lt;code&gt;AI&lt;/code&gt;, broken down into multiple tasks delivered, and manually assembled the project code.&lt;/p&gt;
&lt;h2 id=&#34;previous-context&#34;&gt;Previous Context
&lt;/h2&gt;&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://ttf248.github.io/post/2022/001/%E4%BB%A5%E4%B8%BA%E5%BE%88%E7%AE%80%E5%8D%95%E7%9A%84%E4%BA%8B%E6%83%85%E5%A2%9E%E5%8A%A0%E4%B8%80%E4%B8%AA%E4%BB%A3%E7%A0%81%E5%A4%8D%E5%88%B6%E6%8C%89%E9%92%AE/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Making simple things easier: Adding a code copy button&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;main-text&#34;&gt;Main Text
&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;promot&lt;/strong&gt;: Design CSS and JavaScript code to locate webpage elements with the class &amp;ldquo;highlight&amp;rdquo;, and within their container, add a copy button to enable copying of the content.&lt;/p&gt;
&lt;p&gt;The first draft of the code was immediately problematic; I generated a revised version that is largely functional, but the copied content is incorrect, containing redundant text. Further guidance led to optimization, specifying core color design and effects.&lt;/p&gt;
&lt;p&gt;Having learned your specific requirements for button size and color, we redesigned the button styles to accommodate these demands.  Here’s the adjusted CSS style sheet, incorporating your provided color scheme and optimizing the button size:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-css&#34;&gt;.copy-btn {
  position: absolute;
  top: 2px; /* Adjust position */
  right: 2px;
  padding: 3px 6px; /* Reduce size */
  font-size: 12px; /* Decrease font size */
  background-color: #8FD1E1; /* Button background color */
  color: #272822; /* Text color */
  border: 1px solid #272822; /* Border color */
  border-radius: 3px; /* Rounded corners */
  cursor: pointer;
  opacity: 0.7; /* Default semi-transparent */
  transition: opacity 0.3s ease, background-color 0.3s ease; /* Smooth transition effect */
}

.copy-btn:hover {
  opacity: 1; /* Unhide on hover */
  background-color: #A3D4E5; /* Background color on hover */
}

.highlight {
  position: relative;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this style sheet, I reduced the button&amp;rsquo;s size and font size to create a more refined look. The button’s default state is semi-transparent, becoming fully opaque when hovered over, with a slightly brighter background color for visual feedback. This design meets your color requirements while maintaining modern and user-friendly characteristics.&lt;/p&gt;
&lt;p&gt;Here’s a simpler &lt;code&gt;JavaScript&lt;/code&gt; implementation:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;document.addEventListener(&#39;DOMContentLoaded&#39;, function() {
  // Find all .highlight elements
  const highlights = document.querySelectorAll(&#39;.highlight&#39;);

  highlights.forEach(function(highlight) {
    // Create copy button
    const copyBtn = document.createElement(&#39;button&#39;);
    copyBtn.textContent = &#39;Copy&#39;;
    copyBtn.classList.add(&#39;copy-btn&#39;);

    // Add the button to each .highlight element
    highlight.appendChild(copyBtn);

    // Add click event listener
    copyBtn.addEventListener(&#39;click&#39;, function() {
      // Assume the first &amp;lt;code&amp;gt; element within .highlight contains the code to copy
      const code = highlight.querySelector(&#39;code&#39;) ? highlight.querySelector(&#39;code&#39;).textContent : &#39;&#39;;

      navigator.clipboard.writeText(code).then(() =&amp;gt; {
        // Display success message
        copyBtn.textContent = &#39;Copied!&#39;;
        setTimeout(() =&amp;gt; {
          copyBtn.textContent = &#39;Copy&#39;;
        }, 2000);
      }).catch(err =&amp;gt; {
        console.error(&#39;Copy failed: &#39;, err);
      });
    });
  });
});
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;epilogue&#34;&gt;Epilogue
&lt;/h2&gt;&lt;p&gt;While I had hoped to see a copy button added in the top-right corner of the code area, due to my lack of familiarity with frontend development, I was unable to accurately describe the functionality based on keywords and spent several rounds iterating with &lt;code&gt;AI&lt;/code&gt; without achieving the desired results.&lt;/p&gt;
&lt;p&gt;It remains to be seen what impact &lt;code&gt;AI&lt;/code&gt; will have on software development as time goes on.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Prompt Engineer</title>
        <link>https://ttf248.life/en/p/prompt-engineer/</link>
        <pubDate>Sun, 26 Mar 2023 20:46:53 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/prompt-engineer/</guid>
        <description>&lt;p&gt;Just as we needed to learn the techniques of searching engines back then, we also need to learn some techniques for communicating with &lt;code&gt;AI&lt;/code&gt;, providing reasonable and sufficient constraints, and efficiently obtaining the answers we need.&lt;/p&gt;
&lt;p&gt;If you look at it from a different angle, current &lt;code&gt;AI&lt;/code&gt; is like a very good student with excellent memory – it has the ability to memorize everything. What we need to do is learn how to communicate with &lt;code&gt;AI&lt;/code&gt; correctly, effectively, and precisely describe our needs to help &lt;code&gt;AI&lt;/code&gt; generate the expected results.&lt;/p&gt;
&lt;p&gt;##科普
The incredibly popular &lt;code&gt;AI&lt;/code&gt; – specifically &lt;code&gt;Generative Pre-Training&lt;/code&gt; – literally translates to generative pre-training. It’s a deep learning model that trains text generation based on internet-available data, used for tasks like question answering, text summarization generation, machine translation, classification, code generation, and conversational AI. Currently, there have been various versions of models such as GPT-1, GPT-2, GPT-3, and GPT-4, each larger and more powerful than the previous one.&lt;/p&gt;
&lt;h2 id=&#34;does-it-really-have-intelligence&#34;&gt;Does it Really Have Intelligence?
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;Similarity is high, accuracy increases accordingly.&lt;/li&gt;
&lt;li&gt;Basic, repetitive tasks, after specific training, no longer require human intervention.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Generative AI is a technology that utilizes existing data such as text, audio, and images to create new content. It can be used for various tasks including text generation, speech synthesis, image generation, and dialogue systems. The logical reasoning of Generative AI depends on its training data and model structure. Generally, Generative AI can follow grammar, logic, and common sense to a certain extent, but it may also produce errors, biases, or unrealistic content. Therefore, the output of Generative AI requires human judgment and verification; it should not be blindly trusted or used.&lt;/p&gt;
&lt;h2 id=&#34;prompt-engineer&#34;&gt;Prompt Engineer
&lt;/h2&gt;&lt;p&gt;Time flows only forward, and people need to learn to adapt to the current. You can think of &lt;code&gt;AI&lt;/code&gt; as not intelligent, lacking logic, and often producing unusable code.&lt;/p&gt;
&lt;p&gt;If you shift your perspective, the current &lt;code&gt;AI&lt;/code&gt; is like a child with excellent memory – it has the ability to memorize after reading something repeatedly, much like copying homework. What we need to do is learn how to communicate correctly, effectively, and with &lt;code&gt;AI&lt;/code&gt;, precisely describing our needs to help &lt;code&gt;AI&lt;/code&gt; generate the desired results.&lt;/p&gt;
&lt;h2 id=&#34;dialogue-patterns&#34;&gt;Dialogue Patterns
&lt;/h2&gt;&lt;p&gt;Two years ago, when GitHub Copilot was released, no one could have imagined that two years later, OpenAI would appear out of nowhere, allowing humans to recognize the capabilities of large language models.&lt;/p&gt;
&lt;p&gt;Based on &lt;strong&gt;comment-based programming&lt;/strong&gt; and &lt;strong&gt;conversational programming&lt;/strong&gt;, the interaction logic is completely different. The dialogue pattern is friendly for novice users, and it’s necessary to mention NewBing providing follow-up prompts after each question. Microsoft is trying to guide users to obtain more content from the AI knowledge base.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Example Scenario
&lt;strong&gt;Scene:&lt;/strong&gt; A long high-speed train journey, coding is a good choice. With limited traffic, processing a few images, obtaining image materials, downloading software to process images is a bit extravagant, and network conditions don’t allow it.
&lt;strong&gt;Requirement:&lt;/strong&gt; There are a batch of picture materials with transparent background images used as navigation bars for small programs. The selected state is colored, and the unselected state is black and white. The materials are all in color, and they need to be preprocessed.
&lt;strong&gt;Prompt:&lt;/strong&gt; Write a python code snippet to read all pictures from a folder, convert the pictures into black and white, maintain the transparent background, save the pictures, optimize the image quality, reduce file size, and display it in the console, comparing the sizes before and after optimization.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import necessary libraries
import os
from PIL import Image

# Define a function to convert images to black and white, maintain transparency, save images, and return file sizes
def convert_and_save(image_file):
    # Open the image
    image = Image.open(image_file)
    # Get the image mode, if it&#39;s RGBA mode, indicating a transparent background
    mode = image.mode
    if mode == &amp;quot;RGBA&amp;quot;:
        # Create a white background image with the same size as the original image
        background = Image.new(&amp;quot;RGB&amp;quot;, image.size, (255, 255, 255))
        # Paste the original image onto the white background, ignoring transparent pixels
        background.paste(image, mask=image.split()[3])
        # Convert the composite image to grayscale mode
        gray_image = background.convert(&amp;quot;L&amp;quot;)
        # Convert the grayscale image back to RGBA mode to preserve transparency
        final_image = gray_image.convert(&amp;quot;RGBA&amp;quot;)
    else:
        # If it&#39;s not RGBA mode, directly convert the image to grayscale mode
        final_image = image.convert(&amp;quot;L&amp;quot;)
    # Get the original image file name and extension
    file_name, file_ext = os.path.splitext(image_file)
    # Define the new image file name with _bw suffix for black and white
    new_file_name = file_name + &amp;quot;_bw&amp;quot; + file_ext
    # Save the new image and optimize quality, reduce file size
    final_image.save(new_file_name, optimize=True)
    # Get the original and new image file sizes and return
    old_size = os.path.getsize(image_file)
    new_size = os.path.getsize(new_file_name)
    return old_size, new_size

# Define a folder path to store all images
folder_path = &amp;quot;images&amp;quot;
# Iterate through all files in the folder
for file in os.listdir(folder_path):
    # Concatenate the full file path
    file_path = os.path.join(folder_path, file)
    # Check if the file is an image file based on extension (can add or modify extensions as needed)
    if file_path.endswith((&amp;quot;.png&amp;quot;, &amp;quot;.jpg&amp;quot;, &amp;quot;.jpeg&amp;quot;, &amp;quot;.gif&amp;quot;)):
        # Call the function to convert and save the image, and get the file size
        old_size, new_size = convert_and_save(file_path)
        # Display the comparison of file sizes before and after optimization in the console
        print(f&amp;quot;{file}: {old_size} bytes -&amp;gt; {new_size} bytes&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;AI&lt;/code&gt; quickly provided an answer, looked very beautiful, and worked well, experienced programmers would find that the code below was not very generic, and couldn’t be put into production:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Fixed parameters&lt;/li&gt;
&lt;li&gt;No log files, all information is printed to the console&lt;/li&gt;
&lt;li&gt;The task of image processing is suitable for a multi-threaded model
&lt;code&gt;NewBing&lt;/code&gt; appeared, based on the prompt, chose a question: &lt;strong&gt;What can be improved in this code?&lt;/strong&gt;
The answer given this time was far beyond expectations, and most programmers would think of optimization measures, &lt;code&gt;AI&lt;/code&gt; gave corresponding suggestions.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;This code may still have some improvements, such as:

You can use the argparse library to allow users to specify the folder path and extension through command-line parameters instead of hardcoding them in the code.
You can use the logging library to record the running status of the program instead of printing it directly to the console.
You can use try-except statements to catch potential exceptions, such as file opening failure, image conversion failure, etc., and handle them

## Let&#39;s illustrate with an example

```python
# Import necessary libraries
import argparse
import logging
import multiprocessing
import os

from PIL import Image


# Define a function to convert and save the image, maintaining transparent background and optimizing file size
def convert_and_save(image_file):
    # Open the image
    try:
        image = Image.open(image_file)
    except Exception as e:
        logging.error(f&amp;quot;Failed to open {image_file}: {e}&amp;quot;)
        return None, None
    # Get the image mode; if it&#39;s RGBA, it has a transparent background
    mode = image.mode
    if mode == &amp;quot;RGBA&amp;quot;:
        # Create a white background image with the same size
        background = Image.new(&amp;quot;RGB&amp;quot;, image.size, (255, 255, 255))
        # Paste the original image onto the white background, ignoring transparent pixels
        background.paste(image, mask=image.split()[3])
        # Convert the combined image to grayscale mode
        gray_image = background.convert(&amp;quot;L&amp;quot;)
        # Convert the grayscale image back to RGBA mode to preserve the transparent background
        final_image = gray_image.convert(&amp;quot;RGBA&amp;quot;)
    else:
        # If it&#39;s not RGBA, directly convert the image to grayscale mode
        final_image = image.convert(&amp;quot;L&amp;quot;)
    # Get the original image file name and extension
    file_name, file_ext = os.path.splitext(image_file)
    # Define the new image file name with _bw suffix for black and white
    new_file_name = file_name + &amp;quot;_bw&amp;quot; + file_ext
    # Save the new image, optimizing quality to reduce file size
    try:
        final_image.save(new_file_name, optimize=True)
    except Exception as e:
        logging.error(f&amp;quot;Failed to save {new_file_name}: {e}&amp;quot;)
        return None, None
    # Get the original image and new image file sizes, and return
    old_size = os.path.getsize(image_file)
    new_size = os.path.getsize(new_file_name)
    return file_name, old_size, new_size

# Define a function to parse command-line arguments and return the folder path and extension list
def parse_args():
    # Create an argument parser object
    parser = argparse.ArgumentParser(description=&amp;quot;Convert images to black and white and optimize quality.&amp;quot;)
    # Add a required positional argument, representing the folder path
    parser.add_argument(&amp;quot;folder_path&amp;quot;, help=&amp;quot;The path of the folder that contains the images.&amp;quot;)
    # Add an optional argument, representing the extension list, defaulting to png, jpg, jpeg, and gif
    parser.add_argument(&amp;quot;-e&amp;quot;, &amp;quot;--extensions&amp;quot;, nargs=&amp;quot;+&amp;quot;, default=[&amp;quot;.png&amp;quot;, &amp;quot;.jpg&amp;quot;, &amp;quot;.jpeg&amp;quot;, &amp;quot;.gif&amp;quot;], help=&amp;quot;The extensions of the image files.&amp;quot;)
    # Parse command-line arguments and return the result object
    args = parser.parse_args()
    return args.folder_path, args.extensions

# Define a function to print the comparison of file sizes before and after optimization
def print_result(result):
    # If the result is not empty, indicating successful conversion and saving
    if result:
        # Unpack the result into a tuple of filename and file size tuple
        if len(result) == 3:
            file, old_size, new_size = result
            # Display the comparison of file sizes before and after optimization in the console
            logging.info(f&amp;quot;{file}: {old_size} bytes -&amp;gt; {new_size} bytes&amp;quot;)
        else:
            # Log the result if it&#39;s not a tuple of 3 elements
            logging.info(f&amp;quot;{result}&amp;quot;)

# Configure the logger, outputting logs to the console and files, setting the log level to INFO
logging.basicConfig(level=logging.INFO,
                    format=&amp;quot;%(asctime)s %(levelname)s %(message)s&amp;quot;,
                    handlers=[logging.StreamHandler(),
                              logging.FileHandler(&amp;quot;log.txt&amp;quot;)])

# Call the function to get the folder path and extension list
folder_path, extensions = parse_args()

if __name__ == &amp;quot;__main__&amp;quot;:
    # Windows needs this function because Windows lacks the fork() function (not entirely accurate).
    # Therefore, on Windows, forking is simulated by creating a new process, and the code - Additionally, this new process is instructed to run the code passed through a pipe by passing the `--multiprocessing-fork` command-line argument to it.
- If you examine the implementation of the `freeze_support()` function, its task is to check which process it&#39;s running in and whether it should execute the code passed through a pipe.
- `multiprocessing.freeze_support()`

- A process pool is created, automatically allocating processes based on the number of cores on the computer.
- An empty list, `results`, is created to store the result objects of asynchronous tasks.
- The script iterates through all files in the folder:
- The full file path is constructed using `os.path.join()`.
- It checks if the file ends with any of the extensions specified in the `extensions` list (you can modify this list as needed).
- If it&#39;s an image file, the `convert_and_save` function is called to convert and save the image asynchronously, without blocking the main process. The file size is also obtained.  The result object is then added to the `results` list using `pool.apply_async()` with the callback function `print_result`.
- The process pool is closed to stop accepting new tasks.
- `pool.join()` is called to wait for all tasks in the pool to complete.

## Epilogue
Due to local development being on a `windows` system, the first answer given by `AI` did not include the `main` function and also lacked `multiprocessing.freeze_support`. The code was fixed after following up and encountering an error. 
Just as learning the techniques of search engines required skill, we also need to learn how to communicate with `AI`, providing reasonable and sufficient constraints to efficiently obtain the desired answers.
Note: **If you are a programming beginner, if you still don&#39;t understand certain parts of the code based on the given comments, please continue to ask related questions.**&lt;/code&gt;&lt;/pre&gt;
</description>
        </item>
        <item>
        <title>AI-assisted programming, the evolution of productivity</title>
        <link>https://ttf248.life/en/p/ai-assisted-programming-productivity-evolution/</link>
        <pubDate>Tue, 28 Feb 2023 17:05:17 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/ai-assisted-programming-productivity-evolution/</guid>
        <description>&lt;p&gt;“GitHub Copilot” was released less than two years ago, and “ChatGPT” has appeared. I don’t fully understand the underlying principles; I&amp;rsquo;ve been using them for a while. The two tools’ levels of assistance are completely different, but they have both achieved a significant increase in productivity.&lt;/p&gt;
&lt;p&gt;For things that are too complex, AI still can’t do it, because they lack logic, or they rely on patterns or fixed frameworks – the training data is sufficient, and AI’s effect is nine out of ten.&lt;/p&gt;
&lt;h2 id=&#34;github-copilot&#34;&gt;GitHub Copilot
&lt;/h2&gt;&lt;p&gt;When publishing, I looked at the official introduction’s “demo” and found it wasn&amp;rsquo;t very smart. After trying it out, it wasn’t great, so I gave up on it.&lt;/p&gt;
&lt;p&gt;Before 2022, when I had some free time, I switched to the new version to try it out, and the effect was already quite good. The only issue was that the speed in China was a bit slow, likely due to network reasons. For languages like &lt;code&gt;Python&lt;/code&gt;, the code generated based on comments was pretty good, and much faster than researching and writing code myself.&lt;/p&gt;
&lt;h2 id=&#34;chatgpt&#34;&gt;ChatGPT
&lt;/h2&gt;&lt;p&gt;&lt;code&gt;Github Copilot&lt;/code&gt; still focuses on developers, i.e., from a development perspective when designing the product, requiring you to understand how to program and guide AI assistance through comments.
&lt;code&gt;ChatGPT&lt;/code&gt; has moved to the next level, &lt;strong&gt;allowing non-experts to quickly get started&lt;/strong&gt;.  A conversational chat interface where you can pose questions, describe requirements, and ask for clarification if needed.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s take an example: You know about tokenization but don’t know how to code, wanting to calculate the similarity of strings:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;Python 分词计算 維天運通 LOGORY 和 合肥維天運通信息科技股份有限公司 两个字符串的相似度；
比较前，统一处理字符串，英文转换为小写；
第二个字符中如果能找到第一个字符的首字符，将第二个字符串截位，截位的长度为第一个字符串的长度
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Hello, this is Bing. 😊&lt;/p&gt;
&lt;p&gt;According to online information 123, Python tokenization can be implemented using the &lt;code&gt;jieba&lt;/code&gt; library. The &lt;code&gt;jieba&lt;/code&gt; library supports three segmentation modes: precise mode, full mode, and search engine mode. You can choose an appropriate mode based on your needs.&lt;/p&gt;
&lt;p&gt;To compare two strings, you first need to tokenize them and then calculate their word vectors or edit distance metrics. You can use libraries like NumPy or SciPy for mathematical operations.&lt;/p&gt;
&lt;p&gt;To standardize string processing, converting English to lowercase, you can use Python&amp;rsquo;s built-in &lt;code&gt;lower()&lt;/code&gt; function.&lt;/p&gt;
&lt;p&gt;To slice the second string, you can use Python’s slicing operator [].&lt;/p&gt;
&lt;p&gt;Here is a possible code example:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import jieba
import numpy as np
from scipy.spatial.distance import cosine

# Define two strings
str1 = &amp;quot;維天運通 LOGORY&amp;quot;
str2 = &amp;quot;合肥維天運通信息科技股份有限公司&amp;quot;

# Standardize the string, convert English to lowercase
str1 = str1.lower()
str2 = str2.lower()

# Find the position of the first character of the first string in the second string
index = str2.find(str1[0])

# If found, slice the second string
if index != -1:
    str2 = str2[index:index+len(str1)]

# Tokenize the two strings using the jieba library
words1 = list(jieba.cut(str1))
words2 = list(jieba.cut(str2))

# Calculate the word vectors of the two strings (using frequency as weights)
vector1 = np.zeros(len(words1))
vector2 = np.zeros(len(words1))

for i, word in enumerate(words1):
    vector1[i] += 1 # Count the number of times each word appears in the first string
    if word in words2:
        vector2[i] += 1 # Count the number of times each word appears in the second string

# Calculate the similarity between the two strings (using cosine distance as an index)
similarity = 1 - cosine(vector1, vector2)

print(&amp;quot;The similarity between the two strings is:&amp;quot;, similarity)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Furthermore, consult the results of running and congratulations, you have discovered a problem. The result is fabricated, it does not actually run the code, but will give you a good result.&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&#34;example&#34;&gt;Example
&lt;/h3&gt;&lt;p&gt;Question: SQL statement to retrieve records from the product code table where the product code is within the range 80000-89999.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;Hello, this is Bing. 😊

According to search result 12, you can use the SQL BETWEEN operator to select values within a given range. The BETWEEN operator includes both the start and end values.

You need to first specify the table name and column name that you want to query, and then use the BETWEEN operator and the range value in the WHERE clause.

For example, if you want to retrieve records from the product code table where the product code is between 80000 and 89999, you can use the following statement:

SELECT * FROM product_code
WHERE code BETWEEN 80000 AND 89999;
Here, &#39;*&#39; represents selecting all columns [3]. If you only want to select specific columns, such as name and price, you can separate them with commas, as shown below:

SELECT name, price FROM product_code
WHERE code BETWEEN 80000 AND 89999;

I hope this helps.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Continue questioning, negating it: The product code is a string, your code is wrong&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;Sorry, I didn&#39;t notice that the product code is a string. If that’s the case, you need to add single quotes around both ends of the range values, as follows:

SELECT * FROM product_code
WHERE code BETWEEN &#39;80000&#39; AND &#39;89999&#39;;

Please forgive my mistake. 🙏

Congratulations, it will recognize mistakes.
&lt;/code&gt;&lt;/pre&gt;
</description>
        </item>
        
    </channel>
</rss>
