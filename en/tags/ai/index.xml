<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>AI on Uncle Xiang&#39;s Notebook</title>
        <link>https://blog.ttf248.life/en/tags/ai/</link>
        <description>Recent content in AI on Uncle Xiang&#39;s Notebook</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en</language>
        <lastBuildDate>Thu, 29 May 2025 23:50:36 +0800</lastBuildDate><atom:link href="https://blog.ttf248.life/en/tags/ai/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>Old habits, dazzling flowers captivate the eyes</title>
        <link>https://blog.ttf248.life/en/p/old-ailment-stunning-flowers/</link>
        <pubDate>Mon, 26 May 2025 23:54:12 +0800</pubDate>
        
        <guid>https://blog.ttf248.life/en/p/old-ailment-stunning-flowers/</guid>
        <description>&lt;p&gt;Having focused on backend development for many years, I recently started exploring &lt;strong&gt;&lt;em&gt;italicized and bolded&lt;/em&gt;&lt;/strong&gt; frontend interfaces to implement one, but in reality, these attempts haven&amp;rsquo;t been particularly helpful for my current work and have instead scattered my focus&lt;/p&gt;
&lt;h2 id=&#34;applicable-scenarios-for-ai&#34;&gt;Applicable scenarios for AI
&lt;/h2&gt;&lt;p&gt;AI tools can be particularly effective in small projects, especially when writing functions that are independent, have low system coupling, and feature simple business logic. These tasks typically involve clear inputs and outputs with limited context dependencies, making them well-suited for current AI-assisted programming capabilities.&lt;/p&gt;
&lt;p&gt;However, AI&amp;rsquo;s limitations become apparent when dealing with complex system architectures or deep business logic. It may generate code that appears reasonable but is actually detached from the project’s actual needs, and even introduce potential issues difficult to debug. In these scenarios, AI is better suited as an assistive tool rather than a fully relied-upon code generator. We need to rigorously review and test the generated code to ensure it meets practical requirements.&lt;/p&gt;
&lt;h2 id=&#34;the-cost-of-mistakes-and-learning&#34;&gt;The Cost of Mistakes and Learning
&lt;/h2&gt;&lt;p&gt;While attempting to generate frontend code using AI, I encountered numerous challenges. As frontend isn&amp;rsquo;t my area of expertise, troubleshooting issues proved time-consuming and draining. Even with prompt adjustments for AI rewrites, low-level errors were difficult to avoid. This repeated trial and error not only wasted time but also highlighted that my efforts are better focused on the backend’s business logic rather than exploring unfamiliar territory.&lt;/p&gt;
&lt;p&gt;Reflecting on the project completed this weekend, I&amp;rsquo;m even more convinced that focusing on backend development and user interaction logic, implementing functionality through the console, is currently the most efficient approach. Systematically learning frontend knowledge later, when I have more time and energy, might be a better strategy.&lt;/p&gt;
&lt;h2 id=&#34;frontend-learning-plan&#34;&gt;Frontend learning plan
&lt;/h2&gt;&lt;p&gt;The front-end tech stack is complex and diverse, making it unrealistic to quickly master. I plan to focus on one framework initially, such as Vue.js or React.js, and deeply learn its core concepts and usage. Only after gaining a solid understanding of the fundamentals should I attempt AI-assisted code generation for the front-end, to effectively avoid errors and wasted time due to unfamiliarity.&lt;/p&gt;
&lt;p&gt;For now, the focus should remain on backend development and steadily building core skills. Explore the combination of frontend and AI when the time is right; that may yield greater rewards.&lt;/p&gt;</description>
        </item>
        <item>
        <title>Claude4 released, attempting development: Hugo tag, hyperlink translation assistant</title>
        <link>https://blog.ttf248.life/en/p/claude-4-release-and-experimentation-hugo-tags-hyperlink-translation-assistant/</link>
        <pubDate>Sat, 24 May 2025 03:05:31 +0800</pubDate>
        
        <guid>https://blog.ttf248.life/en/p/claude-4-release-and-experimentation-hugo-tags-hyperlink-translation-assistant/</guid>
        <description>&lt;p&gt;This site is built with Hugo, but I&amp;rsquo;ve been using Chinese titles, which results in unfriendly URLs. To put it simply, when shared, they don&amp;rsquo;t look good because Chinese characters are encoded as things like %E4%BD%A0%E5%A5%BD. While slugs can fix this, manually setting them each time is too much trouble.&lt;/p&gt;
&lt;p&gt;So, today I tried using Claude4 to develop a translation assistant that automatically converts Chinese titles into English slugs and adds hyperlinks within articles. This would eliminate the need for manual setup.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Claude 4 is excellent; its ability to understand context and handle complex tasks has been significantly improved&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;project-address&#34;&gt;Project address
&lt;/h2&gt;&lt;p&gt;Domestic project address: &lt;strong&gt;PROTECTED&lt;/strong&gt;
Overseas project address: &lt;strong&gt;PROTECTED&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;code-implementation&#34;&gt;Code implementation
&lt;/h2&gt;&lt;p&gt;Here&amp;rsquo;s the translation: First, let me outline the approach: We need to scan all articles, extract tag information and article titles, then call a local large language model (like gemma-3-12b-it) for translation&lt;/p&gt;
&lt;p&gt;In practical development, compared to previous generations of large models, it not only supports the statistics and analysis of tags but also includes classification statistics and can even detect &lt;strong&gt;unlabeled article previews&lt;/strong&gt; and generate tag pages&lt;/p&gt;
&lt;p&gt;Whether connecting to local large models, adding translation caches, or conducting large-scale code refactoring, &lt;code&gt;Claude4&lt;/code&gt;Everything was completed in one go, with almost no issues. Although the project wasn&amp;rsquo;t large-scale, it included many small features. Previously, large language models often forgot earlier context during development, but this time&amp;hellip;&lt;code&gt;Claude4&lt;/code&gt; performs exceptionally well, &lt;strong&gt;rarely losing context&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In short, with improved intelligence, we&amp;rsquo;re preparing to switch to &lt;strong&gt;PROTECTD_INLINE_CODE_0&lt;/strong&gt; for more development work as our primary coding model&lt;/p&gt;
&lt;h2 id=&#34;translation-cache&#34;&gt;Translation cache
&lt;/h2&gt;&lt;p&gt;This approach, aside from reducing large model calls, offers good efficiency when running a 12b model locally without causing delays. However, repeatedly calling the large model can still be slow. Secondly, to maintain article links, full updates sometimes produce inconsistent results due to long titles, leading to link changes – which is quite awkward.&lt;/p&gt;
&lt;h2 id=&#34;feature-optimization&#34;&gt;Feature optimization
&lt;/h2&gt;&lt;p&gt;The entire project was handed over to [REDACTED], who analyzed areas for optimization and provided the following suggestions:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;配置外置化 - 提高可维护性和灵活性
结构化日志 - 便于问题排查和监控
性能监控 - 了解系统运行状况
用户体验 - 进度条等视觉反馈
错误处理 - 更完善的异常处理机制
代码组织 - 更清晰的模块划分
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The code review was completed and everything looks good. For example, the configuration file now automatically generates a default if one doesn&amp;rsquo;t exist when reading it, converting previous configurations to defaults – this prevents user errors.&lt;/p&gt;
&lt;h3 id=&#34;heading&#34;&gt;
&lt;/h3&gt;&lt;p&gt;Requirements: While translating the main text, dynamically calculate the current translation efficiency, estimate the remaining time, and output relevant information to the console. This includes tracking the number of characters obtained from the article, the number of characters translated per line, the elapsed time, a fitted calculation for the translation time per 100 characters, and an estimated remaining translation time for the entire article.&lt;/p&gt;
&lt;p&gt;The code is done, but I&amp;rsquo;m not very satisfied with the results, so I asked AI to provide new design suggestions&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Provides multiple efficiency calculation methods: real-time efficiency, average efficiency, sliding window efficiency&lt;/li&gt;
&lt;li&gt;Improved display methods: progress bar, segmented statistics, dynamic refresh&lt;/li&gt;
&lt;li&gt;Add more useful metrics: API call count, success rate, etc&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;After completing the code, I discovered a new surprise – real-time statistics on translation efficiency are displayed without mindless scrolling&lt;/p&gt;
&lt;p&gt;Translating body text to English..
Detected 53 lines needing translation
Translating 354 characters..
Completed (3.1s) | API call #1
Completed (1.5s) | API call #2
Completed (0.9s) | API call #3
Completed (0.2s) | API call #4
Completed
Completed
Completed (0.2s) | API call #7
Progress: 13.2% (7/53) | Characters 12.9% (925/7163) 114.6 characters/second
Efficiency: Real-time 76.4 | Average 117.9 | Recent 109.0 | Stage 113.6 characters/second 📊
Success Rate: 100.0% (7/7) | Remaining: 46 lines, 7 seconds] 9.4% Translation of 110 characters…
Estimated remaining: 55s | Estimated completion: 10:19 8s | 11.3% | Translating 114 characters…
Processing speed: 3211.3 lines/minute | Total time: 8s] 13.2% Translated 16 characters…
Stage 1/6 [███░░░░░░░░░░░░░░░░░░░░░░] 15.1% Translating 166 characters..&lt;/p&gt;
&lt;p&gt;I haven&amp;rsquo;t written much control programs before, curious about how it’s implemented, so I looked at the code&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-golang&#34;&gt;// Clear screen and redisplay (dynamic refresh effect)
if translationCount &amp;gt; 1 {
   fmt.Print(&amp;quot;\033[6A\033[K&amp;quot;) // Move up 6 lines and clear
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;performance-statistics-menu&#34;&gt;Performance Statistics Menu
&lt;/h3&gt;&lt;p&gt;This new feature, allowing me to design it myself, might not even be this well-designed&lt;/p&gt;
&lt;p&gt;Performance Statistics:
Translation count: 360
Cache hit rate: 1.4% (5/365)
Average translation time: 315.927234ms
File Operations: 73
Incorrect attempts: 0&lt;/p&gt;
&lt;h3 id=&#34;progress-bar-display&#34;&gt;Progress bar display
&lt;/h3&gt;&lt;p&gt;New &lt;strong&gt;features&lt;/strong&gt; provide detailed progress, time spent, and estimated remaining time&lt;/p&gt;
&lt;p&gt;Please select function (0-13): 10
Collecting translation target..
Cached file loaded, containing 0 translation records&lt;/p&gt;
&lt;p&gt;Translation cache statistics:
Total labels: 229
Total articles: 131
Cached: 0 items
360 items need translating&lt;/p&gt;
&lt;p&gt;Confirm generating full translation cache? (y/n): y
Generating full translation cache..
Cached file loaded, containing 0 translation records
Checking cached translation..
Need to translate 360 new tags
5/360 (1.4%) - Time taken: 3s - Estimated remaining: 3m8s💾 Saved cache file, containing 5 translation records
10/360 (2.8%) - Time taken: 6s - Estimated remaining time: 3m28s💾 Saved cache file, containing 10 translation records
15/360 (4.2%) - Time taken: 9s - Estimated remaining: 3m30s💾 Saved cache file, containing 15 translation records
20/360 (5.6%) - Time taken: 13s - Estimated remaining time: 3m36s💾 Saved cache file, containing 20 translation records
25/360 (6.9%) - Time taken: 16s - Estimated remaining time: 3m33s💾 Saved cache file, containing 25 translation records
30/360 (8.3%) - Time elapsed: 19s - Estimated remaining: 3m30s💾 Saved cache file, containing 30 translation records
Saved cache file, containing 35 translation records&lt;/p&gt;
&lt;hr&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;=== Hugo 博客管理工具 ===

🚀 核心功能
  1. 一键处理全部 (完整博客处理流程)

📝 内容管理
  2. 生成标签页面
  3. 生成文章Slug
  4. 翻译文章为多语言版本

💾 缓存管理
  5. 查看缓存状态
  6. 生成全量翻译缓存
  7. 清空翻译缓存

  0. 退出程序
&lt;/code&gt;&lt;/pre&gt;</description>
        </item>
        <item>
        <title>Too much AI, some side effects</title>
        <link>https://blog.ttf248.life/en/p/ai-overuse-side-effects/</link>
        <pubDate>Wed, 14 May 2025 19:39:50 +0800</pubDate>
        
        <guid>https://blog.ttf248.life/en/p/ai-overuse-side-effects/</guid>
        <description>&lt;p&gt;Since the establishment of &lt;strong&gt;INLINE_CODE_0&lt;/strong&gt;, there&amp;rsquo;s been a surge in recording and publishing trivial things using AI. Consequently, time for quiet reflection has diminished. It would be better to slightly control the output of this column and consolidate it into a monthly publication, releasing one article per month.&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s like a lingering effect or side effect – efficiency increases, but depth and breadth of thought decrease&lt;/p&gt;
&lt;h2 id=&#34;efficiency-improvement-undeniable&#34;&gt;Efficiency improvement: Undeniable
&lt;/h2&gt;&lt;p&gt;Previously, this column wasn&amp;rsquo;t well-maintained. Due to laziness, I didn’t search for information or compile records on some trending events. Now, with various AI tools, outlining is sufficient; the AI can automatically search and record related events, generate articles, and after simple formatting, they can be published.&lt;/p&gt;
&lt;p&gt;This is like a dream come true for those who don&amp;rsquo;t want to put in too much effort; it significantly boosts efficiency, almost achieving results with half the work&lt;/p&gt;
&lt;p&gt;Efficiency significantly improves when coding instead of writing articles. Many code sections previously required detailed review of API documentation; now, this step can be skipped – a necessary shortcut. Familiarity with APIs is best left to AI.&lt;/p&gt;
&lt;h2 id=&#34;junk-content&#34;&gt;Junk content
&lt;/h2&gt;&lt;p&gt;Many of the submissions lack substance; they feel lifeless and stale, like chewing wax—not a style I previously enjoyed&lt;/p&gt;
&lt;p&gt;To put it another way, AI-generated content often feels like mass-produced work, lacking soul&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Internet junk of the new era&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;forgetfulness&#34;&gt;Forgetfulness
&lt;/h2&gt;&lt;p&gt;This type of article is AI-generated. I don&amp;rsquo;t know the readers, and over time, my own memory will fade or even disappear.&lt;/p&gt;
&lt;p&gt;Similar issues arise when coding – without reviewing the commit history, it&amp;rsquo;s impossible to know your original thought process or rationale for a particular implementation. This is especially true with code generated after extensive revisions and discussions with AI, which can significantly diverge from the initial idea, even becoming entirely different.&lt;/p&gt;
&lt;h2 id=&#34;search&#34;&gt;Search
&lt;/h2&gt;&lt;p&gt;I&amp;rsquo;ve noticed a significant decrease in how often I use Google and Baidu recently; searching with AI is much better than traditional search engines, both in terms of interaction and results&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s commemorate &lt;strong&gt;INLINE_CODE_0&lt;/strong&gt;, one of the earliest AI tools connected to the internet released by a major company&lt;/p&gt;
&lt;p&gt;People are using Google less; visits to &lt;strong&gt;INLINE_CODE_0&lt;/strong&gt; have decreased as well. Many questions can now be answered directly by AI, and this website is gradually being phased out by the times.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion
&lt;/h2&gt;&lt;p&gt;The blog I&amp;rsquo;m still maintaining doesn&amp;rsquo;t get much traffic anyway, and certainly won&amp;rsquo;t now; it’s more of a personal journal for my own reference&lt;/p&gt;</description>
        </item>
        <item>
        <title>Design and develop a customizable stock selection module (without coding)</title>
        <link>https://blog.ttf248.life/en/p/no-code-design-develop-custom-stock-module/</link>
        <pubDate>Thu, 27 Feb 2025 23:20:39 +0800</pubDate>
        
        <guid>https://blog.ttf248.life/en/p/no-code-design-develop-custom-stock-module/</guid>
        <description>&lt;p&gt;We tested Cursor last month, but due to free tier limitations, we didn&amp;rsquo;t develop complex features, just simple testing. We noticed that ByteDance also released a similar product at the time, both using the same underlying large language model: Claude-3.5.&lt;/p&gt;
&lt;p&gt;ByteDance&amp;rsquo;s product, Trae, initially launched a Mac version and finally released a Windows version this February. Big company products are just good – you can use them for free without paying, with unlimited access to Claude-3.5, which is a very impressive model.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Ultimately, development stalled on the K-line chart due to my limited knowledge of React. To continue, I need to supplement my frontend skills and break down tasks into smaller steps instead of tackling a large project like developing a K-line chart directly.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;issues-found&#34;&gt;Issues found
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;Due to insufficient training data for Vue3 + Element-Plus due to reliance on foreign AI models, we chose React as the frontend framework&lt;/li&gt;
&lt;li&gt;There may be occasional grammatical errors that require manual correction&lt;/li&gt;
&lt;li&gt;Some complex problems require human guidance for solutions&lt;/li&gt;
&lt;li&gt;Code structure optimization requires human guidance&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The most time-consuming part was packaging the frontend code into a container. As someone with zero experience, I had no understanding of this process; I only managed to grasp the logic with help from others. There&amp;rsquo;s a significant difference in how frontend development checks code between dev and build modes. The database and service containers for the backend took about five minutes combined.&lt;/p&gt;
&lt;p&gt;Please provide the Chinese text you want me to translate. I am ready when you are!&lt;/p&gt;
&lt;h2 id=&#34;warehouse-address&#34;&gt;Warehouse address
&lt;/h2&gt;&lt;p&gt;As the title suggests, we&amp;rsquo;re skipping the coding and directly discussing with AI to design and develop a custom stock selection module. Let’s see what kind of results we can achieve.&lt;/p&gt;
&lt;p&gt;Warehouse address: &lt;a class=&#34;link&#34; href=&#34;https://github.com/ttf248/trae-demo&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/ttf248/trae-demo&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Detailed usage instructions can be found in the repository&amp;rsquo;s README.md file&lt;/p&gt;
&lt;p&gt;The repository contains numerous submission records, mostly conversations between me and Trae, along with my tests of various features for him. Notes indicate whether manual intervention was required to implement the corresponding functionality.&lt;/p&gt;
&lt;h2 id=&#34;prompt&#34;&gt;Prompt
&lt;/h2&gt;&lt;p&gt;This project was created from scratch. Here&amp;rsquo;s the prompt for the project:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;基于项目原型图，开发功能：自选股，需要支持合约的新增、删除、修改、查询。自选股界面需要展示基础的行情数据。支持多个不同的市场切换。

前端：react
后端：golang gin gorm
数据库：PostgreSQL

服务端需要支持跨域请求，同时需要考虑数据的校验和错误处理，如果后端服务不可用，前端需要告警提示。

后端需要展示请求和应答的日志；前端也打印通讯的日志，方便排查问题。
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;ui-and-interaction-optimization&#34;&gt;UI and interaction optimization
&lt;/h2&gt;&lt;p&gt;The frontend design is entirely dependent on Grok. We initially created a prototype within Trae, but it lacked aesthetic appeal. Due to the model&amp;rsquo;s strong coding capabilities and weaker overall abilities, we need to use Grok to optimize the frontend UI.&lt;/p&gt;
&lt;p&gt;By taking screenshots of the current interface, uploading them to Grok, and having it help us optimize the UI, we can potentially receive numerous optimization suggestions. We will then manually evaluate these suggestions and copy them into Trae for execution, observing the results.&lt;/p&gt;
&lt;h3 id=&#34;technology-stack&#34;&gt;Technology stack
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Frontend: React + TypeScript&lt;/li&gt;
&lt;li&gt;Backend: Golang + Gin + GORM&lt;/li&gt;
&lt;li&gt;Database: PostgreSQL 17&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;system-architecture&#34;&gt;System architecture
&lt;/h2&gt;&lt;h2 id=&#34;backend-architecture&#34;&gt;Backend architecture
&lt;/h2&gt;&lt;p&gt;The backend is implemented using Go&amp;rsquo;s Gin framework for a RESTful API, with modules including:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Please provide the Chinese text you want me to translate. I am ready when you are!&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Use GORM as an ORM framework&lt;/li&gt;
&lt;li&gt;Supports environment variable configuration for database connections&lt;/li&gt;
&lt;li&gt;Automatically migrate database tables&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;Please provide the Chinese text you want me to translate. I am ready when you are!&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;RESTful API Design&lt;/li&gt;
&lt;li&gt;Unified error handling mechanism&lt;/li&gt;
&lt;li&gt;Built-in request logging&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;Please provide the Chinese text you want me to translate. I am ready when you are!&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Support cross-domain for local development environments&lt;/li&gt;
&lt;li&gt;Configurable CORS policy&lt;/li&gt;
&lt;li&gt;Support cross-domain cookies&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;frontend-architecture&#34;&gt;Frontend Architecture
&lt;/h2&gt;&lt;p&gt;Built with React + TypeScript, achieving:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Stock list display&lt;/li&gt;
&lt;li&gt;Self-selected stock management&lt;/li&gt;
&lt;li&gt;Market Data Display&lt;/li&gt;
&lt;li&gt;Error message mechanism&lt;/li&gt;
&lt;/ul&gt;</description>
        </item>
        <item>
        <title>Two years of AI development: Its somewhat like the state before Docker was released</title>
        <link>https://blog.ttf248.life/en/p/ai-development-two-years-docker-pre-release/</link>
        <pubDate>Thu, 20 Feb 2025 18:16:37 +0800</pubDate>
        
        <guid>https://blog.ttf248.life/en/p/ai-development-two-years-docker-pre-release/</guid>
        <description>&lt;p&gt;Artificial intelligence (AI) has undoubtedly been one of the most discussed topics in technology in recent years, especially with its rapid advancements over the past two years. From deep learning and natural language processing to computer vision and automated decision systems, AI applications are constantly emerging. However, despite continuous technological breakthroughs, AI still faces a bottleneck similar to that of Docker before its release – a lack of a killer application to truly ignite the market.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The development of AI over the past two years is similar to the state before Docker&amp;rsquo;s release – lacking a killer application. It needs a perfect practical implementation based on existing technology, like Docker: not relying on groundbreaking new technologies, but offering a complete and reasonable solution that transforms operations and development workflows.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;the-current-state-of-ai-development-technology-is-mature-but-application-still-needs-breakthroughs&#34;&gt;The current state of AI development: technology is mature, but application still needs breakthroughs
&lt;/h2&gt;&lt;p&gt;From a technical perspective, AI has made significant progress in the past two years. Whether it&amp;rsquo;s OpenAI’s GPT series models or Google’s BERT and DeepMind’s Alpha series, AI processing capabilities have far exceeded previous expectations. Particularly in natural language processing, models like GPT-4 not only possess powerful generation abilities but also demonstrate astonishing performance in understanding and reasoning.&lt;/p&gt;
&lt;p&gt;However, despite rapid technological advancements, the practical application of AI faces certain challenges. Similar to the state before Docker&amp;rsquo;s release, while AI has immense potential, a truly widespread and industry-transforming &amp;ldquo;killer&amp;rdquo; application hasn&amp;rsquo;t yet emerged. People discuss AI’s prospects but may struggle to find an application that can bring revolutionary change. Many AI applications remain in early experimental stages and require further integration and optimization.&lt;/p&gt;
&lt;h2 id=&#34;the-similarity-between-docker-and-ai-technology-isnt-necessarily-innovation-solutions-are-key&#34;&gt;The similarity between Docker and AI: Technology isn&amp;rsquo;t necessarily innovation, solutions are key
&lt;/h2&gt;&lt;p&gt;Looking back at the history before Docker&amp;rsquo;s release, we find striking similarities with the current state of AI development. Prior to Docker, container technology wasn&amp;rsquo;t new; early technologies like LXC (Linux Containers) and virtualization already possessed basic containerization capabilities. However, Docker cleverly integrated and optimized existing technologies, proposing a simpler, more intuitive, and efficient solution. This approach didn’t introduce revolutionary technology but addressed many pain points in operations and development processes, significantly simplifying software deployment, scaling, and management.&lt;/p&gt;
&lt;p&gt;Similarly, the AI field faces a similar situation. While current AI technology is no longer &amp;ldquo;new,&amp;rdquo; achieving widespread application still requires a perfect implementation scenario – like Docker, integrating and optimizing existing technologies to form a practical solution. The “killer” application of AI may not depend on breakthrough new technologies, but rather on how to integrate existing ones to solve real-world business pain points and needs.&lt;/p&gt;
&lt;h2 id=&#34;how-to-find-ais-docker-moment&#34;&gt;How to find AI&amp;rsquo;s &amp;ldquo;Docker moment&amp;rdquo;?
&lt;/h2&gt;&lt;p&gt;To achieve widespread application of AI technology, several aspects need to be addressed&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Please provide the Chinese text you want me to translate. I am ready when you are!
Currently, many AI applications remain experimental and lack large-scale practical implementation. While areas like AI customer service and intelligent recommendations are widely used, their functionality is still limited and hasn&amp;rsquo;t yet overcome industry bottlenecks. True breakthroughs may come from industries long burdened by traditional methods—such as healthcare, manufacturing, and logistics—where AI can improve efficiency and reduce costs through more efficient data processing and predictive analysis.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Please provide the Chinese text you want me to translate. I am ready when you are!
Just as Docker improves operational efficiency by streamlining the containerization process, the usability of AI products is equally crucial. The popularization of AI isn&amp;rsquo;t just about technology; it’s about productization. Integrating AI into daily workflows and enabling users to easily utilize these tools without needing a deep understanding of the underlying technology is a key step in its successful implementation.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Please provide the Chinese text you want me to translate. I am ready when you are!
The widespread adoption of any new technology hinges on building a robust ecosystem. Docker&amp;rsquo;s rapid rise is due to its openness and compatibility, allowing developers to easily connect with various cloud platforms, tools, and services. Similarly, the future of AI depends on ecosystem development. Standardization, model sharing, data accessibility, and technical integration will all influence whether AI can achieve broad industry applications.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;conclusion-the-future-of-ai-is-full-of-possibilities-but-still-requires-more-robust-implementation-plans&#34;&gt;Conclusion: The future of AI is full of possibilities, but still requires more robust implementation plans
&lt;/h2&gt;&lt;p&gt;Despite significant advancements in AI technology over the past two years, it remains in a stage without a killer application. Similar to containerization technology before Docker&amp;rsquo;s release, AI needs a practical application scenario that deeply integrates existing technologies with business needs to achieve widespread adoption and scale. While technological innovation is important, solutions that simplify processes and improve efficiency are more likely to drive the popularization and development of the technology.&lt;/p&gt;
&lt;p&gt;In the future, AI may not revolutionize through groundbreaking technology, but rather create a perfect application scenario by integrating existing technologies—ultimately transforming how we work and live&lt;/p&gt;</description>
        </item>
        <item>
        <title>Deploy DeepSeek-R1 locally</title>
        <link>https://blog.ttf248.life/en/p/ollama-local-deployment-deepseek-r1/</link>
        <pubDate>Fri, 07 Feb 2025 22:41:02 +0800</pubDate>
        
        <guid>https://blog.ttf248.life/en/p/ollama-local-deployment-deepseek-r1/</guid>
        <description>&lt;p&gt;Ollama is an open-source AI tool designed to enable users to run and deploy large language models (LLMs) locally. It aims to provide a convenient and efficient way for developers to use models like GPT on their local machines without relying on cloud services. Ollama supports various models and focuses on optimizing performance so that even resource-constrained devices can run them smoothly.&lt;/p&gt;
&lt;p&gt;With Ollama, users can utilize text-based AI applications and interact with locally deployed models without concerns about data privacy or high API usage fees. You can call different models through a command-line interface (CLI) for tasks like natural language processing and question answering.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Ollama is good for trying out different models. The Windows version doesn&amp;rsquo;t fully utilize hardware performance, likely due to Windows itself. The Linux version might be better. Deploying a 32B parameter model results in slow responses even with low memory and GPU load.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;hardware-overview&#34;&gt;Hardware Overview
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;Operating system: win11&lt;/li&gt;
&lt;li&gt;CPU：i7-10700K&lt;/li&gt;
&lt;li&gt;Memory: 40GB&lt;/li&gt;
&lt;li&gt;Graphics card: RTX 3060 12GB&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;environment-setup&#34;&gt;Environment Setup
&lt;/h2&gt;&lt;p&gt;Added system environment variable for future use&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;set OLLAMA_MODELS=E:\ollama&lt;/code&gt;&lt;/strong&gt;&lt;br&gt;
This variable specifies the storage path for Ollama models. &lt;strong&gt;INLINE_CODE_0&lt;/strong&gt; is a folder path where all local model files are stored. Ollama loads and uses your downloaded or deployed language models based on this path. You can store the model files in other locations by changing this path.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;set OLLAMA_HOST=127.0.0.1:8000&lt;/code&gt;&lt;/strong&gt;&lt;br&gt;
This environment variable sets the host and port for the Ollama service&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;127.0.0.1&lt;/code&gt; is a local address (localhost), meaning that the Ollama service will only listen for requests from the local machine&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;INLINE_CODE_0&lt;/strong&gt; is the designated port number, indicating that the Ollama service will listen for and process requests on port 8000. You can change the port number as needed, but ensure it&amp;rsquo;s not occupied by another application.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;set OLLAMA_ORIGINS=*&lt;/code&gt;&lt;/strong&gt;&lt;br&gt;
This environment variable controls which sources of requests are allowed to access the Ollama service&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;INLINE_CODE_0&lt;/strong&gt; indicates that any origin (i.e., all domains and IP addresses) is allowed to access the Ollama service. This is typically used in development and debugging environments; in production, more restrictive source control is usually specified to limit access to specific domains or IPs for enhanced security.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;deepseek-r1-model-deployment&#34;&gt;DeepSeek-R1 Model Deployment
&lt;/h2&gt;&lt;p&gt;Ollama installation is straightforward; details are omitted here&lt;/p&gt;
&lt;p&gt;Post-installation verification&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;C:\Users\core&amp;gt;ollama -v
ollama version is 0.5.11
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Model deployment, refer to the official model page and select the corresponding parameters for the model: &lt;strong&gt;INLINE_CODE_0&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;A 14B parameter model effectively remembers conversation context; smaller versions do not. The 32B parameter version is too slow for local deployment, so further testing was not conducted.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.ollama.com/library/deepseek-r1&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://www.ollama.com/library/deepseek-r1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/SPEvYTmTBxhoEkJqm1yPmw&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://mp.weixin.qq.com/s/SPEvYTmTBxhoEkJqm1yPmw&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://blog.csdn.net/x18990027/article/details/145368094&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://blog.csdn.net/x18990027/article/details/145368094&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
        </item>
        <item>
        <title>DeepSeek unexpectedly surged before the Spring Festival, causing Nvidia stock to plummet: Institutional maneuvering and large model reasoning chains</title>
        <link>https://blog.ttf248.life/en/p/deepseek-chinese-new-year-nvidia-stock-drop-institutional-operations-large-language-model-chain/</link>
        <pubDate>Fri, 07 Feb 2025 20:36:05 +0800</pubDate>
        
        <guid>https://blog.ttf248.life/en/p/deepseek-chinese-new-year-nvidia-stock-drop-institutional-operations-large-language-model-chain/</guid>
        <description>&lt;p&gt;Ahead of the Spring Festival, DeepSeek quickly became a hot topic, garnering widespread attention on social media in just a few days. This sudden surge not only surprised many but also triggered a market chain reaction. Simultaneously, Nvidia&amp;rsquo;s stock plummeted, raising concerns about its future and prompting significant short-selling activity from some institutions, suggesting a potentially orchestrated scenario.&lt;/p&gt;
&lt;h3 id=&#34;please-provide-the-chinese-text-you-want-me-to-translate-i-am-ready-when-you-are&#34;&gt;Please provide the Chinese text you want me to translate. I am ready when you are!
&lt;/h3&gt;&lt;p&gt;DeepSeek is an AI-powered tool focused on optimizing deep learning models, particularly in natural language processing (NLP) and image generation. In the days leading up to Chinese New Year, the project unexpectedly gained significant attention from investors and technical professionals. The team&amp;rsquo;s performance and demonstrated technological achievements have sparked considerable interest. Discussions about DeepSeek dominate tech circles across developer communities and social media platforms.&lt;/p&gt;
&lt;p&gt;However, DeepSeek&amp;rsquo;s sudden surge in popularity wasn&amp;rsquo;t accidental. Analysis suggests potential involvement from certain institutions. Notably, Nvidia’s stock price has since seen a clear decline, indicating some factors are driving this change.&lt;/p&gt;
&lt;h3 id=&#34;please-provide-the-chinese-text-you-want-me-to-translate-i-am-ready-when-you-are-1&#34;&gt;Please provide the Chinese text you want me to translate. I am ready when you are!
&lt;/h3&gt;&lt;p&gt;Nvidia, a leading manufacturer of graphics processing units (GPUs), has long been a key hardware provider for large language models and AI computing. While the company&amp;rsquo;s stock has consistently performed strongly alongside the rapid growth of the AI market, it recently experienced a sharp decline following DeepSeek’s surge in popularity and increased market attention to its technology.&lt;/p&gt;
&lt;p&gt;This phenomenon may involve short-selling strategies by institutional investors. In recent years, as AI technology has become widespread, Nvidia&amp;rsquo;s stock price has been highly inflated, leading many investors to believe its valuation is overhyped. Particularly after the surge in popularity of DeepSeek, some institutions may have profited handsomely by shorting Nvidia’s stock. These institutions successfully capitalized on precise market timing and an understanding of DeepSeek’s influence.&lt;/p&gt;
&lt;h3 id=&#34;please-provide-the-chinese-text-you-want-me-to-translate-i-am-ready-when-you-are-2&#34;&gt;Please provide the Chinese text you want me to translate. I am ready when you are!
&lt;/h3&gt;&lt;p&gt;In traditional AI applications, many practitioners and investors focus on the &amp;ldquo;results&amp;rdquo; of AI models—such as generated images or text. However, discussions surrounding DeepSeek reveal a growing recognition that the underlying reasoning chain behind large language models is the more valuable core element. Previously, we could only see model outputs; now, we need to understand their logic, algorithms, and how to optimize performance by adjusting these factors.&lt;/p&gt;
&lt;p&gt;This shift in thinking represents a deeper consideration of AI research and application. Moving from simple &amp;ldquo;black box&amp;rdquo; operations to genuinely understanding the internal workings of models is prompting many technical experts and investors to re-evaluate the future direction of artificial intelligence. DeepSeek&amp;rsquo;s popularity exemplifies this breakthrough, encouraging attention towards the entire model construction and optimization process, rather than just the final output.&lt;/p&gt;
&lt;h3 id=&#34;please-provide-the-chinese-text-you-want-me-to-translate-i-am-ready-when-you-are-3&#34;&gt;Please provide the Chinese text you want me to translate. I am ready when you are!
&lt;/h3&gt;&lt;p&gt;DeepSeek&amp;rsquo;s sudden rise, Nvidia’s stock plunge, and the market manipulation behind it all appear to be part of a carefully orchestrated scheme. A deep understanding of large language model thinking chains reveals that applying AI technology isn&amp;rsquo;t just about superficial accumulation; it requires in-depth exploration and optimization of internal model logic. As technology advances, we may see more innovative tools like DeepSeek emerge, driving AI research and application to new heights.&lt;/p&gt;
&lt;p&gt;This phenomenon not only reveals the immense potential of AI technology but also prompts us to consider the underlying business competition and capital operations. The future market trends will be a continued focus of this interplay between technology and capital.&lt;/p&gt;</description>
        </item>
        <item>
        <title>Cursor AI programming IDE trial</title>
        <link>https://blog.ttf248.life/en/p/cursor-ai-programming-ide-trial/</link>
        <pubDate>Thu, 23 Jan 2025 19:30:13 +0800</pubDate>
        
        <guid>https://blog.ttf248.life/en/p/cursor-ai-programming-ide-trial/</guid>
        <description>&lt;p&gt;Another year has passed. The biggest change at work is the noticeably increased involvement of AI. Previously, switching between different development languages required developers to be familiar with various APIs. Now, these basic code snippets can be generated by AI – a great boon for developers.&lt;/p&gt;
&lt;h2 id=&#34;chatgpt&#34;&gt;ChatGPT
&lt;/h2&gt;&lt;p&gt;Back in 2023, I revised two introductory articles. Now it&amp;rsquo;s 2025, and frankly, there hasn’t been a significant improvement. It still requires developing one&amp;rsquo;s own understanding, being able to reasonably break down tasks, and most importantly, identifying bugs in AI-generated code.&lt;/p&gt;
&lt;h2 id=&#34;github-copilot&#34;&gt;Github copilot
&lt;/h2&gt;&lt;p&gt;I can&amp;rsquo;t recall the exact date, but I saw information stating that Singapore has deployed servers for domestic use. This means we no longer need to constantly use a VPN; only a VPN is required for login, after which it can be turned off.&lt;/p&gt;
&lt;p&gt;I use Github Copilot more often; this plugin can be used directly in VS Code and Visual Studio without switching between programs. Compared to ChatGPT, Github Copilot offers better project support and a friendlier interface. You can feed it portions of local files &lt;strong&gt;to generate code that aligns with your project.&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;cursor-ai&#34;&gt;Cursor AI
&lt;/h2&gt;&lt;p&gt;I recently saw a new AI programming IDE, Cursor AI. This IDE is based on GitHub Copilot, but it&amp;rsquo;s even more intelligent and can help you create files directly.&lt;/p&gt;
&lt;p&gt;It feels pretty good after a quick try, but my understanding of the existing project is still not enough. When dealing with many local project files and large-scale refactoring and optimization, I still need &lt;strong&gt;开发者拆分任务&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Switch to curso&amp;rsquo;s engineering mode, enter: Create a personal resume webpage with support for multiple style switching. Remember to fill in some personal information for data display.&lt;/p&gt;
&lt;p&gt;After a few tries, you&amp;rsquo;ll be able to get the following webpage. Of course, it’s still quite simple, but it’s pretty good for beginners.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Currently, registered users can enjoy 150 free trials of the premium API; paid users are limited to 5000 calls&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Please provide the Chinese text you want me to translate. I am ready when you are! Just paste the text here.&lt;/p&gt;</description>
        </item>
        <item>
        <title>AI programming and task decomposition</title>
        <link>https://blog.ttf248.life/en/p/ai-programming-and-task-decomposition/</link>
        <pubDate>Fri, 22 Dec 2023 08:44:26 +0800</pubDate>
        
        <guid>https://blog.ttf248.life/en/p/ai-programming-and-task-decomposition/</guid>
        <description>&lt;p&gt;Two years ago, I added a copy function for the site&amp;rsquo;s code area. It took nearly half a day, and the final rendering effect was not ideal. As an amateur front-end developer, I didn’t bother to improve it; as long as it worked, that was fine. This year, using &lt;strong&gt;INLIN_ITALIC_1&lt;/strong&gt;__, I developed a mini program and became more familiar with front-end development. So, I rebuilt it (AI redesign).&lt;/p&gt;
&lt;p&gt;Understanding the Dao: Break down tasks into many smaller ones&lt;/p&gt;
&lt;h2 id=&#34;previous-text&#34;&gt;Previous text
&lt;/h2&gt;&lt;p&gt;Please provide the Chinese text you want me to translate. I am ready when you are! Just paste the text here.&lt;/p&gt;
&lt;h2 id=&#34;main-body&#34;&gt;Main body
&lt;/h2&gt;&lt;p&gt;Design CSS and JavaScript code to locate webpage elements with the class &amp;ldquo;highlight&amp;rdquo;. Within the container of these elements, add a copy button that implements the functionality to copy the code.&lt;/p&gt;
&lt;p&gt;The initial draft of the code had issues. A revised draft was generated, which is now basically functional, but contains incorrect copied content and redundant text. Further optimization is needed, including manual specification of core color design and effects.&lt;/p&gt;
&lt;p&gt;We understand you have specific requirements for button size and color. We can redesign the button style to accommodate these needs. Here&amp;rsquo;s an adjusted CSS style, incorporating your provided color scheme and optimizing the button dimensions:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-css&#34;&gt;.copy-btn {
  position: absolute;
  top: 2px; /* 调整位置 */
  right: 2px;
  padding: 3px 6px; /* 缩小尺寸 */
  font-size: 12px; /* 减小字体大小 */
  background-color: #8FD1E1; /* 按钮背景色 */
  color: #272822; /* 文字颜色 */
  border: 1px solid #272822; /* 边框颜色 */
  border-radius: 3px; /* 圆角 */
  cursor: pointer;
  opacity: 0.7; /* 默认半透明 */
  transition: opacity 0.3s ease, background-color 0.3s ease; /* 平滑过渡效果 */
}

.copy-btn:hover {
  opacity: 1; /* 鼠标悬停时不透明 */
  background-color: #A3D4E5; /* 鼠标悬停时的背景色 */
}

.highlight {
  position: relative;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this style, I’ve reduced the button size and font to make it more refined. The default state is semi-transparent; hovering over the button makes it opaque with a slightly brighter background for visual feedback. This design meets your color requirements while maintaining a modern, user-friendly feel.&lt;/p&gt;
&lt;p&gt;A simpler version was provided accordingly&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-JavaScript&#34;&gt;document.addEventListener(&#39;DOMContentLoaded&#39;, function() {
  // 查找所有 .highlight 元素
  const highlights = document.querySelectorAll(&#39;.highlight&#39;);

  highlights.forEach(function(highlight) {
    // 创建复制按钮
    const copyBtn = document.createElement(&#39;button&#39;);
    copyBtn.textContent = &#39;复制&#39;;
    copyBtn.classList.add(&#39;copy-btn&#39;);

    // 将按钮添加到每个 .highlight 元素中
    highlight.appendChild(copyBtn);

    // 添加点击事件监听器
    copyBtn.addEventListener(&#39;click&#39;, function() {
      // 假设 .highlight 下的第一个 &amp;lt;code&amp;gt; 元素包含要复制的代码
      const code = highlight.querySelector(&#39;code&#39;) ? highlight.querySelector(&#39;code&#39;).textContent : &#39;&#39;;

      navigator.clipboard.writeText(code).then(() =&amp;gt; {
        // 显示复制成功信息
        copyBtn.textContent = &#39;已复制!&#39;;
        setTimeout(() =&amp;gt; {
          copyBtn.textContent = &#39;复制&#39;;
        }, 2000);
      }).catch(err =&amp;gt; {
        console.error(&#39;复制失败: &#39;, err);
      });
    });
  });
});
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;afterword&#34;&gt;Afterword
&lt;/h2&gt;&lt;p&gt;Although I&amp;rsquo;d prefer a copy button added to the top right of the code area, due to my limited frontend knowledge, I haven’t been able to accurately describe the functionality with keywords, and several attempts have not yielded the desired result&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s uncertain what impact &lt;code&gt;AI&lt;/code&gt; will have on program development as time progresses&lt;/p&gt;</description>
        </item>
        <item>
        <title>prompt-engineer</title>
        <link>https://blog.ttf248.life/en/p/prompt-engineer/</link>
        <pubDate>Sun, 26 Mar 2023 20:46:53 +0800</pubDate>
        
        <guid>https://blog.ttf248.life/en/p/prompt-engineer/</guid>
        <description>&lt;p&gt;Just as we learned techniques for using search engines, we also need to learn communication skills – providing reasonable and sufficient constraints to efficiently obtain the answers we need&lt;/p&gt;
&lt;p&gt;If you look at it from a different angle, the current situation will generate the desired result&lt;/p&gt;
&lt;h2 id=&#34;science-communication&#34;&gt;Science communication
&lt;/h2&gt;&lt;p&gt;Generative Pre-trained Transformer (GPT) is a deep learning model trained on publicly available internet data, used for tasks like question answering, text summarization, machine translation, classification, code generation, and conversational AI. There are currently various versions of GPT, including GPT-1, GPT-2, GPT-3, and GPT-4, each larger and more powerful than its predecessor.&lt;/p&gt;
&lt;h2 id=&#34;does-intelligence-truly-exist&#34;&gt;Does intelligence truly exist?
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;The higher the similarity, the greater the accuracy&lt;/li&gt;
&lt;li&gt;Basic, repetitive tasks no longer require human intervention after specific training&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Generative AI is a technology that utilizes existing data such as text, audio, and images to create new content. It can be applied to various tasks including text generation, speech synthesis, image generation, and conversational systems. The logic of generative AI depends on its training data and model structure. Generally, it can follow grammar, logic, and common sense to a certain extent, but may also produce errors, biases, or inaccurate information. Therefore, the output of generative AI requires human judgment and verification; it should not be blindly trusted or used.&lt;/p&gt;
&lt;h2 id=&#34;prompt-engineer&#34;&gt;prompt-engineer
&lt;/h2&gt;&lt;p&gt;The river of time doesn&amp;rsquo;t flow backward; people need to learn to adapt to the trend. You can consider &lt;em&gt;[INLIN]&lt;/em&gt; as unintelligent and lacking logic, often producing unusable code.&lt;/p&gt;
&lt;p&gt;If you look at it from a different angle, the current situation will generate the desired result&lt;/p&gt;
&lt;h2 id=&#34;dialogue-mode&#34;&gt;Dialogue mode
&lt;/h2&gt;&lt;p&gt;Two years ago, [something] dramatically emerged, allowing humanity to recognize the capabilities of large language models&lt;/p&gt;
&lt;p&gt;Based on &lt;strong&gt;基于注释的编程&lt;/strong&gt; and &lt;strong&gt;基于对话的编程&lt;/strong&gt;, the interactive logic is completely different; the dialogue pattern offers a user-friendly experience for novice users. It&amp;rsquo;s essential to mention &lt;code&gt;NewBing&lt;/code&gt;在每个问题之后，给出的追问提示，微软在尝试引导用户，获取&lt;code&gt;AI&lt;/code&gt; within the knowledge base, with more content.&lt;/p&gt;
&lt;h2 id=&#34;for-example&#34;&gt;For example
&lt;/h2&gt;&lt;p&gt;A long journey on high-speed rail; coding is a good option. With limited data, processing a few images and obtaining image resources is manageable, but downloading software to process them is a bit of a luxury given the network conditions.&lt;/p&gt;
&lt;p&gt;A batch of image assets with transparent backgrounds are needed for a mini-program navigation bar. Selected states should be colored, unselected states black and white. The assets are currently in color and require preprocessing.&lt;/p&gt;
&lt;p&gt;Write a Python code snippet that reads all images from a folder, converts them to grayscale, preserves transparency, optimizes image quality and reduces file size upon saving, and displays the comparison of file sizes before and after optimization in the console&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# 导入必要的库
import os
import PIL
from PIL import Image

# 定义一个函数，将图片转换成黑白，维持透明背景，保存图片，并返回文件体积
def convert_and_save(image_file):
    # 打开图片
    image = Image.open(image_file)
    # 获取图片的模式，如果是 RGBA 模式，说明有透明背景
    mode = image.mode
    if mode == &amp;quot;RGBA&amp;quot;:
        # 创建一个和图片大小一致的白色背景图片
        background = Image.new(&amp;quot;RGB&amp;quot;, image.size, (255, 255, 255))
        # 将原图片粘贴到白色背景上，忽略透明像素
        background.paste(image, mask=image.split()[3])
        # 将合成的图片转换成灰度模式
        gray_image = background.convert(&amp;quot;L&amp;quot;)
        # 将灰度图片再转换成 RGBA 模式，以便保留透明背景
        final_image = gray_image.convert(&amp;quot;RGBA&amp;quot;)
    else:
        # 如果不是 RGBA 模式，直接将图片转换成灰度模式
        final_image = image.convert(&amp;quot;L&amp;quot;)
    # 获取原图片的文件名和扩展名
    file_name, file_ext = os.path.splitext(image_file)
    # 定义新图片的文件名，添加 _bw 后缀表示黑白
    new_file_name = file_name + &amp;quot;_bw&amp;quot; + file_ext
    # 保存新图片，并优化质量，减少文件体积
    final_image.save(new_file_name, optimize=True)
    # 获取原图片和新图片的文件体积，并返回
    old_size = os.path.getsize(image_file)
    new_size = os.path.getsize(new_file_name)
    return old_size, new_size

# 定义一个文件夹路径，存放所有的图片
folder_path = &amp;quot;images&amp;quot;
# 遍历文件夹中的所有文件
for file in os.listdir(folder_path):
    # 拼接完整的文件路径
    file_path = os.path.join(folder_path, file)
    # 判断是否是图片文件，根据扩展名判断，可以根据需要添加或修改扩展名
    if file_path.endswith((&amp;quot;.png&amp;quot;, &amp;quot;.jpg&amp;quot;, &amp;quot;.jpeg&amp;quot;, &amp;quot;.gif&amp;quot;)):
        # 调用函数，转换并保存图片，并获取文件体积
        old_size, new_size = convert_and_save(file_path)
        # 在控制台展示优化前后的文件体积大小对比
        print(f&amp;quot;{file}: {old_size} bytes -&amp;gt; {new_size} bytes&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Quickly provided an answer, looks good, and works fine. Experienced programmers will find the code lacks generality and isn&amp;rsquo;t suitable for production environments.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Parameters fixed&lt;/li&gt;
&lt;li&gt;No log files; information is printed to the console&lt;/li&gt;
&lt;li&gt;Image processing tasks are well-suited for multithreaded models&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Are there any improvements that can be made to this code?&lt;/p&gt;
&lt;p&gt;This answer, frankly, far exceeded expectations. It provides suggestions for optimizations that even a typical programmer would consider.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;这段代码可能还有一些可以改进的地方，比如：

可以使用 argparse 库，让用户可以通过命令行参数指定文件夹路径和扩展名，而不是写死在代码里。
可以使用 logging 库，记录程序的运行情况，而不是直接打印到控制台。
可以使用 try-except 语句，捕获可能出现的异常，比如文件打开失败，图片转换失败等，并做相应的处理。
可以使用 multiprocessing 库，利用多核处理器，加速图片的转换和保存过程。
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A friendly response, considering deployment environments may vary, supporting only single-process execution is permitted&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;当然，我很乐意帮你修改一下代码。我尝试添加了 argparse, logging 和 try-except 语句，但没有添加 multiprocessing 库，因为我不确定你的电脑是否支持多核处理。修改后的代码如下：
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# 导入必要的库
import argparse
import logging
import multiprocessing
import os

from PIL import Image


# 定义一个函数，将图片转换成黑白，维持透明背景，保存图片，并返回文件体积
def convert_and_save(image_file):
    # 打开图片
    try:
        image = Image.open(image_file)
    except Exception as e:
        logging.error(f&amp;quot;Failed to open {image_file}: {e}&amp;quot;)
        return None, None
    # 获取图片的模式，如果是 RGBA 模式，说明有透明背景
    mode = image.mode
    if mode == &amp;quot;RGBA&amp;quot;:
        # 创建一个和图片大小一致的白色背景图片
        background = Image.new(&amp;quot;RGB&amp;quot;, image.size, (255, 255, 255))
        # 将原图片粘贴到白色背景上，忽略透明像素
        background.paste(image, mask=image.split()[3])
        # 将合成的图片转换成灰度模式
        gray_image = background.convert(&amp;quot;L&amp;quot;)
        # 将灰度图片再转换成 RGBA 模式，以便保留透明背景
        final_image = gray_image.convert(&amp;quot;RGBA&amp;quot;)
    else:
        # 如果不是 RGBA 模式，直接将图片转换成灰度模式
        final_image = image.convert(&amp;quot;L&amp;quot;)
    # 获取原图片的文件名和扩展名
    file_name, file_ext = os.path.splitext(image_file)
    # 定义新图片的文件名，添加 _bw 后缀表示黑白
    new_file_name = file_name + &amp;quot;_bw&amp;quot; + file_ext
    # 保存新图片，并优化质量，减少文件体积
    try:
        final_image.save(new_file_name, optimize=True)
    except Exception as e:
        logging.error(f&amp;quot;Failed to save {new_file_name}: {e}&amp;quot;)
        return None, None
    # 获取原图片和新图片的文件体积，并返回
    old_size = os.path.getsize(image_file)
    new_size = os.path.getsize(new_file_name)
    return file_name, old_size, new_size

# 定义一个函数，解析命令行参数，并返回文件夹路径和扩展名列表
def parse_args():
    # 创建一个解析器对象
    parser = argparse.ArgumentParser(description=&amp;quot;Convert images to black and white and optimize quality.&amp;quot;)
    # 添加一个必选的位置参数，表示文件夹路径
    parser.add_argument(&amp;quot;folder_path&amp;quot;, help=&amp;quot;The path of the folder that contains the images.&amp;quot;)
    # 添加一个可选的参数，表示扩展名列表，默认为 png, jpg, jpeg 和 gif
    parser.add_argument(&amp;quot;-e&amp;quot;, &amp;quot;--extensions&amp;quot;, nargs=&amp;quot;+&amp;quot;, default=[&amp;quot;.png&amp;quot;, &amp;quot;.jpg&amp;quot;, &amp;quot;.jpeg&amp;quot;, &amp;quot;.gif&amp;quot;], help=&amp;quot;The extensions of the image files.&amp;quot;)
    # 解析命令行参数，并返回结果对象
    args = parser.parse_args()
    return args.folder_path, args.extensions

# 定义一个函数，打印优化前后的文件体积大小对比
def print_result(result):
    # 如果结果不为空，说明转换和保存成功
    if result:
        # 解包结果为文件名和文件体积元组
        if len(result) == 3:
            file, old_size, new_size = result
            # 在控制台展示优化前后的文件体积大小对比
            logging.info(f&amp;quot;{file}: {old_size} bytes -&amp;gt; {new_size} bytes&amp;quot;)
        else:
            logging.info(f&amp;quot;{result}&amp;quot;)

# 配置日志记录器，将日志输出到控制台和文件中，设置日志等级为 INFO
logging.basicConfig(level=logging.INFO,
                    format=&amp;quot;%(asctime)s %(levelname)s %(message)s&amp;quot;,
                    handlers=[logging.StreamHandler(),
                              logging.FileHandler(&amp;quot;log.txt&amp;quot;)])

# 调用函数，获取文件夹路径和扩展名列表
folder_path, extensions = parse_args()

if __name__ == &amp;quot;__main__&amp;quot;:
    # Windows 需要这个函数的原因是 Windows 缺乏 fork() 函数（这不完全正确）。
    # 因此，在 Windows 上，fork() 是通过创建一个新的进程来模拟的，这个新的进程中运行的代码，在 Linux 上是在子进程中运行的。
    # 由于这些代码要在技术上无关的进程中运行，所以它必须在运行之前被传递到那里。
    # 传递的方式是先将它们序列化，然后通过管道从原始进程发送到新的进程。
    # 另外，这个新的进程被通知它必须运行通过管道传递的代码，通过向它传递 --multiprocessing-fork 命令行参数。
    # 如果你看一下 freeze_support() 函数的实现，它的任务是检查它运行在哪个进程中，是否应该运行通过管道传递的代码
    multiprocessing.freeze_support()

    # 创建一个进程池，根据电脑的核心数自动分配进程
    pool = multiprocessing.Pool()
    # 创建一个空列表，用于存放异步任务的结果对象
    results = []
    # 遍历文件夹中的所有文件
    for file in os.listdir(folder_path):
        # 拼接完整的文件路径
        file_path = os.path.join(folder_path, file)
        # 判断是否是图片文件，根据扩展名判断，可以根据需要修改扩展名列表
        if any(file_path.endswith(ext) for ext in extensions):
            # 调用函数，转换并保存图片，并获取文件体积，使用异步方式，不阻塞主进程
            result = pool.apply_async(convert_and_save, args=(file_path,), callback=print_result)
            # 将结果对象添加到列表中
            results.append((file, result))
    # 关闭进程池，不再接受新的任务
    pool.close()
    # 等待所有的任务完成
    pool.join()
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;afterword&#34;&gt;Afterword
&lt;/h2&gt;&lt;p&gt;Due to local development issues, the code was fixed by investigating reported errors&lt;/p&gt;
&lt;p&gt;Just as we learned techniques for using search engines, we also need to learn communication skills – providing reasonable and sufficient constraints to efficiently obtain the answers we need&lt;/p&gt;
&lt;p&gt;Note:&lt;/p&gt;</description>
        </item>
        <item>
        <title>AI-assisted programming, an evolution of productivity</title>
        <link>https://blog.ttf248.life/en/p/ai-assisted-programming-productivity-evolution/</link>
        <pubDate>Tue, 28 Feb 2023 17:05:17 +0800</pubDate>
        
        <guid>https://blog.ttf248.life/en/p/ai-assisted-programming-productivity-evolution/</guid>
        <description>&lt;p&gt;&lt;strong&gt;INLINE_CODE_0&lt;/strong&gt; 发布也不到两年时间，&lt;strong&gt;INLINE_CODE_1&lt;/strong&gt; was released, and I don&amp;rsquo;t fully understand the underlying principles, but I’ve been using them for a while. The two tools offer completely different levels of assistance, but both significantly improve productivity.&lt;/p&gt;
&lt;p&gt;For overly complex matters, the effect can only be about nine-tenths successful&lt;/p&gt;
&lt;h2 id=&#34;github-copilot&#34;&gt;github copilot
&lt;/h2&gt;&lt;p&gt;When it was released, the website description &lt;strong&gt;INLINE_CODE_0&lt;/strong&gt; didn&amp;rsquo;t seem very smart, and after trying it out, it wasn&amp;rsquo;t very usable, so I gave up&lt;/p&gt;
&lt;p&gt;Before 2022, out of boredom, I switched to the new version for a try. The results are already pretty good, though the speed is slow in China, likely due to network issues. Code generated based on comments like &lt;code&gt;Python&lt;/code&gt; is quite decent and much faster than researching and writing it myself.&lt;/p&gt;
&lt;h2 id=&#34;chatgpt&#34;&gt;ChatGPT
&lt;/h2&gt;&lt;p&gt;Still designing products from a programmer&amp;rsquo;s, or development, perspective. You need to understand how to code and gradually guide AI-assisted programming through comments.&lt;/p&gt;
&lt;p&gt;Let non-experts quickly get started. The conversational, interactive design allows you to ask questions, describe your needs, and continue asking if anything is unclear.&lt;/p&gt;
&lt;p&gt;For example, you understand the concept of tokenization but don&amp;rsquo;t know how to code it, and want to calculate string similarity&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;Python 分词计算 維天運通 LOGORY 和 合肥維天運通信息科技股份有限公司 两个字符串的相似度；
比较前，统一处理字符串，英文转换为小写；
第二个字符中如果能找到第一个字符的首字符，将第二个字符串截位，截位的长度为第一个字符串的长度
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Hello, this is Bing. 😊&lt;/p&gt;
&lt;p&gt;According to online information, Python tokenization can be implemented using the jieba library. Jieba supports three modes: precise mode, full mode, and search engine mode. You can choose the appropriate mode based on your needs.&lt;/p&gt;
&lt;p&gt;To compare the similarity of two strings, you first need to tokenize them and then calculate indicators such as word vectors or edit distance. You can use libraries like NumPy or SciPy for mathematical operations.&lt;/p&gt;
&lt;p&gt;To uniformly handle strings, converting them to lowercase can be achieved using Python&amp;rsquo;s built-in &lt;code&gt;lower()&lt;/code&gt; function&lt;/p&gt;
&lt;p&gt;To extract the second string, you can use Python&amp;rsquo;s slicing operator []&lt;/p&gt;
&lt;p&gt;Here is a possible code example:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import jieba
import numpy as np
from scipy.spatial.distance import cosine

# 定义两个字符串
str1 = &amp;quot;維天運通 LOGORY&amp;quot;
str2 = &amp;quot;合肥維天運通信息科技股份有限公司&amp;quot;

# 统一处理字符串，英文转换为小写
str1 = str1.lower()
str2 = str2.lower()

# 找到第二个字符串中第一个字符的位置
index = str2.find(str1[0])

# 如果找到了，截取第二个字符串
if index != -1:
    str2 = str2[index:index+len(str1)]

# 使用 jieba 库进行分词
words1 = list(jieba.cut(str1))
words2 = list(jieba.cut(str2))

# 计算两个字符串的词向量（以词频作为权重）
vector1 = np.zeros(len(words1))
vector2 = np.zeros(len(words1))

for i, word in enumerate(words1):
    vector1[i] += 1 # 统计第一个字符串中每个词出现的次数
    if word in words2:
        vector2[i] += 1 # 统计第二个字符串中每个词出现的次数

# 计算两个字符串的相似度（以余弦距离作为指标）
similarity = 1 - cosine(vector1, vector2)

print(&amp;quot;两个字符串的相似度是：&amp;quot;, similarity)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Please provide the Chinese text you want me to translate. I am ready when you are!&lt;/p&gt;
&lt;h3 id=&#34;chestnut&#34;&gt;Chestnut
&lt;/h3&gt;&lt;p&gt;Find all records from the product code table where the product codes are within the range of 80000-89999&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;你好，这是 Bing。😊

根据搜索结果12，你可以使用 SQL BETWEEN 运算符来选择给定区间内的值。BETWEEN 运算符包含了开始和结束的值。

你需要先指定要查询的表名和列名，然后在 WHERE 子句中使用 BETWEEN 运算符和区间值。

例如，如果你要查询产品代码表中，产品代码在 80000 到 89999 之间的记录，你可以使用以下语句：

SELECT * FROM product_code
WHERE code BETWEEN 80000 AND 89999;
这里，* 表示选择所有列3。如果你只想选择特定的列，比如产品名称和价格，你可以用逗号分隔它们，如下：

SELECT name, price FROM product_code
WHERE code BETWEEN 80000 AND 89999;
希望这能帮助到你。
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Keep asking, deny it: The product code is a string, your code is wrong&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;抱歉，我没有注意到产品代码是字符串。如果是这样的话，你需要在区间值两边加上单引号，如下：

SELECT * FROM product_code
WHERE code BETWEEN &#39;80000&#39; AND &#39;89999&#39;;
请原谅我的错误。🙏
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Congratulations, it will be wrong&lt;/p&gt;</description>
        </item>
        
    </channel>
</rss>
