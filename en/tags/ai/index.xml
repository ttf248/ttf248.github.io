<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>AI on Uncle Xiang&#39;s Notebook</title>
        <link>https://blog.ttf248.life/en/tags/ai/</link>
        <description>Recent content in AI on Uncle Xiang&#39;s Notebook</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en</language>
        <lastBuildDate>Thu, 29 May 2025 23:50:36 +0800</lastBuildDate><atom:link href="https://blog.ttf248.life/en/tags/ai/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>Old habits, dazzling flowers captivate the eyes</title>
        <link>https://blog.ttf248.life/en/p/old-ailment-stunning-flowers/</link>
        <pubDate>Mon, 26 May 2025 23:54:12 +0800</pubDate>
        
        <guid>https://blog.ttf248.life/en/p/old-ailment-stunning-flowers/</guid>
        <description>&lt;p&gt;Having focused on backend development for many years, I recently started exploring &lt;strong&gt;&lt;em&gt;italicized and bolded&lt;/em&gt;&lt;/strong&gt; frontend interfaces to implement one, but in reality, these attempts haven&amp;rsquo;t been particularly helpful for my current work and have instead scattered my focus&lt;/p&gt;
&lt;h2 id=&#34;applicable-scenarios-for-ai&#34;&gt;Applicable scenarios for AI
&lt;/h2&gt;&lt;p&gt;AI tools can be particularly effective in small projects, especially when writing functions that are independent, have low system coupling, and feature simple business logic. These tasks typically involve clear inputs and outputs with limited context dependencies, making them well-suited for current AI-assisted programming capabilities.&lt;/p&gt;
&lt;p&gt;However, AI&amp;rsquo;s limitations become apparent when dealing with complex system architectures or deep business logic. It may generate code that appears reasonable but is actually detached from the projectâ€™s actual needs, and even introduce potential issues difficult to debug. In these scenarios, AI is better suited as an assistive tool rather than a fully relied-upon code generator. We need to rigorously review and test the generated code to ensure it meets practical requirements.&lt;/p&gt;
&lt;h2 id=&#34;the-cost-of-mistakes-and-learning&#34;&gt;The Cost of Mistakes and Learning
&lt;/h2&gt;&lt;p&gt;While attempting to generate frontend code using AI, I encountered numerous challenges. As frontend isn&amp;rsquo;t my area of expertise, troubleshooting issues proved time-consuming and draining. Even with prompt adjustments for AI rewrites, low-level errors were difficult to avoid. This repeated trial and error not only wasted time but also highlighted that my efforts are better focused on the backendâ€™s business logic rather than exploring unfamiliar territory.&lt;/p&gt;
&lt;p&gt;Reflecting on the project completed this weekend, I&amp;rsquo;m even more convinced that focusing on backend development and user interaction logic, implementing functionality through the console, is currently the most efficient approach. Systematically learning frontend knowledge later, when I have more time and energy, might be a better strategy.&lt;/p&gt;
&lt;h2 id=&#34;frontend-learning-plan&#34;&gt;Frontend learning plan
&lt;/h2&gt;&lt;p&gt;The front-end tech stack is complex and diverse, making it unrealistic to quickly master. I plan to focus on one framework initially, such as Vue.js or React.js, and deeply learn its core concepts and usage. Only after gaining a solid understanding of the fundamentals should I attempt AI-assisted code generation for the front-end, to effectively avoid errors and wasted time due to unfamiliarity.&lt;/p&gt;
&lt;p&gt;For now, the focus should remain on backend development and steadily building core skills. Explore the combination of frontend and AI when the time is right; that may yield greater rewards.&lt;/p&gt;</description>
        </item>
        <item>
        <title>Claude4 released, attempting development: Hugo tag, hyperlink translation assistant</title>
        <link>https://blog.ttf248.life/en/p/claude-4-release-and-experimentation-hugo-tags-hyperlink-translation-assistant/</link>
        <pubDate>Sat, 24 May 2025 03:05:31 +0800</pubDate>
        
        <guid>https://blog.ttf248.life/en/p/claude-4-release-and-experimentation-hugo-tags-hyperlink-translation-assistant/</guid>
        <description>&lt;p&gt;This site is built with Hugo, but I&amp;rsquo;ve been using Chinese titles, which results in unfriendly URLs. To put it simply, when shared, they don&amp;rsquo;t look good because Chinese characters are encoded as things like %E4%BD%A0%E5%A5%BD. While slugs can fix this, manually setting them each time is too much trouble.&lt;/p&gt;
&lt;p&gt;So, today I tried using Claude4 to develop a translation assistant that automatically converts Chinese titles into English slugs and adds hyperlinks within articles. This would eliminate the need for manual setup.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Claude 4 is excellent; its ability to understand context and handle complex tasks has been significantly improved&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;project-address&#34;&gt;Project address
&lt;/h2&gt;&lt;p&gt;Domestic project address: &lt;strong&gt;PROTECTED&lt;/strong&gt;
Overseas project address: &lt;strong&gt;PROTECTED&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;code-implementation&#34;&gt;Code implementation
&lt;/h2&gt;&lt;p&gt;Here&amp;rsquo;s the translation: First, let me outline the approach: We need to scan all articles, extract tag information and article titles, then call a local large language model (like gemma-3-12b-it) for translation&lt;/p&gt;
&lt;p&gt;In practical development, compared to previous generations of large models, it not only supports the statistics and analysis of tags but also includes classification statistics and can even detect &lt;strong&gt;unlabeled article previews&lt;/strong&gt; and generate tag pages&lt;/p&gt;
&lt;p&gt;Whether connecting to local large models, adding translation caches, or conducting large-scale code refactoring, &lt;code&gt;Claude4&lt;/code&gt;Everything was completed in one go, with almost no issues. Although the project wasn&amp;rsquo;t large-scale, it included many small features. Previously, large language models often forgot earlier context during development, but this time&amp;hellip;&lt;code&gt;Claude4&lt;/code&gt; performs exceptionally well, &lt;strong&gt;rarely losing context&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In short, with improved intelligence, we&amp;rsquo;re preparing to switch to &lt;strong&gt;PROTECTD_INLINE_CODE_0&lt;/strong&gt; for more development work as our primary coding model&lt;/p&gt;
&lt;h2 id=&#34;translation-cache&#34;&gt;Translation cache
&lt;/h2&gt;&lt;p&gt;This approach, aside from reducing large model calls, offers good efficiency when running a 12b model locally without causing delays. However, repeatedly calling the large model can still be slow. Secondly, to maintain article links, full updates sometimes produce inconsistent results due to long titles, leading to link changes â€“ which is quite awkward.&lt;/p&gt;
&lt;h2 id=&#34;feature-optimization&#34;&gt;Feature optimization
&lt;/h2&gt;&lt;p&gt;The entire project was handed over to [REDACTED], who analyzed areas for optimization and provided the following suggestions:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;é…ç½®å¤–ç½®åŒ– - æé«˜å¯ç»´æŠ¤æ€§å’Œçµæ´»æ€§
ç»“æ„åŒ–æ—¥å¿— - ä¾¿äºé—®é¢˜æ’æŸ¥å’Œç›‘æ§
æ€§èƒ½ç›‘æ§ - äº†è§£ç³»ç»Ÿè¿è¡ŒçŠ¶å†µ
ç”¨æˆ·ä½“éªŒ - è¿›åº¦æ¡ç­‰è§†è§‰åé¦ˆ
é”™è¯¯å¤„ç† - æ›´å®Œå–„çš„å¼‚å¸¸å¤„ç†æœºåˆ¶
ä»£ç ç»„ç»‡ - æ›´æ¸…æ™°çš„æ¨¡å—åˆ’åˆ†
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The code review was completed and everything looks good. For example, the configuration file now automatically generates a default if one doesn&amp;rsquo;t exist when reading it, converting previous configurations to defaults â€“ this prevents user errors.&lt;/p&gt;
&lt;h3 id=&#34;heading&#34;&gt;
&lt;/h3&gt;&lt;p&gt;Requirements: While translating the main text, dynamically calculate the current translation efficiency, estimate the remaining time, and output relevant information to the console. This includes tracking the number of characters obtained from the article, the number of characters translated per line, the elapsed time, a fitted calculation for the translation time per 100 characters, and an estimated remaining translation time for the entire article.&lt;/p&gt;
&lt;p&gt;The code is done, but I&amp;rsquo;m not very satisfied with the results, so I asked AI to provide new design suggestions&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Provides multiple efficiency calculation methods: real-time efficiency, average efficiency, sliding window efficiency&lt;/li&gt;
&lt;li&gt;Improved display methods: progress bar, segmented statistics, dynamic refresh&lt;/li&gt;
&lt;li&gt;Add more useful metrics: API call count, success rate, etc&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;After completing the code, I discovered a new surprise â€“ real-time statistics on translation efficiency are displayed without mindless scrolling&lt;/p&gt;
&lt;p&gt;Translating body text to English..
Detected 53 lines needing translation
Translating 354 characters..
Completed (3.1s) | API call #1
Completed (1.5s) | API call #2
Completed (0.9s) | API call #3
Completed (0.2s) | API call #4
Completed
Completed
Completed (0.2s) | API call #7
Progress: 13.2% (7/53) | Characters 12.9% (925/7163) 114.6 characters/second
Efficiency: Real-time 76.4 | Average 117.9 | Recent 109.0 | Stage 113.6 characters/second ğŸ“Š
Success Rate: 100.0% (7/7) | Remaining: 46 lines, 7 seconds] 9.4% Translation of 110 charactersâ€¦
Estimated remaining: 55s | Estimated completion: 10:19 8s | 11.3% | Translating 114 charactersâ€¦
Processing speed: 3211.3 lines/minute | Total time: 8s] 13.2% Translated 16 charactersâ€¦
Stage 1/6 [â–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 15.1% Translating 166 characters..&lt;/p&gt;
&lt;p&gt;I haven&amp;rsquo;t written much control programs before, curious about how itâ€™s implemented, so I looked at the code&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-golang&#34;&gt;// Clear screen and redisplay (dynamic refresh effect)
if translationCount &amp;gt; 1 {
 Â  fmt.Print(&amp;quot;\033[6A\033[K&amp;quot;) // Move up 6 lines and clear
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;performance-statistics-menu&#34;&gt;Performance Statistics Menu
&lt;/h3&gt;&lt;p&gt;This new feature, allowing me to design it myself, might not even be this well-designed&lt;/p&gt;
&lt;p&gt;Performance Statistics:
Translation count: 360
Cache hit rate: 1.4% (5/365)
Average translation time: 315.927234ms
File Operations: 73
Incorrect attempts: 0&lt;/p&gt;
&lt;h3 id=&#34;progress-bar-display&#34;&gt;Progress bar display
&lt;/h3&gt;&lt;p&gt;New &lt;strong&gt;features&lt;/strong&gt; provide detailed progress, time spent, and estimated remaining time&lt;/p&gt;
&lt;p&gt;Please select function (0-13): 10
Collecting translation target..
Cached file loaded, containing 0 translation records&lt;/p&gt;
&lt;p&gt;Translation cache statistics:
Total labels: 229
Total articles: 131
Cached: 0 items
360 items need translating&lt;/p&gt;
&lt;p&gt;Confirm generating full translation cache? (y/n): y
Generating full translation cache..
Cached file loaded, containing 0 translation records
Checking cached translation..
Need to translate 360 new tags
5/360 (1.4%) - Time taken: 3s - Estimated remaining: 3m8sğŸ’¾ Saved cache file, containing 5 translation records
10/360 (2.8%) - Time taken: 6s - Estimated remaining time: 3m28sğŸ’¾ Saved cache file, containing 10 translation records
15/360 (4.2%) - Time taken: 9s - Estimated remaining: 3m30sğŸ’¾ Saved cache file, containing 15 translation records
20/360 (5.6%) - Time taken: 13s - Estimated remaining time: 3m36sğŸ’¾ Saved cache file, containing 20 translation records
25/360 (6.9%) - Time taken: 16s - Estimated remaining time: 3m33sğŸ’¾ Saved cache file, containing 25 translation records
30/360 (8.3%) - Time elapsed: 19s - Estimated remaining: 3m30sğŸ’¾ Saved cache file, containing 30 translation records
Saved cache file, containing 35 translation records&lt;/p&gt;
&lt;hr&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;=== Hugo åšå®¢ç®¡ç†å·¥å…· ===

ğŸš€ æ ¸å¿ƒåŠŸèƒ½
  1. ä¸€é”®å¤„ç†å…¨éƒ¨ (å®Œæ•´åšå®¢å¤„ç†æµç¨‹)

ğŸ“ å†…å®¹ç®¡ç†
  2. ç”Ÿæˆæ ‡ç­¾é¡µé¢
  3. ç”Ÿæˆæ–‡ç« Slug
  4. ç¿»è¯‘æ–‡ç« ä¸ºå¤šè¯­è¨€ç‰ˆæœ¬

ğŸ’¾ ç¼“å­˜ç®¡ç†
  5. æŸ¥çœ‹ç¼“å­˜çŠ¶æ€
  6. ç”Ÿæˆå…¨é‡ç¿»è¯‘ç¼“å­˜
  7. æ¸…ç©ºç¿»è¯‘ç¼“å­˜

  0. é€€å‡ºç¨‹åº
&lt;/code&gt;&lt;/pre&gt;</description>
        </item>
        <item>
        <title>Too much AI, some side effects</title>
        <link>https://blog.ttf248.life/en/p/ai-overuse-side-effects/</link>
        <pubDate>Wed, 14 May 2025 19:39:50 +0800</pubDate>
        
        <guid>https://blog.ttf248.life/en/p/ai-overuse-side-effects/</guid>
        <description>&lt;p&gt;Since the establishment of &lt;strong&gt;INLINE_CODE_0&lt;/strong&gt;, there&amp;rsquo;s been a surge in recording and publishing trivial things using AI. Consequently, time for quiet reflection has diminished. It would be better to slightly control the output of this column and consolidate it into a monthly publication, releasing one article per month.&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s like a lingering effect or side effect â€“ efficiency increases, but depth and breadth of thought decrease&lt;/p&gt;
&lt;h2 id=&#34;efficiency-improvement-undeniable&#34;&gt;Efficiency improvement: Undeniable
&lt;/h2&gt;&lt;p&gt;Previously, this column wasn&amp;rsquo;t well-maintained. Due to laziness, I didnâ€™t search for information or compile records on some trending events. Now, with various AI tools, outlining is sufficient; the AI can automatically search and record related events, generate articles, and after simple formatting, they can be published.&lt;/p&gt;
&lt;p&gt;This is like a dream come true for those who don&amp;rsquo;t want to put in too much effort; it significantly boosts efficiency, almost achieving results with half the work&lt;/p&gt;
&lt;p&gt;Efficiency significantly improves when coding instead of writing articles. Many code sections previously required detailed review of API documentation; now, this step can be skipped â€“ a necessary shortcut. Familiarity with APIs is best left to AI.&lt;/p&gt;
&lt;h2 id=&#34;junk-content&#34;&gt;Junk content
&lt;/h2&gt;&lt;p&gt;Many of the submissions lack substance; they feel lifeless and stale, like chewing waxâ€”not a style I previously enjoyed&lt;/p&gt;
&lt;p&gt;To put it another way, AI-generated content often feels like mass-produced work, lacking soul&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Internet junk of the new era&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;forgetfulness&#34;&gt;Forgetfulness
&lt;/h2&gt;&lt;p&gt;This type of article is AI-generated. I don&amp;rsquo;t know the readers, and over time, my own memory will fade or even disappear.&lt;/p&gt;
&lt;p&gt;Similar issues arise when coding â€“ without reviewing the commit history, it&amp;rsquo;s impossible to know your original thought process or rationale for a particular implementation. This is especially true with code generated after extensive revisions and discussions with AI, which can significantly diverge from the initial idea, even becoming entirely different.&lt;/p&gt;
&lt;h2 id=&#34;search&#34;&gt;Search
&lt;/h2&gt;&lt;p&gt;I&amp;rsquo;ve noticed a significant decrease in how often I use Google and Baidu recently; searching with AI is much better than traditional search engines, both in terms of interaction and results&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s commemorate &lt;strong&gt;INLINE_CODE_0&lt;/strong&gt;, one of the earliest AI tools connected to the internet released by a major company&lt;/p&gt;
&lt;p&gt;People are using Google less; visits to &lt;strong&gt;INLINE_CODE_0&lt;/strong&gt; have decreased as well. Many questions can now be answered directly by AI, and this website is gradually being phased out by the times.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion
&lt;/h2&gt;&lt;p&gt;The blog I&amp;rsquo;m still maintaining doesn&amp;rsquo;t get much traffic anyway, and certainly won&amp;rsquo;t now; itâ€™s more of a personal journal for my own reference&lt;/p&gt;</description>
        </item>
        <item>
        <title>Design and develop a customizable stock selection module (without coding)</title>
        <link>https://blog.ttf248.life/en/p/no-code-design-develop-custom-stock-module/</link>
        <pubDate>Thu, 27 Feb 2025 23:20:39 +0800</pubDate>
        
        <guid>https://blog.ttf248.life/en/p/no-code-design-develop-custom-stock-module/</guid>
        <description>&lt;p&gt;We tested Cursor last month, but due to free tier limitations, we didn&amp;rsquo;t develop complex features, just simple testing. We noticed that ByteDance also released a similar product at the time, both using the same underlying large language model: Claude-3.5.&lt;/p&gt;
&lt;p&gt;ByteDance&amp;rsquo;s product, Trae, initially launched a Mac version and finally released a Windows version this February. Big company products are just good â€“ you can use them for free without paying, with unlimited access to Claude-3.5, which is a very impressive model.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Ultimately, development stalled on the K-line chart due to my limited knowledge of React. To continue, I need to supplement my frontend skills and break down tasks into smaller steps instead of tackling a large project like developing a K-line chart directly.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;issues-found&#34;&gt;Issues found
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;Due to insufficient training data for Vue3 + Element-Plus due to reliance on foreign AI models, we chose React as the frontend framework&lt;/li&gt;
&lt;li&gt;There may be occasional grammatical errors that require manual correction&lt;/li&gt;
&lt;li&gt;Some complex problems require human guidance for solutions&lt;/li&gt;
&lt;li&gt;Code structure optimization requires human guidance&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The most time-consuming part was packaging the frontend code into a container. As someone with zero experience, I had no understanding of this process; I only managed to grasp the logic with help from others. There&amp;rsquo;s a significant difference in how frontend development checks code between dev and build modes. The database and service containers for the backend took about five minutes combined.&lt;/p&gt;
&lt;p&gt;Please provide the Chinese text you want me to translate. I am ready when you are!&lt;/p&gt;
&lt;h2 id=&#34;warehouse-address&#34;&gt;Warehouse address
&lt;/h2&gt;&lt;p&gt;As the title suggests, we&amp;rsquo;re skipping the coding and directly discussing with AI to design and develop a custom stock selection module. Letâ€™s see what kind of results we can achieve.&lt;/p&gt;
&lt;p&gt;Warehouse address: &lt;a class=&#34;link&#34; href=&#34;https://github.com/ttf248/trae-demo&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/ttf248/trae-demo&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Detailed usage instructions can be found in the repository&amp;rsquo;s README.md file&lt;/p&gt;
&lt;p&gt;The repository contains numerous submission records, mostly conversations between me and Trae, along with my tests of various features for him. Notes indicate whether manual intervention was required to implement the corresponding functionality.&lt;/p&gt;
&lt;h2 id=&#34;prompt&#34;&gt;Prompt
&lt;/h2&gt;&lt;p&gt;This project was created from scratch. Here&amp;rsquo;s the prompt for the project:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;åŸºäºé¡¹ç›®åŸå‹å›¾ï¼Œå¼€å‘åŠŸèƒ½ï¼šè‡ªé€‰è‚¡ï¼Œéœ€è¦æ”¯æŒåˆçº¦çš„æ–°å¢ã€åˆ é™¤ã€ä¿®æ”¹ã€æŸ¥è¯¢ã€‚è‡ªé€‰è‚¡ç•Œé¢éœ€è¦å±•ç¤ºåŸºç¡€çš„è¡Œæƒ…æ•°æ®ã€‚æ”¯æŒå¤šä¸ªä¸åŒçš„å¸‚åœºåˆ‡æ¢ã€‚

å‰ç«¯ï¼šreact
åç«¯ï¼šgolang gin gorm
æ•°æ®åº“ï¼šPostgreSQL

æœåŠ¡ç«¯éœ€è¦æ”¯æŒè·¨åŸŸè¯·æ±‚ï¼ŒåŒæ—¶éœ€è¦è€ƒè™‘æ•°æ®çš„æ ¡éªŒå’Œé”™è¯¯å¤„ç†ï¼Œå¦‚æœåç«¯æœåŠ¡ä¸å¯ç”¨ï¼Œå‰ç«¯éœ€è¦å‘Šè­¦æç¤ºã€‚

åç«¯éœ€è¦å±•ç¤ºè¯·æ±‚å’Œåº”ç­”çš„æ—¥å¿—ï¼›å‰ç«¯ä¹Ÿæ‰“å°é€šè®¯çš„æ—¥å¿—ï¼Œæ–¹ä¾¿æ’æŸ¥é—®é¢˜ã€‚
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;ui-and-interaction-optimization&#34;&gt;UI and interaction optimization
&lt;/h2&gt;&lt;p&gt;The frontend design is entirely dependent on Grok. We initially created a prototype within Trae, but it lacked aesthetic appeal. Due to the model&amp;rsquo;s strong coding capabilities and weaker overall abilities, we need to use Grok to optimize the frontend UI.&lt;/p&gt;
&lt;p&gt;By taking screenshots of the current interface, uploading them to Grok, and having it help us optimize the UI, we can potentially receive numerous optimization suggestions. We will then manually evaluate these suggestions and copy them into Trae for execution, observing the results.&lt;/p&gt;
&lt;h3 id=&#34;technology-stack&#34;&gt;Technology stack
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Frontend: React + TypeScript&lt;/li&gt;
&lt;li&gt;Backend: Golang + Gin + GORM&lt;/li&gt;
&lt;li&gt;Database: PostgreSQL 17&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;system-architecture&#34;&gt;System architecture
&lt;/h2&gt;&lt;h2 id=&#34;backend-architecture&#34;&gt;Backend architecture
&lt;/h2&gt;&lt;p&gt;The backend is implemented using Go&amp;rsquo;s Gin framework for a RESTful API, with modules including:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Please provide the Chinese text you want me to translate. I am ready when you are!&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Use GORM as an ORM framework&lt;/li&gt;
&lt;li&gt;Supports environment variable configuration for database connections&lt;/li&gt;
&lt;li&gt;Automatically migrate database tables&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;Please provide the Chinese text you want me to translate. I am ready when you are!&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;RESTful API Design&lt;/li&gt;
&lt;li&gt;Unified error handling mechanism&lt;/li&gt;
&lt;li&gt;Built-in request logging&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;Please provide the Chinese text you want me to translate. I am ready when you are!&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Support cross-domain for local development environments&lt;/li&gt;
&lt;li&gt;Configurable CORS policy&lt;/li&gt;
&lt;li&gt;Support cross-domain cookies&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;frontend-architecture&#34;&gt;Frontend Architecture
&lt;/h2&gt;&lt;p&gt;Built with React + TypeScript, achieving:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Stock list display&lt;/li&gt;
&lt;li&gt;Self-selected stock management&lt;/li&gt;
&lt;li&gt;Market Data Display&lt;/li&gt;
&lt;li&gt;Error message mechanism&lt;/li&gt;
&lt;/ul&gt;</description>
        </item>
        <item>
        <title>Two years of AI development: Its somewhat like the state before Docker was released</title>
        <link>https://blog.ttf248.life/en/p/ai-development-two-years-docker-pre-release/</link>
        <pubDate>Thu, 20 Feb 2025 18:16:37 +0800</pubDate>
        
        <guid>https://blog.ttf248.life/en/p/ai-development-two-years-docker-pre-release/</guid>
        <description>&lt;p&gt;Artificial intelligence (AI) has undoubtedly been one of the most discussed topics in technology in recent years, especially with its rapid advancements over the past two years. From deep learning and natural language processing to computer vision and automated decision systems, AI applications are constantly emerging. However, despite continuous technological breakthroughs, AI still faces a bottleneck similar to that of Docker before its release â€“ a lack of a killer application to truly ignite the market.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The development of AI over the past two years is similar to the state before Docker&amp;rsquo;s release â€“ lacking a killer application. It needs a perfect practical implementation based on existing technology, like Docker: not relying on groundbreaking new technologies, but offering a complete and reasonable solution that transforms operations and development workflows.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;the-current-state-of-ai-development-technology-is-mature-but-application-still-needs-breakthroughs&#34;&gt;The current state of AI development: technology is mature, but application still needs breakthroughs
&lt;/h2&gt;&lt;p&gt;From a technical perspective, AI has made significant progress in the past two years. Whether it&amp;rsquo;s OpenAIâ€™s GPT series models or Googleâ€™s BERT and DeepMindâ€™s Alpha series, AI processing capabilities have far exceeded previous expectations. Particularly in natural language processing, models like GPT-4 not only possess powerful generation abilities but also demonstrate astonishing performance in understanding and reasoning.&lt;/p&gt;
&lt;p&gt;However, despite rapid technological advancements, the practical application of AI faces certain challenges. Similar to the state before Docker&amp;rsquo;s release, while AI has immense potential, a truly widespread and industry-transforming &amp;ldquo;killer&amp;rdquo; application hasn&amp;rsquo;t yet emerged. People discuss AIâ€™s prospects but may struggle to find an application that can bring revolutionary change. Many AI applications remain in early experimental stages and require further integration and optimization.&lt;/p&gt;
&lt;h2 id=&#34;the-similarity-between-docker-and-ai-technology-isnt-necessarily-innovation-solutions-are-key&#34;&gt;The similarity between Docker and AI: Technology isn&amp;rsquo;t necessarily innovation, solutions are key
&lt;/h2&gt;&lt;p&gt;Looking back at the history before Docker&amp;rsquo;s release, we find striking similarities with the current state of AI development. Prior to Docker, container technology wasn&amp;rsquo;t new; early technologies like LXC (Linux Containers) and virtualization already possessed basic containerization capabilities. However, Docker cleverly integrated and optimized existing technologies, proposing a simpler, more intuitive, and efficient solution. This approach didnâ€™t introduce revolutionary technology but addressed many pain points in operations and development processes, significantly simplifying software deployment, scaling, and management.&lt;/p&gt;
&lt;p&gt;Similarly, the AI field faces a similar situation. While current AI technology is no longer &amp;ldquo;new,&amp;rdquo; achieving widespread application still requires a perfect implementation scenario â€“ like Docker, integrating and optimizing existing technologies to form a practical solution. The â€œkillerâ€ application of AI may not depend on breakthrough new technologies, but rather on how to integrate existing ones to solve real-world business pain points and needs.&lt;/p&gt;
&lt;h2 id=&#34;how-to-find-ais-docker-moment&#34;&gt;How to find AI&amp;rsquo;s &amp;ldquo;Docker moment&amp;rdquo;?
&lt;/h2&gt;&lt;p&gt;To achieve widespread application of AI technology, several aspects need to be addressed&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Please provide the Chinese text you want me to translate. I am ready when you are!
Currently, many AI applications remain experimental and lack large-scale practical implementation. While areas like AI customer service and intelligent recommendations are widely used, their functionality is still limited and hasn&amp;rsquo;t yet overcome industry bottlenecks. True breakthroughs may come from industries long burdened by traditional methodsâ€”such as healthcare, manufacturing, and logisticsâ€”where AI can improve efficiency and reduce costs through more efficient data processing and predictive analysis.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Please provide the Chinese text you want me to translate. I am ready when you are!
Just as Docker improves operational efficiency by streamlining the containerization process, the usability of AI products is equally crucial. The popularization of AI isn&amp;rsquo;t just about technology; itâ€™s about productization. Integrating AI into daily workflows and enabling users to easily utilize these tools without needing a deep understanding of the underlying technology is a key step in its successful implementation.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Please provide the Chinese text you want me to translate. I am ready when you are!
The widespread adoption of any new technology hinges on building a robust ecosystem. Docker&amp;rsquo;s rapid rise is due to its openness and compatibility, allowing developers to easily connect with various cloud platforms, tools, and services. Similarly, the future of AI depends on ecosystem development. Standardization, model sharing, data accessibility, and technical integration will all influence whether AI can achieve broad industry applications.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;conclusion-the-future-of-ai-is-full-of-possibilities-but-still-requires-more-robust-implementation-plans&#34;&gt;Conclusion: The future of AI is full of possibilities, but still requires more robust implementation plans
&lt;/h2&gt;&lt;p&gt;Despite significant advancements in AI technology over the past two years, it remains in a stage without a killer application. Similar to containerization technology before Docker&amp;rsquo;s release, AI needs a practical application scenario that deeply integrates existing technologies with business needs to achieve widespread adoption and scale. While technological innovation is important, solutions that simplify processes and improve efficiency are more likely to drive the popularization and development of the technology.&lt;/p&gt;
&lt;p&gt;In the future, AI may not revolutionize through groundbreaking technology, but rather create a perfect application scenario by integrating existing technologiesâ€”ultimately transforming how we work and live&lt;/p&gt;</description>
        </item>
        <item>
        <title>Deploy DeepSeek-R1 locally</title>
        <link>https://blog.ttf248.life/en/p/ollama-local-deployment-deepseek-r1/</link>
        <pubDate>Fri, 07 Feb 2025 22:41:02 +0800</pubDate>
        
        <guid>https://blog.ttf248.life/en/p/ollama-local-deployment-deepseek-r1/</guid>
        <description>&lt;p&gt;Ollama is an open-source AI tool designed to enable users to run and deploy large language models (LLMs) locally. It aims to provide a convenient and efficient way for developers to use models like GPT on their local machines without relying on cloud services. Ollama supports various models and focuses on optimizing performance so that even resource-constrained devices can run them smoothly.&lt;/p&gt;
&lt;p&gt;With Ollama, users can utilize text-based AI applications and interact with locally deployed models without concerns about data privacy or high API usage fees. You can call different models through a command-line interface (CLI) for tasks like natural language processing and question answering.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Ollama is good for trying out different models. The Windows version doesn&amp;rsquo;t fully utilize hardware performance, likely due to Windows itself. The Linux version might be better. Deploying a 32B parameter model results in slow responses even with low memory and GPU load.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;hardware-overview&#34;&gt;Hardware Overview
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;Operating system: win11&lt;/li&gt;
&lt;li&gt;CPUï¼ši7-10700K&lt;/li&gt;
&lt;li&gt;Memory: 40GB&lt;/li&gt;
&lt;li&gt;Graphics card: RTX 3060 12GB&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;environment-setup&#34;&gt;Environment Setup
&lt;/h2&gt;&lt;p&gt;Added system environment variable for future use&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;set OLLAMA_MODELS=E:\ollama&lt;/code&gt;&lt;/strong&gt;&lt;br&gt;
This variable specifies the storage path for Ollama models. &lt;strong&gt;INLINE_CODE_0&lt;/strong&gt; is a folder path where all local model files are stored. Ollama loads and uses your downloaded or deployed language models based on this path. You can store the model files in other locations by changing this path.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;set OLLAMA_HOST=127.0.0.1:8000&lt;/code&gt;&lt;/strong&gt;&lt;br&gt;
This environment variable sets the host and port for the Ollama service&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;127.0.0.1&lt;/code&gt; is a local address (localhost), meaning that the Ollama service will only listen for requests from the local machine&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;INLINE_CODE_0&lt;/strong&gt; is the designated port number, indicating that the Ollama service will listen for and process requests on port 8000. You can change the port number as needed, but ensure it&amp;rsquo;s not occupied by another application.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;set OLLAMA_ORIGINS=*&lt;/code&gt;&lt;/strong&gt;&lt;br&gt;
This environment variable controls which sources of requests are allowed to access the Ollama service&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;INLINE_CODE_0&lt;/strong&gt; indicates that any origin (i.e., all domains and IP addresses) is allowed to access the Ollama service. This is typically used in development and debugging environments; in production, more restrictive source control is usually specified to limit access to specific domains or IPs for enhanced security.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;deepseek-r1-model-deployment&#34;&gt;DeepSeek-R1 Model Deployment
&lt;/h2&gt;&lt;p&gt;Ollama installation is straightforward; details are omitted here&lt;/p&gt;
&lt;p&gt;Post-installation verification&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;C:\Users\core&amp;gt;ollama -v
ollama version is 0.5.11
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Model deployment, refer to the official model page and select the corresponding parameters for the model: &lt;strong&gt;INLINE_CODE_0&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;A 14B parameter model effectively remembers conversation context; smaller versions do not. The 32B parameter version is too slow for local deployment, so further testing was not conducted.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.ollama.com/library/deepseek-r1&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://www.ollama.com/library/deepseek-r1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/SPEvYTmTBxhoEkJqm1yPmw&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://mp.weixin.qq.com/s/SPEvYTmTBxhoEkJqm1yPmw&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://blog.csdn.net/x18990027/article/details/145368094&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://blog.csdn.net/x18990027/article/details/145368094&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
        </item>
        <item>
        <title>DeepSeek unexpectedly surged before the Spring Festival, causing Nvidia stock to plummet: Institutional maneuvering and large model reasoning chains</title>
        <link>https://blog.ttf248.life/en/p/deepseek-chinese-new-year-nvidia-stock-drop-institutional-operations-large-language-model-chain/</link>
        <pubDate>Fri, 07 Feb 2025 20:36:05 +0800</pubDate>
        
        <guid>https://blog.ttf248.life/en/p/deepseek-chinese-new-year-nvidia-stock-drop-institutional-operations-large-language-model-chain/</guid>
        <description>&lt;p&gt;Ahead of the Spring Festival, DeepSeek quickly became a hot topic, garnering widespread attention on social media in just a few days. This sudden surge not only surprised many but also triggered a market chain reaction. Simultaneously, Nvidia&amp;rsquo;s stock plummeted, raising concerns about its future and prompting significant short-selling activity from some institutions, suggesting a potentially orchestrated scenario.&lt;/p&gt;
&lt;h3 id=&#34;please-provide-the-chinese-text-you-want-me-to-translate-i-am-ready-when-you-are&#34;&gt;Please provide the Chinese text you want me to translate. I am ready when you are!
&lt;/h3&gt;&lt;p&gt;DeepSeek is an AI-powered tool focused on optimizing deep learning models, particularly in natural language processing (NLP) and image generation. In the days leading up to Chinese New Year, the project unexpectedly gained significant attention from investors and technical professionals. The team&amp;rsquo;s performance and demonstrated technological achievements have sparked considerable interest. Discussions about DeepSeek dominate tech circles across developer communities and social media platforms.&lt;/p&gt;
&lt;p&gt;However, DeepSeek&amp;rsquo;s sudden surge in popularity wasn&amp;rsquo;t accidental. Analysis suggests potential involvement from certain institutions. Notably, Nvidiaâ€™s stock price has since seen a clear decline, indicating some factors are driving this change.&lt;/p&gt;
&lt;h3 id=&#34;please-provide-the-chinese-text-you-want-me-to-translate-i-am-ready-when-you-are-1&#34;&gt;Please provide the Chinese text you want me to translate. I am ready when you are!
&lt;/h3&gt;&lt;p&gt;Nvidia, a leading manufacturer of graphics processing units (GPUs), has long been a key hardware provider for large language models and AI computing. While the company&amp;rsquo;s stock has consistently performed strongly alongside the rapid growth of the AI market, it recently experienced a sharp decline following DeepSeekâ€™s surge in popularity and increased market attention to its technology.&lt;/p&gt;
&lt;p&gt;This phenomenon may involve short-selling strategies by institutional investors. In recent years, as AI technology has become widespread, Nvidia&amp;rsquo;s stock price has been highly inflated, leading many investors to believe its valuation is overhyped. Particularly after the surge in popularity of DeepSeek, some institutions may have profited handsomely by shorting Nvidiaâ€™s stock. These institutions successfully capitalized on precise market timing and an understanding of DeepSeekâ€™s influence.&lt;/p&gt;
&lt;h3 id=&#34;please-provide-the-chinese-text-you-want-me-to-translate-i-am-ready-when-you-are-2&#34;&gt;Please provide the Chinese text you want me to translate. I am ready when you are!
&lt;/h3&gt;&lt;p&gt;In traditional AI applications, many practitioners and investors focus on the &amp;ldquo;results&amp;rdquo; of AI modelsâ€”such as generated images or text. However, discussions surrounding DeepSeek reveal a growing recognition that the underlying reasoning chain behind large language models is the more valuable core element. Previously, we could only see model outputs; now, we need to understand their logic, algorithms, and how to optimize performance by adjusting these factors.&lt;/p&gt;
&lt;p&gt;This shift in thinking represents a deeper consideration of AI research and application. Moving from simple &amp;ldquo;black box&amp;rdquo; operations to genuinely understanding the internal workings of models is prompting many technical experts and investors to re-evaluate the future direction of artificial intelligence. DeepSeek&amp;rsquo;s popularity exemplifies this breakthrough, encouraging attention towards the entire model construction and optimization process, rather than just the final output.&lt;/p&gt;
&lt;h3 id=&#34;please-provide-the-chinese-text-you-want-me-to-translate-i-am-ready-when-you-are-3&#34;&gt;Please provide the Chinese text you want me to translate. I am ready when you are!
&lt;/h3&gt;&lt;p&gt;DeepSeek&amp;rsquo;s sudden rise, Nvidiaâ€™s stock plunge, and the market manipulation behind it all appear to be part of a carefully orchestrated scheme. A deep understanding of large language model thinking chains reveals that applying AI technology isn&amp;rsquo;t just about superficial accumulation; it requires in-depth exploration and optimization of internal model logic. As technology advances, we may see more innovative tools like DeepSeek emerge, driving AI research and application to new heights.&lt;/p&gt;
&lt;p&gt;This phenomenon not only reveals the immense potential of AI technology but also prompts us to consider the underlying business competition and capital operations. The future market trends will be a continued focus of this interplay between technology and capital.&lt;/p&gt;</description>
        </item>
        <item>
        <title>Cursor AI programming IDE trial</title>
        <link>https://blog.ttf248.life/en/p/cursor-ai-programming-ide-trial/</link>
        <pubDate>Thu, 23 Jan 2025 19:30:13 +0800</pubDate>
        
        <guid>https://blog.ttf248.life/en/p/cursor-ai-programming-ide-trial/</guid>
        <description>&lt;p&gt;Another year has passed. The biggest change at work is the noticeably increased involvement of AI. Previously, switching between different development languages required developers to be familiar with various APIs. Now, these basic code snippets can be generated by AI â€“ a great boon for developers.&lt;/p&gt;
&lt;h2 id=&#34;chatgpt&#34;&gt;ChatGPT
&lt;/h2&gt;&lt;p&gt;Back in 2023, I revised two introductory articles. Now it&amp;rsquo;s 2025, and frankly, there hasnâ€™t been a significant improvement. It still requires developing one&amp;rsquo;s own understanding, being able to reasonably break down tasks, and most importantly, identifying bugs in AI-generated code.&lt;/p&gt;
&lt;h2 id=&#34;github-copilot&#34;&gt;Github copilot
&lt;/h2&gt;&lt;p&gt;I can&amp;rsquo;t recall the exact date, but I saw information stating that Singapore has deployed servers for domestic use. This means we no longer need to constantly use a VPN; only a VPN is required for login, after which it can be turned off.&lt;/p&gt;
&lt;p&gt;I use Github Copilot more often; this plugin can be used directly in VS Code and Visual Studio without switching between programs. Compared to ChatGPT, Github Copilot offers better project support and a friendlier interface. You can feed it portions of local files &lt;strong&gt;to generate code that aligns with your project.&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;cursor-ai&#34;&gt;Cursor AI
&lt;/h2&gt;&lt;p&gt;I recently saw a new AI programming IDE, Cursor AI. This IDE is based on GitHub Copilot, but it&amp;rsquo;s even more intelligent and can help you create files directly.&lt;/p&gt;
&lt;p&gt;It feels pretty good after a quick try, but my understanding of the existing project is still not enough. When dealing with many local project files and large-scale refactoring and optimization, I still need &lt;strong&gt;å¼€å‘è€…æ‹†åˆ†ä»»åŠ¡&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Switch to curso&amp;rsquo;s engineering mode, enter: Create a personal resume webpage with support for multiple style switching. Remember to fill in some personal information for data display.&lt;/p&gt;
&lt;p&gt;After a few tries, you&amp;rsquo;ll be able to get the following webpage. Of course, itâ€™s still quite simple, but itâ€™s pretty good for beginners.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Currently, registered users can enjoy 150 free trials of the premium API; paid users are limited to 5000 calls&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Please provide the Chinese text you want me to translate. I am ready when you are! Just paste the text here.&lt;/p&gt;</description>
        </item>
        <item>
        <title>AI programming and task decomposition</title>
        <link>https://blog.ttf248.life/en/p/ai-programming-and-task-decomposition/</link>
        <pubDate>Fri, 22 Dec 2023 08:44:26 +0800</pubDate>
        
        <guid>https://blog.ttf248.life/en/p/ai-programming-and-task-decomposition/</guid>
        <description>&lt;p&gt;Two years ago, I added a copy function for the site&amp;rsquo;s code area. It took nearly half a day, and the final rendering effect was not ideal. As an amateur front-end developer, I didnâ€™t bother to improve it; as long as it worked, that was fine. This year, using &lt;strong&gt;INLIN_ITALIC_1&lt;/strong&gt;__, I developed a mini program and became more familiar with front-end development. So, I rebuilt it (AI redesign).&lt;/p&gt;
&lt;p&gt;Understanding the Dao: Break down tasks into many smaller ones&lt;/p&gt;
&lt;h2 id=&#34;previous-text&#34;&gt;Previous text
&lt;/h2&gt;&lt;p&gt;Please provide the Chinese text you want me to translate. I am ready when you are! Just paste the text here.&lt;/p&gt;
&lt;h2 id=&#34;main-body&#34;&gt;Main body
&lt;/h2&gt;&lt;p&gt;Design CSS and JavaScript code to locate webpage elements with the class &amp;ldquo;highlight&amp;rdquo;. Within the container of these elements, add a copy button that implements the functionality to copy the code.&lt;/p&gt;
&lt;p&gt;The initial draft of the code had issues. A revised draft was generated, which is now basically functional, but contains incorrect copied content and redundant text. Further optimization is needed, including manual specification of core color design and effects.&lt;/p&gt;
&lt;p&gt;We understand you have specific requirements for button size and color. We can redesign the button style to accommodate these needs. Here&amp;rsquo;s an adjusted CSS style, incorporating your provided color scheme and optimizing the button dimensions:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-css&#34;&gt;.copy-btn {
  position: absolute;
  top: 2px; /* è°ƒæ•´ä½ç½® */
  right: 2px;
  padding: 3px 6px; /* ç¼©å°å°ºå¯¸ */
  font-size: 12px; /* å‡å°å­—ä½“å¤§å° */
  background-color: #8FD1E1; /* æŒ‰é’®èƒŒæ™¯è‰² */
  color: #272822; /* æ–‡å­—é¢œè‰² */
  border: 1px solid #272822; /* è¾¹æ¡†é¢œè‰² */
  border-radius: 3px; /* åœ†è§’ */
  cursor: pointer;
  opacity: 0.7; /* é»˜è®¤åŠé€æ˜ */
  transition: opacity 0.3s ease, background-color 0.3s ease; /* å¹³æ»‘è¿‡æ¸¡æ•ˆæœ */
}

.copy-btn:hover {
  opacity: 1; /* é¼ æ ‡æ‚¬åœæ—¶ä¸é€æ˜ */
  background-color: #A3D4E5; /* é¼ æ ‡æ‚¬åœæ—¶çš„èƒŒæ™¯è‰² */
}

.highlight {
  position: relative;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this style, Iâ€™ve reduced the button size and font to make it more refined. The default state is semi-transparent; hovering over the button makes it opaque with a slightly brighter background for visual feedback. This design meets your color requirements while maintaining a modern, user-friendly feel.&lt;/p&gt;
&lt;p&gt;A simpler version was provided accordingly&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-JavaScript&#34;&gt;document.addEventListener(&#39;DOMContentLoaded&#39;, function() {
  // æŸ¥æ‰¾æ‰€æœ‰ .highlight å…ƒç´ 
  const highlights = document.querySelectorAll(&#39;.highlight&#39;);

  highlights.forEach(function(highlight) {
    // åˆ›å»ºå¤åˆ¶æŒ‰é’®
    const copyBtn = document.createElement(&#39;button&#39;);
    copyBtn.textContent = &#39;å¤åˆ¶&#39;;
    copyBtn.classList.add(&#39;copy-btn&#39;);

    // å°†æŒ‰é’®æ·»åŠ åˆ°æ¯ä¸ª .highlight å…ƒç´ ä¸­
    highlight.appendChild(copyBtn);

    // æ·»åŠ ç‚¹å‡»äº‹ä»¶ç›‘å¬å™¨
    copyBtn.addEventListener(&#39;click&#39;, function() {
      // å‡è®¾ .highlight ä¸‹çš„ç¬¬ä¸€ä¸ª &amp;lt;code&amp;gt; å…ƒç´ åŒ…å«è¦å¤åˆ¶çš„ä»£ç 
      const code = highlight.querySelector(&#39;code&#39;) ? highlight.querySelector(&#39;code&#39;).textContent : &#39;&#39;;

      navigator.clipboard.writeText(code).then(() =&amp;gt; {
        // æ˜¾ç¤ºå¤åˆ¶æˆåŠŸä¿¡æ¯
        copyBtn.textContent = &#39;å·²å¤åˆ¶!&#39;;
        setTimeout(() =&amp;gt; {
          copyBtn.textContent = &#39;å¤åˆ¶&#39;;
        }, 2000);
      }).catch(err =&amp;gt; {
        console.error(&#39;å¤åˆ¶å¤±è´¥: &#39;, err);
      });
    });
  });
});
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;afterword&#34;&gt;Afterword
&lt;/h2&gt;&lt;p&gt;Although I&amp;rsquo;d prefer a copy button added to the top right of the code area, due to my limited frontend knowledge, I havenâ€™t been able to accurately describe the functionality with keywords, and several attempts have not yielded the desired result&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s uncertain what impact &lt;code&gt;AI&lt;/code&gt; will have on program development as time progresses&lt;/p&gt;</description>
        </item>
        <item>
        <title>prompt-engineer</title>
        <link>https://blog.ttf248.life/en/p/prompt-engineer/</link>
        <pubDate>Sun, 26 Mar 2023 20:46:53 +0800</pubDate>
        
        <guid>https://blog.ttf248.life/en/p/prompt-engineer/</guid>
        <description>&lt;p&gt;Just as we learned techniques for using search engines, we also need to learn communication skills â€“ providing reasonable and sufficient constraints to efficiently obtain the answers we need&lt;/p&gt;
&lt;p&gt;If you look at it from a different angle, the current situation will generate the desired result&lt;/p&gt;
&lt;h2 id=&#34;science-communication&#34;&gt;Science communication
&lt;/h2&gt;&lt;p&gt;Generative Pre-trained Transformer (GPT) is a deep learning model trained on publicly available internet data, used for tasks like question answering, text summarization, machine translation, classification, code generation, and conversational AI. There are currently various versions of GPT, including GPT-1, GPT-2, GPT-3, and GPT-4, each larger and more powerful than its predecessor.&lt;/p&gt;
&lt;h2 id=&#34;does-intelligence-truly-exist&#34;&gt;Does intelligence truly exist?
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;The higher the similarity, the greater the accuracy&lt;/li&gt;
&lt;li&gt;Basic, repetitive tasks no longer require human intervention after specific training&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Generative AI is a technology that utilizes existing data such as text, audio, and images to create new content. It can be applied to various tasks including text generation, speech synthesis, image generation, and conversational systems. The logic of generative AI depends on its training data and model structure. Generally, it can follow grammar, logic, and common sense to a certain extent, but may also produce errors, biases, or inaccurate information. Therefore, the output of generative AI requires human judgment and verification; it should not be blindly trusted or used.&lt;/p&gt;
&lt;h2 id=&#34;prompt-engineer&#34;&gt;prompt-engineer
&lt;/h2&gt;&lt;p&gt;The river of time doesn&amp;rsquo;t flow backward; people need to learn to adapt to the trend. You can consider &lt;em&gt;[INLIN]&lt;/em&gt; as unintelligent and lacking logic, often producing unusable code.&lt;/p&gt;
&lt;p&gt;If you look at it from a different angle, the current situation will generate the desired result&lt;/p&gt;
&lt;h2 id=&#34;dialogue-mode&#34;&gt;Dialogue mode
&lt;/h2&gt;&lt;p&gt;Two years ago, [something] dramatically emerged, allowing humanity to recognize the capabilities of large language models&lt;/p&gt;
&lt;p&gt;Based on &lt;strong&gt;åŸºäºæ³¨é‡Šçš„ç¼–ç¨‹&lt;/strong&gt; and &lt;strong&gt;åŸºäºå¯¹è¯çš„ç¼–ç¨‹&lt;/strong&gt;, the interactive logic is completely different; the dialogue pattern offers a user-friendly experience for novice users. It&amp;rsquo;s essential to mention &lt;code&gt;NewBing&lt;/code&gt;åœ¨æ¯ä¸ªé—®é¢˜ä¹‹åï¼Œç»™å‡ºçš„è¿½é—®æç¤ºï¼Œå¾®è½¯åœ¨å°è¯•å¼•å¯¼ç”¨æˆ·ï¼Œè·å–&lt;code&gt;AI&lt;/code&gt; within the knowledge base, with more content.&lt;/p&gt;
&lt;h2 id=&#34;for-example&#34;&gt;For example
&lt;/h2&gt;&lt;p&gt;A long journey on high-speed rail; coding is a good option. With limited data, processing a few images and obtaining image resources is manageable, but downloading software to process them is a bit of a luxury given the network conditions.&lt;/p&gt;
&lt;p&gt;A batch of image assets with transparent backgrounds are needed for a mini-program navigation bar. Selected states should be colored, unselected states black and white. The assets are currently in color and require preprocessing.&lt;/p&gt;
&lt;p&gt;Write a Python code snippet that reads all images from a folder, converts them to grayscale, preserves transparency, optimizes image quality and reduces file size upon saving, and displays the comparison of file sizes before and after optimization in the console&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# å¯¼å…¥å¿…è¦çš„åº“
import os
import PIL
from PIL import Image

# å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œå°†å›¾ç‰‡è½¬æ¢æˆé»‘ç™½ï¼Œç»´æŒé€æ˜èƒŒæ™¯ï¼Œä¿å­˜å›¾ç‰‡ï¼Œå¹¶è¿”å›æ–‡ä»¶ä½“ç§¯
def convert_and_save(image_file):
    # æ‰“å¼€å›¾ç‰‡
    image = Image.open(image_file)
    # è·å–å›¾ç‰‡çš„æ¨¡å¼ï¼Œå¦‚æœæ˜¯ RGBA æ¨¡å¼ï¼Œè¯´æ˜æœ‰é€æ˜èƒŒæ™¯
    mode = image.mode
    if mode == &amp;quot;RGBA&amp;quot;:
        # åˆ›å»ºä¸€ä¸ªå’Œå›¾ç‰‡å¤§å°ä¸€è‡´çš„ç™½è‰²èƒŒæ™¯å›¾ç‰‡
        background = Image.new(&amp;quot;RGB&amp;quot;, image.size, (255, 255, 255))
        # å°†åŸå›¾ç‰‡ç²˜è´´åˆ°ç™½è‰²èƒŒæ™¯ä¸Šï¼Œå¿½ç•¥é€æ˜åƒç´ 
        background.paste(image, mask=image.split()[3])
        # å°†åˆæˆçš„å›¾ç‰‡è½¬æ¢æˆç°åº¦æ¨¡å¼
        gray_image = background.convert(&amp;quot;L&amp;quot;)
        # å°†ç°åº¦å›¾ç‰‡å†è½¬æ¢æˆ RGBA æ¨¡å¼ï¼Œä»¥ä¾¿ä¿ç•™é€æ˜èƒŒæ™¯
        final_image = gray_image.convert(&amp;quot;RGBA&amp;quot;)
    else:
        # å¦‚æœä¸æ˜¯ RGBA æ¨¡å¼ï¼Œç›´æ¥å°†å›¾ç‰‡è½¬æ¢æˆç°åº¦æ¨¡å¼
        final_image = image.convert(&amp;quot;L&amp;quot;)
    # è·å–åŸå›¾ç‰‡çš„æ–‡ä»¶åå’Œæ‰©å±•å
    file_name, file_ext = os.path.splitext(image_file)
    # å®šä¹‰æ–°å›¾ç‰‡çš„æ–‡ä»¶åï¼Œæ·»åŠ  _bw åç¼€è¡¨ç¤ºé»‘ç™½
    new_file_name = file_name + &amp;quot;_bw&amp;quot; + file_ext
    # ä¿å­˜æ–°å›¾ç‰‡ï¼Œå¹¶ä¼˜åŒ–è´¨é‡ï¼Œå‡å°‘æ–‡ä»¶ä½“ç§¯
    final_image.save(new_file_name, optimize=True)
    # è·å–åŸå›¾ç‰‡å’Œæ–°å›¾ç‰‡çš„æ–‡ä»¶ä½“ç§¯ï¼Œå¹¶è¿”å›
    old_size = os.path.getsize(image_file)
    new_size = os.path.getsize(new_file_name)
    return old_size, new_size

# å®šä¹‰ä¸€ä¸ªæ–‡ä»¶å¤¹è·¯å¾„ï¼Œå­˜æ”¾æ‰€æœ‰çš„å›¾ç‰‡
folder_path = &amp;quot;images&amp;quot;
# éå†æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰æ–‡ä»¶
for file in os.listdir(folder_path):
    # æ‹¼æ¥å®Œæ•´çš„æ–‡ä»¶è·¯å¾„
    file_path = os.path.join(folder_path, file)
    # åˆ¤æ–­æ˜¯å¦æ˜¯å›¾ç‰‡æ–‡ä»¶ï¼Œæ ¹æ®æ‰©å±•ååˆ¤æ–­ï¼Œå¯ä»¥æ ¹æ®éœ€è¦æ·»åŠ æˆ–ä¿®æ”¹æ‰©å±•å
    if file_path.endswith((&amp;quot;.png&amp;quot;, &amp;quot;.jpg&amp;quot;, &amp;quot;.jpeg&amp;quot;, &amp;quot;.gif&amp;quot;)):
        # è°ƒç”¨å‡½æ•°ï¼Œè½¬æ¢å¹¶ä¿å­˜å›¾ç‰‡ï¼Œå¹¶è·å–æ–‡ä»¶ä½“ç§¯
        old_size, new_size = convert_and_save(file_path)
        # åœ¨æ§åˆ¶å°å±•ç¤ºä¼˜åŒ–å‰åçš„æ–‡ä»¶ä½“ç§¯å¤§å°å¯¹æ¯”
        print(f&amp;quot;{file}: {old_size} bytes -&amp;gt; {new_size} bytes&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Quickly provided an answer, looks good, and works fine. Experienced programmers will find the code lacks generality and isn&amp;rsquo;t suitable for production environments.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Parameters fixed&lt;/li&gt;
&lt;li&gt;No log files; information is printed to the console&lt;/li&gt;
&lt;li&gt;Image processing tasks are well-suited for multithreaded models&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Are there any improvements that can be made to this code?&lt;/p&gt;
&lt;p&gt;This answer, frankly, far exceeded expectations. It provides suggestions for optimizations that even a typical programmer would consider.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;è¿™æ®µä»£ç å¯èƒ½è¿˜æœ‰ä¸€äº›å¯ä»¥æ”¹è¿›çš„åœ°æ–¹ï¼Œæ¯”å¦‚ï¼š

å¯ä»¥ä½¿ç”¨ argparse åº“ï¼Œè®©ç”¨æˆ·å¯ä»¥é€šè¿‡å‘½ä»¤è¡Œå‚æ•°æŒ‡å®šæ–‡ä»¶å¤¹è·¯å¾„å’Œæ‰©å±•åï¼Œè€Œä¸æ˜¯å†™æ­»åœ¨ä»£ç é‡Œã€‚
å¯ä»¥ä½¿ç”¨ logging åº“ï¼Œè®°å½•ç¨‹åºçš„è¿è¡Œæƒ…å†µï¼Œè€Œä¸æ˜¯ç›´æ¥æ‰“å°åˆ°æ§åˆ¶å°ã€‚
å¯ä»¥ä½¿ç”¨ try-except è¯­å¥ï¼Œæ•è·å¯èƒ½å‡ºç°çš„å¼‚å¸¸ï¼Œæ¯”å¦‚æ–‡ä»¶æ‰“å¼€å¤±è´¥ï¼Œå›¾ç‰‡è½¬æ¢å¤±è´¥ç­‰ï¼Œå¹¶åšç›¸åº”çš„å¤„ç†ã€‚
å¯ä»¥ä½¿ç”¨ multiprocessing åº“ï¼Œåˆ©ç”¨å¤šæ ¸å¤„ç†å™¨ï¼ŒåŠ é€Ÿå›¾ç‰‡çš„è½¬æ¢å’Œä¿å­˜è¿‡ç¨‹ã€‚
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A friendly response, considering deployment environments may vary, supporting only single-process execution is permitted&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;å½“ç„¶ï¼Œæˆ‘å¾ˆä¹æ„å¸®ä½ ä¿®æ”¹ä¸€ä¸‹ä»£ç ã€‚æˆ‘å°è¯•æ·»åŠ äº† argparse, logging å’Œ try-except è¯­å¥ï¼Œä½†æ²¡æœ‰æ·»åŠ  multiprocessing åº“ï¼Œå› ä¸ºæˆ‘ä¸ç¡®å®šä½ çš„ç”µè„‘æ˜¯å¦æ”¯æŒå¤šæ ¸å¤„ç†ã€‚ä¿®æ”¹åçš„ä»£ç å¦‚ä¸‹ï¼š
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# å¯¼å…¥å¿…è¦çš„åº“
import argparse
import logging
import multiprocessing
import os

from PIL import Image


# å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œå°†å›¾ç‰‡è½¬æ¢æˆé»‘ç™½ï¼Œç»´æŒé€æ˜èƒŒæ™¯ï¼Œä¿å­˜å›¾ç‰‡ï¼Œå¹¶è¿”å›æ–‡ä»¶ä½“ç§¯
def convert_and_save(image_file):
    # æ‰“å¼€å›¾ç‰‡
    try:
        image = Image.open(image_file)
    except Exception as e:
        logging.error(f&amp;quot;Failed to open {image_file}: {e}&amp;quot;)
        return None, None
    # è·å–å›¾ç‰‡çš„æ¨¡å¼ï¼Œå¦‚æœæ˜¯ RGBA æ¨¡å¼ï¼Œè¯´æ˜æœ‰é€æ˜èƒŒæ™¯
    mode = image.mode
    if mode == &amp;quot;RGBA&amp;quot;:
        # åˆ›å»ºä¸€ä¸ªå’Œå›¾ç‰‡å¤§å°ä¸€è‡´çš„ç™½è‰²èƒŒæ™¯å›¾ç‰‡
        background = Image.new(&amp;quot;RGB&amp;quot;, image.size, (255, 255, 255))
        # å°†åŸå›¾ç‰‡ç²˜è´´åˆ°ç™½è‰²èƒŒæ™¯ä¸Šï¼Œå¿½ç•¥é€æ˜åƒç´ 
        background.paste(image, mask=image.split()[3])
        # å°†åˆæˆçš„å›¾ç‰‡è½¬æ¢æˆç°åº¦æ¨¡å¼
        gray_image = background.convert(&amp;quot;L&amp;quot;)
        # å°†ç°åº¦å›¾ç‰‡å†è½¬æ¢æˆ RGBA æ¨¡å¼ï¼Œä»¥ä¾¿ä¿ç•™é€æ˜èƒŒæ™¯
        final_image = gray_image.convert(&amp;quot;RGBA&amp;quot;)
    else:
        # å¦‚æœä¸æ˜¯ RGBA æ¨¡å¼ï¼Œç›´æ¥å°†å›¾ç‰‡è½¬æ¢æˆç°åº¦æ¨¡å¼
        final_image = image.convert(&amp;quot;L&amp;quot;)
    # è·å–åŸå›¾ç‰‡çš„æ–‡ä»¶åå’Œæ‰©å±•å
    file_name, file_ext = os.path.splitext(image_file)
    # å®šä¹‰æ–°å›¾ç‰‡çš„æ–‡ä»¶åï¼Œæ·»åŠ  _bw åç¼€è¡¨ç¤ºé»‘ç™½
    new_file_name = file_name + &amp;quot;_bw&amp;quot; + file_ext
    # ä¿å­˜æ–°å›¾ç‰‡ï¼Œå¹¶ä¼˜åŒ–è´¨é‡ï¼Œå‡å°‘æ–‡ä»¶ä½“ç§¯
    try:
        final_image.save(new_file_name, optimize=True)
    except Exception as e:
        logging.error(f&amp;quot;Failed to save {new_file_name}: {e}&amp;quot;)
        return None, None
    # è·å–åŸå›¾ç‰‡å’Œæ–°å›¾ç‰‡çš„æ–‡ä»¶ä½“ç§¯ï¼Œå¹¶è¿”å›
    old_size = os.path.getsize(image_file)
    new_size = os.path.getsize(new_file_name)
    return file_name, old_size, new_size

# å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œè§£æå‘½ä»¤è¡Œå‚æ•°ï¼Œå¹¶è¿”å›æ–‡ä»¶å¤¹è·¯å¾„å’Œæ‰©å±•ååˆ—è¡¨
def parse_args():
    # åˆ›å»ºä¸€ä¸ªè§£æå™¨å¯¹è±¡
    parser = argparse.ArgumentParser(description=&amp;quot;Convert images to black and white and optimize quality.&amp;quot;)
    # æ·»åŠ ä¸€ä¸ªå¿…é€‰çš„ä½ç½®å‚æ•°ï¼Œè¡¨ç¤ºæ–‡ä»¶å¤¹è·¯å¾„
    parser.add_argument(&amp;quot;folder_path&amp;quot;, help=&amp;quot;The path of the folder that contains the images.&amp;quot;)
    # æ·»åŠ ä¸€ä¸ªå¯é€‰çš„å‚æ•°ï¼Œè¡¨ç¤ºæ‰©å±•ååˆ—è¡¨ï¼Œé»˜è®¤ä¸º png, jpg, jpeg å’Œ gif
    parser.add_argument(&amp;quot;-e&amp;quot;, &amp;quot;--extensions&amp;quot;, nargs=&amp;quot;+&amp;quot;, default=[&amp;quot;.png&amp;quot;, &amp;quot;.jpg&amp;quot;, &amp;quot;.jpeg&amp;quot;, &amp;quot;.gif&amp;quot;], help=&amp;quot;The extensions of the image files.&amp;quot;)
    # è§£æå‘½ä»¤è¡Œå‚æ•°ï¼Œå¹¶è¿”å›ç»“æœå¯¹è±¡
    args = parser.parse_args()
    return args.folder_path, args.extensions

# å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œæ‰“å°ä¼˜åŒ–å‰åçš„æ–‡ä»¶ä½“ç§¯å¤§å°å¯¹æ¯”
def print_result(result):
    # å¦‚æœç»“æœä¸ä¸ºç©ºï¼Œè¯´æ˜è½¬æ¢å’Œä¿å­˜æˆåŠŸ
    if result:
        # è§£åŒ…ç»“æœä¸ºæ–‡ä»¶åå’Œæ–‡ä»¶ä½“ç§¯å…ƒç»„
        if len(result) == 3:
            file, old_size, new_size = result
            # åœ¨æ§åˆ¶å°å±•ç¤ºä¼˜åŒ–å‰åçš„æ–‡ä»¶ä½“ç§¯å¤§å°å¯¹æ¯”
            logging.info(f&amp;quot;{file}: {old_size} bytes -&amp;gt; {new_size} bytes&amp;quot;)
        else:
            logging.info(f&amp;quot;{result}&amp;quot;)

# é…ç½®æ—¥å¿—è®°å½•å™¨ï¼Œå°†æ—¥å¿—è¾“å‡ºåˆ°æ§åˆ¶å°å’Œæ–‡ä»¶ä¸­ï¼Œè®¾ç½®æ—¥å¿—ç­‰çº§ä¸º INFO
logging.basicConfig(level=logging.INFO,
                    format=&amp;quot;%(asctime)s %(levelname)s %(message)s&amp;quot;,
                    handlers=[logging.StreamHandler(),
                              logging.FileHandler(&amp;quot;log.txt&amp;quot;)])

# è°ƒç”¨å‡½æ•°ï¼Œè·å–æ–‡ä»¶å¤¹è·¯å¾„å’Œæ‰©å±•ååˆ—è¡¨
folder_path, extensions = parse_args()

if __name__ == &amp;quot;__main__&amp;quot;:
    # Windows éœ€è¦è¿™ä¸ªå‡½æ•°çš„åŸå› æ˜¯ Windows ç¼ºä¹ fork() å‡½æ•°ï¼ˆè¿™ä¸å®Œå…¨æ­£ç¡®ï¼‰ã€‚
    # å› æ­¤ï¼Œåœ¨ Windows ä¸Šï¼Œfork() æ˜¯é€šè¿‡åˆ›å»ºä¸€ä¸ªæ–°çš„è¿›ç¨‹æ¥æ¨¡æ‹Ÿçš„ï¼Œè¿™ä¸ªæ–°çš„è¿›ç¨‹ä¸­è¿è¡Œçš„ä»£ç ï¼Œåœ¨ Linux ä¸Šæ˜¯åœ¨å­è¿›ç¨‹ä¸­è¿è¡Œçš„ã€‚
    # ç”±äºè¿™äº›ä»£ç è¦åœ¨æŠ€æœ¯ä¸Šæ— å…³çš„è¿›ç¨‹ä¸­è¿è¡Œï¼Œæ‰€ä»¥å®ƒå¿…é¡»åœ¨è¿è¡Œä¹‹å‰è¢«ä¼ é€’åˆ°é‚£é‡Œã€‚
    # ä¼ é€’çš„æ–¹å¼æ˜¯å…ˆå°†å®ƒä»¬åºåˆ—åŒ–ï¼Œç„¶åé€šè¿‡ç®¡é“ä»åŸå§‹è¿›ç¨‹å‘é€åˆ°æ–°çš„è¿›ç¨‹ã€‚
    # å¦å¤–ï¼Œè¿™ä¸ªæ–°çš„è¿›ç¨‹è¢«é€šçŸ¥å®ƒå¿…é¡»è¿è¡Œé€šè¿‡ç®¡é“ä¼ é€’çš„ä»£ç ï¼Œé€šè¿‡å‘å®ƒä¼ é€’ --multiprocessing-fork å‘½ä»¤è¡Œå‚æ•°ã€‚
    # å¦‚æœä½ çœ‹ä¸€ä¸‹ freeze_support() å‡½æ•°çš„å®ç°ï¼Œå®ƒçš„ä»»åŠ¡æ˜¯æ£€æŸ¥å®ƒè¿è¡Œåœ¨å“ªä¸ªè¿›ç¨‹ä¸­ï¼Œæ˜¯å¦åº”è¯¥è¿è¡Œé€šè¿‡ç®¡é“ä¼ é€’çš„ä»£ç 
    multiprocessing.freeze_support()

    # åˆ›å»ºä¸€ä¸ªè¿›ç¨‹æ± ï¼Œæ ¹æ®ç”µè„‘çš„æ ¸å¿ƒæ•°è‡ªåŠ¨åˆ†é…è¿›ç¨‹
    pool = multiprocessing.Pool()
    # åˆ›å»ºä¸€ä¸ªç©ºåˆ—è¡¨ï¼Œç”¨äºå­˜æ”¾å¼‚æ­¥ä»»åŠ¡çš„ç»“æœå¯¹è±¡
    results = []
    # éå†æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰æ–‡ä»¶
    for file in os.listdir(folder_path):
        # æ‹¼æ¥å®Œæ•´çš„æ–‡ä»¶è·¯å¾„
        file_path = os.path.join(folder_path, file)
        # åˆ¤æ–­æ˜¯å¦æ˜¯å›¾ç‰‡æ–‡ä»¶ï¼Œæ ¹æ®æ‰©å±•ååˆ¤æ–­ï¼Œå¯ä»¥æ ¹æ®éœ€è¦ä¿®æ”¹æ‰©å±•ååˆ—è¡¨
        if any(file_path.endswith(ext) for ext in extensions):
            # è°ƒç”¨å‡½æ•°ï¼Œè½¬æ¢å¹¶ä¿å­˜å›¾ç‰‡ï¼Œå¹¶è·å–æ–‡ä»¶ä½“ç§¯ï¼Œä½¿ç”¨å¼‚æ­¥æ–¹å¼ï¼Œä¸é˜»å¡ä¸»è¿›ç¨‹
            result = pool.apply_async(convert_and_save, args=(file_path,), callback=print_result)
            # å°†ç»“æœå¯¹è±¡æ·»åŠ åˆ°åˆ—è¡¨ä¸­
            results.append((file, result))
    # å…³é—­è¿›ç¨‹æ± ï¼Œä¸å†æ¥å—æ–°çš„ä»»åŠ¡
    pool.close()
    # ç­‰å¾…æ‰€æœ‰çš„ä»»åŠ¡å®Œæˆ
    pool.join()
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;afterword&#34;&gt;Afterword
&lt;/h2&gt;&lt;p&gt;Due to local development issues, the code was fixed by investigating reported errors&lt;/p&gt;
&lt;p&gt;Just as we learned techniques for using search engines, we also need to learn communication skills â€“ providing reasonable and sufficient constraints to efficiently obtain the answers we need&lt;/p&gt;
&lt;p&gt;Note:&lt;/p&gt;</description>
        </item>
        <item>
        <title>AI-assisted programming, an evolution of productivity</title>
        <link>https://blog.ttf248.life/en/p/ai-assisted-programming-productivity-evolution/</link>
        <pubDate>Tue, 28 Feb 2023 17:05:17 +0800</pubDate>
        
        <guid>https://blog.ttf248.life/en/p/ai-assisted-programming-productivity-evolution/</guid>
        <description>&lt;p&gt;&lt;strong&gt;INLINE_CODE_0&lt;/strong&gt; å‘å¸ƒä¹Ÿä¸åˆ°ä¸¤å¹´æ—¶é—´ï¼Œ&lt;strong&gt;INLINE_CODE_1&lt;/strong&gt; was released, and I don&amp;rsquo;t fully understand the underlying principles, but Iâ€™ve been using them for a while. The two tools offer completely different levels of assistance, but both significantly improve productivity.&lt;/p&gt;
&lt;p&gt;For overly complex matters, the effect can only be about nine-tenths successful&lt;/p&gt;
&lt;h2 id=&#34;github-copilot&#34;&gt;github copilot
&lt;/h2&gt;&lt;p&gt;When it was released, the website description &lt;strong&gt;INLINE_CODE_0&lt;/strong&gt; didn&amp;rsquo;t seem very smart, and after trying it out, it wasn&amp;rsquo;t very usable, so I gave up&lt;/p&gt;
&lt;p&gt;Before 2022, out of boredom, I switched to the new version for a try. The results are already pretty good, though the speed is slow in China, likely due to network issues. Code generated based on comments like &lt;code&gt;Python&lt;/code&gt; is quite decent and much faster than researching and writing it myself.&lt;/p&gt;
&lt;h2 id=&#34;chatgpt&#34;&gt;ChatGPT
&lt;/h2&gt;&lt;p&gt;Still designing products from a programmer&amp;rsquo;s, or development, perspective. You need to understand how to code and gradually guide AI-assisted programming through comments.&lt;/p&gt;
&lt;p&gt;Let non-experts quickly get started. The conversational, interactive design allows you to ask questions, describe your needs, and continue asking if anything is unclear.&lt;/p&gt;
&lt;p&gt;For example, you understand the concept of tokenization but don&amp;rsquo;t know how to code it, and want to calculate string similarity&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;Python åˆ†è¯è®¡ç®— ç¶­å¤©é‹é€š LOGORY å’Œ åˆè‚¥ç¶­å¤©é‹é€šä¿¡æ¯ç§‘æŠ€è‚¡ä»½æœ‰é™å…¬å¸ ä¸¤ä¸ªå­—ç¬¦ä¸²çš„ç›¸ä¼¼åº¦ï¼›
æ¯”è¾ƒå‰ï¼Œç»Ÿä¸€å¤„ç†å­—ç¬¦ä¸²ï¼Œè‹±æ–‡è½¬æ¢ä¸ºå°å†™ï¼›
ç¬¬äºŒä¸ªå­—ç¬¦ä¸­å¦‚æœèƒ½æ‰¾åˆ°ç¬¬ä¸€ä¸ªå­—ç¬¦çš„é¦–å­—ç¬¦ï¼Œå°†ç¬¬äºŒä¸ªå­—ç¬¦ä¸²æˆªä½ï¼Œæˆªä½çš„é•¿åº¦ä¸ºç¬¬ä¸€ä¸ªå­—ç¬¦ä¸²çš„é•¿åº¦
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Hello, this is Bing. ğŸ˜Š&lt;/p&gt;
&lt;p&gt;According to online information, Python tokenization can be implemented using the jieba library. Jieba supports three modes: precise mode, full mode, and search engine mode. You can choose the appropriate mode based on your needs.&lt;/p&gt;
&lt;p&gt;To compare the similarity of two strings, you first need to tokenize them and then calculate indicators such as word vectors or edit distance. You can use libraries like NumPy or SciPy for mathematical operations.&lt;/p&gt;
&lt;p&gt;To uniformly handle strings, converting them to lowercase can be achieved using Python&amp;rsquo;s built-in &lt;code&gt;lower()&lt;/code&gt; function&lt;/p&gt;
&lt;p&gt;To extract the second string, you can use Python&amp;rsquo;s slicing operator []&lt;/p&gt;
&lt;p&gt;Here is a possible code example:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import jieba
import numpy as np
from scipy.spatial.distance import cosine

# å®šä¹‰ä¸¤ä¸ªå­—ç¬¦ä¸²
str1 = &amp;quot;ç¶­å¤©é‹é€š LOGORY&amp;quot;
str2 = &amp;quot;åˆè‚¥ç¶­å¤©é‹é€šä¿¡æ¯ç§‘æŠ€è‚¡ä»½æœ‰é™å…¬å¸&amp;quot;

# ç»Ÿä¸€å¤„ç†å­—ç¬¦ä¸²ï¼Œè‹±æ–‡è½¬æ¢ä¸ºå°å†™
str1 = str1.lower()
str2 = str2.lower()

# æ‰¾åˆ°ç¬¬äºŒä¸ªå­—ç¬¦ä¸²ä¸­ç¬¬ä¸€ä¸ªå­—ç¬¦çš„ä½ç½®
index = str2.find(str1[0])

# å¦‚æœæ‰¾åˆ°äº†ï¼Œæˆªå–ç¬¬äºŒä¸ªå­—ç¬¦ä¸²
if index != -1:
    str2 = str2[index:index+len(str1)]

# ä½¿ç”¨ jieba åº“è¿›è¡Œåˆ†è¯
words1 = list(jieba.cut(str1))
words2 = list(jieba.cut(str2))

# è®¡ç®—ä¸¤ä¸ªå­—ç¬¦ä¸²çš„è¯å‘é‡ï¼ˆä»¥è¯é¢‘ä½œä¸ºæƒé‡ï¼‰
vector1 = np.zeros(len(words1))
vector2 = np.zeros(len(words1))

for i, word in enumerate(words1):
    vector1[i] += 1 # ç»Ÿè®¡ç¬¬ä¸€ä¸ªå­—ç¬¦ä¸²ä¸­æ¯ä¸ªè¯å‡ºç°çš„æ¬¡æ•°
    if word in words2:
        vector2[i] += 1 # ç»Ÿè®¡ç¬¬äºŒä¸ªå­—ç¬¦ä¸²ä¸­æ¯ä¸ªè¯å‡ºç°çš„æ¬¡æ•°

# è®¡ç®—ä¸¤ä¸ªå­—ç¬¦ä¸²çš„ç›¸ä¼¼åº¦ï¼ˆä»¥ä½™å¼¦è·ç¦»ä½œä¸ºæŒ‡æ ‡ï¼‰
similarity = 1 - cosine(vector1, vector2)

print(&amp;quot;ä¸¤ä¸ªå­—ç¬¦ä¸²çš„ç›¸ä¼¼åº¦æ˜¯ï¼š&amp;quot;, similarity)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Please provide the Chinese text you want me to translate. I am ready when you are!&lt;/p&gt;
&lt;h3 id=&#34;chestnut&#34;&gt;Chestnut
&lt;/h3&gt;&lt;p&gt;Find all records from the product code table where the product codes are within the range of 80000-89999&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;ä½ å¥½ï¼Œè¿™æ˜¯ Bingã€‚ğŸ˜Š

æ ¹æ®æœç´¢ç»“æœ12ï¼Œä½ å¯ä»¥ä½¿ç”¨ SQL BETWEEN è¿ç®—ç¬¦æ¥é€‰æ‹©ç»™å®šåŒºé—´å†…çš„å€¼ã€‚BETWEEN è¿ç®—ç¬¦åŒ…å«äº†å¼€å§‹å’Œç»“æŸçš„å€¼ã€‚

ä½ éœ€è¦å…ˆæŒ‡å®šè¦æŸ¥è¯¢çš„è¡¨åå’Œåˆ—åï¼Œç„¶ååœ¨ WHERE å­å¥ä¸­ä½¿ç”¨ BETWEEN è¿ç®—ç¬¦å’ŒåŒºé—´å€¼ã€‚

ä¾‹å¦‚ï¼Œå¦‚æœä½ è¦æŸ¥è¯¢äº§å“ä»£ç è¡¨ä¸­ï¼Œäº§å“ä»£ç åœ¨ 80000 åˆ° 89999 ä¹‹é—´çš„è®°å½•ï¼Œä½ å¯ä»¥ä½¿ç”¨ä»¥ä¸‹è¯­å¥ï¼š

SELECT * FROM product_code
WHERE code BETWEEN 80000 AND 89999;
è¿™é‡Œï¼Œ* è¡¨ç¤ºé€‰æ‹©æ‰€æœ‰åˆ—3ã€‚å¦‚æœä½ åªæƒ³é€‰æ‹©ç‰¹å®šçš„åˆ—ï¼Œæ¯”å¦‚äº§å“åç§°å’Œä»·æ ¼ï¼Œä½ å¯ä»¥ç”¨é€—å·åˆ†éš”å®ƒä»¬ï¼Œå¦‚ä¸‹ï¼š

SELECT name, price FROM product_code
WHERE code BETWEEN 80000 AND 89999;
å¸Œæœ›è¿™èƒ½å¸®åŠ©åˆ°ä½ ã€‚
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Keep asking, deny it: The product code is a string, your code is wrong&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;æŠ±æ­‰ï¼Œæˆ‘æ²¡æœ‰æ³¨æ„åˆ°äº§å“ä»£ç æ˜¯å­—ç¬¦ä¸²ã€‚å¦‚æœæ˜¯è¿™æ ·çš„è¯ï¼Œä½ éœ€è¦åœ¨åŒºé—´å€¼ä¸¤è¾¹åŠ ä¸Šå•å¼•å·ï¼Œå¦‚ä¸‹ï¼š

SELECT * FROM product_code
WHERE code BETWEEN &#39;80000&#39; AND &#39;89999&#39;;
è¯·åŸè°…æˆ‘çš„é”™è¯¯ã€‚ğŸ™
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Congratulations, it will be wrong&lt;/p&gt;</description>
        </item>
        
    </channel>
</rss>
