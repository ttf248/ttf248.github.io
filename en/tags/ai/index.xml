<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Ai on Uncle Xiang&#39;s Notebook</title>
        <link>https://ttf248.life/en/tags/ai/</link>
        <description>Recent content in Ai on Uncle Xiang&#39;s Notebook</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en</language>
        <lastBuildDate>Mon, 26 May 2025 00:49:57 +0800</lastBuildDate><atom:link href="https://ttf248.life/en/tags/ai/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>Claude4 released, attempting development: Hugo tag, hyperlink translation assistant</title>
        <link>https://ttf248.life/en/p/claude-4-release-and-experimentation-hugo-tags-hyperlink-translation-assistant/</link>
        <pubDate>Sat, 24 May 2025 03:05:31 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/claude-4-release-and-experimentation-hugo-tags-hyperlink-translation-assistant/</guid>
        <description>&lt;p&gt;This site is built with Hugo, but I&amp;rsquo;ve been using Chinese titles, which results in unfriendly URLs. To put it simply, when shared, they don&amp;rsquo;t look good because Chinese characters are encoded as things like %E4%BD%A0%E5%A5%BD. While slugs can fix this, manually setting them each time is too much trouble.&lt;/p&gt;
&lt;p&gt;So, today I tried using Claude4 to develop a translation assistant that automatically converts Chinese titles into English slugs and adds hyperlinks within articles. This would eliminate the need for manual setup.&lt;/p&gt;
&lt;p&gt;Claude 4 is excellent; its ability to understand context and handle complex tasks has been significantly improved&lt;/p&gt;
&lt;p&gt;Project Address&lt;/p&gt;
&lt;p&gt;Domestic project address: [https://cnb.cool/ttf248/hugo-content-suite]
Foreign project address: [https://github.com/ttf248/hugo-content-suite]&lt;/p&gt;
&lt;p&gt;Code Implementation&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s the translation: First, let me outline the approach: We need to scan all articles, extract tag information and article titles, then call a local large language model (like gemma-3-12b-it) for translation&lt;/p&gt;
&lt;p&gt;In practical development, &lt;code&gt;Claude4&lt;/code&gt; demonstrates several significant improvements compared to previous generations of large language models. Due to extensive functional requirements, &lt;code&gt;Claude4&lt;/code&gt; automatically designs interactive menus and comprehensively considers various use cases. For example, in tag processing, &lt;code&gt;Claude4&lt;/code&gt; not only supports tag statistics and analysis but also includes classification statistics and can even detect &lt;strong&gt;untagged articles&lt;/strong&gt;. Furthermore, it provides &lt;strong&gt;preview&lt;/strong&gt; and tag page generation functionalities.&lt;/p&gt;
&lt;p&gt;Whether it&amp;rsquo;s integrating with local large language models, adding translation caching, or conducting extensive code refactoring, &lt;code&gt;Claude4&lt;/code&gt; completed everything in one go with virtually no issues. Despite the project’s modest size and numerous small features, large language models often forget previous context during development. This time, however, &lt;code&gt;Claude4&lt;/code&gt; performed exceptionally well, &lt;strong&gt;showing almost no contextual forgetting.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In short, intelligence has improved, and we&amp;rsquo;re preparing to switch to &lt;code&gt;Claude4&lt;/code&gt; for more development work as our primary coding model&lt;/p&gt;
&lt;p&gt;Translation Cache&lt;/p&gt;
&lt;p&gt;This approach, aside from reducing large model calls, offers good efficiency when running a 12b model locally without causing delays. However, repeatedly calling the large model can still be slow. Secondly, to maintain article links, full updates sometimes produce inconsistent results due to long titles, leading to link changes – which is quite awkward.&lt;/p&gt;
&lt;p&gt;Feature Optimization&lt;/p&gt;
&lt;p&gt;The entire project was given to Claude4 for analysis, resulting in these optimization suggestions:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;配置外置化 - 提高可维护性和灵活性
结构化日志 - 便于问题排查和监控
性能监控 - 了解系统运行状况
用户体验 - 进度条等视觉反馈
错误处理 - 更完善的异常处理机制
代码组织 - 更清晰的模块划分
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The code review was completed and everything looks good. For example, the configuration file now automatically generates a default if one doesn&amp;rsquo;t exist when reading it, converting previous configurations to defaults – preventing user errors.&lt;/p&gt;
&lt;h3 id=&#34;heading&#34;&gt;
&lt;/h3&gt;&lt;p&gt;Requirements: While translating the main text, dynamically calculate the current translation efficiency, estimate the remaining time, and output relevant information to the console. This includes tracking the number of characters obtained from the article, the number of characters translated per line, the elapsed time, a fitted calculation for the translation time per 100 characters, and an estimated remaining translation time for the entire article.&lt;/p&gt;
&lt;p&gt;The code is done, but I&amp;rsquo;m not very satisfied with the results, so I asked AI to provide new design suggestions&lt;/p&gt;
&lt;p&gt;Provides multiple efficiency calculation methods: real-time efficiency, average efficiency, sliding window efficiency
Improved display methods: progress bar, segmented statistics, dynamic refresh
Add more useful metrics: API call count, success rate, etc&lt;/p&gt;
&lt;p&gt;After completing the code, I discovered a new surprise – real-time statistics on translation efficiency are displayed without mindless scrolling&lt;/p&gt;
&lt;p&gt;Translating body text to English..
Detected 53 lines needing translation
Translating 354 characters..
Completed (3.1s) | API call #1
Completed (1.5s) | API call #2
Completed (0.9s) | API call #3
Completed (0.2s) | API call #4
Completed
Completed
Completed (0.2s) | API call #7
Progress: 13.2% (7/53) | Characters 12.9% (925/7163) 114.6 characters/second
Efficiency: Real-time 76.4 | Average 117.9 | Recent 109.0 | Stage 113.6 characters/second 📊
Success Rate: 100.0% (7/7) | Remaining: 46 lines, 7 seconds] 9.4% Translation of 110 characters…
Estimated remaining: 55s | Estimated completion: 10:19 8s | 11.3% | Translating 114 characters…
Processing speed: 3211.3 lines/minute | Total time: 8s] 13.2% Translated 16 characters…
Stage 1/6 [███░░░░░░░░░░░░░░░░░░░░░░] 15.1% Translating 166 characters…&lt;/p&gt;
&lt;p&gt;I haven&amp;rsquo;t controlled many programs before, but I&amp;rsquo;m curious about how it&amp;rsquo;s implemented, so I looked at the code&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-golang&#34;&gt;// 清屏并重新显示 (动态刷新效果)
if translationCount &amp;gt; 1 {
   fmt.Print(&amp;quot;\033[6A\033[K&amp;quot;) // 上移6行并清除
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Performance Statistics Menu&lt;/p&gt;
&lt;p&gt;The new &lt;strong&gt;performance statistics menu&lt;/strong&gt;, which I designed myself, might not even be this comprehensive&lt;/p&gt;
&lt;p&gt;Performance Statistics:
Translation count: 360
Cache hit rate: 1.4% (5/365)
Average translation time: 315.927234ms
File Operations: 73
Incorrect attempts: 0&lt;/p&gt;
&lt;p&gt;Progress bar display&lt;/p&gt;
&lt;p&gt;New &lt;strong&gt;progress bar display&lt;/strong&gt;, showing detailed progress, elapsed time, and estimated remaining time&lt;/p&gt;
&lt;p&gt;Please select function (0-13): 10
Collecting translation target..
Cached file loaded, containing 0 translation records&lt;/p&gt;
&lt;p&gt;Translation cache statistics:
Total labels: 229
Total articles: 131
Cached: 0 items
360 items need translating&lt;/p&gt;
&lt;p&gt;Confirm generating full translation cache? (y/n): y
Generating full translation cache..
Cached file loaded, containing 0 translation records
Checking cached translations..
Need to translate 360 new tags
5/360 (1.4%) - Time taken: 3s - Estimated remaining: 3m8s💾 Saved cache file, containing 5 translation records
10/360 (2.8%) - Time taken: 6s - Estimated remaining time: 3m28s💾 Saved cache file, containing 10 translation records
15/360 (4.2%) - Time taken: 9s - Estimated remaining: 3m30s💾 Saved cache file, containing 15 translation records
20/360 (5.6%) - Time taken: 13s - Estimated remaining time: 3m36s💾 Saved cache file, containing 20 translation records
25/360 (6.9%) - Time taken: 16s - Estimated remaining time: 3m33s💾 Saved cache file, containing 25 translation records
30/360 (8.3%) - Time elapsed: 19s - Estimated remaining: 3m30s💾 Saved cache file, containing 30 translation records
Saved cache file, containing 35 translation records&lt;/p&gt;
&lt;hr&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;=== Hugo 博客管理工具 ===

🚀 核心功能
  1. 一键处理全部 (完整博客处理流程)

📝 内容管理
  2. 生成标签页面
  3. 生成文章Slug
  4. 翻译文章为多语言版本

💾 缓存管理
  5. 查看缓存状态
  6. 生成全量翻译缓存
  7. 清空翻译缓存

  0. 退出程序
&lt;/code&gt;&lt;/pre&gt;</description>
        </item>
        <item>
        <title>Too much AI, a bit of a side effect</title>
        <link>https://ttf248.life/en/p/ai-overuse-side-effects/</link>
        <pubDate>Wed, 14 May 2025 19:39:50 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/ai-overuse-side-effects/</guid>
        <description>&lt;p&gt;Since the new &lt;code&gt;AI Inspiration Hub&lt;/code&gt; was established, a lot of trivial things have been tried to be recorded and published using AI. As a result, there&amp;rsquo;s less time for quiet reflection and contemplation. It would be good to slightly control the output of this column in the future and integrate it into a monthly publication format, releasing one article per month.&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s like a kind of aftereffect, or perhaps a side effect – efficiency is improved, but the depth and breadth of thinking have declined&lt;/p&gt;
&lt;h2 id=&#34;efficiency-improvement-undeniable&#34;&gt;Efficiency improvement: undeniable
&lt;/h2&gt;&lt;p&gt;The &amp;ldquo;Seven Seconds of Fish&amp;rdquo; column hasn&amp;rsquo;t been well-maintained previously. Due to laziness, I didn’t search for information or compile records on some trending events. Now that various AI tools are available, outlining is sufficient; the AI can automatically connect to the internet, search for related event records, and generate articles as needed. A simple formatting adjustment, and it&amp;rsquo;s ready for publication.&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s like a godsend for lazy people, it significantly improves efficiency, and you could even say it achieves results with half the effort&lt;/p&gt;
&lt;p&gt;Throwing aside the need to write articles, when writing code, the efficiency boost is real. Many coding tasks used to require detailed reading of API interface documentation, but now you can skip them directly – and this skipping is quite necessary. Familiarity with APIs is a &lt;code&gt;physical labor&lt;/code&gt;, not a &lt;code&gt;mental labor&lt;/code&gt;; AI handling this part is just right.&lt;/p&gt;
&lt;h2 id=&#34;garbage-content&#34;&gt;Garbage content
&lt;/h2&gt;&lt;p&gt;Many of the submissions are low in quality; they aren&amp;rsquo;t entirely without merit, but they lack a sense of life and warmth. It’s a style I used to dislike – dry and lifeless.&lt;/p&gt;
&lt;p&gt;To put it another way, AI-generated content does feel a bit like something produced on an assembly line – lacking soul&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Internet junk of the new era&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;forgetfulness&#34;&gt;Forgetfulness
&lt;/h2&gt;&lt;p&gt;This type of article is all AI-generated, and I don’t know the readers&amp;rsquo; situation, but over time, my own impressions will become blurred, or even forgotten&lt;/p&gt;
&lt;p&gt;Similar issues arise when writing code as well. Without checking the commit history, you have no idea what you were thinking at the time and why you wrote it that way. This is especially true for code generated after repeated communication with yourself and AI – the final version can be very different from the initial idea, even completely different.&lt;/p&gt;
&lt;h2 id=&#34;search&#34;&gt;Search
&lt;/h2&gt;&lt;p&gt;I&amp;rsquo;ve noticed a significant decrease in how often I open Google and Baidu recently. Many questions are now searched for using AI, and the interaction and search results are much better than traditional search engines.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s pay our respects to &lt;code&gt;bing ai&lt;/code&gt;, a tool from a major company that was among the first released with internet search capabilities, whether it’s still alive or not&lt;/p&gt;
&lt;p&gt;People are using Google less, and visiting &lt;code&gt;stackoverflow&lt;/code&gt; less frequently too. Many questions can now be directly asked to AI, so this website is gradually being phased out by the times.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion
&lt;/h2&gt;&lt;p&gt;The blog I’m still maintaining didn&amp;rsquo;t have much traffic to begin with, and now even less so; it’s more of a place for keeping records, writing for myself&lt;/p&gt;</description>
        </item>
        <item>
        <title>Design and develop a customizable stock selection module without coding</title>
        <link>https://ttf248.life/en/p/no-code-design-develop-custom-stock-module/</link>
        <pubDate>Thu, 27 Feb 2025 23:20:39 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/no-code-design-develop-custom-stock-module/</guid>
        <description>&lt;p&gt;Last month we tested Cursor, but due to limitations on the free tier, we didn&amp;rsquo;t do any complex feature development, just some simple testing. At that time, we also discovered that ByteDance released a similar product, and both used the same underlying large language model – Claude-3.5.&lt;/p&gt;
&lt;p&gt;Byte&amp;rsquo;s product is called Trae, first launched for Mac, and finally released a Windows version this February. Products from big companies are just good – you can use them for free, without paying, with unlimited access to Claude-3.5. This model has a very nice effect.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Ultimately, I got stuck on the development of the K-line chart. Due to my basic lack of knowledge in React, I had to give up. To continue developing it, I need to supplement some fundamental front-end knowledge and break down the tasks more finely, rather than giving a large task: develop the K-line chart.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;issues-found&#34;&gt;Issues found
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;Due to insufficient training data for Vue3 + Element-Plus due to the use of foreign AI models, we chose React as the front-end framework&lt;/li&gt;
&lt;li&gt;There may be occasional grammatical errors that need manual correction&lt;/li&gt;
&lt;li&gt;Solutions to some complex problems require human guidance&lt;/li&gt;
&lt;li&gt;Code structure optimization requires human guidance&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The most time-consuming part was packaging the frontend code into a container. As the author has no foundation, I had no concept of &lt;code&gt;.env.production&lt;/code&gt; or &lt;code&gt;tsconfig.json&lt;/code&gt;. I only managed to understand the corresponding logic with help from Bean. There&amp;rsquo;s a big difference in how the frontend development (dev) mode and build mode check the code. The container scripts for the backend database and services took about five minutes altogether.&lt;/p&gt;
&lt;p&gt;Currently, AI primarily improves development efficiency; having a foundation is best, as AI won&amp;rsquo;t solve all your problems&lt;/p&gt;
&lt;h2 id=&#34;warehouse-address&#34;&gt;Warehouse address
&lt;/h2&gt;&lt;p&gt;As the title says, this time we&amp;rsquo;re avoiding writing code and directly engaging with AI to design and develop a customizable stock selection module. Let’s see what kind of results we can achieve in the end.&lt;/p&gt;
&lt;p&gt;Repository address: [https://github.com/ttf248/trae-demo]&lt;/p&gt;
&lt;p&gt;Detailed usage instructions can be found in the README.md file in the repository&lt;/p&gt;
&lt;p&gt;The warehouse contains many submission records, mostly conversations between me and Trae, as well as my tests of some of Trae&amp;rsquo;s functions, with notes indicating whether manual intervention was required to achieve the corresponding functionality&lt;/p&gt;
&lt;h2 id=&#34;prompt&#34;&gt;Prompt
&lt;/h2&gt;&lt;p&gt;The project was created from scratch, here&amp;rsquo;s the prompt for the project:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;基于项目原型图，开发功能：自选股，需要支持合约的新增、删除、修改、查询。自选股界面需要展示基础的行情数据。支持多个不同的市场切换。

前端：react
后端：golang gin gorm
数据库：PostgreSQL

服务端需要支持跨域请求，同时需要考虑数据的校验和错误处理，如果后端服务不可用，前端需要告警提示。

后端需要展示请求和应答的日志；前端也打印通讯的日志，方便排查问题。
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;ui-and-interaction-optimization&#34;&gt;UI and interaction optimization
&lt;/h2&gt;&lt;p&gt;The design of the front-end interface is completely dependent on Grok. We first created a prototype in Trae, but it lacked aesthetic appeal. Due to the model&amp;rsquo;s strong coding capabilities but relatively weak other abilities, we need to use Grok to optimize the front-end UI.&lt;/p&gt;
&lt;p&gt;By taking a screenshot of the current interface, uploading it to Grok, and having it help us optimize the UI, we can potentially receive many optimization suggestions at once. We will manually evaluate them and then copy them into Trae for execution, observing the effects of the optimization.&lt;/p&gt;
&lt;h3 id=&#34;technology-stack&#34;&gt;Technology stack
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Frontend: React + TypeScript&lt;/li&gt;
&lt;li&gt;Backend: Golang + Gin + GORM&lt;/li&gt;
&lt;li&gt;Database: PostgreSQL 17&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;system-architecture&#34;&gt;System architecture
&lt;/h2&gt;&lt;h2 id=&#34;backend-architecture&#34;&gt;Backend architecture
&lt;/h2&gt;&lt;p&gt;The backend is implemented using the Go Gin framework for a RESTful API, with main modules including:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Database module&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Use GORM as an ORM framework&lt;/li&gt;
&lt;li&gt;Support configuring database connections through environment variables&lt;/li&gt;
&lt;li&gt;Automatically migrate database tables&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Routing module&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;RESTful API Design&lt;/li&gt;
&lt;li&gt;Unified error handling mechanism&lt;/li&gt;
&lt;li&gt;Built-in request logging&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Cross-domain handling&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Support cross-domain in local development environments&lt;/li&gt;
&lt;li&gt;Configurable CORS policy&lt;/li&gt;
&lt;li&gt;Support cross-domain cookies&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;frontend-architecture&#34;&gt;Frontend Architecture
&lt;/h2&gt;&lt;p&gt;Built with React + TypeScript, it achieves:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Stock list display&lt;/li&gt;
&lt;li&gt;Self-selected stock management&lt;/li&gt;
&lt;li&gt;Market Data Display&lt;/li&gt;
&lt;li&gt;Error prompt mechanism&lt;/li&gt;
&lt;/ul&gt;</description>
        </item>
        <item>
        <title>Two years of AI development: somewhat similar to the state before Docker was released</title>
        <link>https://ttf248.life/en/p/ai-development-two-years-docker-pre-release/</link>
        <pubDate>Thu, 20 Feb 2025 18:16:37 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/ai-development-two-years-docker-pre-release/</guid>
        <description>&lt;p&gt;Artificial intelligence (AI) has undoubtedly been one of the most discussed topics in the technology field in recent years, especially with the rapid advancements in AI technology over the past two years. From deep learning and natural language processing to computer vision and automated decision-making systems, applications of AI are constantly emerging. However, despite continuous technological breakthroughs, AI still faces a bottleneck similar to that before Docker&amp;rsquo;s release – a lack of a killer application to truly ignite the market.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The development of AI over the past two years is somewhat similar to the state before Docker was released – it lacks a killer application. Based on existing technologies, we need to create a perfect practical scenario. Docker didn&amp;rsquo;t rely heavily on new technologies, but the entire solution was very reasonable and changed the workflows for operations and development.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;the-current-state-of-ai-development-technology-is-mature-but-application-still-needs-breakthroughs&#34;&gt;The current state of AI development: technology is mature, but application still needs breakthroughs
&lt;/h2&gt;&lt;p&gt;From a technical perspective, AI has made considerable progress in the past two years. Whether it&amp;rsquo;s OpenAI&amp;rsquo;s GPT series models or Google&amp;rsquo;s BERT and DeepMind&amp;rsquo;s Alpha series, AI&amp;rsquo;s processing capabilities have far exceeded previous expectations. Especially in the field of natural language processing, models like GPT-4 not only possess powerful generation capabilities but also demonstrate astonishing performance in understanding and reasoning.&lt;/p&gt;
&lt;p&gt;However, despite the rapid advancement of technology, the practical application of AI faces certain challenges. Similar to the state before Docker was released, although AI has great potential, a truly widespread and industry-transforming killer application hasn&amp;rsquo;t yet emerged. People are talking about the prospects of AI, but may not be able to find an application scenario that can directly bring revolutionary change. Many AI applications are still in the initial trial stage, and most require further integration and optimization.&lt;/p&gt;
&lt;h2 id=&#34;the-similarity-between-docker-and-ai-technology-isnt-necessarily-innovation-solutions-are-key&#34;&gt;The similarity between Docker and AI: Technology isn&amp;rsquo;t necessarily innovation, solutions are key
&lt;/h2&gt;&lt;p&gt;Looking back at the history before Docker&amp;rsquo;s release, we can easily see that the technological environment and the current state of AI development share many similarities. Before Docker was released, container technology wasn&amp;rsquo;t a new concept; early LXC (Linux Containers) and virtualization technologies already possessed basic containerization capabilities. However, Docker cleverly integrated and optimized existing technologies to propose a simpler, more intuitive, and efficient solution. This approach didn’t introduce any groundbreaking technologies but solved many pain points in operations and development processes, greatly simplifying the deployment, scaling, and management of software.&lt;/p&gt;
&lt;p&gt;Similarly, the AI field faces similar circumstances. While current AI technology is no longer a &amp;ldquo;novelty,&amp;rdquo; achieving truly large-scale applications still requires a perfect landing scenario – like Docker – to integrate and optimize existing technologies into a reasonable application plan. The killer application of AI may not depend on entirely new technological breakthroughs, but rather on how to solve real business pain points and needs by integrating existing technologies.&lt;/p&gt;
&lt;h2 id=&#34;how-to-find-ais-docker-moment&#34;&gt;How to find AI&amp;rsquo;s &amp;ldquo;Docker moment&amp;rdquo;?
&lt;/h2&gt;&lt;p&gt;To truly achieve widespread application of AI technology, efforts need to be made in several areas:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;In-depth exploration of real-world scenarios
Currently, many AI applications are still experimental in nature and lack large-scale practical implementation. For example, while AI customer service and intelligent recommendation fields have widespread applications, their functions still have many limitations and haven&amp;rsquo;t yet overcome industry bottlenecks. Real breakthroughs may come from industries long troubled by traditional methods, such as healthcare, manufacturing, and logistics, where AI can help businesses improve efficiency and reduce costs through more efficient data processing and predictive analysis in these complex scenarios.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Productization and Usability
Just as Docker improves operational efficiency by simplifying the containerization process, the usability of AI products is equally crucial. The popularization of AI isn&amp;rsquo;t just about the spread of technology; it’s about the popularization of its productization. Integrating AI into daily workflows and allowing users to easily use these tools without needing a deep understanding of the underlying technology is an important step in implementing AI.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Ecological Construction and Standardization
The widespread adoption of any new technology is inseparable from ecosystem development. Docker&amp;rsquo;s rapid rise is precisely due to its openness and compatibility, allowing developers to easily connect with various cloud platforms, tools, and services. Similarly, the future of AI depends on building an ecosystem. The standardization of AI, model sharing, data openness, and technical integrability will all influence whether AI can form widespread industry applications.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;conclusion-the-future-of-ai-is-full-of-possibilities-but-a-more-complete-landing-plan-is-still-needed&#34;&gt;Conclusion: The future of AI is full of possibilities, but a more complete landing plan is still needed
&lt;/h2&gt;&lt;p&gt;Although AI technology has made significant progress in the past two years, it is still in a stage of “lacking a killer application.” Similar to containerization technology before Docker’s release, AI needs a reasonable application scenario that deeply integrates existing technologies with business needs in order to truly achieve large-scale application and popularization. While technological innovation is important, solutions that simplify processes and improve efficiency are better at driving the adoption and development of technology.&lt;/p&gt;
&lt;p&gt;In the future, AI may be like Docker—not through revolutionary technological breakthroughs, but by integrating existing technologies to create a perfect application scenario, ultimately changing the way we work and live&lt;/p&gt;</description>
        </item>
        <item>
        <title>Deploy DeepSeek-R1 locally</title>
        <link>https://ttf248.life/en/p/ollama-local-deployment-deepseek-r1/</link>
        <pubDate>Fri, 07 Feb 2025 22:41:02 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/ollama-local-deployment-deepseek-r1/</guid>
        <description>&lt;p&gt;Ollama is an open-source AI tool designed to enable users to run and deploy large language models (LLMs) locally. Its goal is to provide a convenient and efficient way for developers to use models like GPT on their local machines without relying on cloud services. Ollama supports various models and focuses on optimizing performance, ensuring that even resource-constrained devices can run these models smoothly.&lt;/p&gt;
&lt;p&gt;With Ollama, users can use text-based AI applications and interact with locally deployed models without worrying about data privacy or high API usage fees. You can call different models through the command-line interface (CLI) to perform tasks such as natural language processing and question answering.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Ollama is suitable for trying out different models. The Windows version, after testing, doesn&amp;rsquo;t fully utilize the hardware&amp;rsquo;s performance; this may be due to the Windows version itself. The Linux version might be better. When deploying a 32b parameter model, with low memory and GPU load, the response speed is very slow.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;hardware-overview&#34;&gt;Hardware Overview
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;Operating system: win11&lt;/li&gt;
&lt;li&gt;CPU：i7-10700K&lt;/li&gt;
&lt;li&gt;Memory: 40GB&lt;/li&gt;
&lt;li&gt;Graphics card: RTX 3060 12GB&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;environmental-preparation&#34;&gt;Environmental preparation
&lt;/h2&gt;&lt;p&gt;Add a new system environment variable for convenient use later&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;set OLLAMA_MODELS=E:\ollama&lt;/code&gt;&lt;/strong&gt;&lt;br&gt;
This variable specifies the storage path for the Ollama model. &lt;code&gt;E:\ollama&lt;/code&gt; is a folder path indicating that all local model files are stored in this directory. Ollama will load and use your downloaded or deployed language models based on this path. You can store the model files in other locations, just change this path.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;set OLLAMA_HOST=127.0.0.1:8000&lt;/code&gt;&lt;/strong&gt;&lt;br&gt;
This environment variable sets the host and port for the Ollama service&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;127.0.0.1&lt;/code&gt; is the local address (localhost), meaning that the Ollama service will only listen for requests from the local machine&lt;/li&gt;
&lt;li&gt;The port number 8000 is the designated port, indicating that the Ollama service will listen for and process requests on port 8000. You can change the port number as needed, but make sure it is not occupied by other applications.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;set OLLAMA_ORIGINS=*&lt;/code&gt;&lt;/strong&gt;&lt;br&gt;
This environment variable controls which sources of requests are allowed to access the Ollama service&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;code&gt;*&lt;/code&gt; indicates that any source (i.e., all domains and IP addresses) is allowed to access the Ollama service. This is typically used in development and debugging environments; in production, you would usually specify stricter source control, limiting access only to specific domains or IPs to improve security.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;deepseek-r1-model-deployment&#34;&gt;DeepSeek-R1 Model Deployment
&lt;/h2&gt;&lt;p&gt;Ollama installation is straightforward and will not be elaborated on here&lt;/p&gt;
&lt;p&gt;Post-installation verification&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;C:\Users\core&amp;gt;ollama -v
ollama version is 0.5.11
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Model deployment, refer to the official website model page and select the corresponding parameters for the model: &lt;code&gt;ollama run deepseek-r1:14b&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;The 14B parameter model can effectively remember conversation context, while smaller parameter versions cannot. The 32B parameter version is very slow when deployed locally, so I didn&amp;rsquo;t conduct further testing.&lt;/p&gt;
&lt;h2 id=&#34;reference-materials&#34;&gt;Reference materials
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.ollama.com/library/deepseek-r1&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://www.ollama.com/library/deepseek-r1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/SPEvYTmTBxhoEkJqm1yPmw&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://mp.weixin.qq.com/s/SPEvYTmTBxhoEkJqm1yPmw&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://blog.csdn.net/x18990027/article/details/145368094&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://blog.csdn.net/x18990027/article/details/145368094&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
        </item>
        <item>
        <title>DeepSeek suddenly surged before the Spring Festival, Nvidia stock plummeted: institutional operations and large model thinking chains</title>
        <link>https://ttf248.life/en/p/deepseek-chinese-new-year-nvidia-stock-drop-institutional-operations-large-language-model-chain/</link>
        <pubDate>Fri, 07 Feb 2025 20:36:05 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/deepseek-chinese-new-year-nvidia-stock-drop-institutional-operations-large-language-model-chain/</guid>
        <description>&lt;p&gt;Ahead of the Spring Festival, DeepSeek briefly became a hot topic, garnering widespread attention on social media in just a few days. This sudden surge in popularity was not only surprising but also triggered a chain reaction in the market. At the same time, Nvidia&amp;rsquo;s stock experienced a sharp decline, raising concerns about its prospects among many investors. Some institutions conducted large-scale short selling during this period, suggesting everything pointed towards a &amp;ldquo;well-planned&amp;rdquo; scenario.&lt;/p&gt;
&lt;h3 id=&#34;deepseeks-rapid-rise-to-prominence-quickly-becoming-the-focus-of-attention&#34;&gt;DeepSeek&amp;rsquo;s rapid rise to prominence: quickly becoming the focus of attention
&lt;/h3&gt;&lt;p&gt;DeepSeek is an AI-based tool focused on optimizing deep learning models, particularly in the fields of natural language processing (NLP) and image generation. In the days leading up to the Spring Festival, this project suddenly garnered significant attention from a large number of investors and technical professionals. The performance of its team and the technological achievements demonstrated have sparked strong interest among many people. Discussions about DeepSeek dominate all topics in the tech circle, whether on developer communities or social media platforms.&lt;/p&gt;
&lt;p&gt;However, DeepSeek&amp;rsquo;s sudden surge in popularity is not accidental. After analysis, many people suspect that some institutions may be involved behind the scenes. Especially after its rise, Nvidia’s stock price experienced a noticeable decline, and there are clearly factors driving this change.&lt;/p&gt;
&lt;h3 id=&#34;nvidia-stock-plummets-the-force-behind-the-short-selling-operation&#34;&gt;Nvidia stock plummets: The force behind the short-selling operation
&lt;/h3&gt;&lt;p&gt;Nvidia, a global leader in graphics processing units (GPUs), has long been a key hardware provider for many large language models and AI computing. With the rapid development of the AI market, Nvidia&amp;rsquo;s stock has consistently performed strongly and even become a favored investment target for many investors. However, with the rise of DeepSeek and the market’s high attention to its technology, Nvidia&amp;rsquo;s stock experienced a sharp decline.&lt;/p&gt;
&lt;p&gt;Behind this phenomenon, there may be involved the short-selling strategies of institutional investors. In recent years, with the popularization of AI technology, Nvidia&amp;rsquo;s stock price has been highly driven up, and many investors began to believe that its stock price was overhyped. Especially after DeepSeek’s explosive success, some institutions may have gained substantial profits by shorting Nvidia&amp;rsquo;s stock. By seizing precise market timing and predicting the influence of DeepSeek, these institutions successfully profited from it.&lt;/p&gt;
&lt;h3 id=&#34;exploring-large-model-thinking-chains-from-results-to-process&#34;&gt;Exploring Large Model Thinking Chains: From &amp;ldquo;Results&amp;rdquo; to &amp;ldquo;Process&amp;rdquo;
&lt;/h3&gt;&lt;p&gt;In traditional AI applications, many practitioners and investors focus more on the &amp;ldquo;results&amp;rdquo; of AI models—such as generated images, text, and other direct outputs. However, in discussions related to DeepSeek, an increasing number of people are realizing that the thinking chain hidden behind large models is the core content that deserves more attention. In the past, we could only see the results of model output, but now, we need to understand the underlying logic, algorithms, and how to optimize model performance by adjusting these factors.&lt;/p&gt;
&lt;p&gt;This shift in thinking is, in essence, a deep reflection on AI research and application. Moving from simple black-box operations to truly understanding the internal workings of models has led many technical personnel and investors to begin reassessing the future direction of artificial intelligence. DeepSeek&amp;rsquo;s popularity exemplifies this breakthrough application, prompting people to focus on the entire model construction and optimization process, rather than just the final output.&lt;/p&gt;
&lt;h3 id=&#34;summary&#34;&gt;Summary
&lt;/h3&gt;&lt;p&gt;DeepSeek&amp;rsquo;s sudden surge in popularity, Nvidia’s stock plunge, and the short-selling operations of institutions behind the market – all this seems to be part of a carefully designed scheme. Through an in-depth understanding of large language model thinking chains, we can see that the application of AI technology is not merely a superficial accumulation of features but rather a deep exploration and optimization of the model&amp;rsquo;s internal logic. As technology advances, we may witness more innovative tools like DeepSeek, driving AI research and applications to higher levels.&lt;/p&gt;
&lt;p&gt;This phenomenon not only shows us the immense potential of AI technology but also prompts us to begin thinking about the commercial competition and capital operations behind the technology. The future trend of the market will be a continued focus of the interplay between technology and capital.&lt;/p&gt;</description>
        </item>
        <item>
        <title>Cursor AI programming IDE trial</title>
        <link>https://ttf248.life/en/p/cursor-ai-programming-ide-trial/</link>
        <pubDate>Thu, 23 Jan 2025 19:30:13 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/cursor-ai-programming-ide-trial/</guid>
        <description>&lt;p&gt;Another year has gone by. The biggest change at work is the noticeably increased involvement of AI. Previously, switching between different development languages required developers to be familiar with various languages&amp;rsquo; distinct APIs. Now, these basic codes can all be generated through AI, which is a great boon for developers.&lt;/p&gt;
&lt;h2 id=&#34;chatgpt&#34;&gt;ChatGPT
&lt;/h2&gt;&lt;p&gt;Back in 2023, I revised two simple introductory articles. Now it&amp;rsquo;s 2025, and honestly, I haven’t perceived any significant progress. It still requires developing your own understanding, being able to reasonably break down tasks, and most importantly, identifying whether there are bugs in the AI-generated code.&lt;/p&gt;
&lt;h2 id=&#34;github-copilot&#34;&gt;Github copilot
&lt;/h2&gt;&lt;p&gt;I forgot what day it is, but I saw in the documentation that Singapore deployed servers. For domestic use, we no longer need to constantly use a VPN. Of course, you still need a VPN to log in, but this VPN only needs to be used once for login and can then be turned off.&lt;/p&gt;
&lt;p&gt;I use Github Copilot more often on a daily basis; this plugin can be used directly in VS Code and Visual Studio, without switching between the two programs. Compared to ChatGPT, Github Copilot offers better project support and a friendlier interaction. You can choose parts of local files to &amp;ldquo;train&amp;rdquo; the AI, so the generated code is more aligned with your project.&lt;/p&gt;
&lt;h2 id=&#34;cursor-ai&#34;&gt;Cursor AI
&lt;/h2&gt;&lt;p&gt;I recently saw a new AI programming IDE, Cursor AI. This IDE is also based on GitHub Copilot, but this IDE is even more intelligent and can help you create files directly.&lt;/p&gt;
&lt;p&gt;I tried it briefly, and it feels pretty good, but my understanding of the existing project is still not enough. When there are many local project files, large refactoring and optimization adjustments still require &lt;strong&gt;developers to break down tasks&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s an example: Switch to curso’s engineering mode, enter the following content: Create a personal resume webpage that supports switching between multiple different styles. Remember to fill in some personal information for data display.&lt;/p&gt;
&lt;p&gt;After a few back-and-forths, you can obtain the following webpage. Of course, this webpage is still relatively simple, but it&amp;rsquo;s quite good for beginners.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Currently, registered users can enjoy 150 free trials of the advanced API, while paying users are limited to 5000 calls&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Resume&lt;/p&gt;</description>
        </item>
        <item>
        <title>AI programming and task decomposition</title>
        <link>https://ttf248.life/en/p/ai-programming-and-task-decomposition/</link>
        <pubDate>Fri, 22 Dec 2023 08:44:26 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/ai-programming-and-task-decomposition/</guid>
        <description>&lt;p&gt;Two years ago, I added a copy function for the code area on the site. It took almost half a day of effort, and the final rendering effect was not ideal. As a novice front-end developer, I didn&amp;rsquo;t bother to improve it; as long as it worked, that was fine. This year, using &lt;code&gt;AI&lt;/code&gt; to develop a mini program, I’m also more familiar with front-end development, so I rebuilt (redesigned by AI) it.&lt;/p&gt;
&lt;p&gt;Understanding the Dao: The task has been broken down into many small tasks, and AI development is still smooth. For a previously defunct mini-program, 75% of the code was provided by AI, and it was delivered through multiple tasks, with manual assembly of project code.&lt;/p&gt;
&lt;h2 id=&#34;previous-text&#34;&gt;Previous text
&lt;/h2&gt;&lt;p&gt;Adding a code copy button to what seems like a simple thing&lt;/p&gt;
&lt;h2 id=&#34;main-body&#34;&gt;Main body
&lt;/h2&gt;&lt;p&gt;Design CSS and JavaScript code to locate webpage elements with the class &amp;ldquo;highlight&amp;rdquo;. Within the container of these elements, add a copy button to implement the functionality of copying the code.&lt;/p&gt;
&lt;p&gt;The first draft of the code looks problematic at a glance. A second draft was generated, which is basically usable, but contains incorrect copied content and redundant text. Further optimization is needed, with manual specification of core color design and included effects.&lt;/p&gt;
&lt;p&gt;We understand you have specific requirements for the button&amp;rsquo;s size and color. We can redesign the button style to accommodate these requirements. Here is an adjusted CSS style, which includes the color scheme you provided and optimizes the button’s dimensions.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-css&#34;&gt;.copy-btn {
  position: absolute;
  top: 2px; /* 调整位置 */
  right: 2px;
  padding: 3px 6px; /* 缩小尺寸 */
  font-size: 12px; /* 减小字体大小 */
  background-color: #8FD1E1; /* 按钮背景色 */
  color: #272822; /* 文字颜色 */
  border: 1px solid #272822; /* 边框颜色 */
  border-radius: 3px; /* 圆角 */
  cursor: pointer;
  opacity: 0.7; /* 默认半透明 */
  transition: opacity 0.3s ease, background-color 0.3s ease; /* 平滑过渡效果 */
}

.copy-btn:hover {
  opacity: 1; /* 鼠标悬停时不透明 */
  background-color: #A3D4E5; /* 鼠标悬停时的背景色 */
}

.highlight {
  position: relative;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this style, I have reduced the size of the buttons and font size to make them more refined. At the same time, the default state of the button is semi-transparent, and when the mouse hovers over it, it becomes opaque, and the background color will become slightly brighter to provide visual feedback. This design both meets your color requirements and maintains a modern and user-friendly feel.&lt;/p&gt;
&lt;p&gt;A simpler JavaScript was provided accordingly&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-JavaScript&#34;&gt;document.addEventListener(&#39;DOMContentLoaded&#39;, function() {
  // 查找所有 .highlight 元素
  const highlights = document.querySelectorAll(&#39;.highlight&#39;);

  highlights.forEach(function(highlight) {
    // 创建复制按钮
    const copyBtn = document.createElement(&#39;button&#39;);
    copyBtn.textContent = &#39;复制&#39;;
    copyBtn.classList.add(&#39;copy-btn&#39;);

    // 将按钮添加到每个 .highlight 元素中
    highlight.appendChild(copyBtn);

    // 添加点击事件监听器
    copyBtn.addEventListener(&#39;click&#39;, function() {
      // 假设 .highlight 下的第一个 &amp;lt;code&amp;gt; 元素包含要复制的代码
      const code = highlight.querySelector(&#39;code&#39;) ? highlight.querySelector(&#39;code&#39;).textContent : &#39;&#39;;

      navigator.clipboard.writeText(code).then(() =&amp;gt; {
        // 显示复制成功信息
        copyBtn.textContent = &#39;已复制!&#39;;
        setTimeout(() =&amp;gt; {
          copyBtn.textContent = &#39;复制&#39;;
        }, 2000);
      }).catch(err =&amp;gt; {
        console.error(&#39;复制失败: &#39;, err);
      });
    });
  });
});
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;afterword&#34;&gt;Afterword
&lt;/h2&gt;&lt;p&gt;Although I would prefer a copy button to be added in the top area of the code region, due to my lack of understanding of frontend development, I couldn&amp;rsquo;t accurately describe the function based on keywords. After several rounds of troubleshooting with &lt;code&gt;AI&lt;/code&gt;, it still didn’t reach the expected result.&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s just that I don&amp;rsquo;t know how much impact AI will have on software development as time goes on&lt;/p&gt;</description>
        </item>
        <item>
        <title>Prompt Engineer</title>
        <link>https://ttf248.life/en/p/prompt-engineer/</link>
        <pubDate>Sun, 26 Mar 2023 20:46:53 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/prompt-engineer/</guid>
        <description>&lt;p&gt;Just as we learned techniques for using search engines back in the day, we also need to learn some techniques for communicating with AI – providing reasonable and sufficient constraints to efficiently obtain the answers we need&lt;/p&gt;
&lt;p&gt;If you look at it from a different angle, the current AI is like a child with excellent memory; it has the ability to remember everything and can copy homework. What we need to do is learn how to communicate with AI correctly and effectively, describe our needs precisely, and help AI generate the expected results.&lt;/p&gt;
&lt;h2 id=&#34;popular-science&#34;&gt;Popular science
&lt;/h2&gt;&lt;p&gt;The &lt;code&gt;AI&lt;/code&gt; that bursts forth like a shooting star, more accurately known as &lt;code&gt;Generative Pre-Training&lt;/code&gt;, translates literally to generative pre-training. It is a text generation deep learning model trained on internet-available data and used for tasks such as question answering, text summarization, machine translation, classification, code generation, and conversational AI. There are currently different versions of the model, including GPT-1, GPT-2, GPT-3, and GPT-4, each version being larger and more powerful than its predecessor.&lt;/p&gt;
&lt;h2 id=&#34;does-it-really-have-intelligence&#34;&gt;Does it really have intelligence?
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;The higher the similarity, the higher the accuracy&lt;/li&gt;
&lt;li&gt;Basic, repetitive tasks no longer require human intervention after specific training&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Generative AI is a technology that utilizes existing data such as text, audio, and images to create new content. It can be used for various tasks including text generation, speech synthesis, image generation, and dialogue systems. The logic of generative AI depends on its training data and model structure. Generally speaking, generative AI can follow grammar, logic, and common sense to a certain extent, but it may also produce errors, biases, or untrue content. Therefore, the output of generative AI requires human judgment and verification and should not be blindly trusted or used.&lt;/p&gt;
&lt;h2 id=&#34;prompt-engineer&#34;&gt;Prompt Engineer
&lt;/h2&gt;&lt;p&gt;The river of time doesn&amp;rsquo;t flow backward; people need to learn to adapt to the trend. You can think of &lt;code&gt;AI&lt;/code&gt; as not intelligent, lacking logic, and often producing unusable code.&lt;/p&gt;
&lt;p&gt;If you look at it from a different angle, the current AI is like a child with excellent memory; it has the ability to remember everything and can copy homework. What we need to do is learn how to communicate with AI correctly and effectively, describe our needs precisely, and help AI generate the expected results.&lt;/p&gt;
&lt;h2 id=&#34;dialogue-mode&#34;&gt;Dialogue mode
&lt;/h2&gt;&lt;p&gt;Two years after &lt;code&gt;GitHub Copilot&lt;/code&gt; was released, no one could have predicted that two years later, &lt;code&gt;OpenAI&lt;/code&gt; would emerge, allowing humans to recognize the capabilities of large language models&lt;/p&gt;
&lt;p&gt;Based on &lt;strong&gt;annotation-based programming&lt;/strong&gt; and &lt;strong&gt;dialogue-based programming&lt;/strong&gt;, the interactive logic is completely different. Dialogue&amp;rsquo;s pattern provides a user-friendly interaction experience for novice users. It must be mentioned that &lt;code&gt;NewBing&lt;/code&gt; offers follow-up prompts after each question, with Microsoft attempting to guide users to obtain more content from the &lt;code&gt;AI&lt;/code&gt; knowledge base.&lt;/p&gt;
&lt;h2 id=&#34;for-example&#34;&gt;For example
&lt;/h2&gt;&lt;p&gt;The high-speed train journey is long, and coding is a good option. With limited data, processing several images and obtaining image resources is fine, but downloading software to process them would be a bit extravagant, and the network conditions don&amp;rsquo;t allow it.&lt;/p&gt;
&lt;p&gt;We have a batch of image assets with transparent backgrounds to be used as navigation bar icons for a mini-program. The selected state should be in color, and the unselected state should be black and white. The original assets are all in color and need some preprocessing.&lt;/p&gt;
&lt;p&gt;Write a Python code to read all images from a folder, convert them to black and white, maintain the transparent background, optimize the image quality when saving to reduce file size, and display the comparison of the file sizes before and after optimization in the console&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# 导入必要的库
import os
import PIL
from PIL import Image

# 定义一个函数，将图片转换成黑白，维持透明背景，保存图片，并返回文件体积
def convert_and_save(image_file):
    # 打开图片
    image = Image.open(image_file)
    # 获取图片的模式，如果是 RGBA 模式，说明有透明背景
    mode = image.mode
    if mode == &amp;quot;RGBA&amp;quot;:
        # 创建一个和图片大小一致的白色背景图片
        background = Image.new(&amp;quot;RGB&amp;quot;, image.size, (255, 255, 255))
        # 将原图片粘贴到白色背景上，忽略透明像素
        background.paste(image, mask=image.split()[3])
        # 将合成的图片转换成灰度模式
        gray_image = background.convert(&amp;quot;L&amp;quot;)
        # 将灰度图片再转换成 RGBA 模式，以便保留透明背景
        final_image = gray_image.convert(&amp;quot;RGBA&amp;quot;)
    else:
        # 如果不是 RGBA 模式，直接将图片转换成灰度模式
        final_image = image.convert(&amp;quot;L&amp;quot;)
    # 获取原图片的文件名和扩展名
    file_name, file_ext = os.path.splitext(image_file)
    # 定义新图片的文件名，添加 _bw 后缀表示黑白
    new_file_name = file_name + &amp;quot;_bw&amp;quot; + file_ext
    # 保存新图片，并优化质量，减少文件体积
    final_image.save(new_file_name, optimize=True)
    # 获取原图片和新图片的文件体积，并返回
    old_size = os.path.getsize(image_file)
    new_size = os.path.getsize(new_file_name)
    return old_size, new_size

# 定义一个文件夹路径，存放所有的图片
folder_path = &amp;quot;images&amp;quot;
# 遍历文件夹中的所有文件
for file in os.listdir(folder_path):
    # 拼接完整的文件路径
    file_path = os.path.join(folder_path, file)
    # 判断是否是图片文件，根据扩展名判断，可以根据需要添加或修改扩展名
    if file_path.endswith((&amp;quot;.png&amp;quot;, &amp;quot;.jpg&amp;quot;, &amp;quot;.jpeg&amp;quot;, &amp;quot;.gif&amp;quot;)):
        # 调用函数，转换并保存图片，并获取文件体积
        old_size, new_size = convert_and_save(file_path)
        # 在控制台展示优化前后的文件体积大小对比
        print(f&amp;quot;{file}: {old_size} bytes -&amp;gt; {new_size} bytes&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;AI quickly provided an answer, it looks good and works fine, but experienced programmers will find the code below not very general and unsuitable for production environments&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Parameters are fixed&lt;/li&gt;
&lt;li&gt;There are no log files; the information is printed to the console&lt;/li&gt;
&lt;li&gt;Image processing tasks are suitable for multithreaded models&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;NewBing is here, following prompts, and selected a question: &lt;strong&gt;Are there any areas for improvement in this code?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Honestly, this answer completely exceeded expectations. The AI provided corresponding suggestions for optimizations that a typical programmer would think of.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;这段代码可能还有一些可以改进的地方，比如：

可以使用 argparse 库，让用户可以通过命令行参数指定文件夹路径和扩展名，而不是写死在代码里。
可以使用 logging 库，记录程序的运行情况，而不是直接打印到控制台。
可以使用 try-except 语句，捕获可能出现的异常，比如文件打开失败，图片转换失败等，并做相应的处理。
可以使用 multiprocessing 库，利用多核处理器，加速图片的转换和保存过程。
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A very friendly response, considering the differences in deployment environments, supporting single-process execution is permitted&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;当然，我很乐意帮你修改一下代码。我尝试添加了 argparse, logging 和 try-except 语句，但没有添加 multiprocessing 库，因为我不确定你的电脑是否支持多核处理。修改后的代码如下：
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# 导入必要的库
import argparse
import logging
import multiprocessing
import os

from PIL import Image


# 定义一个函数，将图片转换成黑白，维持透明背景，保存图片，并返回文件体积
def convert_and_save(image_file):
    # 打开图片
    try:
        image = Image.open(image_file)
    except Exception as e:
        logging.error(f&amp;quot;Failed to open {image_file}: {e}&amp;quot;)
        return None, None
    # 获取图片的模式，如果是 RGBA 模式，说明有透明背景
    mode = image.mode
    if mode == &amp;quot;RGBA&amp;quot;:
        # 创建一个和图片大小一致的白色背景图片
        background = Image.new(&amp;quot;RGB&amp;quot;, image.size, (255, 255, 255))
        # 将原图片粘贴到白色背景上，忽略透明像素
        background.paste(image, mask=image.split()[3])
        # 将合成的图片转换成灰度模式
        gray_image = background.convert(&amp;quot;L&amp;quot;)
        # 将灰度图片再转换成 RGBA 模式，以便保留透明背景
        final_image = gray_image.convert(&amp;quot;RGBA&amp;quot;)
    else:
        # 如果不是 RGBA 模式，直接将图片转换成灰度模式
        final_image = image.convert(&amp;quot;L&amp;quot;)
    # 获取原图片的文件名和扩展名
    file_name, file_ext = os.path.splitext(image_file)
    # 定义新图片的文件名，添加 _bw 后缀表示黑白
    new_file_name = file_name + &amp;quot;_bw&amp;quot; + file_ext
    # 保存新图片，并优化质量，减少文件体积
    try:
        final_image.save(new_file_name, optimize=True)
    except Exception as e:
        logging.error(f&amp;quot;Failed to save {new_file_name}: {e}&amp;quot;)
        return None, None
    # 获取原图片和新图片的文件体积，并返回
    old_size = os.path.getsize(image_file)
    new_size = os.path.getsize(new_file_name)
    return file_name, old_size, new_size

# 定义一个函数，解析命令行参数，并返回文件夹路径和扩展名列表
def parse_args():
    # 创建一个解析器对象
    parser = argparse.ArgumentParser(description=&amp;quot;Convert images to black and white and optimize quality.&amp;quot;)
    # 添加一个必选的位置参数，表示文件夹路径
    parser.add_argument(&amp;quot;folder_path&amp;quot;, help=&amp;quot;The path of the folder that contains the images.&amp;quot;)
    # 添加一个可选的参数，表示扩展名列表，默认为 png, jpg, jpeg 和 gif
    parser.add_argument(&amp;quot;-e&amp;quot;, &amp;quot;--extensions&amp;quot;, nargs=&amp;quot;+&amp;quot;, default=[&amp;quot;.png&amp;quot;, &amp;quot;.jpg&amp;quot;, &amp;quot;.jpeg&amp;quot;, &amp;quot;.gif&amp;quot;], help=&amp;quot;The extensions of the image files.&amp;quot;)
    # 解析命令行参数，并返回结果对象
    args = parser.parse_args()
    return args.folder_path, args.extensions

# 定义一个函数，打印优化前后的文件体积大小对比
def print_result(result):
    # 如果结果不为空，说明转换和保存成功
    if result:
        # 解包结果为文件名和文件体积元组
        if len(result) == 3:
            file, old_size, new_size = result
            # 在控制台展示优化前后的文件体积大小对比
            logging.info(f&amp;quot;{file}: {old_size} bytes -&amp;gt; {new_size} bytes&amp;quot;)
        else:
            logging.info(f&amp;quot;{result}&amp;quot;)

# 配置日志记录器，将日志输出到控制台和文件中，设置日志等级为 INFO
logging.basicConfig(level=logging.INFO,
                    format=&amp;quot;%(asctime)s %(levelname)s %(message)s&amp;quot;,
                    handlers=[logging.StreamHandler(),
                              logging.FileHandler(&amp;quot;log.txt&amp;quot;)])

# 调用函数，获取文件夹路径和扩展名列表
folder_path, extensions = parse_args()

if __name__ == &amp;quot;__main__&amp;quot;:
    # Windows 需要这个函数的原因是 Windows 缺乏 fork() 函数（这不完全正确）。
    # 因此，在 Windows 上，fork() 是通过创建一个新的进程来模拟的，这个新的进程中运行的代码，在 Linux 上是在子进程中运行的。
    # 由于这些代码要在技术上无关的进程中运行，所以它必须在运行之前被传递到那里。
    # 传递的方式是先将它们序列化，然后通过管道从原始进程发送到新的进程。
    # 另外，这个新的进程被通知它必须运行通过管道传递的代码，通过向它传递 --multiprocessing-fork 命令行参数。
    # 如果你看一下 freeze_support() 函数的实现，它的任务是检查它运行在哪个进程中，是否应该运行通过管道传递的代码
    multiprocessing.freeze_support()

    # 创建一个进程池，根据电脑的核心数自动分配进程
    pool = multiprocessing.Pool()
    # 创建一个空列表，用于存放异步任务的结果对象
    results = []
    # 遍历文件夹中的所有文件
    for file in os.listdir(folder_path):
        # 拼接完整的文件路径
        file_path = os.path.join(folder_path, file)
        # 判断是否是图片文件，根据扩展名判断，可以根据需要修改扩展名列表
        if any(file_path.endswith(ext) for ext in extensions):
            # 调用函数，转换并保存图片，并获取文件体积，使用异步方式，不阻塞主进程
            result = pool.apply_async(convert_and_save, args=(file_path,), callback=print_result)
            # 将结果对象添加到列表中
            results.append((file, result))
    # 关闭进程池，不再接受新的任务
    pool.close()
    # 等待所有的任务完成
    pool.join()
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;afterword&#34;&gt;Afterword
&lt;/h2&gt;&lt;p&gt;Because local development is on a &lt;code&gt;Windows&lt;/code&gt; system, the AI&amp;rsquo;s initial answer didn&amp;rsquo;t include a &lt;code&gt;main&lt;/code&gt; function or &lt;code&gt;multiprocessing.freeze_support&lt;/code&gt;. The code was fixed by tracing the error messages that occurred through follow-up questions.&lt;/p&gt;
&lt;p&gt;Just as we learned techniques for using search engines back in the day, we also need to learn some techniques for communicating with AI – providing reasonable and sufficient constraints to efficiently obtain the answers we need&lt;/p&gt;
&lt;p&gt;Note: &lt;strong&gt;If you are a programming novice and have any questions about the code based on the comments, feel free to ask for clarification.&lt;/strong&gt;&lt;/p&gt;</description>
        </item>
        <item>
        <title>AI-assisted programming, an evolution of productivity</title>
        <link>https://ttf248.life/en/p/ai-assisted-programming-productivity-evolution/</link>
        <pubDate>Tue, 28 Feb 2023 17:05:17 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/ai-assisted-programming-productivity-evolution/</guid>
        <description>&lt;p&gt;GitHub Copilot was released less than two years ago, and then ChatGPT appeared. I don&amp;rsquo;t fully understand the underlying principles, but I’ve used both for a while. The two tools offer completely different levels of assistance, but they both significantly improve productivity.&lt;/p&gt;
&lt;p&gt;Very complex things, AI still can&amp;rsquo;t do. After all, they lack logic; things that are routine or have a fixed paradigm, if the training data is sufficient, AI can achieve about 90% effectiveness.&lt;/p&gt;
&lt;h2 id=&#34;github-copilot&#34;&gt;github copilot
&lt;/h2&gt;&lt;p&gt;When it was released, looking at the demo introduction on the official website, it didn&amp;rsquo;t seem very smart, and after trying it out, it wasn&amp;rsquo;t very usable, so I gave up&lt;/p&gt;
&lt;p&gt;Before 2022, when I had nothing else to do, I switched to the new version to try it out, and the effect is already pretty good. The only problem is that the speed in China is a bit slow, probably due to network issues. Code written based on comments using languages like &lt;code&gt;Python&lt;/code&gt; is quite decent; it&amp;rsquo;s much faster than looking up information myself and writing code from scratch.&lt;/p&gt;
&lt;h2 id=&#34;chatgpt&#34;&gt;ChatGPT
&lt;/h2&gt;&lt;p&gt;GitHub Copilot is still designed from a programmer&amp;rsquo;s, or developer&amp;rsquo;s, perspective. You need to understand how to program and gradually guide the AI with comments to assist in programming.&lt;/p&gt;
&lt;p&gt;ChatGPT has reached the next level, &lt;strong&gt;making it easy for even non-experts to get started&lt;/strong&gt;. With its conversational and interactive design, you can freely ask questions, describe your needs, and continue asking if there&amp;rsquo;s anything you don&amp;rsquo;t understand.&lt;/p&gt;
&lt;p&gt;For example, you understand the concept of tokenization but don&amp;rsquo;t know how to code it, and want to calculate string similarity:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;Python 分词计算 維天運通 LOGORY 和 合肥維天運通信息科技股份有限公司 两个字符串的相似度；
比较前，统一处理字符串，英文转换为小写；
第二个字符中如果能找到第一个字符的首字符，将第二个字符串截位，截位的长度为第一个字符串的长度
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Hello, this is Bing. 😊&lt;/p&gt;
&lt;p&gt;According to online information 123, Python tokenization can be implemented using the jieba library. The jieba library supports three segmentation modes: precise mode, full mode, and search engine mode. You can choose the appropriate mode based on your needs.&lt;/p&gt;
&lt;p&gt;To compare the similarity of two strings, you first need to tokenize them and then calculate indicators such as word vectors or edit distance. You can use libraries like NumPy or SciPy for mathematical operations.&lt;/p&gt;
&lt;p&gt;To uniformly handle strings, converting them to lowercase can be achieved using Python&amp;rsquo;s built-in function lower()&lt;/p&gt;
&lt;p&gt;To extract the second string, you can use Python&amp;rsquo;s slicing operator []&lt;/p&gt;
&lt;p&gt;Here is a possible code example:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import jieba
import numpy as np
from scipy.spatial.distance import cosine

# 定义两个字符串
str1 = &amp;quot;維天運通 LOGORY&amp;quot;
str2 = &amp;quot;合肥維天運通信息科技股份有限公司&amp;quot;

# 统一处理字符串，英文转换为小写
str1 = str1.lower()
str2 = str2.lower()

# 找到第二个字符串中第一个字符的位置
index = str2.find(str1[0])

# 如果找到了，截取第二个字符串
if index != -1:
    str2 = str2[index:index+len(str1)]

# 使用 jieba 库进行分词
words1 = list(jieba.cut(str1))
words2 = list(jieba.cut(str2))

# 计算两个字符串的词向量（以词频作为权重）
vector1 = np.zeros(len(words1))
vector2 = np.zeros(len(words1))

for i, word in enumerate(words1):
    vector1[i] += 1 # 统计第一个字符串中每个词出现的次数
    if word in words2:
        vector2[i] += 1 # 统计第二个字符串中每个词出现的次数

# 计算两个字符串的相似度（以余弦距离作为指标）
similarity = 1 - cosine(vector1, vector2)

print(&amp;quot;两个字符串的相似度是：&amp;quot;, similarity)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To take it a step further, I consulted the results of the run and congratulated you on discovering a problem – the results were fabricated. It didn&amp;rsquo;t actually execute any code but gave a decent result.&lt;/p&gt;
&lt;h3 id=&#34;chestnuts&#34;&gt;Chestnuts
&lt;/h3&gt;&lt;p&gt;Question: SQL statement, retrieve records from the product code table where the product codes are within the range of 80000-89999&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;你好，这是 Bing。😊

根据搜索结果12，你可以使用 SQL BETWEEN 运算符来选择给定区间内的值。BETWEEN 运算符包含了开始和结束的值。

你需要先指定要查询的表名和列名，然后在 WHERE 子句中使用 BETWEEN 运算符和区间值。

例如，如果你要查询产品代码表中，产品代码在 80000 到 89999 之间的记录，你可以使用以下语句：

SELECT * FROM product_code
WHERE code BETWEEN 80000 AND 89999;
这里，* 表示选择所有列3。如果你只想选择特定的列，比如产品名称和价格，你可以用逗号分隔它们，如下：

SELECT name, price FROM product_code
WHERE code BETWEEN 80000 AND 89999;
希望这能帮助到你。
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Keep asking, negate it: The product code is a string, your code is wrong&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;抱歉，我没有注意到产品代码是字符串。如果是这样的话，你需要在区间值两边加上单引号，如下：

SELECT * FROM product_code
WHERE code BETWEEN &#39;80000&#39; AND &#39;89999&#39;;
请原谅我的错误。🙏
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Congratulations, it will make a mistake&lt;/p&gt;</description>
        </item>
        
    </channel>
</rss>
