<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Large Model on Uncle Xiang&#39;s Notebook</title>
        <link>https://ttf248.life/en/tags/large-model/</link>
        <description>Recent content in Large Model on Uncle Xiang&#39;s Notebook</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en</language>
        <lastBuildDate>Thu, 04 Dec 2025 22:08:05 +0800</lastBuildDate><atom:link href="https://ttf248.life/en/tags/large-model/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>Is AI Getting “Dumber”? Uncovering the Game Between Parameter Accuracy and Inference Costs</title>
        <link>https://ttf248.life/en/p/is-ai-getting-dumber-uncovering-the-game-between-parameter-precision-and-inference-costs/</link>
        <pubDate>Thu, 04 Dec 2025 21:58:09 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/is-ai-getting-dumber-uncovering-the-game-between-parameter-precision-and-inference-costs/</guid>
        <description>&lt;p&gt;Recently mingling in various programming large model communication circles, the most complained about thing is model degradation.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Models deployed on local desktop computers are quantized models, essentially downgraded versions.&lt;/li&gt;
&lt;li&gt;With “vibe coding” so popular, could it be that the content output by current large models is the most valuable product – code?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This round of prompts received one optimization, which coincided with model degradation, and the large model’s solution was to optimize the prompt, with more detailed task planning and clearer output requirements.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Addressing this issue, provide suitable prompts: Many large factories have now provided large model services, and users sometimes feedback that models are degrading; from a professional perspective, considering parameter precision and inference cost, write an article;科普文 (科普 article), don’t be too long.
Optimized version:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;You are a senior AI industry technical column writer. Your goal is to write a professional but accessible Chinese科普 article for general readers.

Please step by step think and write:
1. Outline drafting: First, please draft a clear three-paragraph article structure outline (e.g., Introduction, Precision Analysis, Cost &amp;amp; Architecture Analysis, Conclusion).
2. Content focus: The article must deeply and clearly explain the phenomenon of “model degradation” reported by users, and the core analysis points must revolve around **parameter precision (quantization)** and **inference operational cost**.
3. Word count and format: The final article length must be controlled at 600 words or less. Please use clear `##` headings to divide paragraphs, and use **bold** to emphasize key professional terms (such as: quantization, MoE, FP16).
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction
&lt;/h2&gt;&lt;p&gt;To ensure clarity and align with a科普 (explanatory) approach, I’ve established the following structure:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Introduction:&lt;/strong&gt; Confirming user perception of reality and introducing the core conflict – in the context of rapidly growing user scales, service providers must balance “intellectual peak” with “service availability.”&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Precision Analysis (Core Point 1):&lt;/strong&gt; Explaining the relationship between &lt;strong&gt;parameters&lt;/strong&gt; and &lt;strong&gt;precision&lt;/strong&gt;. By comparing &lt;strong&gt;FP16&lt;/strong&gt; with &lt;strong&gt;quantization&lt;/strong&gt; (INT8/INT4), we use the analogy of &amp;ldquo;image compression&amp;rdquo; to illustrate why a drop in precision leads to degraded logic.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Cost &amp;amp; Architecture (Core Point 2):&lt;/strong&gt; Analyzing &lt;strong&gt;inference cost&lt;/strong&gt;. We introduce the &lt;strong&gt;MoE&lt;/strong&gt; architecture and &lt;strong&gt;sparse activation&lt;/strong&gt; mechanisms, explaining how models “slack off” to save money and accelerate performance.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Conclusion:&lt;/strong&gt; Summarizing that this &amp;ldquo;degradation of intelligence&amp;rdquo; is an inevitable path for commercialization and widespread adoption (the impossibility triangle).&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;article-body&#34;&gt;Article Body
&lt;/h2&gt;&lt;p&gt;Recently, many users have reported that large models seem less “intelligent” than before: logic has become simpler, instruction following has deteriorated, and even empty talk (“car轱辘话”) has emerged. This isn’t entirely an illusion. From a technical perspective, this “decline in intelligence” phenomenon is often a technological compromise made by major companies when facing massive user bases, balancing &lt;strong&gt;inference costs&lt;/strong&gt; with service speed.&lt;/p&gt;
&lt;h2 id=&#34;precision-shrinking-the-cost-of-4k-to-720p&#34;&gt;Precision “Shrinking”: The Cost of 4K to 720P
&lt;/h2&gt;&lt;p&gt;The “intelligence” of large models is primarily stored in trillions of &lt;strong&gt;parameters&lt;/strong&gt;. Ideally, these parameters run in high-precision &lt;strong&gt;FP16&lt;/strong&gt; (16-bit floating-point) format, capturing extremely subtle semantic differences. However, this high precision means significant &lt;strong&gt;memory&lt;/strong&gt; usage and slow computation speeds.&lt;/p&gt;
&lt;p&gt;To allow hundreds of millions of users to use the service smoothly, service providers generally adopt &lt;strong&gt;quantization&lt;/strong&gt; technology. This is a technique that compresses parameter precision from &lt;strong&gt;FP16&lt;/strong&gt; to &lt;strong&gt;INT8&lt;/strong&gt; or even &lt;strong&gt;INT4&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;This is like compressing a 4K high-definition movie into a 720P stream: although the plot (overall logic) hasn&amp;rsquo;t changed, the visual details (small logical associations, complex instruction execution details) are lost. This “lossy compression” directly leads to a decline in model performance when handling complex tasks, giving users the sensation of “getting dumber.”&lt;/p&gt;
&lt;h2 id=&#34;cost-pressure-letting-the-brain-partially-rest&#34;&gt;Cost Pressure: Letting the Brain “Partially Rest”
&lt;/h2&gt;&lt;p&gt;Besides accuracy, &lt;strong&gt;the operational cost of inference&lt;/strong&gt; is another key factor. Every time you ask an AI a question, servers perform massive matrix calculations, leading to exorbitant electricity bills and hardware wear and tear.&lt;/p&gt;
&lt;p&gt;To reduce costs, modern large models often adopt &lt;strong&gt;MoE&lt;/strong&gt; (Mixture of Experts) architecture. Unlike traditional models that activate all neurons every time, &lt;strong&gt;MoE&lt;/strong&gt; employs a &lt;strong&gt;sparse activation&lt;/strong&gt; strategy: for your question, the system only activates a small subset of relevant “expert” networks, while the rest remain dormant.&lt;/p&gt;
&lt;p&gt;Although this significantly reduces computational load, if the &lt;strong&gt;routing algorithm&lt;/strong&gt; (Router) distributes your complex mathematical problems to “literary experts” for load balancing or power saving, or sacrifices the number of expert calls in favor of speed, the output quality will inevitably fluctuate and decline.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion
&lt;/h2&gt;&lt;p&gt;Therefore, what is being referred to as “diminishment” – in essence, is the inevitable pain associated with AI’s transition from the laboratory to large-scale commercialization. The restraint on &lt;strong&gt;parameter precision&lt;/strong&gt; and the efficient design of &lt;strong&gt;model architecture&lt;/strong&gt; are what manufacturers seek to balance between “absolute intelligence” and “cost efficiency,” allowing everyone to affordably use AI.&lt;/p&gt;</description>
        </item>
        <item>
        <title>Ultimately, it’s returning to domestic models.</title>
        <link>https://ttf248.life/en/p/ultimately-its-returning-to-domestic-models/</link>
        <pubDate>Wed, 03 Dec 2025 22:33:47 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/ultimately-its-returning-to-domestic-models/</guid>
        <description>&lt;p&gt;“Previously, it was mentioned that when logging into Gemini Cli, you needed to configure the Google Cloud Project ID. This is already wrong – if it’s a personal account, there shouldn&amp;rsquo;t be this restriction. The fact that this restriction exists indicates that you’ve started triggering Google’s security system and are being identified as not being a personal account.&lt;/p&gt;
&lt;p&gt;It’s frustrating; after using it for half a month and getting used to it, now I have to return to the embrace of cc + domestic models.”&lt;/p&gt;
&lt;h2 id=&#34;overview&#34;&gt;Overview
&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;You are currently configured to use a Google Cloud Project but lack a Gemini Code Assist license. Please contact your administrator to request a license. (#3501)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;troubleshooting&#34;&gt;Troubleshooting
&lt;/h2&gt;&lt;p&gt;It was still accessible and usable during the day, but stopped working when I got home in the evening, initially suspected a bug caused by an update. Switching to an older version didn’t fix it, so I raised an issue on GitHub. The system automatically pulled a large number of similar issues for me.
&lt;a class=&#34;link&#34; href=&#34;https://github.com/google-gemini/gemini-cli/issues/14447&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/google-gemini/gemini-cli/issues/14447&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;After reviewing the related information, I realized something was wrong: the Gemini CLI website hadn’t been updated, and the documentation on GitHub mentioned that if you need to log in with a Google Cloud ID, you are being identified as an &lt;strong&gt;enterprise user&lt;/strong&gt;, not a personal developer.&lt;/p&gt;
&lt;p&gt;I continued searching, and there were many similar cases in the community: [https://discuss.google.dev/t/is-gemini-code-assist-incorrectly-identifying-my-personal-account-as-an-enterprise-account/287654/2], which also reported issues with usage after login.&lt;/p&gt;
&lt;h2 id=&#34;solution&#34;&gt;Solution
&lt;/h2&gt;&lt;p&gt;M2 has seen a lot of updates in the past half month, supported by &lt;strong&gt;MCP&lt;/strong&gt; for online search and image recognition. Try it out first, and if it doesn&amp;rsquo;t work later, consider finding a way to pay Google for the basic paid version. My existing US dollar credit card may not be successful in payment; last time I tried paying ChatGPT failed.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Alibaba Large Model Strategy</title>
        <link>https://ttf248.life/en/p/alibaba-large-model-strategy/</link>
        <pubDate>Tue, 18 Nov 2025 22:07:10 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/alibaba-large-model-strategy/</guid>
        <description>&lt;p&gt;Alibaba (Ali) has released numerous large models, not simply a matter of “volume chasing,” but a carefully planned &lt;strong&gt;“Model-as-a-Service” (MaaS) ecosystem strategy.&lt;/strong&gt; There are multiple considerations behind this, which can be summarized as “internal empowerment and external ecosystem building.”&lt;/p&gt;
&lt;h2 id=&#34;internal-business-driven-inward-empowerment&#34;&gt;Internal Business Driven (Inward Empowerment)
&lt;/h2&gt;&lt;p&gt;Alibaba possesses an extremely vast and diverse business landscape, including e-commerce (Taobao &amp;amp; Tmall), finance (Ant Financial), logistics (Cainiao), cloud computing (Aliyun), and entertainment (Youku), among others.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Scenario Customization:&lt;/strong&gt; A single, general-purpose large model cannot efficiently meet the fine-grained needs of all vertical scenarios. For example, the requirements for an e-commerce customer service model, an advertising creative generation model, a financial risk control model, and a logistics route planning model are entirely different.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Efficiency &amp;amp; Cost:&lt;/strong&gt; Training specialized “small models” for specific tasks (such as coding or diagramming) is more cost-effective and responsive than continuously calling upon a large “all-in-one” model.&lt;/li&gt;
&lt;li&gt;Therefore, Alibaba needs a &amp;ldquo;Model Matrix&amp;rdquo; with “Tongyi” (Tongyi) foundational model at its core, to derive specialized models tailored to different businesses, enabling an AI transformation of internal operations.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;building-a-cloud-service-ecosystem-external-ecosystem-building&#34;&gt;Building a Cloud Service Ecosystem (External Ecosystem Building)
&lt;/h2&gt;&lt;p&gt;This is Alibaba’s core strategy. Large Models are the “operating system” of the AI era, while computing power (cloud services) are like “water and electricity.” The goal of Alibaba Cloud is to become a leading “compute + model” infrastructure provider in the AI era.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Building a &amp;ldquo;Model Supermarket&amp;rdquo;:&lt;/strong&gt;  Alibaba not only provides its own “Tongyi” series models (such as Tongyi Qianwen, Tongyi Wanxiang, and Tongyi Lingma), but also gathers a large number of third-party and open-source models through the “MoDaBu”(ModelScope) community. By releasing numerous models, it’s intended to showcase its powerful model research capabilities and rich model library, attracting enterprises to move to the cloud.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;“Open Source + Closed Source” Parallel Strategy:&lt;/strong&gt; Alibaba&amp;rsquo;s (such as open-sourcing the Qwen series of “Tongyi Qianwen”) open-source strategy is aimed at rapidly building a developer community, capturing market share, accelerating technological iteration, and competing with international open-source forces like Meta’s Llama. After developers “practice” on open-source models, they are likely to choose Alibaba Cloud&amp;rsquo;s paid computing power and closed-source commercial models for final deployment.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;summary&#34;&gt;Summary
&lt;/h2&gt;&lt;p&gt;In short, Alibaba’s release of numerous large models is characterized by a strategy of: building a core self-developed model matrix centered around the “Tongyu” series to deeply empower its vast and diverse internal businesses. Simultaneously, through “open source” and “Model as a Service” platforms, it&amp;rsquo;s constructing an open and thriving AI ecosystem on Alibaba Cloud, with the ultimate goal of &lt;strong&gt;driving growth in its core cloud computing business.&lt;/strong&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Recent Usage Experiences of Large Models</title>
        <link>https://ttf248.life/en/p/recent-usage-experiences-of-large-models/</link>
        <pubDate>Wed, 09 Jul 2025 21:09:01 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/recent-usage-experiences-of-large-models/</guid>
        <description>&lt;p&gt;Currently, no large model stands out as particularly superior; each company has its own strengths and preferred use cases.&lt;/p&gt;
&lt;h2 id=&#34;technical-documentation&#34;&gt;Technical Documentation
&lt;/h2&gt;&lt;p&gt;For feeding code or asking IT technical questions: ChatGPT and Gemini&lt;/p&gt;
&lt;h2 id=&#34;write-code&#34;&gt;Write Code
&lt;/h2&gt;&lt;p&gt;Gather requirements and request code modifications: Claude&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Blog Translation Project Musings: Historical Conversations</title>
        <link>https://ttf248.life/en/p/blog-translation-project-musings-historical-conversations/</link>
        <pubDate>Mon, 02 Jun 2025 21:16:24 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/blog-translation-project-musings-historical-conversations/</guid>
        <description>&lt;p&gt;The initial design of the blog translation project was overly complex – first parsing Markdown format, then using placeholders to protect the content, and finally sending it to a large model for translation. This was entirely unnecessary; large models inherently possess the ability to recognize Markdown syntax and can directly process the original content while maintaining formatting during translation.&lt;/p&gt;
&lt;p&gt;Our work shifted from debugging code to debugging the &lt;strong&gt;prompting&lt;/strong&gt; of the model.
Model: &lt;code&gt;google/gemma-3-4b&lt;/code&gt;
Hardware: &lt;code&gt;Nvidia 3060 12GB&lt;/code&gt;
Indeed, we chose a non-thinking model – thinking models were inefficient when executing translation tasks. We compared the performance of 4b and 12b parameters, and for translation purposes, gemma3’s 4b parameter was sufficient; there was no significant advantage in terms of 12b parameters.
12b parameter speed: &lt;strong&gt;11.32 tok/sec&lt;/strong&gt; , 4b parameter speed: &lt;strong&gt;75.21 tok/sec&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;background-introduction&#34;&gt;Background Introduction
&lt;/h2&gt;&lt;p&gt;Despite adding various constraints within the &lt;strong&gt;system&lt;/strong&gt;, the output translation results still presented some issues, such as: lack of formatting protection, inclusion of extraneous explanatory content. When defining roles, it was already stated to protect Markdown format and only output translation results; ultimately, the translation remained unstable.&lt;/p&gt;
&lt;p&gt;At this point, I remembered encountering a comic translation project previously, which also leveraged the capabilities of large models. Its translation effect seemed better than mine. Upon reviewing the code and comparing the request data, the comic translation project would include a set of context with each request, in addition to the current translation content, it would also include previous translation content.&lt;/p&gt;
&lt;p&gt;What were the benefits? Not only did this improve the coherence between preceding and following translations, but it also ensured the stability of the output format.&lt;/p&gt;
&lt;h2 id=&#34;the-importance-of-historical-conversations&#34;&gt;The Importance of Historical Conversations
&lt;/h2&gt;&lt;p&gt;As large AI models (such as the GPT series, Claude, Gemini, etc.) become more prevalent, an increasing number of businesses and developers are accessing these models via APIs to build intelligent customer service, content generation, code assistant, and other applications. However, many people encounter a common issue during initial access: &lt;strong&gt;model outputs are disjointed, lack contextual understanding, and even answer the wrong questions.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;A key reason for this phenomenon is – &lt;strong&gt;not including historical conversation content in API requests.&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;what-is-a-history-dialogue&#34;&gt;What is a History Dialogue?
&lt;/h2&gt;&lt;p&gt;A history dialogue refers to the exchange records between a model and a user within a single conversation session. In most large language model APIs (such as OpenAI’s Chat Completions API), developers need to construct the complete &lt;code&gt;messages&lt;/code&gt; array themselves, passing the historical dialogue in turn as &lt;code&gt;user&lt;/code&gt; and &lt;code&gt;assistant&lt;/code&gt; message format.&lt;/p&gt;
&lt;h3 id=&#34;example&#34;&gt;Example
&lt;/h3&gt;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{
  &amp;quot;model&amp;quot;: &amp;quot;gpt-4&amp;quot;,
  &amp;quot;messages&amp;quot;: [
    {&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;Write me a resignation letter&amp;quot;},
    {&amp;quot;role&amp;quot;: &amp;quot;assistant&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;Okay, what do you want to write about as the reason for your resignation?&amp;quot;},
    {&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;I want to pursue personal career development&amp;quot;}
  ]
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you only send the last sentence:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;I want to pursue personal career development&amp;quot;}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The model won&amp;rsquo;t know you are writing a resignation letter, and its output quality will be very poor because it doesn’t understand the context.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;why-historical-dialogue-is-so-important&#34;&gt;Why Historical Dialogue is So Important?
&lt;/h2&gt;&lt;h3 id=&#34;1-build-context-enhance-coherence&#34;&gt;1. &lt;strong&gt;Build Context, Enhance Coherence&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;AI models are inherently “context-driven.” They cannot remember anything that has happened “previously,” unless you explicitly tell it. By passing in the dialogue history, the model can better understand your intent and topic context, resulting in outputs more aligned with expectations.&lt;/p&gt;
&lt;h3 id=&#34;2-reduce-misunderstanding-rate&#34;&gt;2. &lt;strong&gt;Reduce Misunderstanding Rate&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;If you want the model to complete a multi-turn instruction, such as writing, summarizing, or debugging code, historical context allows the model to gradually accumulate understanding and avoid going off-topic or losing focus midway through.&lt;/p&gt;
&lt;h3 id=&#34;3-simulating-realistic-human-dialogue-behavior&#34;&gt;3. &lt;strong&gt;Simulating Realistic Human Dialogue Behavior&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;In practical applications such as customer service systems, educational assistants, and health consultations, user questions often unfold gradually rather than being expressed clearly in a single instance. Preserving dialogue history allows the AI to behave more like a “memoryful assistant.”&lt;/p&gt;
&lt;h2 id=&#34;how-to-correctly-add-historical-conversations-in-an-api&#34;&gt;How to Correctly Add Historical Conversations in an API?
&lt;/h2&gt;&lt;p&gt;Using OpenAI&amp;rsquo;s API as an example, we recommend following the structure below:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;messages = [
    {&amp;quot;role&amp;quot;: &amp;quot;system&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;You are a professional legal assistant&amp;quot;},
    {&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;What are the essential conditions for a contract?&amp;quot;},
    {&amp;quot;role&amp;quot;: &amp;quot;assistant&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;Contract validity requires fulfilling several conditions: ...&amp;quot;},
    {&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;Does an oral agreement count?&amp;quot;}
]

response = openai.ChatCompletion.create(
    model=&amp;quot;gpt-4&amp;quot;,
    messages=messages
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Use the &lt;code&gt;system&lt;/code&gt; message to set the model&amp;rsquo;s behavior and identity.&lt;/li&gt;
&lt;li&gt;Only retain recent key conversations, not necessarily the entire history (to avoid exceeding token limits).&lt;/li&gt;
&lt;li&gt;In long sessions, truncate early content and maintain core information summaries to control token consumption.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;practical-recommendations&#34;&gt;Practical Recommendations
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Dialogue State Management&lt;/strong&gt;: The backend needs to design caching mechanisms to record each user’s conversation history (e.g., Redis, database).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Limit Length&lt;/strong&gt;: OpenAI GPT-4 has a context length of 128k tokens, Claude 3 can reach 200k~1M, and requires reasonable truncation.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dynamic Summarization of History&lt;/strong&gt;: When the historical content is too long, use a model to first summarize the old conversations before adding them to the dialogue context.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;summary&#34;&gt;Summary
&lt;/h2&gt;&lt;p&gt;AI large model capabilities are powerful, but developers need to “feed” it sufficient contextual information. &lt;strong&gt;By adding historical conversations within API requests, not only can the quality and coherence of the model’s output be significantly improved, but users can also experience a more natural and realistic conversation.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Regardless of whether you&amp;rsquo;re building AI customer service, writing assistants, coding helpers, or educational applications, this is an optimization technique that cannot be ignored.&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
