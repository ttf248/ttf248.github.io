<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Docker on Uncle Xiang&#39;s Notebook</title>
        <link>https://ttf248.life/en/tags/docker/</link>
        <description>Recent content in Docker on Uncle Xiang&#39;s Notebook</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en</language>
        <lastBuildDate>Mon, 02 Jun 2025 19:00:25 +0800</lastBuildDate><atom:link href="https://ttf248.life/en/tags/docker/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>AI Development Over Two Years: Somewhat similar to the state before a Docker release – fragmented and rapidly evolving.</title>
        <link>https://ttf248.life/en/p/ai-development-two-years-like-docker-pre-release/</link>
        <pubDate>Thu, 20 Feb 2025 18:16:37 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/ai-development-two-years-like-docker-pre-release/</guid>
        <description>&lt;p&gt;Artificial Intelligence (AI) has undoubtedly been one of the most hotly debated topics in the technology field in recent years, particularly over the past two years, with AI technology experiencing rapid advancements. Whether it’s deep learning, natural language processing, or computer vision and automated decision systems, the application scenarios for AI are constantly emerging. However, despite continuous technological breakthroughs, AI still faces a bottleneck similar to that of a Docker release – a lack of a killer app to truly ignite the market.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Prompt: Write an article: AI has developed for two years, which is somewhat like the state of Docker’s pre-release, lacking a killer application, based on existing technologies, create a perfect landing scenario, Docker doesn&amp;rsquo;t use too many new technologies, but the overall solution is reasonable, and it changes the workflow of operations and development.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;ai-development-status-the-technology-is-mature-but-applications-still-need-breakthroughs&#34;&gt;AI Development Status: The Technology is Mature, but Applications Still Need Breakthroughs
&lt;/h2&gt;&lt;p&gt;From a technical perspective, AI has made significant progress in the past two years. Whether it’s OpenAI&amp;rsquo;s GPT series models or Google&amp;rsquo;s BERT and DeepMind&amp;rsquo;s Alpha series, AI processing capabilities have far exceeded previous expectations. Particularly in the field of natural language processing, models like GPT-4 not only possess powerful generative abilities but also demonstrate astonishing performance in understanding and reasoning.&lt;/p&gt;
&lt;p&gt;However, despite technological advancements, AI’s implementation in practical applications faces certain challenges. Similar to Docker&amp;rsquo;s state before its release, although AI has enormous potential, there isn&amp;rsquo;t yet a killer application that can be widely adopted and transform industries. Everyone is talking about the future of AI, but it may not always be possible to find an application scenario that can directly bring about revolutionary change.&lt;/p&gt;
&lt;h2 id=&#34;docker-and-ai-parallels-technology-doesnt-have-to-be-innovation-its-about-the-solution&#34;&gt;Docker and AI Parallels: Technology Doesn&amp;rsquo;t Have to Be Innovation, It’s About the Solution
&lt;/h2&gt;&lt;p&gt;Looking back at the history leading up to Docker’s release, we can readily see many similarities between the technological environment at that time and the current state of AI development. Before Docker, container technology wasn’t a new concept; early technologies like LXC (Linux Containers) and virtualization already possessed the basic capabilities for containerization. However, Docker didn&amp;rsquo;t introduce disruptive technology itself. Instead, it presented a simpler, more intuitive, and efficient solution through the clever integration and optimization of existing technologies. This solution didn’t bring about revolutionary technology, but it addressed many pain points in operations and development processes, dramatically simplifying software deployment, scaling, and management workflows.&lt;/p&gt;
&lt;p&gt;Similarly, the AI field is facing a similar situation. Currently, AI technology is no longer a “new thing,” but to truly achieve large-scale applications, it still requires a perfect landing scenario. Like Docker, integrating and optimizing existing technologies to form a reasonable application solution. A killer AI application may not rely on groundbreaking technological breakthroughs, but rather how to integrate existing technologies to solve pain points and needs in actual business operations.&lt;/p&gt;
&lt;h2 id=&#34;how-to-find-ais-docker-moment&#34;&gt;How to Find AI&amp;rsquo;s &amp;ldquo;Docker Moment&amp;rdquo;?
&lt;/h2&gt;&lt;p&gt;To truly enable the widespread adoption of AI technology, we need to focus on several key areas:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Deep Exploration of Real-World Scenarios&lt;/strong&gt;
Currently, many AI applications are still primarily experimental in nature and lack large-scale real-world implementation. Areas such as AI customer service and intelligent recommendations are widely used, but their functionality is still limited and has not yet broken through industry bottlenecks. True breakthroughs may come from industries that have been plagued by traditional methods for a long time, such as healthcare, manufacturing, and logistics. AI can help these companies improve efficiency and reduce costs in complex scenarios through more efficient data processing and predictive analytics.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Productization and Ease of Use&lt;/strong&gt;
Similar to how Docker simplifies containerization processes to improve operations efficiency, the ease of use of AI products is equally crucial. AI’s proliferation isn&amp;rsquo;t just about the spread of technology; it’s also about the proliferation of its products. Integrating AI into daily workflows, allowing users to easily utilize these tools without needing to deeply understand the underlying technology, is a crucial step in AI adoption.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Ecosystem Building &amp;amp; Standardization&lt;/strong&gt;
Any new technology’s widespread application relies on ecosystem building. Docker’s rapid rise is due to its openness and compatibility, enabling developers to seamlessly connect with various cloud platforms, tools, and services. Similarly, AI&amp;rsquo;s future depends on the construction of an ecosystem. The standardization of AI, model sharing, data openness, and technological integration will all influence whether AI can form widespread industry applications.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;conclusion-the-future-of-ai-is-full-of-possibilities-but-requires-more-refined-implementation&#34;&gt;Conclusion: The Future of AI is Full of Possibilities, but Requires More Refined Implementation
&lt;/h2&gt;&lt;p&gt;Despite the significant advancements in AI technology over the past two years, it remains in the stage of “lacking killer applications” at present. Similar to containerization technology before Docker’s release, AI needs a reasonable application scenario to deeply integrate existing technologies with business requirements in order to truly achieve large-scale adoption and widespread use. While technological innovation is important, solutions that can simplify processes and improve efficiency are more likely to drive the popularization and development of the technology.&lt;/p&gt;
&lt;p&gt;In the future, AI may evolve like Docker – not through disruptive technological breakthroughs, but by integrating existing technologies to create a perfect application scenario, ultimately changing the way we work and live.&lt;/p&gt;</description>
        </item>
        <item>
        <title>Docker Domestic Mirror Proxy Failed</title>
        <link>https://ttf248.life/en/p/docker-domestic-image-proxy-failure/</link>
        <pubDate>Sat, 04 Jan 2025 18:29:25 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/docker-domestic-image-proxy-failure/</guid>
        <description>&lt;p&gt;Deploying Docker on domestic servers, after deployment, if the company doesn’t provide a registry center, the first thing developers need to do is configure a domestic registry acceleration address. It&amp;rsquo;s lucky that today there was a server configured with a registry acceleration address, but when pulling images, it kept failing to pull.&lt;/p&gt;
&lt;p&gt;Error response from daemon: “https://registry-1.docker.io/v2/”: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)&lt;/p&gt;
&lt;!-- 20250106 – Two days after, all servers were restored, and this didn’t even make the news! All domestic registry proxies went down --&gt;
&lt;h2 id=&#34;troubleshooting-and-repair-attempts&#34;&gt;Troubleshooting and Repair Attempts
&lt;/h2&gt;&lt;p&gt;Initially, we attempted to switch to alternative mirror acceleration addresses, hoping to resolve the issue. However, contrary to our expectations, the problem persisted.&lt;/p&gt;
&lt;p&gt;Subsequently, we began modifying the local DNS configuration, attempting to find a breakthrough at the network resolution level. Unfortunately, after some debugging, the fault remained.&lt;/p&gt;
&lt;p&gt;At this point, the stability of the local network was heavily questioned, so we decisively switched to a mobile hotspot, attempting to bypass potential local network faults, but the results were discouraging – there was no sign of improvement.&lt;/p&gt;
&lt;h2 id=&#34;problem-propagation&#34;&gt;Problem Propagation
&lt;/h2&gt;&lt;p&gt;We currently have &lt;strong&gt;a few servers deployed domestically&lt;/strong&gt; with Docker environments, and all of them failed to successfully pull the image. We initially hoped to find an alternative solution, but we found that they all consistently failed with identical error messages, indicating that the issue isn&amp;rsquo;t isolated to a single device.&lt;/p&gt;
&lt;p&gt;Further investigation revealed that the image proxy seemingly malfunctioned instantaneously. In this critical moment, we quickly switched to a machine outside of the country, and thankfully, image pulls were restored at this location. This suggests that the problem is most likely related to network links or configurations within China.&lt;/p&gt;
&lt;h2 id=&#34;strategy-adjustment-circumventing-the-issue&#34;&gt;Strategy Adjustment: Circumventing the Issue
&lt;/h2&gt;&lt;p&gt;Given that direct image pulling methods within China have been heavily restricted while foreign mirrors remain accessible, in order to expedite project progress, we’ve decided to employ a circumvention tactic. Initially, we switched to a foreign server to successfully pull the required images, subsequently pushing them to domestic mirror repositories to establish a “data bridge.”&lt;/p&gt;
&lt;p&gt;At the same time, we synchronized modifications to the Dockerfile files, replacing image addresses with those adapted for the Chinese environment and then rebuilt the images, ultimately achieving successful deployment.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Office migration, the servers are inaccessible.</title>
        <link>https://ttf248.life/en/p/office-move-server-inaccessible/</link>
        <pubDate>Sat, 11 Mar 2023 01:42:05 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/office-move-server-inaccessible/</guid>
        <description>&lt;p&gt;Administrative notice, office relocation from the second floor to the fifteenth floor – a standard, routine move of an individual workstation.&lt;/p&gt;
&lt;h2 id=&#34;design-sense&#34;&gt;Design Sense
&lt;/h2&gt;&lt;p&gt;&lt;img src=&#34;https://ttf248.life/p/office-move-server-inaccessible/20230311014537.png&#34;
	width=&#34;511&#34;
	height=&#34;916&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;Office Building&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;55&#34;
		data-flex-basis=&#34;133px&#34;
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;migration&#34;&gt;Migration
&lt;/h2&gt;&lt;p&gt;Closing up shop, packing everything away, a familiar route, a new workstation – adjusting computer cabling, finding a comfortable posture to start working.
(ÒωÓױ)! – Connecting the network cable, accessing the servers frequently used by the team, but it was inaccessible. I tried switching to wireless networking, and access became normal again. Initially, I thought it was a problem with the server’s IP address settings, the wired network at the new workstation wasn&amp;rsquo;t included in the firewall configuration, so adjusting it solved the issue; this subnet isn’t just one server – when trying to access other servers, they were all working normally. Gradually, confusion arose? Let professional people handle professional matters, and finally, a colleague from the operations department pinpointed the cause: This server was deployed with &lt;code&gt;docker&lt;/code&gt;, and the default network of the &lt;code&gt;docker0&lt;/code&gt; service conflicted with the wired network configuration of the office, causing data packets sent to it not to receive responses and being routed to the &lt;code&gt;docker&lt;/code&gt; service. Other servers didn’t deploy &lt;code&gt;docker&lt;/code&gt; services, so I use this one more frequently. Occasionally, I use containers to deploy some test services, and I didn&amp;rsquo;t expect to encounter this scenario. Looking back, considering the entire group is located in a single office building, it’s not strange that IT colleagues divided network segments using addresses starting with &lt;code&gt;172&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&#34;docker0&#34;&gt;docker0
&lt;/h2&gt;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# vim /etc/docker/daemon.json
{
    &amp;quot;bip&amp;quot;:&amp;quot;172.200.0.1/24&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Restart the service and switch to the new network; the server recovers normal access.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References
&lt;/h2&gt;&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://yeasy.gitbook.io/docker_practice/advanced_network/docker0&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Docker From Beginner to Advanced - docker0&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Docker Basics, Intermediate, and Advanced</title>
        <link>https://ttf248.life/en/p/docker-two-three-things/</link>
        <pubDate>Thu, 21 Jan 2021 09:26:07 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/docker-two-three-things/</guid>
        <description>&lt;p&gt;Having worked with CentOS for many years, content may not apply to macOS or Ubuntu users in some cases.&lt;/p&gt;
&lt;p&gt;You can refer to the documentation from Tsinghua University for installation guidance: &lt;a class=&#34;link&#34; href=&#34;https://mirrors.tuna.tsinghua.edu.cn/help/docker-ce/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://mirrors.tuna.tsinghua.edu.cn/help/docker-ce/&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;installation&#34;&gt;Installation
&lt;/h2&gt;&lt;p&gt;Due to unknown mysterious forces, installing Docker domestically is recommended to set the cloud vendor&amp;rsquo;s repository address. Here we recommend using &lt;strong&gt;Alibaba Cloud&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;set-repository-source-address&#34;&gt;Set Repository Source Address
&lt;/h3&gt;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;yum install yum-utils device-mapper-persistent-data lvm2 &amp;amp;&amp;amp; \
sudo yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;deploy-the-latest-version&#34;&gt;Deploy the Latest Version
&lt;/h3&gt;&lt;p&gt;Docker is a commonly used background service, we recommend setting it to start on boot. The following command applies to CentOS 7:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;sudo yum install -y docker-ce docker-ce-cli containerd.io &amp;amp;&amp;amp; systemctl enable --now docker
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;deploying-a-specific-version&#34;&gt;Deploying a Specific Version
&lt;/h3&gt;&lt;p&gt;The releases of &lt;code&gt;kubernetes&lt;/code&gt; and &lt;code&gt;docker&lt;/code&gt; are not fully synchronized. If you need to deploy &lt;code&gt;kubernetes&lt;/code&gt; subsequently, refer to the &lt;code&gt;kubernetes&lt;/code&gt; deployment instructions and install a specific version of &lt;code&gt;docker&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;yum list docker-ce --showduplicates | sort -r
sudo yum install -y docker-ce-18.09.2-3.el7 docker-ce-cli-18.09.2-3.el7 containerd.io-18.09.2-3.el7 &amp;amp;&amp;amp; systemctl enable --now docker
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;adding-docker-permissions-for-regular-users&#34;&gt;Adding Docker Permissions for Regular Users
&lt;/h3&gt;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;sudo usermod -aG docker ${USER}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;uninstall&#34;&gt;Uninstall
&lt;/h3&gt;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;sudo yum erase -y docker-ce docker-ce-cli containerd.io
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;everyday-use&#34;&gt;Everyday Use
&lt;/h2&gt;&lt;h3 id=&#34;mirror-acceleration&#34;&gt;Mirror Acceleration
&lt;/h3&gt;&lt;p&gt;There’s still an unknown mysterious force that causes slow image pulls. At this time, domestic cloud vendors have emerged and provided many acceleration services, which are still recommended – &lt;strong&gt;Alibaba Cloud&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The acceleration addresses can be managed by you registering an Alibaba Cloud account; this service is free. Alibaba Cloud also offers a free image build service.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cat &amp;gt; /etc/docker/daemon.json &amp;lt;&amp;lt;EOF
{
  &amp;quot;registry-mirrors&amp;quot;: [
    &amp;quot;https://docker.nju.edu.cn&amp;quot;,
    &amp;quot;https://mirror.baidubce.com&amp;quot;,
    &amp;quot;https://docker.m.daocloud.io&amp;quot;,
    &amp;quot;https://docker.mirrors.sjtug.sjtu.edu.cn&amp;quot;
  ]
}
EOF
systemctl daemon-reload &amp;amp;&amp;amp; \
systemctl restart docker
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;recommended-control-panels&#34;&gt;Recommended Control Panels
&lt;/h3&gt;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;docker volume create portainer_data &amp;amp;&amp;amp; \
docker run -d --name=portainer --restart=always -p 9000:9000 -v /var/run/docker.sock:/var/run/docker.sock -v portainer_data:/data portainer/portainer-ce:2.20.3-alpine
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;frequently-used-image-pull-list&#34;&gt;Frequently Used Image Pull List
&lt;/h3&gt;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;docker pull rancher/rancher:stable &amp;amp;&amp;amp; docker pull portainer/portainer-ce:2.0.1 &amp;amp;&amp;amp; \
docker pull centos:7 &amp;amp;&amp;amp; docker pull ubuntu:20.04 &amp;amp;&amp;amp; docker pull ubuntu:18.04 &amp;amp;&amp;amp; \
docker pull redis:5 &amp;amp;&amp;amp; docker pull redis:6 &amp;amp;&amp;amp; \
docker pull alpine:3.11 &amp;amp;&amp;amp; docker pull busybox:1.32 &amp;amp;&amp;amp; \
docker pull rabbitmq:3.7-management &amp;amp;&amp;amp; \
docker pull mariadb:10.2 &amp;amp;&amp;amp; \
docker pull nginx:1.18 &amp;amp;&amp;amp; docker pull nginx:1.19 &amp;amp;&amp;amp; \
docker pull mysql:5.6 &amp;amp;&amp;amp; docker pull mysql:8 &amp;amp;&amp;amp; \
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;frequently-used-image-pull-list-1&#34;&gt;Frequently Used Image Pull List
&lt;/h3&gt;&lt;p&gt;docker pull mysql:5.6 &amp;amp;&amp;amp; docker pull mysql:8 &amp;amp;&amp;amp; &lt;br&gt;
docker pull elasticsearch:6.8.11 &amp;amp;&amp;amp; docker pull logstash:6.8.11 &amp;amp;&amp;amp; docker pull kibana:6.8.11 &amp;amp;&amp;amp; &lt;br&gt;
docker pull zookeeper:3.4 &amp;amp;&amp;amp; &lt;br&gt;
docker pull influxdb:1.7 &amp;amp;&amp;amp; docker pull grafana/grafana:7.3.1 &amp;amp;&amp;amp; &lt;br&gt;
docker pull percona:8 &amp;amp;&amp;amp; docker pull percona:5.6 &amp;amp;&amp;amp; &lt;br&gt;
docker pull cloverzrg/frps-docker:0.34.3 &amp;amp;&amp;amp; docker pull cloverzrg/frpc-docker:0.34.3&lt;/p&gt;
&lt;h3 id=&#34;common-command-combinations&#34;&gt;Common Command Combinations
&lt;/h3&gt;&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://docs.docker.com/engine/reference/commandline/docker/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://docs.docker.com/engine/reference/commandline/docker/&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;View container running status, append the &lt;code&gt;format&lt;/code&gt; parameter to view detailed container information, without focusing on image information:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;docker ps --format &amp;quot;{{.Names}}: {{.Ports}}: {{.Size}}&amp;quot;
#portainer: 0.0.0.0:8000-&amp;gt;8000/tcp, 0.0.0.0:9000-&amp;gt;9000/tcp: 0B (virtual 172MB)
#influxdb: 0.0.0.0:8086-&amp;gt;8086/tcp: 183B (virtual 311MB)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;p&gt;Stop all containers with a single command:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;docker stop $(docker ps -a -q)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Delete all images with a single command:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;dokcer rmi $(docker images -a -q)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;p&gt;Export image&lt;/p&gt;
&lt;h3 id=&#34;common-combination-commands&#34;&gt;Common Combination Commands
&lt;/h3&gt;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;docker save &amp;lt;IMAGE NAME&amp;gt;:&amp;lt;IMAGE TAG&amp;gt; &amp;gt; -o XXX.tar
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Export image and compress&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;docker save &amp;lt;IMAGE NAME&amp;gt;:&amp;lt;IMAGE TAG&amp;gt; | gzip &amp;gt; XXX.tar
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Import image&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;docker load -i XXX.tar
&lt;/code&gt;&lt;/pre&gt;
</description>
        </item>
        <item>
        <title>Linux Setup JMeter Test Environment</title>
        <link>https://ttf248.life/en/p/linux-setup-jmeter-testing-environment/</link>
        <pubDate>Tue, 22 Dec 2020 10:12:50 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/linux-setup-jmeter-testing-environment/</guid>
        <description>&lt;p&gt;The author has a strong interest in hardware and used JMeter to conduct load testing, documenting the process of deploying JMeter, InfluxDB, and Grafana on CentOS 7. They shared installation and command usage for JMeter, InfluxDB’s features and Docker installation method, as well as simple deployment and configuration for Grafana. They summarized experience and references related to high-performance programming patterns.&lt;/p&gt;
&lt;h2 id=&#34;background&#34;&gt;Background
&lt;/h2&gt;&lt;p&gt;As widely known, I have a very strong interest in hardware. By chance, the test team was using &lt;code&gt;JMeter&lt;/code&gt; to perform load tests and discovered that performance wasn&amp;rsquo;t improving. As a curious individual, I decisively took action to see how the company conducted its testing. There’s also a small story: at some point in the distant past, I read a post on OpenChina about how to create more impressive-looking performance test graphs – after observing &lt;code&gt;Windows&lt;/code&gt; versions execute tests and achieving visualized &lt;code&gt;TPS&lt;/code&gt; data display, what&amp;rsquo;s the use of configuring a web panel?&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Thinking is all well and good, but you have to try it yourself to understand.
Don’t use GUI mode for load testing! only for Test creation and Test debugging.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;background-1&#34;&gt;Background
&lt;/h2&gt;&lt;p&gt;Officially, it’s recommended to obtain test reports via the command line and display them using a GUI, which introduces data errors.  I don&amp;rsquo;t have deep knowledge of JMeter – at least I found a reason to tinker with a &lt;code&gt;Linux&lt;/code&gt; version console panel. The openchinese post’s core component deployment isn’t friendly; you need to follow their WeChat channel to download the required files, and as a millennial, of course I used &lt;code&gt;Docker&lt;/code&gt; instead. Basically, my server is located domestically, and accessing the overseas source addresses is very slow – at least using an image service, Alibaba Cloud has a free acceleration.&lt;/p&gt;
&lt;p&gt;Regarding &lt;code&gt;docker&lt;/code&gt; installation and deployment, this will not be elaborated on here; please refer to previous articles for recommendations.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;The following content focuses on two main areas: setting up the basic test environment components and a simple explanation of each component.&lt;/p&gt;
&lt;h2 id=&#34;jmeter&#34;&gt;JMeter
&lt;/h2&gt;&lt;p&gt;Apache JMeter is a Java-based load testing tool developed by the Apache Software Foundation. It’s used to perform stress tests on software, initially designed for web application testing but later expanded to other testing domains. It can be used to test static and dynamic resources such as static files, Java microservices, CGI scripts, Java objects, databases, FTP servers, etc. JMeter can simulate massive loads from various stress categories onto servers, networks, or objects to test their strength and analyze overall performance. Furthermore, JMeter can perform functional/regression testing on applications by creating scripts with assertions to validate that your program returns the expected results. To maximize flexibility, JMeter allows using regular expressions to create assertions. Apache JMeter can be used to perform performance testing of static and dynamic resources (files, Servlets, Perl scripts, Java objects, databases and queries, FTP servers, etc.). It can be used to simulate heavy loads on servers, networks, or objects to test their strength or analyze overall performance under different stress types. You can use it for graphical analysis of performance metrics or for large concurrent load testing of your server/script/object.&lt;/p&gt;
&lt;h3 id=&#34;jmeter-deployment-on-centos7&#34;&gt;Jmeter Deployment on CentOS7
&lt;/h3&gt;&lt;p&gt;Install the &lt;code&gt;JDK&lt;/code&gt; runtime environment, download the &lt;code&gt;Jmeter&lt;/code&gt; installation package:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;yum install java-1.8.0-openjdk -y &amp;amp;&amp;amp; \
wget https://mirrors.bfsu.edu.cn/apache//jmeter/binaries/apache-jmeter-5.4.tgz &amp;amp;&amp;amp; tar -xf apache-jmeter-5.4.tgz
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Configure environment variables:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;export JMETER_HOME=$HOME/jmeter/apache-jmeter-5.4
export PATH=$JMETER_HOME/bin:$PATH
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;jmeter-commands&#34;&gt;JMeter Commands
&lt;/h3&gt;&lt;p&gt;Finally, it will be connected to the &lt;code&gt;Grafana&lt;/code&gt; dashboard, and you don&amp;rsquo;t need to input the &lt;code&gt;-l&lt;/code&gt; parameter to observe data in the &lt;code&gt;web&lt;/code&gt; console.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;jmeter -n -t /tmp/order-500-10s.jmx -l /tmp/jmeter-order-report-20200109/order-500-10s.jtl
# Generally, don&#39;t use test results and test reports to simplify the command
jmeter -n -t /tmp/order-500-10s.jmx
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;influxdb&#34;&gt;InfluxDB
&lt;/h2&gt;&lt;p&gt;InfluxDB is an open-source distributed time series database written in Go. It requires no external dependencies. The database is now primarily used for storing large volumes of timestamped data such as DevOps monitoring data, app metrics, IoT sensor data, and real-time analytics data.&lt;/p&gt;
&lt;h3 id=&#34;influxdb-features&#34;&gt;InfluxDB Features
&lt;/h3&gt;&lt;p&gt;InfluxDB’s features can be summarized into the following 9 aspects:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Schema-less (Schemaless):&lt;/strong&gt; Can accommodate an arbitrary number of columns;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Metric Retention Time Setting:&lt;/strong&gt;  Allows setting retention times for metrics;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Support for Time-Related Functions:&lt;/strong&gt; Supports functions related to time (such as min, max, sum, count, mean, median, etc.) for convenient statistical analysis;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Storage Policy Support:&lt;/strong&gt; Can be used for data deletion and modification (InfluxDB does not provide direct methods for deleting or modifying data);&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Continuous Query Support:&lt;/strong&gt;  Automatically scheduled sets of queries that run continuously, combined with storage policies to reduce InfluxDB’s system footprint;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Native HTTP Support:&lt;/strong&gt; Built-in HTTP API;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Support for Similar SQL Syntax:&lt;/strong&gt; Supports a syntax similar to SQL;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Support for Setting Replica Count in Clusters:&lt;/strong&gt; Allows setting the number of replicas for data within clusters;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Support for Periodic Data Sampling:&lt;/strong&gt;  Allows sampling data periodically and writing it to another measurement, facilitating granular data storage.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;influxdb-docker-installation&#34;&gt;InfluxDB Docker Installation
&lt;/h3&gt;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;mkdir influxdb &amp;amp;&amp;amp; cd influxdb &amp;amp;&amp;amp; \
docker run -p 8086:8086 -d --name influxdb -v $PWD:/var/lib/influxdb influxdb:1.7
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;docker exec -it influxdb /bin/bash&lt;/code&gt; enters the container, executes commands, and manually creates a database&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;root@bce0a55bbc72:/# influx
Connected to http://localhost:8086 version 1.7.10
InfluxDB shell version: 1.7.10
&amp;gt; Execute commands in the interactive shell
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;influxdb-database-and-user-creation&#34;&gt;InfluxDB Database and User Creation
&lt;/h3&gt;&lt;p&gt;Create database: &lt;code&gt;create database jmeter_t2&lt;/code&gt;
View databases: &lt;code&gt;show databases&lt;/code&gt;
Switch to database: &lt;code&gt;use jmeter_t2&lt;/code&gt;
Create user: &lt;code&gt;create user &amp;quot;admin&amp;quot; with password &#39;admin&#39; with all privileges&lt;/code&gt;
View users: &lt;code&gt;show users&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;&amp;gt; show users
user  admin
----  -----
admin true
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If the user permissions for &lt;code&gt;admin&lt;/code&gt; are displayed as &lt;code&gt;true&lt;/code&gt;, the database setup is complete.&lt;/p&gt;
&lt;h2 id=&#34;grafana&#34;&gt;Grafana
&lt;/h2&gt;&lt;p&gt;When writing test cases, it was found that the effect of chart visualization is not very necessary; the &lt;code&gt;tps&lt;/code&gt; data from the interface can be observed when executed in the command line, and more importantly, we wanted to know the internal timing of the program.&lt;/p&gt;
&lt;p&gt;A simple deployment of the &lt;code&gt;grafana&lt;/code&gt; console panel and importing a configuration file to connect with &lt;code&gt;InfluxDB&lt;/code&gt; was performed. The console supports filtering test results through tags; generally, only one &lt;code&gt;InfluxDB&lt;/code&gt; database needs to be configured:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Application Name&lt;/li&gt;
&lt;li&gt;Test Case Name&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://ttf248.life/p/linux-setup-jmeter-testing-environment/Snipaste_2021-03-09_19-44-22.png&#34;
	width=&#34;861&#34;
	height=&#34;357&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;grafana&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;241&#34;
		data-flex-basis=&#34;578px&#34;
	
&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;docker run -d --name=grafana -p 3000:3000 grafana/grafana:7.3.1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Due to the sampling interval in the web version, the calculated &lt;code&gt;TPS&lt;/code&gt; and related values do not match the aggregated report from &lt;code&gt;JMeter&lt;/code&gt;. Please refer to this link: &lt;a class=&#34;link&#34; href=&#34;https://www.vinsguru.com/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://www.vinsguru.com/jmeter-real-time-results-influxdb-grafana/&lt;/a&gt; ## Grafana&lt;/p&gt;
&lt;p&gt;When writing test cases, it was found that the chart visualization effect is not very necessary; the &lt;code&gt;tps&lt;/code&gt; data from the interface can be observed when executed in the command line, and more importantly, we wanted to know the internal timing of the program.&lt;/p&gt;
&lt;p&gt;A simple deployment of the &lt;code&gt;grafana&lt;/code&gt; console panel and importing a configuration file to connect with &lt;code&gt;InfluxDB&lt;/code&gt; was performed. The console supports filtering test results through tags; generally, only one &lt;code&gt;InfluxDB&lt;/code&gt; database needs to be configured:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Application Name&lt;/li&gt;
&lt;li&gt;Test Case Name&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://ttf248.life/p/linux-setup-jmeter-testing-environment/Snipaste_2021-03-09_19-44-22.png&#34;
	width=&#34;861&#34;
	height=&#34;357&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;grafana&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;241&#34;
		data-flex-basis=&#34;578px&#34;
	
&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;docker run -d --name=grafana -p 3000:3000 grafana/grafana:7.3.1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Due to the sampling interval in the web version, the calculated &lt;code&gt;TPS&lt;/code&gt; and related values do not match the &lt;code&gt;JMeter&lt;/code&gt; aggregated report. Please refer to this link: &lt;a class=&#34;link&#34; href=&#34;https://www.vinsguru.com/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://www.vinsguru.com/jmeter-real-time-results-influxdb-grafana/&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;epilogue&#34;&gt;Epilogue
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;High-performance program patterns invariably are one-loop thread; any locks, enqueueing, and dequeueing will cause unnecessary performance loss.&lt;/li&gt;
&lt;li&gt;The time spent on core business logic is greater than the time spent introducing other code; concurrency can effectively improve efficiency only when the core latency is sufficiently large; otherwise, it’s best to be cautious about introducing other code.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;references&#34;&gt;References
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://my.oschina.net/u/4617935/blog/4680856&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;JMeter Series - JMeter + Grafana + InfluxDB Real-time Monitoring&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://hub.docker.com/_/influxdb&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;InfluxDB Official Image&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://hub.docker.com/r/grafana/grafana&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Grafana Official Image&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://jmeter.apache.org/download_jmeter.cgi&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;JMeter Website&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://medium.com/@jasonli.studio/to-install-apache-jmeter-in-centos7-294bc72a97ba&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;To Install Apache JMeter in CentOS7&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        
    </channel>
</rss>
