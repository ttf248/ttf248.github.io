<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Docker on Uncle Xiang&#39;s Notebook</title>
        <link>https://ttf248.life/en/tags/docker/</link>
        <description>Recent content in Docker on Uncle Xiang&#39;s Notebook</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en</language>
        <lastBuildDate>Sun, 25 May 2025 02:57:45 +0800</lastBuildDate><atom:link href="https://ttf248.life/en/tags/docker/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>Two years of AI development: somewhat similar to the state before Docker was released</title>
        <link>https://ttf248.life/en/p/ai-development-two-years-docker-pre-release/</link>
        <pubDate>Thu, 20 Feb 2025 18:16:37 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/ai-development-two-years-docker-pre-release/</guid>
        <description>&lt;p&gt;Artificial intelligence (AI) has undoubtedly been one of the most discussed topics in the technology field in recent years, especially with the rapid advancements in AI technology over the past two years. From deep learning and natural language processing to computer vision and automated decision-making systems, applications of AI are constantly emerging. However, despite continuous technological breakthroughs, AI still faces a bottleneck similar to that before Docker&amp;rsquo;s release – a lack of a killer application to truly ignite the market.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The development of AI over the past two years is somewhat similar to the state before Docker was released – it lacks a killer application. Based on existing technologies, we need to create a perfect practical scenario. Docker didn&amp;rsquo;t rely heavily on new technologies, but the entire solution was very reasonable and changed the workflows for operations and development.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;the-current-state-of-ai-development-technology-is-mature-but-application-still-needs-breakthroughs&#34;&gt;The current state of AI development: technology is mature, but application still needs breakthroughs
&lt;/h2&gt;&lt;p&gt;From a technical perspective, AI has made considerable progress in the past two years. Whether it&amp;rsquo;s OpenAI&amp;rsquo;s GPT series models or Google&amp;rsquo;s BERT and DeepMind&amp;rsquo;s Alpha series, AI&amp;rsquo;s processing capabilities have far exceeded previous expectations. Especially in the field of natural language processing, models like GPT-4 not only possess powerful generation capabilities but also demonstrate astonishing performance in understanding and reasoning.&lt;/p&gt;
&lt;p&gt;However, despite the rapid advancement of technology, the practical application of AI faces certain challenges. Similar to the state before Docker was released, although AI has great potential, a truly widespread and industry-transforming killer application hasn&amp;rsquo;t yet emerged. People are talking about the prospects of AI, but may not be able to find an application scenario that can directly bring revolutionary change. Many AI applications are still in the initial trial stage, and most require further integration and optimization.&lt;/p&gt;
&lt;h2 id=&#34;the-similarity-between-docker-and-ai-technology-isnt-necessarily-innovation-solutions-are-key&#34;&gt;The similarity between Docker and AI: Technology isn&amp;rsquo;t necessarily innovation, solutions are key
&lt;/h2&gt;&lt;p&gt;Looking back at the history before Docker&amp;rsquo;s release, we can easily see that the technological environment and the current state of AI development share many similarities. Before Docker was released, container technology wasn&amp;rsquo;t a new concept; early LXC (Linux Containers) and virtualization technologies already possessed basic containerization capabilities. However, Docker cleverly integrated and optimized existing technologies to propose a simpler, more intuitive, and efficient solution. This approach didn’t introduce any groundbreaking technologies but solved many pain points in operations and development processes, greatly simplifying the deployment, scaling, and management of software.&lt;/p&gt;
&lt;p&gt;Similarly, the AI field faces similar circumstances. While current AI technology is no longer a &amp;ldquo;novelty,&amp;rdquo; achieving truly large-scale applications still requires a perfect landing scenario – like Docker – to integrate and optimize existing technologies into a reasonable application plan. The killer application of AI may not depend on entirely new technological breakthroughs, but rather on how to solve real business pain points and needs by integrating existing technologies.&lt;/p&gt;
&lt;h2 id=&#34;how-to-find-ais-docker-moment&#34;&gt;How to find AI&amp;rsquo;s &amp;ldquo;Docker moment&amp;rdquo;?
&lt;/h2&gt;&lt;p&gt;To truly achieve widespread application of AI technology, efforts need to be made in several areas:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;In-depth exploration of real-world scenarios
Currently, many AI applications are still experimental in nature and lack large-scale practical implementation. For example, while AI customer service and intelligent recommendation fields have widespread applications, their functions still have many limitations and haven&amp;rsquo;t yet overcome industry bottlenecks. Real breakthroughs may come from industries long troubled by traditional methods, such as healthcare, manufacturing, and logistics, where AI can help businesses improve efficiency and reduce costs through more efficient data processing and predictive analysis in these complex scenarios.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Productization and Usability
Just as Docker improves operational efficiency by simplifying the containerization process, the usability of AI products is equally crucial. The popularization of AI isn&amp;rsquo;t just about the spread of technology; it’s about the popularization of its productization. Integrating AI into daily workflows and allowing users to easily use these tools without needing a deep understanding of the underlying technology is an important step in implementing AI.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Ecological Construction and Standardization
The widespread adoption of any new technology is inseparable from ecosystem development. Docker&amp;rsquo;s rapid rise is precisely due to its openness and compatibility, allowing developers to easily connect with various cloud platforms, tools, and services. Similarly, the future of AI depends on building an ecosystem. The standardization of AI, model sharing, data openness, and technical integrability will all influence whether AI can form widespread industry applications.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;conclusion-the-future-of-ai-is-full-of-possibilities-but-a-more-complete-landing-plan-is-still-needed&#34;&gt;Conclusion: The future of AI is full of possibilities, but a more complete landing plan is still needed
&lt;/h2&gt;&lt;p&gt;Although AI technology has made significant progress in the past two years, it is still in a stage of “lacking a killer application.” Similar to containerization technology before Docker’s release, AI needs a reasonable application scenario that deeply integrates existing technologies with business needs in order to truly achieve large-scale application and popularization. While technological innovation is important, solutions that simplify processes and improve efficiency are better at driving the adoption and development of technology.&lt;/p&gt;
&lt;p&gt;In the future, AI may be like Docker—not through revolutionary technological breakthroughs, but by integrating existing technologies to create a perfect application scenario, ultimately changing the way we work and live&lt;/p&gt;</description>
        </item>
        <item>
        <title>Docker domestic image proxy failure</title>
        <link>https://ttf248.life/en/p/docker-domestic-mirror-failure/</link>
        <pubDate>Sat, 04 Jan 2025 18:29:25 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/docker-domestic-mirror-failure/</guid>
        <description>&lt;p&gt;Deploying Docker on domestic servers. If the company doesn&amp;rsquo;t provide an image registry, developers must first configure a domestic mirror acceleration address. Conveniently, I have a server today that has been configured with a mirror acceleration address, but it can’t pull images.&lt;/p&gt;
&lt;p&gt;Error response from daemon: Get &amp;ldquo;&lt;a class=&#34;link&#34; href=&#34;https://registry-1.docker.io/v2/%22&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://registry-1.docker.io/v2/&#34;&lt;/a&gt;: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)&lt;/p&gt;
&lt;p&gt;After two days, all servers have been restored, and surprisingly this didn&amp;rsquo;t trend. All domestic mirror proxies are down.&lt;/p&gt;
&lt;h2 id=&#34;troubleshooting-and-repair-attempts&#34;&gt;Troubleshooting and Repair Attempts
&lt;/h2&gt;&lt;p&gt;Initially, I tried switching to other mirror acceleration addresses hoping to resolve the issue, but it backfired and the problem persists&lt;/p&gt;
&lt;p&gt;Immediately, I began modifying the local DNS configuration, attempting to find a breakthrough at the network resolution level; unfortunately, after some debugging, the fault still persisted&lt;/p&gt;
&lt;p&gt;At this point, the stability of the local network was seriously questioned, so I immediately switched to my phone&amp;rsquo;s hotspot in an attempt to bypass any potential issues with the local network. However, the result was still frustrating; there were no signs of improvement.&lt;/p&gt;
&lt;h2 id=&#34;the-problem-is-spreading&#34;&gt;The problem is spreading
&lt;/h2&gt;&lt;p&gt;We still have several servers deployed domestically, all with Docker environments installed. I tried pulling images on these servers, hoping to find an alternative solution, but without exception, they failed to pull successfully. The error messages were identical across all devices, indicating the problem wasn&amp;rsquo;t limited to a single machine.&lt;/p&gt;
&lt;p&gt;Further investigation revealed that the mirror proxy seemed to have failed instantly. At this critical moment, we quickly switched to an overseas machine for a trial attempt. Fortunately, image pulling resumed here, which suggests the problem likely lies within the domestic network link or related configuration.&lt;/p&gt;
&lt;h2 id=&#34;strategic-adjustment-resolving-issues-indirectly&#34;&gt;Strategic adjustment: resolving issues indirectly
&lt;/h2&gt;&lt;p&gt;Given the numerous obstacles to directly pulling images within China, while foreign mirrors are accessible, in order to expedite project progress, we have decided to adopt a circumspect approach. First, switch to a foreign server to successfully pull the required images, then push them to a domestic mirror repository, thereby establishing a &amp;ldquo;data bridge.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;At the same time, I modified the Dockerfile and replaced the image address with one suitable for domestic environments, then rebuilt the image and successfully deployed it&lt;/p&gt;</description>
        </item>
        <item>
        <title>The office relocation caused server inaccessibility</title>
        <link>https://ttf248.life/en/p/office-migration-server-unavailable/</link>
        <pubDate>Sat, 11 Mar 2023 01:42:05 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/office-migration-server-unavailable/</guid>
        <description>&lt;p&gt;Administrative notice, office relocation. Moving from the original second floor to the fifteenth floor, a routine desk move.&lt;/p&gt;
&lt;h2 id=&#34;sense-of-design&#34;&gt;Sense of design
&lt;/h2&gt;&lt;p&gt;Office building&lt;/p&gt;
&lt;h2 id=&#34;migration&#34;&gt;Migration
&lt;/h2&gt;&lt;p&gt;Packing up and moving on, setting up a new workstation, adjusting computer cables, and starting work in a comfortable position&lt;/p&gt;
&lt;p&gt;Oh no! I connected the network cable, but the server commonly used by our group is inaccessible. I tried switching to a wireless network and it works fine again.&lt;/p&gt;
&lt;p&gt;Initially, I thought it was a server network segment configuration issue. The wired network of the new workstation wasn&amp;rsquo;t on the firewall’s configured list, and contacting IT colleagues to make adjustments should have resolved it. However, this network segment hosts more than one server, and accessing other servers worked fine. This gradually raised my suspicions. Professional matters should be handled by professionals. Eventually, the operations team identified that because this server had &lt;code&gt;docker&lt;/code&gt; deployed, there was a conflict between the default Docker network &lt;code&gt;docker0&lt;/code&gt; and the network segment configured for the office’s wired network. As a result, data packets sent to the server received no response and were routed to the &lt;code&gt;docker&lt;/code&gt; service.&lt;/p&gt;
&lt;p&gt;The other servers don&amp;rsquo;t have &lt;code&gt;docker&lt;/code&gt; deployed, so it’s just this one that I use frequently. I occasionally deploy some test services using containers, and never expected to encounter this situation. Thinking about it more carefully, since the entire group is in the same office building, and the IT department divided network segments using addresses starting with &lt;code&gt;172&lt;/code&gt;, it&amp;rsquo;s not surprising.&lt;/p&gt;
&lt;h2 id=&#34;docker0&#34;&gt;docker0
&lt;/h2&gt;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# vim /etc/docker/daemon.json
{
    &amp;quot;bip&amp;quot;:&amp;quot;172.200.0.1/24&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Restart the service, switch to the new network, and the server will return to normal access&lt;/p&gt;
&lt;h2 id=&#34;reference-materials&#34;&gt;Reference materials
&lt;/h2&gt;&lt;p&gt;Docker from Entry to Practice - docker0&lt;/p&gt;</description>
        </item>
        <item>
        <title>Docker tales</title>
        <link>https://ttf248.life/en/p/docker-two-three-things/</link>
        <pubDate>Thu, 21 Jan 2021 09:26:07 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/docker-two-three-things/</guid>
        <description>&lt;p&gt;Having worked for many years, I&amp;rsquo;ve primarily encountered the &lt;code&gt;CentOS&lt;/code&gt; operating system. If you are a &lt;code&gt;Mac&lt;/code&gt; or &lt;code&gt;Ubuntu&lt;/code&gt; user, some of this content may not be applicable.&lt;/p&gt;
&lt;p&gt;The installation section can refer to the manual from Tsinghua University: [https://mirrors.tuna.tsinghua.edu.cn/help/docker-ce/]&lt;/p&gt;
&lt;h2 id=&#34;installation&#34;&gt;Installation
&lt;/h2&gt;&lt;p&gt;Due to an unknown and mysterious force, it is recommended to use the repository addresses provided by cloud vendors for Docker installations within China. Here, &lt;strong&gt;Alibaba Cloud&lt;/strong&gt; is recommended.&lt;/p&gt;
&lt;h3 id=&#34;set-warehouse-source-address&#34;&gt;Set warehouse source address
&lt;/h3&gt;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;yum install yum-utils device-mapper-persistent-data lvm2 &amp;amp;&amp;amp; \
sudo yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;deploy-the-latest-version&#34;&gt;Deploy the latest version
&lt;/h3&gt;&lt;p&gt;As a commonly used background service, Docker is recommended to be set as a startup item; the current command applies to CentOS 7&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;sudo yum install -y docker-ce docker-ce-cli containerd.io &amp;amp;&amp;amp; systemctl enable --now docker
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;deploy-specified-version&#34;&gt;Deploy specified version
&lt;/h3&gt;&lt;p&gt;The release of &lt;code&gt;kubernetes&lt;/code&gt; and &lt;code&gt;docker&lt;/code&gt; are not fully synchronized. If you need to deploy &lt;code&gt;kubernetes&lt;/code&gt; next, please refer to the &lt;code&gt;kubernetes&lt;/code&gt; deployment instructions and install the specified version of &lt;code&gt;docker&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;yum list docker-ce --showduplicates | sort -r
sudo yum install -y docker-ce-18.09.2-3.el7 docker-ce-cli-18.09.2-3.el7 containerd.io-18.09.2-3.el7 &amp;amp;&amp;amp; systemctl enable --now docker
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;adding-docker-permissions-for-regular-users&#34;&gt;Adding Docker permissions for regular users
&lt;/h3&gt;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;sudo usermod -aG docker ${USER}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;uninstall&#34;&gt;Uninstall
&lt;/h3&gt;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;sudo yum erase -y docker-ce docker-ce-cli containerd.io
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;everyday-use&#34;&gt;Everyday use
&lt;/h2&gt;&lt;h3 id=&#34;mirror-acceleration&#34;&gt;Mirror acceleration
&lt;/h3&gt;&lt;p&gt;There are still unknown mysterious forces causing slow image pulls. At this time, domestic cloud vendors have stepped up and provided many acceleration services. I still recommend &lt;strong&gt;Alibaba Cloud&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;To obtain the accelerated address, please register for an Alibaba Cloud account yourself. This service is free, and Alibaba Cloud also provides a free image building service.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cat &amp;gt; /etc/docker/daemon.json &amp;lt;&amp;lt;EOF
{
  &amp;quot;registry-mirrors&amp;quot;: [
    &amp;quot;https://docker.nju.edu.cn&amp;quot;,
    &amp;quot;https://mirror.baidubce.com&amp;quot;,
    &amp;quot;https://docker.m.daocloud.io&amp;quot;,
    &amp;quot;https://docker.mirrors.sjtug.sjtu.edu.cn&amp;quot;
  ]
}
EOF
systemctl daemon-reload &amp;amp;&amp;amp; \
systemctl restart docker
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;highly-recommended-control-panel&#34;&gt;Highly recommended control panel
&lt;/h3&gt;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;docker volume create portainer_data &amp;amp;&amp;amp; \
docker run -d --name=portainer --restart=always -p 9000:9000 -v /var/run/docker.sock:/var/run/docker.sock -v portainer_data:/data portainer/portainer-ce:2.20.3-alpine
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;commonly-used-image-pull-collection&#34;&gt;Commonly Used Image Pull Collection
&lt;/h3&gt;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;docker pull rancher/rancher:stable &amp;amp;&amp;amp; docker pull  portainer/portainer-ce:2.0.1 &amp;amp;&amp;amp; \
docker pull centos:7 &amp;amp;&amp;amp; docker pull ubuntu:20.04 &amp;amp;&amp;amp; docker pull ubuntu:18.04 &amp;amp;&amp;amp; \
docker pull redis:5 &amp;amp;&amp;amp; docker pull redis:6 &amp;amp;&amp;amp; \
docker pull alpine:3.11 &amp;amp;&amp;amp; docker pull busybox:1.32 &amp;amp;&amp;amp; \
docker pull rabbitmq:3.7-management &amp;amp;&amp;amp; \
docker pull mariadb:10.2 &amp;amp;&amp;amp; \
docker pull nginx:1.18 &amp;amp;&amp;amp; docker pull nginx:1.19 &amp;amp;&amp;amp; \
docker pull mysql:5.6 &amp;amp;&amp;amp; docker pull mysql:8 &amp;amp;&amp;amp; \
docker pull elasticsearch:6.8.11 &amp;amp;&amp;amp; docker pull logstash:6.8.11 &amp;amp;&amp;amp; docker pull kibana:6.8.11 &amp;amp;&amp;amp; \
docker pull zookeeper:3.4 &amp;amp;&amp;amp; \
docker pull influxdb:1.7 &amp;amp;&amp;amp; docker pull grafana/grafana:7.3.1 &amp;amp;&amp;amp; \
docker pull percona:8 &amp;amp;&amp;amp; docker pull percona:5.6 &amp;amp;&amp;amp; \
docker pull cloverzrg/frps-docker:0.34.3 &amp;amp;&amp;amp; docker pull cloverzrg/frpc-docker:0.34.3
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;commonly-used-command-combinations&#34;&gt;Commonly Used Command Combinations
&lt;/h3&gt;&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://docs.docker.com/engine/reference/commandline/docker/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://docs.docker.com/engine/reference/commandline/docker/&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Check the container running status, add the &lt;code&gt;format&lt;/code&gt; parameter to view detailed container information; ignore image information at this time&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;docker ps --format &amp;quot;{{.Names}}: {{.Ports}}: {{.Size}}&amp;quot;
#portainer: 0.0.0.0:8000-&amp;gt;8000/tcp, 0.0.0.0:9000-&amp;gt;9000/tcp: 0B (virtual 172MB)
#influxdb: 0.0.0.0:8086-&amp;gt;8086/tcp: 183B (virtual 311MB)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;p&gt;Stop all containers with one click&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;docker stop $(docker ps -a -q)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Delete all images with one click&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;dokcer rmi $(docker images -a -q)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;p&gt;Export image&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;docker save &amp;lt;IMAGE NAME&amp;gt;:&amp;lt;IMAGE TAG&amp;gt; &amp;gt; -o XXX.tar
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Export image and compress&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;docker save &amp;lt;IMAGE NAME&amp;gt;:&amp;lt;IMAGE TAG&amp;gt; | gzip &amp;gt; XXX.tar
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Import image&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;docker load -i XXX.tar
&lt;/code&gt;&lt;/pre&gt;</description>
        </item>
        <item>
        <title>Setting up a JMeter testing environment on Linux</title>
        <link>https://ttf248.life/en/p/linux-setup-jmeter-testing-environment/</link>
        <pubDate>Tue, 22 Dec 2020 10:12:50 +0800</pubDate>
        
        <guid>https://ttf248.life/en/p/linux-setup-jmeter-testing-environment/</guid>
        <description>&lt;p&gt;The author has a strong interest in hardware and used JMeter to conduct stress testing, documenting the process of deploying JMeter, InfluxDB, and Grafana on CentOS 7. They shared information on JMeter installation and command usage, InfluxDB features and Docker installation methods, as well as simple deployment and configuration of Grafana. The document summarizes experiences and references for high-performance programming patterns.&lt;/p&gt;
&lt;h2 id=&#34;background&#34;&gt;Background
&lt;/h2&gt;&lt;p&gt;As you all know, I have a strong interest in hardware. By chance, the testing team was using JMeter for performance testing and found that the performance wasn&amp;rsquo;t improving. As a curious person, I decided to jump in and see how our company does pressure testing. There’s also a little story here: at some point in the distant past, I saw a post on Open Source China about how to create high-end performance test charts. The testers have already seen the Windows version execute tests and achieve visualized TPS data display. What use would there be for configuring another web panel?&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Whatever you think is right, you won&amp;rsquo;t understand until you try it yourself
Don&amp;rsquo;t use GUI mode for load testing! only for Test creation and Test debuggin.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The official recommendation is to obtain the load test report through the command line and display it with a GUI, which may introduce data errors? I don&amp;rsquo;t have a very deep understanding of JMeter, but at least it gives me a reason to tinker with the &lt;code&gt;Linux&lt;/code&gt; console panel&lt;/p&gt;
&lt;p&gt;The posts on Open Source China indicate that the deployment method for the core components is not user-friendly, and you need to follow a public account to download the required files. As a young generation, of course, I replaced it with &lt;code&gt;Docker&lt;/code&gt;. To put it simply, the servers are still located within mainland China, so accessing cross-border sources is very slow. At least for image services, Alibaba Cloud has a free accelerator.&lt;/p&gt;
&lt;p&gt;Regarding the installation and deployment of &lt;code&gt;docker&lt;/code&gt;, details will not be repeated here; please refer to previous articles for more information&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;The following content is divided into two main parts: building the basic test environment components and a simple explanation of each component&lt;/p&gt;
&lt;h2 id=&#34;jmeter&#34;&gt;Jmeter
&lt;/h2&gt;&lt;p&gt;Apache JMeter is a load testing tool developed by the Apache organization based on Java. It is used for stress testing software and was originally designed for web application testing, but has since been extended to other testing areas. It can be used to test static and dynamic resources such as static files, small Java services, CGI scripts, Java objects, databases, FTP servers, etc. JMeter can simulate a huge load on servers, networks or objects to test their strength under different stress categories and analyze overall performance. In addition, JMeter can perform functional/regression testing of applications by creating scripts with assertions to verify that your program returns the expected results. For maximum flexibility, JMeter allows the use of regular expressions to create assertions.&lt;/p&gt;
&lt;p&gt;Apache JMeter can be used to test the performance of static and dynamic resources (files, Servlets, Perl scripts, Java objects, databases and queries, FTP servers, etc.). It can be used to simulate heavy loads on servers, networks or objects to test their strength or analyze overall performance under different types of stress. You can use it for graphical analysis of performance or for load testing your servers/scripts/objects with high concurrency.&lt;/p&gt;
&lt;h3 id=&#34;jmeter-deployment-on-centos-7&#34;&gt;JMeter deployment on CentOS 7
&lt;/h3&gt;&lt;p&gt;Install the &lt;code&gt;JDK&lt;/code&gt; runtime environment, download the &lt;code&gt;JMeter&lt;/code&gt; installation package&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;yum install java-1.8.0-openjdk -y &amp;amp;&amp;amp; \
wget https://mirrors.bfsu.edu.cn/apache//jmeter/binaries/apache-jmeter-5.4.tgz &amp;amp;&amp;amp; tar -xf apache-jmeter-5.4.tgz
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Configure environment variables&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;export JMETER_HOME=$HOME/jmeter/apache-jmeter-5.4
export PATH=$JMETER_HOME/bin:$PATH
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;jmeter-commands&#34;&gt;JMeter commands
&lt;/h3&gt;&lt;p&gt;Finally, it will connect to the &lt;code&gt;Grafana&lt;/code&gt; control panel. You can omit the &lt;code&gt;-l&lt;/code&gt; parameter and observe the data in the &lt;code&gt;web&lt;/code&gt; console.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;jmeter -n -t /tmp/order-500-10s.jmx -l /tmp/jmeter-order-report-20200109/order-500-10s.jtl
# 一般不用测试结果和测试报告，简化命令
jmeter -n -t /tmp/order-500-10s.jmx
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;influxdb&#34;&gt;InfluxDB
&lt;/h2&gt;&lt;p&gt;InfluxDB is an open-source distributed time series, event and metrics database written in Go, requiring no external dependencies. This database is now primarily used to store large volumes of timestamped data, such as DevOps monitoring data, APP metrics, IoT sensor data, and real-time analytics data.&lt;/p&gt;
&lt;h3 id=&#34;influxdb-features&#34;&gt;InfluxDB Features
&lt;/h3&gt;&lt;p&gt;The characteristics of InfluxDB can be summarized into 9 aspects:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Unstructured (patternless): can be any number of columns;&lt;/li&gt;
&lt;li&gt;The retention period for metrics can be configured&lt;/li&gt;
&lt;li&gt;Supports time-related functions (such as min, max, sum, count, mean, median, etc.) for statistical analysis&lt;/li&gt;
&lt;li&gt;Support for storage policies: can be used for data deletion and modification. (InfluxDB does not provide methods for deleting or modifying data.)&lt;/li&gt;
&lt;li&gt;Supporting continuous queries: a set of statements that are automatically started on a schedule in the database, and when paired with storage policies, can reduce InfluxDB&amp;rsquo;s system footprint&lt;/li&gt;
&lt;li&gt;Native HTTP support, built-in HTTP API&lt;/li&gt;
&lt;li&gt;Supports SQL-like syntax&lt;/li&gt;
&lt;li&gt;Supports setting the number of data replicas in the cluster&lt;/li&gt;
&lt;li&gt;Support periodic sampling data and write it to another measurement for granular storage of data&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;influxdb-docker-installation&#34;&gt;InfluxDB Docker installation
&lt;/h3&gt;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;mkdir influxdb &amp;amp;&amp;amp; cd influxdb &amp;amp;&amp;amp; \
docker run -p 8086:8086 -d --name influxdb -v $PWD:/var/lib/influxdb influxdb:1.7
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Enter the container, execute a command, and manually create a database&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;root@bce0a55bbc72:/# influx
Connected to http://localhost:8086 version 1.7.10
InfluxDB shell version: 1.7.10
&amp;gt; 交互面板执行命令
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;creating-databases-and-users-in-influxdb&#34;&gt;Creating Databases and Users in InfluxDB
&lt;/h3&gt;&lt;p&gt;Create database: create database jmeter_t2
View databases
Switch database: use jmeter_t2
Create user &amp;ldquo;admin&amp;rdquo; with password &amp;lsquo;admin&amp;rsquo; with all privileges
View users&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;&amp;gt; show users
user  admin
----  -----
admin true
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If the user permission &lt;code&gt;admin&lt;/code&gt; is displayed as &lt;code&gt;true&lt;/code&gt;, the database preparation is complete&lt;/p&gt;
&lt;h2 id=&#34;grafana&#34;&gt;Grafana
&lt;/h2&gt;&lt;p&gt;When writing test cases, I realized that the chart display isn&amp;rsquo;t really necessary. The &lt;code&gt;tps&lt;/code&gt; data for the interface can already be observed when executed in the command line; mostly, I just want to know the internal execution time of the program.&lt;/p&gt;
&lt;p&gt;Quickly deploy a Grafana dashboard, import configuration files, and connect to InfluxDB&lt;/p&gt;
&lt;p&gt;The console supports filtering test results by tag, and generally only an &lt;code&gt;InfluxDB&lt;/code&gt; database needs to be configured&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Application Name&lt;/li&gt;
&lt;li&gt;Test Case Name&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://ttf248.life/p/linux-setup-jmeter-testing-environment/Snipaste_2021-03-09_19-44-22.png&#34;
	width=&#34;861&#34;
	height=&#34;357&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;grafana&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;241&#34;
		data-flex-basis=&#34;578px&#34;
	
&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;docker run -d --name=grafana -p 3000:3000 grafana/grafana:7.3.1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;TPS&lt;/code&gt; and related values calculated in the web version can be inconsistent with the aggregated report in &lt;code&gt;JMeter&lt;/code&gt; due to the sampler interval, see reference link: &lt;a class=&#34;link&#34; href=&#34;https://www.vinsguru.com/jmeter-real-time-results-influxdb-grafana/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://www.vinsguru.com/jmeter-real-time-results-influxdb-grafana/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The document also describes how to customize listeners&lt;/p&gt;
&lt;h2 id=&#34;afterword&#34;&gt;Afterword
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;High-performance programming inherently relies on a single loop thread; any locks, queuing for entry, and queuing for exit will cause unnecessary performance loss&lt;/li&gt;
&lt;li&gt;The time spent on core business logic is greater than the time spent introducing other code; concurrency can effectively improve efficiency, and if the core processing time is sufficiently small, it&amp;rsquo;s best to be cautious when introducing other code&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;reference-materials&#34;&gt;Reference materials
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;Real-time Monitoring with JMeter + Grafana + InfluxDB&lt;/li&gt;
&lt;li&gt;InfluxDB official image&lt;/li&gt;
&lt;li&gt;Grafana official image&lt;/li&gt;
&lt;li&gt;Apache JMeter Official Website&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://medium.com/@jasonli.studio/to-install-apache-jmeter-in-centos7-294bc72a97ba&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;To install Apache JMeter in CentOS7&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
        </item>
        
    </channel>
</rss>
