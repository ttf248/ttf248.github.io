<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>ai on 向叔记事簿</title>
        <link>https://ttf248.life/tags/ai/</link>
        <description>Recent content in ai on 向叔记事簿</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language><atom:link href="https://ttf248.life/tags/ai/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>近期大模型的一些使用经验</title>
        <link>https://ttf248.life/p/recent-usage-experiences-of-large-models/</link>
        <pubDate>Wed, 09 Jul 2025 21:09:01 +0800</pubDate>
        
        <guid>https://ttf248.life/p/recent-usage-experiences-of-large-models/</guid>
        <description>&lt;p&gt;现在用下来并没有哪个大模型特别好，各家都有自己的优势场景。&lt;/p&gt;
&lt;h2 id=&#34;技术文档&#34;&gt;技术文档
&lt;/h2&gt;&lt;p&gt;投喂代码或者咨询IT技术类的问题：ChatGPT 和 Gemini&lt;/p&gt;
&lt;h2 id=&#34;写代码&#34;&gt;写代码
&lt;/h2&gt;&lt;p&gt;整理需求，要求修改代码：Claude&lt;/p&gt;
&lt;h2 id=&#34;金融类咨询&#34;&gt;金融类咨询
&lt;/h2&gt;&lt;p&gt;这里仅测试了混元和deepseek，都是在腾讯的平台，deepseek 明显更加专业一些&lt;/p&gt;</description>
        </item>
        <item>
        <title>日常碎碎念</title>
        <link>https://ttf248.life/p/daily-musings/</link>
        <pubDate>Thu, 19 Jun 2025 19:07:33 +0800</pubDate>
        
        <guid>https://ttf248.life/p/daily-musings/</guid>
        <description>&lt;p&gt;AI已经融入到日常开发的工作流、投资最近切换了思路，从场外基金，切换到了场内的股票和ETF。&lt;/p&gt;
&lt;h2 id=&#34;开源项目&#34;&gt;开源项目
&lt;/h2&gt;&lt;h3 id=&#34;项目记录&#34;&gt;项目记录
&lt;/h3&gt;&lt;p&gt;上周无聊时尝试获取 Github 勋章，开始使用 Issue 模块。以前写代码时，总想找个地方记录每次 AI 改动的内容，但单独弄个文档记录显得杂乱。现在有了 Issue 模块，打上标签区分 bug、feature、enhancement 等，记录变得清晰且高效。即使以后可能用不到，记录下来也是一种积累。&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/ttf248/comic-reader/issues?q=is%3Aissue&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;查看 Issue 列表&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;版本发布&#34;&gt;版本发布
&lt;/h3&gt;&lt;p&gt;版本发布记录，找到最近相关的提交，由于都是AI生成的提交记录，从网页端，复制近期所有的提交记录，扔给AI让它整理下，就是一份不错的发布记录。&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/ttf248/comic-reader/releases/tag/v1.9.0&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/ttf248/comic-reader/releases/tag/v1.9.0&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;活跃度&#34;&gt;活跃度
&lt;/h3&gt;&lt;p&gt;那次整上了Github的个人主页，被动的提高了写代码的积极性，毕竟数据可视化了，有些东西，就是那么奇妙，简单的正反馈，能让人有持续下去的动力。&lt;/p&gt;
&lt;h3 id=&#34;trae&#34;&gt;Trae
&lt;/h3&gt;&lt;p&gt;付费买了首月体验，怎么说呢，在 Vscode 里面也是用的 Claude4 模型，字节的 IDE 体验上还是更好，实战的效果也是更好，有时候同样的问题，Trae 能给出更好的答案，后续要不要买个年费呢？按照现在瞎搞的频率，Trae 次数可能不够用，不用想那么多，等用完再说，字节应该还有另外的的付费方案，购买更多的调用次数。&lt;/p&gt;
&lt;p&gt;&lt;del&gt;简单的小问题，用微软的，github copilot 各个模型也能都能调用。&lt;/del&gt;&lt;/p&gt;
&lt;p&gt;计划夭折，github copilot 也开始限制调用次数，&lt;a class=&#34;link&#34; href=&#34;https://docs.github.com/zh/copilot/about-github-copilot/plans-for-github-copilot#models&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;618 开启限制次数&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;如何查看&lt;a class=&#34;link&#34; href=&#34;https://docs.github.com/zh/copilot/managing-copilot/understanding-and-managing-copilot-usage/monitoring-your-copilot-usage-and-entitlements#downloading-a-monthly-usage-report&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;当前使用量&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;投资&#34;&gt;投资
&lt;/h2&gt;&lt;p&gt;终归还是没忍住，自从开通了港股通，还没在港股交易过，想着小米要发布新车，买了点小米，涨了点就卖了，跌了又买回来，来回操作几次，还没等到新车发布，股票上小赚。&lt;/p&gt;
&lt;p&gt;这个时候没事看什么跨境通的资金流，看着美团都是资金的净流入，跟风买进去，成功当上了股东，忘记看总的资金流，国内买入的资金，只是一部分，港股还有很多外资。这次刚好实践下，蓝筹股，慢慢持有最后的收益会怎么样。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;仓位的控制和亏损，&lt;strong&gt;反人性的东西&lt;/strong&gt;，慢慢来，慢慢来，不能急。小米如果新车不太行，到底要不要卖掉呢？这是个问题。投资的认知还是不够，还是要多看书，多学习。美联储不准备降息，港股大跌一波，建仓的时候如果再晚一些？也不对，如果传出来的消息是降息，港股大涨怎么办？这就是投资，考验人性。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;常念叨的一句话，买的是国运，但自己好像不太信国运。&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;抛开国运的信仰问题，&lt;strong&gt;另外一个就是注意力&lt;/strong&gt;，既然是要做长线，频繁看盘是没意义，每天早上十分钟，收盘十分钟差不多了。最终预期的收益率是多少？还没一个明确的止盈位置。&lt;/p&gt;
&lt;p&gt;除开止盈的问题，配置部分股票，还是老脑子里面有个固有的观念，资产配置需要一部分股票。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;市场大跌的时候，腾讯还是香饽饽，资金扎堆&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;代码&lt;/th&gt;
&lt;th&gt;名称&lt;/th&gt;
&lt;th&gt;最新价&lt;/th&gt;
&lt;th&gt;成交额&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;00700&lt;/td&gt;
&lt;td&gt;腾讯控股&lt;/td&gt;
&lt;td&gt;498.600&lt;/td&gt;
&lt;td&gt;80.08亿&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;03690&lt;/td&gt;
&lt;td&gt;美团-W&lt;/td&gt;
&lt;td&gt;128.100&lt;/td&gt;
&lt;td&gt;68.81亿&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;09992&lt;/td&gt;
&lt;td&gt;泡泡玛特&lt;/td&gt;
&lt;td&gt;247.200&lt;/td&gt;
&lt;td&gt;57.03亿&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;09988&lt;/td&gt;
&lt;td&gt;阿里巴巴-W&lt;/td&gt;
&lt;td&gt;109.800&lt;/td&gt;
&lt;td&gt;53.22亿&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;01810&lt;/td&gt;
&lt;td&gt;小米集团-W&lt;/td&gt;
&lt;td&gt;53.050&lt;/td&gt;
&lt;td&gt;41.05亿&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;p&gt;这段是好几天后更新的，港股一波大涨，那天跌下去的基本都涨回来了。脑子突然想到去年亏损的恒生电子，怎么说呢，从老东家走了，就是对未来的预期不看好，选择买入恒生碰到下跌，心理就会拿不住。两波节奏，股价彻底涨回去，一个是去年的牛市，还有一个就是过年那会的杭州六小龙概念。今年的稳定币也算一个？&lt;/p&gt;
&lt;p&gt;从哪里跌到从哪里爬起来是没错，短时间没机会重新建仓恒生电子了。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;update 20250625&lt;/p&gt;
&lt;p&gt;小米发布会还没开始，上次清空场内ETF，券商这两天大涨，还是没做到长期持有，中间随意变仓了，要买小米，也不应该动券商ETF的资金。&lt;/p&gt;</description>
        </item>
        <item>
        <title>博客翻译项目碎碎念：文化传播、AI编程</title>
        <link>https://ttf248.life/p/blog-translation-project-musings-cultural-transmission-ai-programming/</link>
        <pubDate>Mon, 02 Jun 2025 21:41:00 +0800</pubDate>
        
        <guid>https://ttf248.life/p/blog-translation-project-musings-cultural-transmission-ai-programming/</guid>
        <description>&lt;p&gt;文化传播：意识形态上的影响，潜移默化。
AI编程：不做软件设计，返工很多&lt;/p&gt;
&lt;h2 id=&#34;文化传播&#34;&gt;文化传播
&lt;/h2&gt;&lt;p&gt;起初项目仅仅支持了英语、日语、韩语三种语言，后面想着反正都是AI翻译，何不多支持几种语言呢？于是又增加了法语、俄罗斯语、印度语。此时还没发现问题，程序执行翻译的时候，由于历史的代码存在问题，翻译出来的格式不对，存档的文章需要重新翻译。&lt;/p&gt;
&lt;p&gt;统计耗时提醒，全部翻译完成需要将近20小时，毕竟是本地部署的大模型，那会还是用 12b 参数的大模型。想着干掉几个不常用的语言，减少翻译时间。于是删除了法语、俄罗斯语、印度语。这时候感觉到了不对劲，为什么开始的语言里面，我会选择韩语、日语。&lt;/p&gt;
&lt;p&gt;按照全球人口的分布，这两个语言的受众人群并不多，尤其是韩语，全球使用人数大概只有8000万左右。日语稍微多一点，大概1.2亿人。相比之下，法语、俄罗斯语、印度语的使用人数都在1亿以上。&lt;/p&gt;
&lt;p&gt;这时候才意识到，韩语、日语的受众人群并不是因为语言使用人数多，而是因为文化传播的影响。韩国和日本的文化在全球范围内有着广泛的影响力，尤其是在亚洲地区。K-pop、动漫、影视剧等文化产品吸引了大量粉丝，这些粉丝自然也会对相关语言产生兴趣。&lt;/p&gt;
&lt;p&gt;回顾成长的历程，小时经常看日本的动漫、漫画，长大了看了很多韩国的电影、电视剧。导致我在项目设置初始语言的时候，下意识的选择了这两个熟悉的语言。&lt;/p&gt;
&lt;h3 id=&#34;软件设计与ai编程&#34;&gt;软件设计与AI编程
&lt;/h3&gt;&lt;p&gt;翻译助手最初只是一个简单的小工具，但在体验了 Claude4 的编码能力后，逐渐扩展了功能，增加了文章翻译、标签翻译等模块。随着功能的增加，代码复杂度也随之提升。虽然 AI 重构代码后目录结构显得更清晰，但在扩展新功能或修复缺陷时，AI生成的代码常常存在重复问题。&lt;/p&gt;
&lt;p&gt;AI在生成代码时，缺乏对整体结构和设计理念的理解。它通常基于已有代码进行修改和扩展，却未能有效复用已有模块，导致代码冗余。每次都需要手动清理重复代码，这无形中增加了开发成本。&lt;/p&gt;
&lt;p&gt;此外，AI生成的代码虽然语法正确，但在逻辑和设计上往往存在问题。例如，在另一个项目中稍微调整提示词后，生成的网页结构完全不同，缺乏一致性。这反映了项目初期缺乏合理设计，功能的增加更多是随意堆砌，导致代码结构混乱。&lt;/p&gt;
&lt;p&gt;这也提醒我们，软件工程的核心经验仍然不可忽视。合理的设计不仅能减少返工，还能提升代码的可维护性和扩展性。AI虽然是强大的工具，但它无法替代人类对系统设计的深刻理解和规划。&lt;/p&gt;</description>
        </item>
        <item>
        <title>博客翻译项目碎碎念：历史会话</title>
        <link>https://ttf248.life/p/blog-translation-project-musings-historical-conversations/</link>
        <pubDate>Mon, 02 Jun 2025 21:16:24 +0800</pubDate>
        
        <guid>https://ttf248.life/p/blog-translation-project-musings-historical-conversations/</guid>
        <description>&lt;p&gt;博客翻译项目最初设计过于复杂——先解析 Markdown 格式，再用占位符保护内容，最后送给大模型翻译。其实这完全是多此一举，大模型本身就具备识别 Markdown 语法的能力，可以直接处理原始内容并在翻译时保持格式完整。&lt;/p&gt;
&lt;p&gt;我们的工作就从调试代码，切换到调试大模型的&lt;strong&gt;提示词&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;模型：&lt;code&gt;google/gemma-3-4b&lt;/code&gt;
硬件：&lt;code&gt;Nvdia 3060 12GB&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;没错，选的非思考模型，思考模型在执行翻译任务时，效率不够高，对比了 4b 参数和 12b 参数的效果，针对翻译任务来说 gemma3 的 4b 参数已经足够了，12b 的参数在翻译任务上并没有明显的优势。&lt;/p&gt;
&lt;p&gt;12b 参数的速度：&lt;strong&gt;11.32 tok/sec&lt;/strong&gt;，4b 参数的速度：&lt;strong&gt;75.21 tok/sec&lt;/strong&gt;。&lt;/p&gt;
&lt;h2 id=&#34;背景介绍&#34;&gt;背景介绍
&lt;/h2&gt;&lt;p&gt;尽管在&lt;strong&gt;system&lt;/strong&gt;里面加上了各种条件限制，输出的翻译结果，还是会出现一些问题，比如：格式没有保护，多出来了一些解释的内容。角色定义的时候，已经声明过，记得保护 Markdown 格式、仅输出翻译结果，最终的翻译还是不太稳定。&lt;/p&gt;
&lt;p&gt;此时想起来，以前接触过一个漫画翻译的项目，也用到了大模型的能力，它的翻译效果好像比我的效果更好一些，翻看代码，对比请求的数据，漫画翻译的项目，每次请求都会带上一组上下文，除了当前的翻译内容，还会带上之前的翻译内容。&lt;/p&gt;
&lt;p&gt;好处是什么，不仅能提升前后翻译的连贯性，还剩确保输出格式的稳定性。&lt;/p&gt;
&lt;h2 id=&#34;历史会话的重要性&#34;&gt;历史会话的重要性
&lt;/h2&gt;&lt;p&gt;随着 AI 大模型（如 GPT 系列、Claude、Gemini 等）的普及，越来越多企业和开发者通过 API 接入这些模型，构建智能客服、内容生成、代码助手等应用。然而，许多人在接入初期会遇到一个常见问题：&lt;strong&gt;模型输出不连贯、缺乏上下文理解，甚至答非所问&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;造成这种现象的一个关键原因是——&lt;strong&gt;没有在 API 请求中包含历史对话内容&lt;/strong&gt;。&lt;/p&gt;
&lt;h2 id=&#34;什么是历史对话&#34;&gt;什么是历史对话？
&lt;/h2&gt;&lt;p&gt;历史对话是指在一次对话会话中，模型和用户之间之前的交流记录。在大多数大模型 API（如 OpenAI 的 Chat Completions API）中，开发者需要自己在请求中构建完整的 &lt;code&gt;messages&lt;/code&gt; 数组，将历史对话以轮流的 &lt;code&gt;user&lt;/code&gt; 和 &lt;code&gt;assistant&lt;/code&gt; 消息形式传入。&lt;/p&gt;
&lt;h3 id=&#34;示例&#34;&gt;示例
&lt;/h3&gt;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{
  &amp;quot;model&amp;quot;: &amp;quot;gpt-4&amp;quot;,
  &amp;quot;messages&amp;quot;: [
    {&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;帮我写一封辞职信&amp;quot;},
    {&amp;quot;role&amp;quot;: &amp;quot;assistant&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;好的，你希望辞职的原因写些什么？&amp;quot;},
    {&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;说我想追求个人职业发展&amp;quot;}
  ]
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;如果你只发送最后一句话：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;说我想追求个人职业发展&amp;quot;}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;模型根本不知道你在说辞职信，它可能完全无法理解上下文，输出质量自然很差。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;为什么历史对话如此重要&#34;&gt;为什么历史对话如此重要？
&lt;/h2&gt;&lt;h3 id=&#34;1-构建上下文提升连贯性&#34;&gt;1. &lt;strong&gt;构建上下文，提升连贯性&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;AI 模型本质上是“上下文驱动”的。它无法记住“之前”发生的任何事情，除非你&lt;strong&gt;显式告诉它&lt;/strong&gt;。通过传入对话历史，模型可以更好地理解你的意图和话题背景，输出更符合预期。&lt;/p&gt;
&lt;h3 id=&#34;2-降低误解率&#34;&gt;2. &lt;strong&gt;降低误解率&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;如果你希望模型完成一个多轮指令，如写作、总结、调试代码，历史记录能让模型逐步积累理解，避免在中途“跑题”或丢失重点。&lt;/p&gt;
&lt;h3 id=&#34;3-模拟真实人类对话行为&#34;&gt;3. &lt;strong&gt;模拟真实人类对话行为&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;在实际应用中，如客服系统、教育助手、健康咨询等，用户的问题往往是逐步展开的，而不是一次性表达清楚。保留对话历史，可以让 AI 更像一个“有记忆力的助理”。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;如何在-api-中正确添加历史对话&#34;&gt;如何在 API 中正确添加历史对话？
&lt;/h2&gt;&lt;p&gt;以 OpenAI 的 API 为例，建议遵循以下结构：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;messages = [
    {&amp;quot;role&amp;quot;: &amp;quot;system&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;你是一个专业的法律助手&amp;quot;},
    {&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;什么是合同的有效条件？&amp;quot;},
    {&amp;quot;role&amp;quot;: &amp;quot;assistant&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;合同有效需要满足以下几个条件：……&amp;quot;},
    {&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;那口头协议算吗？&amp;quot;}
]

response = openai.ChatCompletion.create(
    model=&amp;quot;gpt-4&amp;quot;,
    messages=messages
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;注意：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;使用 &lt;code&gt;system&lt;/code&gt; 消息设定模型行为和身份。&lt;/li&gt;
&lt;li&gt;保留最近几轮关键对话即可，不需要每次传入全部历史（避免超过 token 限制）。&lt;/li&gt;
&lt;li&gt;在长会话中，可通过截断早期内容，保留核心信息摘要，控制 token 消耗。&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;实践建议&#34;&gt;实践建议
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;对话状态管理&lt;/strong&gt;：后端需设计缓存机制，记录每个用户的会话历史（如 Redis、数据库）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;限制长度&lt;/strong&gt;：OpenAI GPT-4 的上下文长度为 128k tokens，Claude 3 可达 200k~1M，需合理裁剪。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;动态摘要历史&lt;/strong&gt;：当历史内容过长时，使用模型先对旧对话做摘要，再添加进对话上下文。&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;总结&#34;&gt;总结
&lt;/h2&gt;&lt;p&gt;AI 大模型的能力强大，但也需要开发者“喂”给它足够的上下文信息。&lt;strong&gt;通过在 API 请求中添加历史对话，不仅能显著提升模型输出的质量和连贯性，也能让用户体验更自然、更贴近真实对话。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;无论你是在构建 AI 客服、写作助手、编程帮手，还是教育类应用，这都是一个不可忽视的优化技巧。&lt;/p&gt;</description>
        </item>
        <item>
        <title>老毛病，繁花迷人眼</title>
        <link>https://ttf248.life/p/old-ailment-stunning-flowers/</link>
        <pubDate>Mon, 26 May 2025 23:54:12 +0800</pubDate>
        
        <guid>https://ttf248.life/p/old-ailment-stunning-flowers/</guid>
        <description>&lt;p&gt;多年来一直专注于后端开发，最近开始尝试探索 &lt;code&gt;AI&lt;/code&gt; 编程，并涉足了一些前端相关的内容。然而，在这段折腾的过程中，我逐渐意识到自己又陷入了一个老毛病——繁花迷人眼。总想着用 &lt;code&gt;AI&lt;/code&gt; 来实现一个前端界面，但实际上，这样的尝试对于当前的工作并没有太大的实际帮助，反而分散了精力。&lt;/p&gt;
&lt;h2 id=&#34;ai-的适用场景&#34;&gt;AI 的适用场景
&lt;/h2&gt;&lt;p&gt;在小型项目中，AI 工具的确能够发挥巨大的作用，尤其是在编写那些独立性强、与系统耦合度低、业务逻辑简单的函数时更显得得心应手。这类任务通常有明确的输入输出，且上下文依赖较少，非常适合当前 AI 辅助编程的能力范围。&lt;/p&gt;
&lt;p&gt;然而，当面对复杂的系统架构或深度的业务逻辑时，AI 的局限性就会逐渐显现。它可能会生成看似合理但实际上脱离项目真实需求的代码，甚至引入一些难以排查的潜在问题。在这些场景下，AI 更适合作为辅助工具，而非完全依赖的代码生成器。我们需要对生成的代码进行严格的审查和测试，确保其符合实际需求。&lt;/p&gt;
&lt;h2 id=&#34;错误与学习的代价&#34;&gt;错误与学习的代价
&lt;/h2&gt;&lt;p&gt;在尝试用 AI 生成前端代码的过程中，我发现自己面临了许多挑战。由于前端并不是我熟悉的领域，排查问题的过程往往耗时耗力。即使通过调整提示词让 AI 重写代码，也难以避免一些低级错误的出现。这种反复尝试不仅浪费了时间，还让我意识到，当前的精力更应该集中在后端的业务逻辑上，而不是在不熟悉的领域中摸索。&lt;/p&gt;
&lt;p&gt;回想起周末完成的那个项目，我更加确信，专注于后端开发和用户交互逻辑，通过控制台来实现功能，才是当前最有效率的选择。等到有更多的时间和精力时，再系统地学习前端知识，或许会是更好的策略。&lt;/p&gt;
&lt;h2 id=&#34;前端学习的计划&#34;&gt;前端学习的计划
&lt;/h2&gt;&lt;p&gt;前端技术栈繁杂多样，想要快速上手并不现实。我计划先选择一个框架，比如 Vue.js 或 React.js，深入学习其核心概念和使用方法。只有在熟悉了基础知识之后，再尝试用 AI 辅助生成前端代码，才能有效避免因不熟悉而导致的错误和时间浪费。&lt;/p&gt;
&lt;p&gt;总之，当前阶段的重点还是要放在后端开发上，稳扎稳打地提升自己的核心技能。等到时机成熟，再去探索前端和 AI 的结合，或许会有更大的收获。&lt;/p&gt;</description>
        </item>
        <item>
        <title>Claude4发布，尝试开发：hugo标签、超链接翻译助手</title>
        <link>https://ttf248.life/p/claude-4-release-hugo-tags-hyperlink-translation-assistant/</link>
        <pubDate>Sat, 24 May 2025 03:05:31 +0800</pubDate>
        
        <guid>https://ttf248.life/p/claude-4-release-hugo-tags-hyperlink-translation-assistant/</guid>
        <description>&lt;p&gt;本站点基于 hugo 开发，但是笔者一直用的都是中文标题，导致生成的文章超链接不太友好，说人话就是，发出去的时候，看起来不太友好，中文的字符在超链接中会被转义成 %E4%BD%A0%E5%A5%BD 这种形式，虽然可以通过设置 slug 来解决，但是每次都要手动设置，太麻烦了。&lt;/p&gt;
&lt;p&gt;所以，今天尝试用 Claude4 来开发一个翻译助手，自动将中文标题转换为英文 slug，并且在文章中添加超链接。这样就可以避免手动设置了。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;claude4 真香，上下文的能力大幅提升，复杂任务的处理效率也大幅提升。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;项目地址&#34;&gt;项目地址
&lt;/h2&gt;&lt;p&gt;国内项目地址：&lt;a class=&#34;link&#34; href=&#34;https://cnb.cool/ttf248/hugo-content-suite&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://cnb.cool/ttf248/hugo-content-suite&lt;/a&gt;
国外项目地址：&lt;a class=&#34;link&#34; href=&#34;https://github.com/ttf248/hugo-content-suite&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/ttf248/hugo-content-suite&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;代码实现&#34;&gt;代码实现
&lt;/h2&gt;&lt;p&gt;先说一下实现思路：我们需要扫描所有文章，提取标签信息和文章标题，然后调用本地的大模型（如 gemma-3-12b-it）进行翻译。&lt;/p&gt;
&lt;p&gt;在实际开发中，与前代大模型相比，&lt;code&gt;Claude4&lt;/code&gt; 展现了几个显著的亮点。由于功能需求较多，&lt;code&gt;Claude4&lt;/code&gt; 自动设计了交互式菜单，全面考虑了各种使用场景。例如，在标签处理方面，&lt;code&gt;Claude4&lt;/code&gt; 不仅支持标签的统计与分析，还包括分类统计，甚至能够检测&lt;strong&gt;无标签文章&lt;/strong&gt;。此外，它还提供了&lt;strong&gt;预览&lt;/strong&gt;和生成标签页面的功能。&lt;/p&gt;
&lt;p&gt;无论是对接本地大模型、新增翻译缓存，还是进行大范围的代码重构，&lt;code&gt;Claude4&lt;/code&gt; 都一次性完成，几乎没有出现任何问题。尽管项目规模不大，但包含了许多小功能。以往在开发过程中，大模型经常会遗忘前面的内容，而这次的 &lt;code&gt;Claude4&lt;/code&gt; 表现非常出色，&lt;strong&gt;几乎没有出现遗忘上下文的情况&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;简而言之，智能程度提高了，后续准备切换到 &lt;code&gt;Claude4&lt;/code&gt; 进行更多的开发工作，作为日常编码的主力模型。&lt;/p&gt;
&lt;h2 id=&#34;翻译缓存&#34;&gt;翻译缓存
&lt;/h2&gt;&lt;p&gt;这个点单说，除了减少大模型调用次数，实际本地跑 12b 模型，效率挺高，不耽误事，但是如果每次都要调用大模型，还是会有点慢的。其次就是，为了固定文章的连接，如果执行全量更新操作，文章的标题很长，偶尔就会出现，两次翻译出来的结果不一样的情况，导致文章的链接变了，这个就很尴尬了。&lt;/p&gt;
&lt;h2 id=&#34;功能优化&#34;&gt;功能优化
&lt;/h2&gt;&lt;p&gt;整个项目扔给 &lt;code&gt;Claude4&lt;/code&gt;，分析优化的空间，拿到如下的建议：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;配置外置化 - 提高可维护性和灵活性
结构化日志 - 便于问题排查和监控
性能监控 - 了解系统运行状况
用户体验 - 进度条等视觉反馈
错误处理 - 更完善的异常处理机制
代码组织 - 更清晰的模块划分
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;审核了代码，全部都没什么毛病，比如配置文件，知道自己将原来代码中的配置，转换成默认配置，读取配置文件的时候，如果没有对应的配置文件，就会自动生成一个默认的配置文件，避免了用户的操作失误。&lt;/p&gt;
&lt;h3 id=&#34;heading&#34;&gt;
&lt;/h3&gt;&lt;p&gt;需求：翻译正文的时候，动态计算当前翻译的效率，预估剩余的时间，相关信息输出到控制台：现在获取了文章的字符数量，每行进行翻译的时候，当前翻译的字符数量，时间，拟合计算每100字符的翻译时间，同时计算文章剩余的翻译预估时间&lt;/p&gt;
&lt;p&gt;代码搞定了，效果不是很满意，我就让AI提供新的设计方案：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;提供多种效率计算方式：实时效率、平均效率、滑动窗口效率&lt;/li&gt;
&lt;li&gt;改进显示方式：进度条、分段统计、动态刷新&lt;/li&gt;
&lt;li&gt;增加更多有用的指标：API调用次数、成功率等&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;代码完成以后，发现了新的惊喜，翻译效率的统计信息实时刷屏，但是没有无脑向下滚动。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;翻译正文到 English (总计 7163 字符)...
检测到 53 行需要翻译
  [1/53] Stage1/6 [░░░░░░░░░░░░░░░░░░░░░░░░░] 1.9% 翻译 354 字符...
    ✅ 完成 (3.1s) | API调用 #1
    ✅ 完成 (1.5s) | API调用 #2
    ✅ 完成 (0.9s) | API调用 #3
    ✅ 完成 (0.2s) | API调用 #4
    ✅ 完成 (1.0s) | API调用 #5
    ✅ 完成 (1.0s) | API调用 #6
    ✅ 完成 (0.2s) | API调用 #7
    📊 进度: 行 13.2% (7/53) | 字符 12.9% (925/7163) 114.6 字符/秒 📊
    ⚡ 效率: 实时76.4 | 平均117.9 | 最近109.0 | 阶段113.6 字符/秒 📊
    🎯 成功率: 100.0% (7/7) | 剩余: 46行7 7s] 9.4% 翻译 110 字符...
    ⏱️  预估剩余: 55s | 预计完成: 00:10:19 8s] 11.3% 翻译 114 字符...
    💾 处理速度: 3211.3 行/分钟 | 总用时: 8s] 13.2% 翻译 16 字符...
  [8/53] Stage1/6 [███░░░░░░░░░░░░░░░░░░░░░░] 15.1% 翻译 166 字符...
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;以前控制程序写的也不是很多，好奇如何实现的，翻看代码&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-golang&#34;&gt;// 清屏并重新显示 (动态刷新效果)
if translationCount &amp;gt; 1 {
   fmt.Print(&amp;quot;\033[6A\033[K&amp;quot;) // 上移6行并清除
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;性能统计菜单&#34;&gt;性能统计菜单
&lt;/h3&gt;&lt;p&gt;新增的&lt;strong&gt;性能统计菜单&lt;/strong&gt;，让我自己设计，都不一定能设计的这么完善&lt;/p&gt;
&lt;p&gt;📊 性能统计:
🔄 翻译次数: 360
⚡ 缓存命中率: 1.4% (5/365)
⏱️  平均翻译时间: 315.927234ms
📁 文件操作: 73
❌ 错误次数: 0&lt;/p&gt;
&lt;h3 id=&#34;进度条显示&#34;&gt;进度条显示
&lt;/h3&gt;&lt;p&gt;新增的&lt;strong&gt;进度条显示&lt;/strong&gt;，详细的进度、已用时间、剩余时间预估&lt;/p&gt;
&lt;p&gt;请选择功能 (0-13): 10
🔍 正在收集翻译目标&amp;hellip;
📄 已加载缓存文件，包含 0 个翻译记录&lt;/p&gt;
&lt;p&gt;📊 翻译缓存统计:
🏷️  标签总数: 229 个
📝 文章总数: 131 篇
✅ 已缓存: 0 个
🔄 需翻译: 360 个&lt;/p&gt;
&lt;p&gt;确认生成全量翻译缓存？(y/n): y
🚀 正在生成全量翻译缓存&amp;hellip;
📄 已加载缓存文件，包含 0 个翻译记录
🔍 检查缓存中的翻译&amp;hellip;
🔄 需要翻译 360 个新标签
[░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 5/360 (1.4%) - 用时: 3s - 预计剩余: 3m8s💾 已保存缓存文件，包含 5 个翻译记录
[█░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 10/360 (2.8%) - 用时: 6s - 预计剩余: 3m28s💾 已保存缓存文件，包含 10 个翻译记录
[██░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 15/360 (4.2%) - 用时: 9s - 预计剩余: 3m30s💾 已保存缓存文件，包含 15 个翻译记录
[██░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 20/360 (5.6%) - 用时: 13s - 预计剩余: 3m36s💾 已保存缓存文件，包含 20 个翻译记录
[███░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 25/360 (6.9%) - 用时: 16s - 预计剩余: 3m33s💾 已保存缓存文件，包含 25 个翻译记录
[████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 30/360 (8.3%) - 用时: 19s - 预计剩余: 3m30s💾 已保存缓存文件，包含 30 个翻译记录
[████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 35/360 (9.7%) - 用时: 22s - 预计剩余: 3m25s💾 已保存缓存文件，包含 35 个翻译记录&lt;/p&gt;
&lt;hr&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;=== Hugo 博客管理工具 ===

🚀 核心功能
  1. 一键处理全部 (完整博客处理流程)

📝 内容管理
  2. 生成标签页面
  3. 生成文章Slug
  4. 翻译文章为多语言版本

💾 缓存管理
  5. 查看缓存状态
  6. 生成全量翻译缓存
  7. 清空翻译缓存

  0. 退出程序
&lt;/code&gt;&lt;/pre&gt;</description>
        </item>
        <item>
        <title>AI用多了，有点后遗症</title>
        <link>https://ttf248.life/p/ai-overuse-side-effects/</link>
        <pubDate>Wed, 14 May 2025 19:39:50 +0800</pubDate>
        
        <guid>https://ttf248.life/p/ai-overuse-side-effects/</guid>
        <description>&lt;p&gt;自从新建了&lt;code&gt;AI 灵感碰撞坊&lt;/code&gt;，很多有的没的东西，都在试着用AI记录，发布出来，静下来自己思考的东西，反而越来越少了，以后需要稍微控制下此栏目的产出，整合为月刊的形式不错，每月发布一篇就行。&lt;/p&gt;
&lt;p&gt;这就像是某种后遗症，或者说是副作用，效率是提高了，但是思考的深度和广度却下降了。&lt;/p&gt;
&lt;h2 id=&#34;效率提升不可否认&#34;&gt;效率提升：不可否认
&lt;/h2&gt;&lt;p&gt;&lt;code&gt;鱼的七秒钟见闻&lt;/code&gt;以前这个栏目维护的不多，有些热点事件，由于懒并没与去互联网搜索资料、整理记录下来，现在有了各种AI工具，整理好大纲就行，AI能做到自动联网搜索记录相关的事件，生成我需要的文章，简单的整理下格式，发布出来就行。&lt;/p&gt;
&lt;p&gt;这就像是一个懒人福音，效率提高了不少，甚至可以说是事半功倍。&lt;/p&gt;
&lt;p&gt;抛开写稿子，写代码的时候，效率提升是实打实的，很多代码的编写，以前是需要详细阅读API接口文档说明，现在直接跳过，这种跳过相当的有必要，熟悉API属于&lt;code&gt;体力劳动&lt;/code&gt;，而不是&lt;code&gt;脑力劳动&lt;/code&gt;，AI来搞定这部分内容，刚刚好。&lt;/p&gt;
&lt;h2 id=&#34;垃圾内容&#34;&gt;垃圾内容
&lt;/h2&gt;&lt;p&gt;很多稿子，内容质量不高，不能说没有东西，读起来怎么说，没有烟火气息，是我以前不喜欢的风格，味同嚼蜡。&lt;/p&gt;
&lt;p&gt;换个角度来说，AI生成的内容，确实是有点像是流水线生产出来的东西，缺乏灵魂。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;新时代的互联网垃圾&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;遗忘性&#34;&gt;遗忘性
&lt;/h2&gt;&lt;p&gt;此类型的稿子，都是AI生成的，读者的情况不清楚，但是时间长了，我自己的印象也会模糊，甚至是忘记了。&lt;/p&gt;
&lt;p&gt;类似的问题，写代码的时候也会发生，不翻看代码提交记录，根本不知道自己当时是怎么想的，为什么要这么写。特别是代码通过和AI反复沟通，最后生成的代码，和最初的想法差别很大，甚至是完全不一样。&lt;/p&gt;
&lt;h2 id=&#34;搜索&#34;&gt;搜索
&lt;/h2&gt;&lt;p&gt;最近打开谷歌、百度的次数明显减少了，很多问题用AI来搜索，交互也好、搜索的结果也好，都比传统的搜索引擎要好很多。&lt;/p&gt;
&lt;p&gt;现在让我们祭奠下不知道是否还活着的&lt;code&gt;bing ai&lt;/code&gt;，属于大厂里面最早发布的，能联网搜索的AI工具。&lt;/p&gt;
&lt;p&gt;谷歌用的少了，访问&lt;code&gt;stackoverflow&lt;/code&gt;的次数也少了，很多问题直接问AI就行了，这网站也逐渐要被时代所淘汰。&lt;/p&gt;
&lt;h2 id=&#34;结尾&#34;&gt;结尾
&lt;/h2&gt;&lt;p&gt;笔者还在维护的博客，原本也没多少访问量，现在就更不指望了，更多是个记录的地方，写给自己看的。&lt;/p&gt;</description>
        </item>
        <item>
        <title>不写代码，设计开发自选股模块</title>
        <link>https://ttf248.life/p/design-develop-custom-stock-module-no-code/</link>
        <pubDate>Thu, 27 Feb 2025 23:20:39 +0800</pubDate>
        
        <guid>https://ttf248.life/p/design-develop-custom-stock-module-no-code/</guid>
        <description>&lt;p&gt;上个月我们试用了 cursor，但是由于免费额度的限制，并没有做太复杂的功能开发，只是简单的测试了一下。那会就发现，字节也发布了类似的产品，两者底层调用的大模型一样，都是 Claude-3.5。&lt;/p&gt;
&lt;p&gt;字节产品叫做 Trae，先发布的 mac 版本，今年二月份，终于发布了 windows 版本。大厂的东西就是好，能免费白嫖，不用掏钱，无限量使用 Claude-3.5，这个模型的效果还是很不错的。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;最终卡在了K线图的开发上，由于本人基本不懂 react，所以只能放弃了。想要继续开发，需要笔者补充一些前端的基础知识，将任务拆分的更细，而不是直接给一个大任务：开发K线图。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;发现的问题&#34;&gt;发现的问题
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;由于使用国外的 AI 模型，Vue3 + Element-Plus 的训练数据不足，因此选择了 React 作为前端框架&lt;/li&gt;
&lt;li&gt;可能存在偶发的语法错误，需要人工修复&lt;/li&gt;
&lt;li&gt;部分复杂问题的解决方案需要人工指引&lt;/li&gt;
&lt;li&gt;代码结构优化需要人工指导&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;其中耗时最多的地方，打包前端代码到容器中，由于笔者零基础 &lt;code&gt;.env.production&lt;/code&gt; &lt;code&gt;tsconfig.json&lt;/code&gt;，完全是没有概念的，这些还是中途求助豆包，才捋顺对应的逻辑。前端开发 dev 模式和 build 模式，对于代码的检查，差异很大。后端数据库和服务的容器脚本，合计五分钟就搞定了。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;AI 目前更多的提高开发的效率，你有基础是最好的，并不是 AI 会帮你解决所有的问题&lt;/strong&gt;。&lt;/p&gt;
&lt;h2 id=&#34;仓库地址&#34;&gt;仓库地址
&lt;/h2&gt;&lt;p&gt;正如标题说的，我们这次是能不写就不动手，和AI硬聊，设计开发自选股模块。看最终能做出来什么效果。&lt;/p&gt;
&lt;p&gt;仓库地址：&lt;a class=&#34;link&#34; href=&#34;https://github.com/ttf248/trae-demo&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/ttf248/trae-demo&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;详细的使用方法，可以看仓库的 README.md 文件。&lt;/p&gt;
&lt;p&gt;仓库包含很多递交记录，大部分都是我和 Trae 的对话记录，以及我对 Trae 的一些功能的测试，备注了是否人工干预来实现对应的功能。&lt;/p&gt;
&lt;h2 id=&#34;prompt&#34;&gt;Prompt
&lt;/h2&gt;&lt;p&gt;项目是从零开始创建，下面是项目的 Prompt：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;基于项目原型图，开发功能：自选股，需要支持合约的新增、删除、修改、查询。自选股界面需要展示基础的行情数据。支持多个不同的市场切换。

前端：react
后端：golang gin gorm
数据库：PostgreSQL

服务端需要支持跨域请求，同时需要考虑数据的校验和错误处理，如果后端服务不可用，前端需要告警提示。

后端需要展示请求和应答的日志；前端也打印通讯的日志，方便排查问题。
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;ui和交互优化&#34;&gt;UI和交互优化
&lt;/h2&gt;&lt;p&gt;前端界面的设计完全依赖的 Grok，我们首先在 Trae 里面做出来产物的雏形，但是没有审美，由于使用的模型，代码能力很强，但是其他能力比较弱，所以我们需要使用 Grok 来优化前端的 UI。&lt;/p&gt;
&lt;p&gt;通过将当前的界面截图，上传到 Grok 里面，然后让它帮我们优化 UI，可能一次性拿到很多的优化建议，我们人工评估，然后拷贝到 Trae 中执行，观察优化的效果。&lt;/p&gt;
&lt;h3 id=&#34;技术栈&#34;&gt;技术栈
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;前端：React + TypeScript&lt;/li&gt;
&lt;li&gt;后端：Golang + Gin + GORM&lt;/li&gt;
&lt;li&gt;数据库：PostgreSQL 17&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;系统架构&#34;&gt;系统架构
&lt;/h2&gt;&lt;h2 id=&#34;后端架构&#34;&gt;后端架构
&lt;/h2&gt;&lt;p&gt;后端采用 Golang 的 Gin 框架实现 RESTful API，主要模块包括：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;数据库模块&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;使用 GORM 作为 ORM 框架&lt;/li&gt;
&lt;li&gt;支持环境变量配置数据库连接&lt;/li&gt;
&lt;li&gt;自动进行数据库表迁移&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;路由模块&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;RESTful API 设计&lt;/li&gt;
&lt;li&gt;统一的错误处理机制&lt;/li&gt;
&lt;li&gt;内置请求日志记录&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;跨域处理&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;支持本地开发环境跨域&lt;/li&gt;
&lt;li&gt;可配置的 CORS 策略&lt;/li&gt;
&lt;li&gt;支持 Cookie 跨域&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;前端架构&#34;&gt;前端架构
&lt;/h2&gt;&lt;p&gt;前端使用 React + TypeScript 构建，实现了：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;股票列表展示&lt;/li&gt;
&lt;li&gt;自选股管理&lt;/li&gt;
&lt;li&gt;行情数据展示&lt;/li&gt;
&lt;li&gt;错误提示机制&lt;/li&gt;
&lt;/ul&gt;</description>
        </item>
        <item>
        <title>AI发展两年：有点类似Docker发布前的状态</title>
        <link>https://ttf248.life/p/ai-development-two-years-like-docker-pre-release/</link>
        <pubDate>Thu, 20 Feb 2025 18:16:37 +0800</pubDate>
        
        <guid>https://ttf248.life/p/ai-development-two-years-like-docker-pre-release/</guid>
        <description>&lt;p&gt;人工智能（AI）近年来无疑是技术领域最为热议的话题之一，尤其是在过去的两年里，AI技术得到了飞速的进展。无论是深度学习、自然语言处理，还是计算机视觉、自动化决策系统，AI的应用场景层出不穷。然而，尽管技术不断取得突破，AI仍然面临着一个类似于Docker发布前的瓶颈——缺乏一个杀手级的应用来真正引爆市场。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Prompt: 编写文章：AI发展两年，有点类似 docker 发布前的状态，缺乏一个杀手级应用，基于现有的技术，弄出来一个完美的落地场景，docker 是没用到太多新技术，但是整套方案很合理，改变了运维、开发的工作流程&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;ai发展现状技术已经成熟但应用尚需突破&#34;&gt;AI发展现状：技术已经成熟，但应用尚需突破
&lt;/h2&gt;&lt;p&gt;从技术层面来看，AI在过去的两年里已经取得了相当大的进展。无论是OpenAI推出的GPT系列模型，还是Google的BERT、DeepMind的Alpha系列，AI的处理能力已经远超之前的预期。尤其是自然语言处理领域，GPT-4等模型不仅具备了强大的生成能力，还在理解和推理上展现了令人惊叹的表现。&lt;/p&gt;
&lt;p&gt;然而，尽管技术日新月异，AI在实际应用上的落地却面临一定的挑战。与Docker发布前的状态类似，虽然AI的潜力巨大，但目前尚未出现一个真正能够广泛普及、改变产业的杀手级应用。大家都在谈论AI的前景，却未必能够找到一个可以直接带来革命性改变的应用场景。很多AI应用仍然停留在初步的尝试阶段，且大部分需要进一步的整合与优化。&lt;/p&gt;
&lt;h2 id=&#34;docker与ai的相似性技术不一定是创新方案才是关键&#34;&gt;Docker与AI的相似性：技术不一定是创新，方案才是关键
&lt;/h2&gt;&lt;p&gt;如果回顾Docker发布前的历史，我们不难发现，当时的技术环境和AI的发展现状有诸多相似之处。Docker发布之前，容器技术并不是新鲜事物，早期的LXC（Linux Containers）和虚拟化技术都具备了容器化的基本能力。但Docker通过对现有技术的巧妙整合和优化，提出了一种更简单、直观且高效的方案。这套方案并没有引入颠覆性的技术，但却解决了许多运维和开发过程中的痛点，极大地简化了软件的部署、扩展和管理流程。&lt;/p&gt;
&lt;p&gt;同样，AI领域也面临类似的情形。目前的AI技术虽然已不再是“新鲜事物”，但要想真正实现大规模的应用，仍然需要一个完美的落地场景，像Docker一样，将现有技术融合并优化，形成一个合理的应用方案。AI的杀手级应用，可能并不依赖于全新的技术突破，而是如何通过整合现有的技术，解决实际业务中的痛点和需求。&lt;/p&gt;
&lt;h2 id=&#34;如何找到ai的docker时刻&#34;&gt;如何找到AI的“Docker时刻”？
&lt;/h2&gt;&lt;p&gt;要让AI技术真正得到广泛应用，需要从几个方面着手：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;实际场景的深度挖掘&lt;/strong&gt;&lt;br&gt;
目前，许多AI的应用场景仍然偏向于实验性质，缺乏大规模的实际落地。例如，AI客服、智能推荐等领域虽然应用广泛，但其功能仍有许多局限，尚未能突破行业的瓶颈。真正的突破，可能来自于那些被传统方法困扰已久的行业，比如医疗、制造业、物流等领域，AI可以通过更高效的数据处理、预测分析，帮助企业在这些复杂的场景中提升效率和降低成本。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;产品化与易用性&lt;/strong&gt;&lt;br&gt;
类似于Docker通过简化容器化流程来提升运维的效率，AI产品的易用性同样至关重要。AI的普及不仅仅是技术的普及，更是其产品化的普及。将AI集成到日常工作流中，让用户在不需要深入理解技术的前提下，能够轻松使用这些工具，这是AI落地的重要一步。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;生态建设与标准化&lt;/strong&gt;&lt;br&gt;
任何一项新技术的广泛应用，都离不开生态的建设。Docker之所以能快速崛起，正是因为它的开放性和兼容性，使得开发者能够轻松地与各种云平台、工具和服务对接。同样，AI的未来也依赖于生态系统的建设。AI的标准化、模型的共享、数据的开放，以及技术的可集成性，都将影响AI是否能够形成广泛的行业应用。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;结语ai的未来充满可能性但仍需更完善的落地方案&#34;&gt;结语：AI的未来充满可能性，但仍需更完善的落地方案
&lt;/h2&gt;&lt;p&gt;尽管AI的技术在过去两年里取得了长足的进步，但目前它依然处于“没有杀手级应用”的阶段。与Docker发布前的容器化技术相似，AI需要一个合理的应用场景，将现有的技术与业务需求深度融合，才能真正实现大规模的应用和普及。技术创新固然重要，但能够简化流程、提高效率的解决方案，更能推动技术的普及与发展。&lt;/p&gt;
&lt;p&gt;未来，AI可能会像Docker一样，不是通过颠覆性的技术突破，而是通过整合现有的技术，打造出一个完美的应用场景，最终改变我们工作和生活的方式。&lt;/p&gt;</description>
        </item>
        <item>
        <title>ollama 本地部署 deepseek-R1</title>
        <link>https://ttf248.life/p/ollama-local-deployment-deepseek-r1/</link>
        <pubDate>Fri, 07 Feb 2025 22:41:02 +0800</pubDate>
        
        <guid>https://ttf248.life/p/ollama-local-deployment-deepseek-r1/</guid>
        <description>&lt;p&gt;Ollama 是一个开源的 AI 工具，旨在使用户能够本地运行和部署大型语言模型（LLM）。它的目标是提供一个方便且高效的方式，让开发者可以在本地机器上使用像 GPT 这样的模型，而不需要依赖云端服务。Ollama 支持多种模型，并且专注于优化性能，使得即使是资源有限的设备也能顺畅运行这些模型。&lt;/p&gt;
&lt;p&gt;通过 Ollama，用户可以使用基于文本的 AI 应用程序，并能够与本地部署的模型进行交互，而无需担心数据隐私或是高昂的 API 使用费用。你可以通过命令行界面（CLI）调用不同的模型，进行自然语言处理、问答等任务。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;ollama 适合不同模型尝鲜，windows 版本测试下来，无法充分发挥硬件的性能，可能是因为 windows 版本的原因，linux 版本可能会更好。部署32b参数的模型，内存、显卡负载都有不高的情况下，回复速度很慢。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;硬件概述&#34;&gt;硬件概述
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;操作系统：win11&lt;/li&gt;
&lt;li&gt;CPU：i7-10700K&lt;/li&gt;
&lt;li&gt;内存：40GB&lt;/li&gt;
&lt;li&gt;显卡：RTX 3060 12GB&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;环境准备&#34;&gt;环境准备
&lt;/h2&gt;&lt;p&gt;新增系统环境变量，方便后续使用：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;set OLLAMA_MODELS=E:\ollama&lt;/code&gt;&lt;/strong&gt;&lt;br&gt;
这个变量指定了 Ollama 模型的存放路径。&lt;code&gt;E:\ollama&lt;/code&gt; 是一个文件夹路径，表示所有本地模型文件都存储在该目录下。Ollama 会根据这个路径加载和使用你下载或部署的语言模型。你可以将模型文件存放在其他位置，只需要更改这个路径。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;set OLLAMA_HOST=127.0.0.1:8000&lt;/code&gt;&lt;/strong&gt;&lt;br&gt;
这个环境变量设置了 Ollama 服务的主机和端口。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;127.0.0.1&lt;/code&gt; 是本地地址（localhost），意味着 Ollama 服务会只监听来自本机的请求。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;8000&lt;/code&gt; 是指定的端口号，表示 Ollama 服务将在 8000 端口上等待和处理请求。你可以根据需要更改端口号，但需要确保该端口没有被其他应用占用。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;set OLLAMA_ORIGINS=*&lt;/code&gt;&lt;/strong&gt;&lt;br&gt;
这个环境变量控制允许哪些来源的请求访问 Ollama 服务。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;*&lt;/code&gt; 表示允许任何来源（即所有域名和 IP 地址）都可以访问 Ollama 服务。这通常用于开发和调试环境，在生产环境中，通常会指定更严格的来源控制，限制只有特定的域或 IP 才能访问你的服务，以提高安全性。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;deepseek-r1-模型部署&#34;&gt;deepseek-R1 模型部署
&lt;/h2&gt;&lt;p&gt;ollama 安装属于傻瓜式，此处不在赘述。&lt;/p&gt;
&lt;p&gt;安装后的校验：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;C:\Users\core&amp;gt;ollama -v
ollama version is 0.5.11
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;模型部署，参考官网模型页面，选择对应参数的模型：&lt;code&gt;ollama run deepseek-r1:14b&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;14b 参数能有效的记住会话上下文，更小的参数版本，无法记住会话上下文。32b 参数版本，本机部署很卡顿，没有再深入进行测试。&lt;/p&gt;
&lt;h2 id=&#34;参考资料&#34;&gt;参考资料
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.ollama.com/library/deepseek-r1&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://www.ollama.com/library/deepseek-r1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/SPEvYTmTBxhoEkJqm1yPmw&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://mp.weixin.qq.com/s/SPEvYTmTBxhoEkJqm1yPmw&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://blog.csdn.net/x18990027/article/details/145368094&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://blog.csdn.net/x18990027/article/details/145368094&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
        </item>
        <item>
        <title>Cursor AI 编程 IDE 试用</title>
        <link>https://ttf248.life/p/cursor-ai-programming-ide-trial/</link>
        <pubDate>Thu, 23 Jan 2025 19:30:13 +0800</pubDate>
        
        <guid>https://ttf248.life/p/cursor-ai-programming-ide-trial/</guid>
        <description>&lt;p&gt;转眼又是一年过去了，工作上的最大的变动，还是AI参与度明显提高了，相当以往来说，不同的开发语言之间切换，需要开发者熟悉的各种语言不同的 api 接口，现在这些基础代码都可以通过 AI 生成代码了，对于开发者来说，就是一个很大的福音。&lt;/p&gt;
&lt;h2 id=&#34;chatgpt&#34;&gt;ChatGPT
&lt;/h2&gt;&lt;p&gt;早在 23 年的时候，整过两篇简单的入门介绍，转眼已经到了 25 年，怎么说呢，并没有感知到显著的进步，还是需要开发有自己的认知，能合理的拆分任务，当然，最重要的是识别 AI 生成的代码是否存在 bug。&lt;/p&gt;
&lt;h2 id=&#34;github-copilot&#34;&gt;Github copilot
&lt;/h2&gt;&lt;p&gt;忘记是是哪天了，看到资料说新加坡部署了服务器，国内使用，再也不用长期挂梯子，当然，登录的时候，还是需要挂个梯子，不过这个梯子只需要登录的时候用一下，之后就可以关闭了。&lt;/p&gt;
&lt;p&gt;日常用的更多的也是 Github copilot，这个插件，可以直接在 vscode、visual studio 中直接使用。不用两个软件之间切换。相对于 ChatGPT，Github copilot 对于项目的支持更好，交互上更友好，能你能选择部分本地的文件投喂，&lt;strong&gt;“训练”AI&lt;/strong&gt;，这样生成的代码更符合你的项目。&lt;/p&gt;
&lt;h2 id=&#34;cursor-ai&#34;&gt;Cursor AI
&lt;/h2&gt;&lt;p&gt;最近看到了一个新的 AI 编程 IDE，Cursor AI，这个 IDE 也是基于 Github copilot 的，不过这个 IDE 更加的智能，可以帮你直接创建文件。&lt;/p&gt;
&lt;p&gt;简单的用了一下，感觉还是不错的，不过对于现有项目的理解还是不够，本地项目文件多的时候，大的重构优化调整，还是需要&lt;strong&gt;开发者拆分任务&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;附上一个例子：切换到 curso 的工程模式，输入内容：新建个人简历网页，支持多个不同风格切换，记得填充一些个人信息用于数据展示。&lt;/p&gt;
&lt;p&gt;经过几次来回的&lt;strong&gt;拉扯&lt;/strong&gt;，你就能拿到如下的网页，当然，这个网页还是比较简单的，不过对于新手来说，还是很不错的。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;目前注册用户能免费试用 150 次高级 api，付费用户限制 5000 次高级 api&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://ttf248.life/cursor/index.html&#34; &gt;个人简历&lt;/a&gt;&lt;/p&gt;</description>
        </item>
        <item>
        <title>AI编程和任务拆解</title>
        <link>https://ttf248.life/p/ai-programming-and-task-decomposition/</link>
        <pubDate>Fri, 22 Dec 2023 08:44:26 +0800</pubDate>
        
        <guid>https://ttf248.life/p/ai-programming-and-task-decomposition/</guid>
        <description>&lt;p&gt;两年前给站点添加代码区域的拷贝功能，前后折腾了小半天，最终渲染效果有点不如意，作为半吊子前端开发，也没想着改进，能用就行，今年用&lt;code&gt;AI&lt;/code&gt;开发了小程序，对于前端的开发也更加熟悉了，重构一波（AI重新设计）。&lt;/p&gt;
&lt;p&gt;悟道：任务拆解开，很多小任务，&lt;code&gt;AI&lt;/code&gt;开发起来还是顺手的。已经倒闭的小程序，&lt;code&gt;75%&lt;/code&gt;代码由&lt;code&gt;AI&lt;/code&gt;提供，拆解了多个任务交付，人工进行项目代码的&lt;code&gt;组装&lt;/code&gt;。&lt;/p&gt;
&lt;h2 id=&#34;前文&#34;&gt;前文
&lt;/h2&gt;&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://ttf248.github.io/post/2022/001-%E4%BB%A5%E4%B8%BA%E5%BE%88%E7%AE%80%E5%8D%95%E7%9A%84%E4%BA%8B%E6%83%85%E5%A2%9E%E5%8A%A0%E4%B8%80%E4%B8%AA%E4%BB%A3%E7%A0%81%E5%A4%8D%E5%88%B6%E6%8C%89%E9%92%AE/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;以为很简单的事情，增加一个代码复制按钮&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;正文&#34;&gt;正文
&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;promot&lt;/strong&gt;: 设计 css 和 JavaScript 代码，定位网页元素 class=&amp;ldquo;highlight&amp;rdquo;，这个元素的容器内，添加一个 复制按钮，实现复制代码的功能&lt;/p&gt;
&lt;p&gt;给的第一稿代码一眼看上去就有问题，重新生成一稿，基本可用，复制的内容不对，有冗余的文字，再次引导优化，人工指定核心的颜色设计，包含哪些特效&lt;/p&gt;
&lt;p&gt;了解到您对按钮的尺寸和颜色有特定的要求，我们可以重新设计按钮的样式以适应这些要求。以下是一个调整后的 CSS 样式，其中包括了您提供的颜色方案，并对按钮的尺寸进行了优化：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-css&#34;&gt;.copy-btn {
  position: absolute;
  top: 2px; /* 调整位置 */
  right: 2px;
  padding: 3px 6px; /* 缩小尺寸 */
  font-size: 12px; /* 减小字体大小 */
  background-color: #8FD1E1; /* 按钮背景色 */
  color: #272822; /* 文字颜色 */
  border: 1px solid #272822; /* 边框颜色 */
  border-radius: 3px; /* 圆角 */
  cursor: pointer;
  opacity: 0.7; /* 默认半透明 */
  transition: opacity 0.3s ease, background-color 0.3s ease; /* 平滑过渡效果 */
}

.copy-btn:hover {
  opacity: 1; /* 鼠标悬停时不透明 */
  background-color: #A3D4E5; /* 鼠标悬停时的背景色 */
}

.highlight {
  position: relative;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;在这个样式中，我减小了按钮的尺寸和字体大小，使其更加精致。同时，按钮的默认状态是半透明的，当鼠标悬停在按钮上时，它会变为不透明，并且背景色会变得稍微亮一些，以提供视觉反馈。这样的设计既符合您的颜色要求，又保持了现代化和用户友好的特点。&lt;/p&gt;
&lt;p&gt;对应的提供了一份更简单的 &lt;code&gt;JavaScript&lt;/code&gt;：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-JavaScript&#34;&gt;document.addEventListener(&#39;DOMContentLoaded&#39;, function() {
  // 查找所有 .highlight 元素
  const highlights = document.querySelectorAll(&#39;.highlight&#39;);

  highlights.forEach(function(highlight) {
    // 创建复制按钮
    const copyBtn = document.createElement(&#39;button&#39;);
    copyBtn.textContent = &#39;复制&#39;;
    copyBtn.classList.add(&#39;copy-btn&#39;);

    // 将按钮添加到每个 .highlight 元素中
    highlight.appendChild(copyBtn);

    // 添加点击事件监听器
    copyBtn.addEventListener(&#39;click&#39;, function() {
      // 假设 .highlight 下的第一个 &amp;lt;code&amp;gt; 元素包含要复制的代码
      const code = highlight.querySelector(&#39;code&#39;) ? highlight.querySelector(&#39;code&#39;).textContent : &#39;&#39;;

      navigator.clipboard.writeText(code).then(() =&amp;gt; {
        // 显示复制成功信息
        copyBtn.textContent = &#39;已复制!&#39;;
        setTimeout(() =&amp;gt; {
          copyBtn.textContent = &#39;复制&#39;;
        }, 2000);
      }).catch(err =&amp;gt; {
        console.error(&#39;复制失败: &#39;, err);
      });
    });
  });
});
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;后记&#34;&gt;后记
&lt;/h2&gt;&lt;p&gt;虽说我更期望在代码区域的右上角，顶部区域新增复制按钮，由于对前端不够了解，无法基于关键词准确描述功能，和&lt;code&gt;AI&lt;/code&gt;来回折腾几个回合也没有达到预期。&lt;/p&gt;
&lt;p&gt;就是不知道随着时间的发展，&lt;code&gt;AI&lt;/code&gt;对于程序开发会有多大的影响。&lt;/p&gt;</description>
        </item>
        <item>
        <title>Prompt 工程师</title>
        <link>https://ttf248.life/p/prompt-engineer/</link>
        <pubDate>Sun, 26 Mar 2023 20:46:53 +0800</pubDate>
        
        <guid>https://ttf248.life/p/prompt-engineer/</guid>
        <description>&lt;p&gt;就如当年学习搜索引擎的技巧，我们也需要学习一些和&lt;code&gt;AI&lt;/code&gt;沟通的技巧，给出合理且充分的限定条件，高效的获取需要的答案。&lt;/p&gt;
&lt;p&gt;如果你换个角度呢，当前的&lt;code&gt;AI&lt;/code&gt;属于一个记忆力很好的小孩子，它拥有过目不忘的能力，有抄作业的能力。我们需要做的是学会如何正确、有效的和&lt;code&gt;AI&lt;/code&gt;沟通，精准的描述需求，帮助&lt;code&gt;AI&lt;/code&gt;生成预期的结果。&lt;/p&gt;
&lt;h2 id=&#34;科普&#34;&gt;科普
&lt;/h2&gt;&lt;p&gt;火出天际的&lt;code&gt;AI&lt;/code&gt;确切的说&lt;code&gt;Generative Pre-Training&lt;/code&gt;，直译过来就是生成式的预训练。它是一种基于互联网可用数据训练的文本生成深度学习模型，用于问答、文本摘要生成、机器翻译、分类、代码生成和对话 AI 等任务。目前已经有 GPT-1、GPT-2、GPT-3 和 GPT-4 等不同版本的模型，每个版本都比前一个版本更大、更强大。&lt;/p&gt;
&lt;h2 id=&#34;到底有没有智能&#34;&gt;到底有没有智能
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;相似度足够高，准确率也就越高&lt;/li&gt;
&lt;li&gt;基础的、重复性的工作，进过特定训练，不再需要人工的介入&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;生成式AI是一种利用现有的文本、音频、图像等数据来创建新内容的技术。它可以用于文本生成、语音合成、图像生成、对话系统等多种任务。生成式AI的逻辑性取决于它的训练数据和模型结构。一般来说，生成式AI可以在一定程度上遵循语法、逻辑和常识，但也可能出现错误、偏差或不真实的内容。因此，生成式AI的输出需要人类的判断和验证，不能盲目相信或使用。&lt;/p&gt;
&lt;h2 id=&#34;prompt-工程师&#34;&gt;Prompt 工程师
&lt;/h2&gt;&lt;p&gt;时间河流不会逆流，人需要学会适应潮流。你可以认为&lt;code&gt;AI&lt;/code&gt;不智能，没有逻辑性，它经常写出来不能用的代码。&lt;/p&gt;
&lt;p&gt;如果你换个角度呢，当前的&lt;code&gt;AI&lt;/code&gt;属于一个记忆力很好的小孩子，它拥有过目不忘的能力，有抄作业的能力。我们需要做的是学会如何正确、有效的和&lt;code&gt;AI&lt;/code&gt;沟通，精准的描述需求，帮助&lt;code&gt;AI&lt;/code&gt;生成预期的结果。&lt;/p&gt;
&lt;h2 id=&#34;对话模式&#34;&gt;对话模式
&lt;/h2&gt;&lt;p&gt;两年前&lt;code&gt;Github Copilot&lt;/code&gt;发布，谁也想不到两年后，&lt;code&gt;OpenAI&lt;/code&gt;横空出世，让人类认知到了大语言模型的能力。&lt;/p&gt;
&lt;p&gt;基于&lt;strong&gt;基于注释的编程&lt;/strong&gt;和&lt;strong&gt;基于对话的编程&lt;/strong&gt;，交互逻辑上完全不同，对话的模式，小白用户的交互体验友好，此处必须提到&lt;code&gt;NewBing&lt;/code&gt;在每个问题之后，给出的追问提示，微软在尝试引导用户，获取&lt;code&gt;AI&lt;/code&gt;知识库里面，更多的内容。&lt;/p&gt;
&lt;h2 id=&#34;举个栗子&#34;&gt;举个栗子
&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;场景&lt;/strong&gt;：高铁旅途时间漫漫，编码是个不错的选择。流量有限的情况下，处理几个图片，获取了图片素材，下载软件处理图片稍微有点奢侈，网络条件也不允许。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;需求&lt;/strong&gt;：有一批图片素材，带透明背景图，用来作为小程序的导航栏，选中的状态是彩色的，没选中的状态是黑白的，素材都是彩色的，需要预处理一下。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Prompt&lt;/strong&gt;：编写一段 python 代码，从一个文件夹中读取所有的图片，将图片转换成黑白，维持透明背景，保存图片的时候，优化图片的质量，减少文件体积，同时在控制台展示，优化前后的文件体积大小对比&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# 导入必要的库
import os
import PIL
from PIL import Image

# 定义一个函数，将图片转换成黑白，维持透明背景，保存图片，并返回文件体积
def convert_and_save(image_file):
    # 打开图片
    image = Image.open(image_file)
    # 获取图片的模式，如果是 RGBA 模式，说明有透明背景
    mode = image.mode
    if mode == &amp;quot;RGBA&amp;quot;:
        # 创建一个和图片大小一致的白色背景图片
        background = Image.new(&amp;quot;RGB&amp;quot;, image.size, (255, 255, 255))
        # 将原图片粘贴到白色背景上，忽略透明像素
        background.paste(image, mask=image.split()[3])
        # 将合成的图片转换成灰度模式
        gray_image = background.convert(&amp;quot;L&amp;quot;)
        # 将灰度图片再转换成 RGBA 模式，以便保留透明背景
        final_image = gray_image.convert(&amp;quot;RGBA&amp;quot;)
    else:
        # 如果不是 RGBA 模式，直接将图片转换成灰度模式
        final_image = image.convert(&amp;quot;L&amp;quot;)
    # 获取原图片的文件名和扩展名
    file_name, file_ext = os.path.splitext(image_file)
    # 定义新图片的文件名，添加 _bw 后缀表示黑白
    new_file_name = file_name + &amp;quot;_bw&amp;quot; + file_ext
    # 保存新图片，并优化质量，减少文件体积
    final_image.save(new_file_name, optimize=True)
    # 获取原图片和新图片的文件体积，并返回
    old_size = os.path.getsize(image_file)
    new_size = os.path.getsize(new_file_name)
    return old_size, new_size

# 定义一个文件夹路径，存放所有的图片
folder_path = &amp;quot;images&amp;quot;
# 遍历文件夹中的所有文件
for file in os.listdir(folder_path):
    # 拼接完整的文件路径
    file_path = os.path.join(folder_path, file)
    # 判断是否是图片文件，根据扩展名判断，可以根据需要添加或修改扩展名
    if file_path.endswith((&amp;quot;.png&amp;quot;, &amp;quot;.jpg&amp;quot;, &amp;quot;.jpeg&amp;quot;, &amp;quot;.gif&amp;quot;)):
        # 调用函数，转换并保存图片，并获取文件体积
        old_size, new_size = convert_and_save(file_path)
        # 在控制台展示优化前后的文件体积大小对比
        print(f&amp;quot;{file}: {old_size} bytes -&amp;gt; {new_size} bytes&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;AI&lt;/code&gt;迅速给出了答案，看着很漂亮，用起来也没问题，经验丰富的程序源，会觉得下面的代码不够通用，也没法上生产环境：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;参数固定了&lt;/li&gt;
&lt;li&gt;没有日志文件，信息都打印在控制台&lt;/li&gt;
&lt;li&gt;图片处理的任务，适合多线程模型&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;NewBing&lt;/code&gt;登场，依据提示，选择了一个问题：&lt;strong&gt;这段代码有什么可以改进的地方吗&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;这次给的答案，实话，完全超出了预期，一般程序员能想到的优化措施，&lt;code&gt;AI&lt;/code&gt;都给出对应的建议&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;这段代码可能还有一些可以改进的地方，比如：

可以使用 argparse 库，让用户可以通过命令行参数指定文件夹路径和扩展名，而不是写死在代码里。
可以使用 logging 库，记录程序的运行情况，而不是直接打印到控制台。
可以使用 try-except 语句，捕获可能出现的异常，比如文件打开失败，图片转换失败等，并做相应的处理。
可以使用 multiprocessing 库，利用多核处理器，加速图片的转换和保存过程。
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;很友好的应答，考虑到部署的环境存在差异，允许不支持多进程&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;当然，我很乐意帮你修改一下代码。我尝试添加了 argparse, logging 和 try-except 语句，但没有添加 multiprocessing 库，因为我不确定你的电脑是否支持多核处理。修改后的代码如下：
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# 导入必要的库
import argparse
import logging
import multiprocessing
import os

from PIL import Image


# 定义一个函数，将图片转换成黑白，维持透明背景，保存图片，并返回文件体积
def convert_and_save(image_file):
    # 打开图片
    try:
        image = Image.open(image_file)
    except Exception as e:
        logging.error(f&amp;quot;Failed to open {image_file}: {e}&amp;quot;)
        return None, None
    # 获取图片的模式，如果是 RGBA 模式，说明有透明背景
    mode = image.mode
    if mode == &amp;quot;RGBA&amp;quot;:
        # 创建一个和图片大小一致的白色背景图片
        background = Image.new(&amp;quot;RGB&amp;quot;, image.size, (255, 255, 255))
        # 将原图片粘贴到白色背景上，忽略透明像素
        background.paste(image, mask=image.split()[3])
        # 将合成的图片转换成灰度模式
        gray_image = background.convert(&amp;quot;L&amp;quot;)
        # 将灰度图片再转换成 RGBA 模式，以便保留透明背景
        final_image = gray_image.convert(&amp;quot;RGBA&amp;quot;)
    else:
        # 如果不是 RGBA 模式，直接将图片转换成灰度模式
        final_image = image.convert(&amp;quot;L&amp;quot;)
    # 获取原图片的文件名和扩展名
    file_name, file_ext = os.path.splitext(image_file)
    # 定义新图片的文件名，添加 _bw 后缀表示黑白
    new_file_name = file_name + &amp;quot;_bw&amp;quot; + file_ext
    # 保存新图片，并优化质量，减少文件体积
    try:
        final_image.save(new_file_name, optimize=True)
    except Exception as e:
        logging.error(f&amp;quot;Failed to save {new_file_name}: {e}&amp;quot;)
        return None, None
    # 获取原图片和新图片的文件体积，并返回
    old_size = os.path.getsize(image_file)
    new_size = os.path.getsize(new_file_name)
    return file_name, old_size, new_size

# 定义一个函数，解析命令行参数，并返回文件夹路径和扩展名列表
def parse_args():
    # 创建一个解析器对象
    parser = argparse.ArgumentParser(description=&amp;quot;Convert images to black and white and optimize quality.&amp;quot;)
    # 添加一个必选的位置参数，表示文件夹路径
    parser.add_argument(&amp;quot;folder_path&amp;quot;, help=&amp;quot;The path of the folder that contains the images.&amp;quot;)
    # 添加一个可选的参数，表示扩展名列表，默认为 png, jpg, jpeg 和 gif
    parser.add_argument(&amp;quot;-e&amp;quot;, &amp;quot;--extensions&amp;quot;, nargs=&amp;quot;+&amp;quot;, default=[&amp;quot;.png&amp;quot;, &amp;quot;.jpg&amp;quot;, &amp;quot;.jpeg&amp;quot;, &amp;quot;.gif&amp;quot;], help=&amp;quot;The extensions of the image files.&amp;quot;)
    # 解析命令行参数，并返回结果对象
    args = parser.parse_args()
    return args.folder_path, args.extensions

# 定义一个函数，打印优化前后的文件体积大小对比
def print_result(result):
    # 如果结果不为空，说明转换和保存成功
    if result:
        # 解包结果为文件名和文件体积元组
        if len(result) == 3:
            file, old_size, new_size = result
            # 在控制台展示优化前后的文件体积大小对比
            logging.info(f&amp;quot;{file}: {old_size} bytes -&amp;gt; {new_size} bytes&amp;quot;)
        else:
            logging.info(f&amp;quot;{result}&amp;quot;)

# 配置日志记录器，将日志输出到控制台和文件中，设置日志等级为 INFO
logging.basicConfig(level=logging.INFO,
                    format=&amp;quot;%(asctime)s %(levelname)s %(message)s&amp;quot;,
                    handlers=[logging.StreamHandler(),
                              logging.FileHandler(&amp;quot;log.txt&amp;quot;)])

# 调用函数，获取文件夹路径和扩展名列表
folder_path, extensions = parse_args()

if __name__ == &amp;quot;__main__&amp;quot;:
    # Windows 需要这个函数的原因是 Windows 缺乏 fork() 函数（这不完全正确）。
    # 因此，在 Windows 上，fork() 是通过创建一个新的进程来模拟的，这个新的进程中运行的代码，在 Linux 上是在子进程中运行的。
    # 由于这些代码要在技术上无关的进程中运行，所以它必须在运行之前被传递到那里。
    # 传递的方式是先将它们序列化，然后通过管道从原始进程发送到新的进程。
    # 另外，这个新的进程被通知它必须运行通过管道传递的代码，通过向它传递 --multiprocessing-fork 命令行参数。
    # 如果你看一下 freeze_support() 函数的实现，它的任务是检查它运行在哪个进程中，是否应该运行通过管道传递的代码
    multiprocessing.freeze_support()

    # 创建一个进程池，根据电脑的核心数自动分配进程
    pool = multiprocessing.Pool()
    # 创建一个空列表，用于存放异步任务的结果对象
    results = []
    # 遍历文件夹中的所有文件
    for file in os.listdir(folder_path):
        # 拼接完整的文件路径
        file_path = os.path.join(folder_path, file)
        # 判断是否是图片文件，根据扩展名判断，可以根据需要修改扩展名列表
        if any(file_path.endswith(ext) for ext in extensions):
            # 调用函数，转换并保存图片，并获取文件体积，使用异步方式，不阻塞主进程
            result = pool.apply_async(convert_and_save, args=(file_path,), callback=print_result)
            # 将结果对象添加到列表中
            results.append((file, result))
    # 关闭进程池，不再接受新的任务
    pool.close()
    # 等待所有的任务完成
    pool.join()
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;后记&#34;&gt;后记
&lt;/h2&gt;&lt;p&gt;由于本地开发是&lt;code&gt;windows&lt;/code&gt;系统，&lt;code&gt;AI&lt;/code&gt;第一次给出的答案，并没有加上&lt;code&gt;main&lt;/code&gt;函数，也没有&lt;code&gt;multiprocessing.freeze_support&lt;/code&gt;。通过追问发生的报错，修复了代码。&lt;/p&gt;
&lt;p&gt;就如当年学习搜索引擎的技巧，我们也需要学习一些和&lt;code&gt;AI&lt;/code&gt;沟通的技巧，给出合理且充分的限定条件，高效的获取需要的答案。&lt;/p&gt;
&lt;p&gt;注意：&lt;strong&gt;如果你是个编程新手，基于给出的注释，还有看不懂的地方，可以继续追问相关代码&lt;/strong&gt;&lt;/p&gt;</description>
        </item>
        <item>
        <title>AI辅助编程，生产力的进化</title>
        <link>https://ttf248.life/p/ai-assisted-programming-productivity-evolution/</link>
        <pubDate>Tue, 28 Feb 2023 17:05:17 +0800</pubDate>
        
        <guid>https://ttf248.life/p/ai-assisted-programming-productivity-evolution/</guid>
        <description>&lt;p&gt;&lt;code&gt;GitHub Copilot&lt;/code&gt; 发布也不到两年时间，&lt;code&gt;ChatGPT&lt;/code&gt; 问世了，不是很懂背后的原理，都用了一段时间。两个工具的辅助层面完全不同，但是都做到了大幅提高生产力。&lt;/p&gt;
&lt;p&gt;太复杂的事情，&lt;code&gt;AI&lt;/code&gt;还做不到，毕竟他们没有逻辑，有套路的或者说范式固定的事情，训练的语料足够，&lt;code&gt;AI&lt;/code&gt;的效果能打个九分。&lt;/p&gt;
&lt;h2 id=&#34;github-copilot&#34;&gt;github copilot
&lt;/h2&gt;&lt;p&gt;发布的时候，看着官网的介绍&lt;code&gt;demo&lt;/code&gt;，不是很智能，试用了一下，不是很好用，放弃了。&lt;/p&gt;
&lt;p&gt;2022年前，闲来无事，切换到新版本体验一下，效果已经不错了，就是国内的速度有点慢，大概率是网络的原因。&lt;code&gt;Python&lt;/code&gt;之类的语言，依据注释写出来的代码都还不错，比我自己查资料然后编写的速度快了很多。&lt;/p&gt;
&lt;h2 id=&#34;chatgpt&#34;&gt;ChatGPT
&lt;/h2&gt;&lt;p&gt;&lt;code&gt;Github Copilot&lt;/code&gt;还停留在程序员，也就是开发的角度来设计产品，你需要懂的如何编程，慢慢的通过注释来引导AI辅助编程。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;ChatGPT&lt;/code&gt; 走到了下一个层面，&lt;strong&gt;让外行人也能迅速上手&lt;/strong&gt;。对话聊天交互式的设计，你完全可以提出问题，描述需求，不懂的地方，继续提问。&lt;/p&gt;
&lt;p&gt;简单来个例子，知道分词的概念，但是不知道如何编码，想要计算字符串的相似度：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;Python 分词计算 維天運通 LOGORY 和 合肥維天運通信息科技股份有限公司 两个字符串的相似度；
比较前，统一处理字符串，英文转换为小写；
第二个字符中如果能找到第一个字符的首字符，将第二个字符串截位，截位的长度为第一个字符串的长度
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;你好，这是Bing。😊&lt;/p&gt;
&lt;p&gt;根据网上的信息123，Python 分词计算可以使用 jieba 库来实现。jieba 库支持三种分词模式：精确模式、全模式和搜索引擎模式。你可以根据你的需求选择合适的模式。&lt;/p&gt;
&lt;p&gt;为了比较两个字符串的相似度，你需要先对它们进行分词，然后计算它们的词向量或者编辑距离等指标。你可以使用 numpy 或者 scipy 等库来进行数学运算。&lt;/p&gt;
&lt;p&gt;为了统一处理字符串，英文转换为小写，你可以使用 Python 的内置函数 lower() 来实现。&lt;/p&gt;
&lt;p&gt;为了截取第二个字符串，你可以使用 Python 的切片操作符 [ ] 来实现。&lt;/p&gt;
&lt;p&gt;以下是一个可能的代码示例：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import jieba
import numpy as np
from scipy.spatial.distance import cosine

# 定义两个字符串
str1 = &amp;quot;維天運通 LOGORY&amp;quot;
str2 = &amp;quot;合肥維天運通信息科技股份有限公司&amp;quot;

# 统一处理字符串，英文转换为小写
str1 = str1.lower()
str2 = str2.lower()

# 找到第二个字符串中第一个字符的位置
index = str2.find(str1[0])

# 如果找到了，截取第二个字符串
if index != -1:
    str2 = str2[index:index+len(str1)]

# 使用 jieba 库进行分词
words1 = list(jieba.cut(str1))
words2 = list(jieba.cut(str2))

# 计算两个字符串的词向量（以词频作为权重）
vector1 = np.zeros(len(words1))
vector2 = np.zeros(len(words1))

for i, word in enumerate(words1):
    vector1[i] += 1 # 统计第一个字符串中每个词出现的次数
    if word in words2:
        vector2[i] += 1 # 统计第二个字符串中每个词出现的次数

# 计算两个字符串的相似度（以余弦距离作为指标）
similarity = 1 - cosine(vector1, vector2)

print(&amp;quot;两个字符串的相似度是：&amp;quot;, similarity)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;更进一步，咨询运行的结果，恭喜你，发现问题了，结果是瞎编的，它并没有实际运行代码，但是会给你一个不错的结果&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&#34;栗子&#34;&gt;栗子
&lt;/h3&gt;&lt;p&gt;提问：sql 语句，查出产品代码表中，产品代码在这个区间 80000-89999 的记录&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;你好，这是 Bing。😊

根据搜索结果12，你可以使用 SQL BETWEEN 运算符来选择给定区间内的值。BETWEEN 运算符包含了开始和结束的值。

你需要先指定要查询的表名和列名，然后在 WHERE 子句中使用 BETWEEN 运算符和区间值。

例如，如果你要查询产品代码表中，产品代码在 80000 到 89999 之间的记录，你可以使用以下语句：

SELECT * FROM product_code
WHERE code BETWEEN 80000 AND 89999;
这里，* 表示选择所有列3。如果你只想选择特定的列，比如产品名称和价格，你可以用逗号分隔它们，如下：

SELECT name, price FROM product_code
WHERE code BETWEEN 80000 AND 89999;
希望这能帮助到你。
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;继续提问，否定它：产品代码是字符串，你的代码错了&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;抱歉，我没有注意到产品代码是字符串。如果是这样的话，你需要在区间值两边加上单引号，如下：

SELECT * FROM product_code
WHERE code BETWEEN &#39;80000&#39; AND &#39;89999&#39;;
请原谅我的错误。🙏
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;恭喜你，它会认错的。&lt;/p&gt;</description>
        </item>
        
    </channel>
</rss>
