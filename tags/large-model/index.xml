<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>大模型 on 向叔记事簿</title>
        <link>https://ttf248.life/tags/large-model/</link>
        <description>Recent content in 大模型 on 向叔记事簿</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language><atom:link href="https://ttf248.life/tags/large-model/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>AI 变“笨”了吗？揭秘参数精度与推理成本的博弈</title>
        <link>https://ttf248.life/p/is-ai-getting-dumber-uncovering-the-game-between-parameter-precision-and-inference-costs/</link>
        <pubDate>Thu, 04 Dec 2025 21:58:09 +0800</pubDate>
        
        <guid>https://ttf248.life/p/is-ai-getting-dumber-uncovering-the-game-between-parameter-precision-and-inference-costs/</guid>
        <description>&lt;p&gt;近期混迹在各个编程大模型的交流圈，模型降智是大家吐槽最多的事情。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;本地台式机部署的都是量化以后的模型，可以说是降智后的版本&lt;/li&gt;
&lt;li&gt;vibe coding 这么热门，有没有可能当前大模型输出的内容，代码是最值钱的产物？&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;本轮的提示词有过一次优化，刚好是模型降智，大模型给的方案是优化提示词，更详细的任务规划、更明确的输出要求&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;针对这个问题，给出合适的提示词：现在很多大厂都提供了大模型服务，偶尔会有用户反馈模型降智问题；从专业角度，参数的精度、推理的成本出发，编写文章；科普文，不要太长。&lt;/p&gt;
&lt;p&gt;优化后的版本：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;你是一位资深的 AI 行业技术专栏作家。你的目标是撰写一篇针对普通读者但内容专业的中文科普文章。

请一步一步思考并撰写：
1. 大纲制定：首先，请拟定一个清晰的三段式文章结构大纲（例如：引言、精度分析、成本与架构分析、结论）。
2. 内容聚焦：文章必须深入浅出地解释用户反馈的“模型降智”现象，核心分析点必须围绕 **参数精度（量化）** 和 **推理的运营成本** 展开。
3. 字数与格式：最终文章长度必须控制在 600 字以内。请使用清晰的 `##` 标题 来划分段落，并使用 **粗体** 来强调文中的关键专业术语（如：量化、MoE、FP16）。
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;引言&#34;&gt;引言
&lt;/h2&gt;&lt;p&gt;为了确保逻辑清晰且符合科普要求，我制定了以下结构：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;引言：&lt;/strong&gt; 确认用户感知的真实性，引出核心矛盾——在用户规模激增的背景下，服务商必须在“智力巅峰”与“服务可用性”之间做平衡。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;精度分析（核心点1）：&lt;/strong&gt; 解释&lt;strong&gt;参数&lt;/strong&gt;与&lt;strong&gt;精度&lt;/strong&gt;的关系。通过对比 &lt;strong&gt;FP16&lt;/strong&gt; 与 &lt;strong&gt;量化&lt;/strong&gt;（INT8/INT4），用“图片压缩”的类比来说明为什么精度下降会导致逻辑变差。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;成本与架构（核心点2）：&lt;/strong&gt; 剖析&lt;strong&gt;推理成本&lt;/strong&gt;。介绍 &lt;strong&gt;MoE&lt;/strong&gt; 架构与&lt;strong&gt;稀疏激活&lt;/strong&gt;机制，解释为了省钱和加速，模型是如何“偷懒”的。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;结论：&lt;/strong&gt; 总结这种“降智”是商业化普及的必经之路（不可能三角）。&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;文章正文&#34;&gt;文章正文
&lt;/h2&gt;&lt;p&gt;近期，不少用户反馈大模型似乎没有以前“聪明”了：逻辑变简单、指令遵循变差，甚至出现车轱辘话。这并非全是错觉。从技术视角看，这种“降智”现象往往是大厂在面对海量用户时，为了平衡&lt;strong&gt;推理成本&lt;/strong&gt;与服务速度而做出的技术妥协。&lt;/p&gt;
&lt;h2 id=&#34;精度缩水从-4k-到-720p-的代价&#34;&gt;精度“缩水”：从 4K 到 720P 的代价
&lt;/h2&gt;&lt;p&gt;大模型的“智商”主要存储在数千亿个&lt;strong&gt;参数&lt;/strong&gt;中。在理想状态下，这些参数以高精度的 &lt;strong&gt;FP16&lt;/strong&gt;（16位浮点数）格式运行，能捕捉极细微的语义差别。但这种高精度意味着巨大的&lt;strong&gt;显存&lt;/strong&gt;占用和缓慢的计算速度。&lt;/p&gt;
&lt;p&gt;为了让数亿用户都能流畅使用，服务商普遍采用了&lt;strong&gt;量化&lt;/strong&gt;（Quantization）技术。这是一种将参数精度从 &lt;strong&gt;FP16&lt;/strong&gt; 压缩到 &lt;strong&gt;INT8&lt;/strong&gt; 甚至 &lt;strong&gt;INT4&lt;/strong&gt; 的手段。&lt;/p&gt;
&lt;p&gt;这就好比将一部 4K 高清电影压缩成 720P 流媒体：虽然剧情（大体逻辑）没变，但画面细节（微小的逻辑关联、复杂指令的执行细节）却丢失了。这种“有损压缩”直接导致模型在处理复杂任务时表现力下降，给用户一种“变笨”的体感。&lt;/p&gt;
&lt;h2 id=&#34;成本压力让大脑部分休息&#34;&gt;成本压力：让大脑“部分休息”
&lt;/h2&gt;&lt;p&gt;除了精度，&lt;strong&gt;推理的运营成本&lt;/strong&gt;是另一大核心因素。每次你向 AI 提问，服务器都要进行庞大的矩阵运算，电费和硬件损耗惊人。&lt;/p&gt;
&lt;p&gt;为了降低成本，现代大模型多采用 &lt;strong&gt;MoE&lt;/strong&gt;（Mixture of Experts，混合专家模型）架构。与传统模型每次都激活所有神经元不同，&lt;strong&gt;MoE&lt;/strong&gt; 采用&lt;strong&gt;稀疏激活&lt;/strong&gt;策略：针对你的问题，系统只唤醒模型中一小部分相关的“专家”网络，其余部分保持休眠。&lt;/p&gt;
&lt;p&gt;虽然这极大降低了计算量，但如果&lt;strong&gt;路由算法&lt;/strong&gt;（Router）为了负载均衡或节省算力，将你的复杂数学问题派发给了“文学专家”，或者为了速度牺牲了专家的调用数量，输出质量自然就会出现波动和下滑。&lt;/p&gt;
&lt;h2 id=&#34;结论&#34;&gt;结论
&lt;/h2&gt;&lt;p&gt;因此，所谓的“降智”，本质上是 AI 从实验室走向大规模商业化时的必然阵痛。在&lt;strong&gt;参数精度&lt;/strong&gt;上的克制和&lt;strong&gt;模型架构&lt;/strong&gt;上的精打细算，是厂商为了让每个人都能用得起 AI，而在“绝对聪明”与“成本效率”之间寻找的微妙平衡。&lt;/p&gt;</description>
        </item>
        <item>
        <title>Gemini CLI 安装部署，谷歌追上来了</title>
        <link>https://ttf248.life/p/gemini-cli-installation-and-deployment-google-is-catching-up/</link>
        <pubDate>Sat, 22 Nov 2025 00:13:49 +0800</pubDate>
        
        <guid>https://ttf248.life/p/gemini-cli-installation-and-deployment-google-is-catching-up/</guid>
        <description>&lt;p&gt;小半个月过去了，国内的 MiniMax，GLM4.6 都付费体验了一波，差距还是存在，cc 工具挺好用的，昨晚折腾前端界面的优化，你懂得，笔者基本不懂前端，vibe coding 以后，才开始接触前端技术栈。国内的大模型没搞定，尝试刚发布的 gemini3，五分钟搞定了，切换到站点的归档页面，你就能看到“书架”。&lt;/p&gt;
&lt;p&gt;常年做的后端C++开发，谷歌在这块的影响力太大了，默认谷歌的产品不会太差，大模型前期是落后，不到两年的时间，现在已经追赶上来。&lt;/p&gt;
&lt;p&gt;首页的 AI 搜索，百度不知道猴年马月能搞出来，不是说百度不行，是国内的产品没去思考，搜索里面嵌入 AI，最重要的是速度，谷歌做到了。&lt;/p&gt;
&lt;h2 id=&#34;安装部署&#34;&gt;安装部署
&lt;/h2&gt;&lt;p&gt;谷歌的模型既然不错，还给了免费的额度，什么取消也没通知，等通知了再说，先用着。个人用户登录，每天有 1000 的免费额度，pro 用户 1500 免费额度，说白了，谷歌暂时还没想从编码这块赚钱。&lt;/p&gt;
&lt;p&gt;类似 cc 的安装部署，官方文档挺详细这就不多说：https://geminicli.com/docs/&lt;/p&gt;
&lt;p&gt;重点是针对国内的用户，首先登录的时候，你需要给当前终端挂上代理，它会访问谷歌的服务器，登录以后也需要保持代理。&lt;/p&gt;
&lt;p&gt;如果是远程服务器登录，没有浏览器、没有界面，执行 &lt;code&gt;gemini login&lt;/code&gt;，访问对应的网址，能获取到一个登录 code。&lt;/p&gt;
&lt;p&gt;gemini3 需要加入排队申请，gemini2-pro 和其他模型随便使用。&lt;/p&gt;
&lt;p&gt;使用谷歌账号登录以后，你需要绑定谷歌云的项目ID，免费归免费，走了付费的流程，记得不要走 api 的流程，这块是计费的。个人账号登录，设置环境变量 &lt;code&gt;GOOGLE_CLOUD_PROJECT&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;谷歌云这块，抖音和国内的博主都没提，说到了能免费用，那些视频怎么说，有点洗稿的嫌疑，流程没有完整的走过。&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;使用&#34;&gt;使用
&lt;/h2&gt;&lt;p&gt;功能上和 cc 存在差距，每周迭代中，记得按时更新。基础的用法和 cc 类型，切换没有太多成本。&lt;/p&gt;
&lt;p&gt;差距最大的地方在 cli 插件无法不知道你在 ide 选中了那行，目前仅感知文件级别，不知道是产品设计的问题，还是在开发中。&lt;/p&gt;
&lt;p&gt;剩余额度没有地方能查询，在 &lt;code&gt;google cloud&lt;/code&gt; 后台能查到调用记录，顺带一提绑定了项目，你的项目需要开通 gemini api 权限，别担心扣费，你都不用绑定信用卡，权限开通以后能正常调用模型。&lt;/p&gt;
&lt;p&gt;交互上有点不同，终端的屏幕划分成了两部分，最下面是输入框，上面是交流过程，默认展开的，想要给上面的区域翻页，滚动鼠标是不行的，需要通过键盘的 Pages 按键。&lt;/p&gt;
&lt;p&gt;每次会话结束，默认会弹出来本次任务的一个汇总。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;   ░░░            ░░░░░░░░░  ░░░░░░░░░░ ░░░░░░   ░░░░░░ ░░░░░ ░░░░░░   ░░░░░ ░░░░░
     ░░░         ░░░     ░░░ ░░░        ░░░░░░   ░░░░░░  ░░░  ░░░░░░   ░░░░░  ░░░
       ░░░      ░░░          ░░░        ░░░ ░░░ ░░░ ░░░  ░░░  ░░░ ░░░  ░░░    ░░░
 ███     ░░░    █████████░░██████████ ██████ ░░██████░█████░██████ ░░█████ █████░
   ███ ░░░     ███░    ███░███░░      ██████  ░██████░░███░░██████  ░█████  ███░░
     ███      ███░░░     ░░███░░      ███░███ ███ ███░░███░░███░███  ███░░  ███░░
   ░░░ ███    ███ ░░░█████░██████░░░░░███░░█████  ███░░███░░███░░███ ███░░░ ███░░░
     ███      ███      ███ ███        ███   ███   ███  ███  ███   ██████    ███
   ███         ███     ███ ███        ███         ███  ███  ███    █████    ███
 ███            █████████  ██████████ ███         ███ █████ ███     █████  █████

Tips for getting started:
1. Ask questions, edit files, or run commands.
2. Be specific for the best results.
3. Create GEMINI.md files to customize your interactions with Gemini.
4. /help for more information.
&lt;/code&gt;&lt;/pre&gt;</description>
        </item>
        <item>
        <title>阿里巴巴大模型策略</title>
        <link>https://ttf248.life/p/alibaba-large-model-strategy/</link>
        <pubDate>Tue, 18 Nov 2025 22:07:10 +0800</pubDate>
        
        <guid>https://ttf248.life/p/alibaba-large-model-strategy/</guid>
        <description>&lt;p&gt;阿里巴巴（阿里）发布众多大模型，并非简单的“刷数量”，而是一种精心布局的**“模型即服务”(MaaS)生态策略**。这背后有多重考量，可概述为“对内赋能、对外建生态”：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;阿里巴巴怎么会发布那么多大模型？这是个什么策略？五百字左右，概述一下&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;内部业务驱动-对内赋能&#34;&gt;内部业务驱动 (对内赋能)
&lt;/h2&gt;&lt;p&gt;阿里拥有极其庞大且多元的业务版图，包括电商（淘宝天猫）、金融（蚂蚁）、物流（菜鸟）、云计算（阿里云）、文娱（优酷）等。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;场景定制化：&lt;/strong&gt; 单一的通用大模型无法高效满足所有垂直场景的精细化需求。例如，电商客服模型、广告创意生成模型、金融风控模型和物流路径规划模型所需的能力截然不同。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;效率与成本：&lt;/strong&gt; 针对特定任务（如编码、作图）训练专门的“小模型”，比始终调用一个庞大的“全能模型”更具成本效益和响应速度。&lt;/li&gt;
&lt;li&gt;因此，阿里需要一个“模型矩阵”（Model Matrix），以“通义” (Tongyi) 基础模型为底座，衍生出适应不同业务的专业模型，实现对内业务的AI化再造。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;构建云服务生态-对外建生态&#34;&gt;构建云服务生态 (对外建生态)
&lt;/h2&gt;&lt;p&gt;这是阿里的核心战略。大模型是AI时代的“操作系统”，而算力（云服务）是“水和电”。阿里云的目标是成为AI时代领先的“算力+模型”基础设施提供商。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;打造“模型超市”：&lt;/strong&gt; 阿里云不仅提供自家的“通义”系列模型（如通义千问、通义万相、通义灵码等），还通过“魔搭”(ModelScope)社区汇聚了大量第三方和开源模型。发布众多模型，是为了向客户展示其强大的模型研发能力和丰富的模型库，吸引企业上云。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;“开源+闭源”并行：&lt;/strong&gt; 阿里（如开源“通义千问”Qwen系列）的开源策略，是为了快速构建开发者社区，抢占市场心智，加速技术迭代，并与Meta的Llama等国际开源力量竞争。开发者在开源模型上“练手”后，最终进行商业部署时，很可能会选择阿里云的付费算力和闭源商业模型。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;总结&#34;&gt;总结
&lt;/h2&gt;&lt;p&gt;简而言之，阿里发布众多大模型，其策略是：以“通义”系列为核心自研模型矩阵，深度赋能内部庞杂业务；同时，通过“开源”和“模型即服务”平台，在阿里云上构建一个开放、繁荣的AI生态系统，最终目的是&lt;strong&gt;驱动其核心的云计算业务增长&lt;/strong&gt;。&lt;/p&gt;</description>
        </item>
        <item>
        <title>近期大模型的一些使用经验</title>
        <link>https://ttf248.life/p/recent-usage-experiences-of-large-models/</link>
        <pubDate>Wed, 09 Jul 2025 21:09:01 +0800</pubDate>
        
        <guid>https://ttf248.life/p/recent-usage-experiences-of-large-models/</guid>
        <description>&lt;p&gt;现在用下来并没有哪个大模型特别好，各家都有自己的优势场景。&lt;/p&gt;
&lt;h2 id=&#34;技术文档&#34;&gt;技术文档
&lt;/h2&gt;&lt;p&gt;投喂代码或者咨询IT技术类的问题：ChatGPT 和 Gemini&lt;/p&gt;
&lt;h2 id=&#34;写代码&#34;&gt;写代码
&lt;/h2&gt;&lt;p&gt;整理需求，要求修改代码：Claude&lt;/p&gt;</description>
        </item>
        <item>
        <title>ollama 本地部署 deepseek-R1</title>
        <link>https://ttf248.life/p/ollama-local-deployment-deepseek-r1/</link>
        <pubDate>Fri, 07 Feb 2025 22:41:02 +0800</pubDate>
        
        <guid>https://ttf248.life/p/ollama-local-deployment-deepseek-r1/</guid>
        <description>&lt;p&gt;Ollama 是一个开源的 AI 工具，旨在使用户能够本地运行和部署大型语言模型（LLM）。它的目标是提供一个方便且高效的方式，让开发者可以在本地机器上使用像 GPT 这样的模型，而不需要依赖云端服务。Ollama 支持多种模型，并且专注于优化性能，使得即使是资源有限的设备也能顺畅运行这些模型。&lt;/p&gt;
&lt;p&gt;通过 Ollama，用户可以使用基于文本的 AI 应用程序，并能够与本地部署的模型进行交互，而无需担心数据隐私或是高昂的 API 使用费用。你可以通过命令行界面（CLI）调用不同的模型，进行自然语言处理、问答等任务。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;ollama 适合不同模型尝鲜，windows 版本测试下来，无法充分发挥硬件的性能，可能是因为 windows 版本的原因，linux 版本可能会更好。部署32b参数的模型，内存、显卡负载都有不高的情况下，回复速度很慢。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;硬件概述&#34;&gt;硬件概述
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;操作系统：win11&lt;/li&gt;
&lt;li&gt;CPU：i7-10700K&lt;/li&gt;
&lt;li&gt;内存：40GB&lt;/li&gt;
&lt;li&gt;显卡：RTX 3060 12GB&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;环境准备&#34;&gt;环境准备
&lt;/h2&gt;&lt;p&gt;新增系统环境变量，方便后续使用：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;set OLLAMA_MODELS=E:\ollama&lt;/code&gt;&lt;/strong&gt;&lt;br&gt;
这个变量指定了 Ollama 模型的存放路径。&lt;code&gt;E:\ollama&lt;/code&gt; 是一个文件夹路径，表示所有本地模型文件都存储在该目录下。Ollama 会根据这个路径加载和使用你下载或部署的语言模型。你可以将模型文件存放在其他位置，只需要更改这个路径。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;set OLLAMA_HOST=127.0.0.1:8000&lt;/code&gt;&lt;/strong&gt;&lt;br&gt;
这个环境变量设置了 Ollama 服务的主机和端口。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;127.0.0.1&lt;/code&gt; 是本地地址（localhost），意味着 Ollama 服务会只监听来自本机的请求。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;8000&lt;/code&gt; 是指定的端口号，表示 Ollama 服务将在 8000 端口上等待和处理请求。你可以根据需要更改端口号，但需要确保该端口没有被其他应用占用。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;set OLLAMA_ORIGINS=*&lt;/code&gt;&lt;/strong&gt;&lt;br&gt;
这个环境变量控制允许哪些来源的请求访问 Ollama 服务。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;*&lt;/code&gt; 表示允许任何来源（即所有域名和 IP 地址）都可以访问 Ollama 服务。这通常用于开发和调试环境，在生产环境中，通常会指定更严格的来源控制，限制只有特定的域或 IP 才能访问你的服务，以提高安全性。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;deepseek-r1-模型部署&#34;&gt;deepseek-R1 模型部署
&lt;/h2&gt;&lt;p&gt;ollama 安装属于傻瓜式，此处不在赘述。&lt;/p&gt;
&lt;p&gt;安装后的校验：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;C:\Users\core&amp;gt;ollama -v
ollama version is 0.5.11
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;模型部署，参考官网模型页面，选择对应参数的模型：&lt;code&gt;ollama run deepseek-r1:14b&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;14b 参数能有效的记住会话上下文，更小的参数版本，无法记住会话上下文。32b 参数版本，本机部署很卡顿，没有再深入进行测试。&lt;/p&gt;
&lt;h2 id=&#34;参考资料&#34;&gt;参考资料
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.ollama.com/library/deepseek-r1&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://www.ollama.com/library/deepseek-r1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/SPEvYTmTBxhoEkJqm1yPmw&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://mp.weixin.qq.com/s/SPEvYTmTBxhoEkJqm1yPmw&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://blog.csdn.net/x18990027/article/details/145368094&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://blog.csdn.net/x18990027/article/details/145368094&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
        </item>
        
    </channel>
</rss>
