<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>工具 on 向叔记事簿</title>
        <link>https://ttf248.life/tags/%E5%B7%A5%E5%85%B7/</link>
        <description>Recent content in 工具 on 向叔记事簿</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <lastBuildDate>Sat, 28 Feb 2026 18:47:02 +0800</lastBuildDate><atom:link href="https://ttf248.life/tags/%E5%B7%A5%E5%85%B7/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>wrk 与 JMeter 的深度较量</title>
        <link>https://ttf248.life/p/wrk-vs-jmeter-deep-benchmarking/</link>
        <pubDate>Fri, 19 Dec 2025 01:14:49 +0800</pubDate>
        
        <guid>https://ttf248.life/p/wrk-vs-jmeter-deep-benchmarking/</guid>
        <description>&lt;p&gt;在互联网系统的压力测试中，我们经常会遇到两款风格迥异的工具：一个是极其轻量、追求极致吞吐量的 wrk；另一个是功能大而全、模拟真实业务流的 JMeter。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;提示词：梳理核心思路，撰写科普文章：http 压力测试工具，wrk vs Jmeter 有什么不同，我了解到的，wrk 倾向于 一个线程多个 connect 进行测试，Jmeter 更多是短连接模式，能通过配置调整为长联机&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;核心架构多线程-vs-事件驱动&#34;&gt;核心架构：多线程 vs 事件驱动
&lt;/h2&gt;&lt;p&gt;这是两者性能差距的根本原因。&lt;/p&gt;
&lt;h3 id=&#34;1-jmeter传统的一人一岗制-thread-per-request&#34;&gt;1. JMeter：传统的“一人一岗”制 (Thread-per-Request)
&lt;/h3&gt;&lt;p&gt;JMeter 基于 Java 开发，采用的是经典的&lt;strong&gt;多线程模型&lt;/strong&gt;。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;逻辑：&lt;/strong&gt; 每一个并发用户（Virtual User）都对应 JVM 中的一个物理线程。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;代价：&lt;/strong&gt; 线程是非常昂贵的资源。当并发数达到几千时，上下文切换（Context Switch）和内存消耗会显著拖慢测试机本身，导致“压测机还没把服务器压死，自己先崩了”的现象。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;2-wrk现代的多面手制-event-driven&#34;&gt;2. wrk：现代的“多面手”制 (Event-driven)
&lt;/h3&gt;&lt;p&gt;wrk 采用 C 语言编写，核心逻辑基于 Redis 同款的 &lt;code&gt;ae&lt;/code&gt; 事件循环框架（利用 &lt;code&gt;epoll/kqueue&lt;/code&gt;）。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;逻辑：&lt;/strong&gt; wrk 并不为每个连接创建线程。它只启动极少数的线程（通常等于你的 CPU 核心数），每个线程内部通过&lt;strong&gt;非阻塞 I/O&lt;/strong&gt; 同时管理成千上万个连接。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;优势：&lt;/strong&gt; 这就是你提到的“一个线程多个 connection”。它极大地减少了线程切换开销，单机即可跑出百万级别的 RPS（每秒请求数）。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;连接模型短连接-vs-长连接&#34;&gt;连接模型：短连接 vs 长连接
&lt;/h2&gt;&lt;p&gt;关于你提到的连接模式，这里有更深层的细节：&lt;/p&gt;
&lt;h3 id=&#34;1-jmeter-的重与灵&#34;&gt;1. JMeter 的“重”与“灵”
&lt;/h3&gt;&lt;p&gt;JMeter 默认确实更倾向于模拟真实用户的行为。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;短连接倾向：&lt;/strong&gt; 在默认配置下，JMeter 的某些旧版本或特定配置可能不会积极复用连接，导致大量的 TCP 握手。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;可调校性：&lt;/strong&gt; 你可以通过在 &lt;code&gt;HTTP Request&lt;/code&gt; 中勾选 &lt;strong&gt;&amp;ldquo;KeepAlive&amp;rdquo;&lt;/strong&gt;，或者在 &lt;code&gt;user.properties&lt;/code&gt; 中调整连接池参数来开启长连接。但即便是长连接，受限于线程模型，它也难以维持数十万级别的并发长连接。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;2-wrk-的快与狠&#34;&gt;2. wrk 的“快”与“狠”
&lt;/h3&gt;&lt;p&gt;wrk 设计的初衷就是为了测试 &lt;strong&gt;HTTP Keep-Alive&lt;/strong&gt; 的性能。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;长连接策略：&lt;/strong&gt; wrk 在测试开始时会建立好指定的连接数（&lt;code&gt;-c&lt;/code&gt; 参数），并在整个测试过程中尽可能复用这些连接。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;应用场景：&lt;/strong&gt; 它非常适合测试 Nginx、网关（Gateway）或高并发 API 在极端长连接压力下的吞吐极限。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;深度对比表&#34;&gt;深度对比表
&lt;/h2&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;特性&lt;/th&gt;
&lt;th&gt;wrk&lt;/th&gt;
&lt;th&gt;Apache JMeter&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;开发语言&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;C / Lua (脚本)&lt;/td&gt;
&lt;td&gt;Java (GUI)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;并发模型&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;事件驱动 (epoll/kqueue)&lt;/td&gt;
&lt;td&gt;多线程 (Thread-per-user)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;资源消耗&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;极低，单机吞吐量巨大&lt;/td&gt;
&lt;td&gt;较高，大并发需分布式集群&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;业务复杂度&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;较低，主要针对单个 URL&lt;/td&gt;
&lt;td&gt;极高，支持多步脚本、断言、提取器&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;测试场景&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;静态 API 压测、容量评估&lt;/td&gt;
&lt;td&gt;复杂业务链路、功能回归压测&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;报表能力&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;仅文本摘要&lt;/td&gt;
&lt;td&gt;极丰富，支持各类图表和 HTML 报告&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;总结我该选哪一个&#34;&gt;总结：我该选哪一个？
&lt;/h2&gt;&lt;p&gt;这两款工具并不是替代关系，而是互补关系：&lt;/p&gt;
&lt;h3 id=&#34;选-wrk&#34;&gt;选 wrk
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;想要测试服务器的&lt;strong&gt;极限吞吐量&lt;/strong&gt;（RPS）。&lt;/li&gt;
&lt;li&gt;测试对象是单一的 API 或静态资源。&lt;/li&gt;
&lt;li&gt;希望用最少的测试服务器压出最大的流量。&lt;/li&gt;
&lt;li&gt;熟悉 Lua 脚本来定制化请求。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;选-jmeter&#34;&gt;选 JMeter
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;需要模拟&lt;strong&gt;复杂的业务流程&lt;/strong&gt;（如：登录 -&amp;gt; 搜商品 -&amp;gt; 下单 -&amp;gt; 支付）。&lt;/li&gt;
&lt;li&gt;需要可视化的界面来观察响应时间分布、错误率等详细指标。&lt;/li&gt;
&lt;li&gt;测试需要处理动态参数（如从上一个接口提取 Token 传递给下一个接口）。&lt;/li&gt;
&lt;li&gt;团队更习惯使用图形化工具而非命令行。&lt;/li&gt;
&lt;/ul&gt;</description>
        </item>
        <item>
        <title>终归还是回到国产模型</title>
        <link>https://ttf248.life/p/ultimately-its-returning-to-domestic-models/</link>
        <pubDate>Wed, 03 Dec 2025 22:33:47 +0800</pubDate>
        
        <guid>https://ttf248.life/p/ultimately-its-returning-to-domestic-models/</guid>
        <description>&lt;p&gt;前文提到 Gemini Cli 登录的时候需要配置谷歌云的项目 ID，这里就已经不对劲，如果是个人账号不会有这个限制，能出现这个限制，已经开始进入谷歌的风控系统，认为你不是个人账号。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;麻烦事，用了小半个月，刚适应，现在要回到 cc + 国产模型的怀抱。&lt;/li&gt;
&lt;li&gt;谷歌自研芯片成本优势那么大吗？市面上主流的模式都是 Tokens 积分，谷歌现在还是按次计费。&lt;/li&gt;
&lt;li&gt;GLM4.6 图片识别不太行，响应速度够快，照葫芦画瓢能力不行，模仿已有接口能力较弱；M2 图片识别凑合，指令遵循不够强, 照葫芦画瓢能力强。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;update: 不知道是谷歌自己修复了，还是由于切换港区绑定了信用卡，账号又能正常使用了&lt;/p&gt;
&lt;h2 id=&#34;梳理&#34;&gt;梳理
&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;You are currently configured to use a Google Cloud Project but lack a Gemini Code Assist license. Please contact your administrator to request a license. (#3501)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;白天还能登录使用，晚上回家就不行了，开始以为更新导致的 bug，切换到老版本，还是不行，Github 提了 Issue，下面机器人自动给我牵扯出来一堆类似的问题。&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/google-gemini/gemini-cli/issues/14447&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/google-gemini/gemini-cli/issues/14447&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;看完相关信息，发现不对劲，gemini cli 官网没更新，Github 上的文档提到，如果你登录的时候需要谷歌云ID，那就是被识别了，&lt;strong&gt;你不是个人开发者&lt;/strong&gt;。国内登录都是通过科学上网，登录IP信息都是机房的地址，被识别也很正常，谷歌也没想着让中国内地随便用，毕竟需要用到科学上网的地方就那么多了，大部分地区都能直接访问。&lt;/p&gt;
&lt;p&gt;继续搜索，社区里面类似的情况很多：&lt;a class=&#34;link&#34; href=&#34;https://discuss.google.dev/t/is-gemini-code-assist-incorrectly-identifying-my-personal-account-as-an-enterprise-account/287654/2&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://discuss.google.dev/t/is-gemini-code-assist-incorrectly-identifying-my-personal-account-as-an-enterprise-account/287654/2&lt;/a&gt;，也都是用着用着就不能用了。&lt;/p&gt;
&lt;h2 id=&#34;方案&#34;&gt;方案
&lt;/h2&gt;&lt;p&gt;M2 最近半月更新的内容也不少，通过&lt;strong&gt;MCP&lt;/strong&gt;支持联网搜索、支持图像识别。先用用看，后续不行了，再考虑找个法子给谷歌付费，切换到基础的付费版本，手上的美元信用卡不一定能付费成功，上次尝试付费ChatGPT就失败了。&lt;/p&gt;
&lt;h2 id=&#34;后记&#34;&gt;后记
&lt;/h2&gt;&lt;p&gt;回退代码，将昨天的任务，重新用 M2 重做一遍，问题不少，Gemini 那边一次搞定的东西，这边反复了很多次，提示词需要很精准，不能有歧义。对整体项目的理解不到位，需要尽可能的拆成小任务执行。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;上面的都是小问题，minimaxi 给的额度看着不少，实测下来，项目开发中，消耗的很快&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;谷歌自研芯片成本优势那么大吗？市面上主流的模式都是 Tokens 积分，谷歌现在还是按次计费。想要正常使用，minimaxi 第二档的年费五百能搞定，相比谷歌 pro 版本呢，确实便宜不少，谷歌 20 美金一个月，包年也没优惠，但是没限制新模式的使用，从当前发布节奏，发布了新模型，你都能用上。结合实际开发体验，貌似国内的性价比也不太行。&lt;/p&gt;
&lt;p&gt;怎么安慰自己呢，也许模型更蠢点，人脑介入更多，也不是坏事？&lt;/p&gt;
&lt;p&gt;之前不仅付费买了 M2，还买了 GLM4.6，两家都是通过 MCP 实现图片解析，怎么说呢，M2 的测试下来还行，图片理解没太大问题， GLM4.6 不知道在搞什么，理解的东西完全不对。&lt;/p&gt;</description>
        </item>
        <item>
        <title>Gemini CLI 安装部署，谷歌追上来了</title>
        <link>https://ttf248.life/p/gemini-cli-installation-and-deployment-google-is-catching-up/</link>
        <pubDate>Sat, 22 Nov 2025 00:13:49 +0800</pubDate>
        
        <guid>https://ttf248.life/p/gemini-cli-installation-and-deployment-google-is-catching-up/</guid>
        <description>&lt;p&gt;小半个月过去了，国内的 MiniMax，GLM4.6 都付费体验了一波，差距还是存在，cc 工具挺好用的，昨晚折腾前端界面的优化，你懂得，笔者基本不懂前端，vibe coding 以后，才开始接触前端技术栈。国内的大模型没搞定，尝试刚发布的 gemini3，五分钟搞定了，切换到站点的归档页面，你就能看到“书架”。&lt;/p&gt;
&lt;p&gt;常年做的后端C++开发，谷歌在这块的影响力太大了，默认谷歌的产品不会太差，大模型前期是落后，不到两年的时间，现在已经追赶上来。&lt;/p&gt;
&lt;p&gt;首页的 AI 搜索，百度不知道猴年马月能搞出来，不是说百度不行，是国内的产品没去思考，搜索里面嵌入 AI，最重要的是速度，谷歌做到了。&lt;/p&gt;
&lt;h2 id=&#34;安装部署&#34;&gt;安装部署
&lt;/h2&gt;&lt;p&gt;谷歌的模型既然不错，还给了免费的额度，什么取消也没通知，等通知了再说，先用着。个人用户登录，每天有 1000 的免费额度，pro 用户 1500 免费额度，说白了，谷歌暂时还没想从编码这块赚钱。&lt;/p&gt;
&lt;p&gt;类似 cc 的安装部署，官方文档挺详细这就不多说：https://geminicli.com/docs/&lt;/p&gt;
&lt;p&gt;重点是针对国内的用户，首先登录的时候，你需要给当前终端挂上代理，它会访问谷歌的服务器，登录以后也需要保持代理。&lt;/p&gt;
&lt;p&gt;如果是远程服务器登录，没有浏览器、没有界面，执行 &lt;code&gt;gemini login&lt;/code&gt;，访问对应的网址，能获取到一个登录 code。&lt;/p&gt;
&lt;p&gt;gemini3 需要加入排队申请，gemini2-pro 和其他模型随便使用。&lt;/p&gt;
&lt;p&gt;使用谷歌账号登录以后，你需要绑定谷歌云的项目ID，免费归免费，走了付费的流程，记得不要走 api 的流程，这块是计费的。个人账号登录，设置环境变量 &lt;code&gt;GOOGLE_CLOUD_PROJECT&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;谷歌云这块，抖音和国内的博主都没提，说到了能免费用，那些视频怎么说，有点洗稿的嫌疑，流程没有完整的走过。&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;使用&#34;&gt;使用
&lt;/h2&gt;&lt;p&gt;功能上和 cc 存在差距，每周迭代中，记得按时更新。基础的用法和 cc 类型，切换没有太多成本。&lt;/p&gt;
&lt;p&gt;差距最大的地方在 cli 插件无法不知道你在 ide 选中了那行，目前仅感知文件级别，不知道是产品设计的问题，还是在开发中。&lt;/p&gt;
&lt;p&gt;剩余额度没有地方能查询，在 &lt;code&gt;google cloud&lt;/code&gt; 后台能查到调用记录，顺带一提绑定了项目，你的项目需要开通 gemini api 权限，别担心扣费，你都不用绑定信用卡，权限开通以后能正常调用模型。&lt;/p&gt;
&lt;p&gt;交互上有点不同，终端的屏幕划分成了两部分，最下面是输入框，上面是交流过程，默认展开的，想要给上面的区域翻页，滚动鼠标是不行的，需要通过键盘的 Pages 按键。&lt;/p&gt;
&lt;p&gt;每次会话结束，默认会弹出来本次任务的一个汇总。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;   ░░░            ░░░░░░░░░  ░░░░░░░░░░ ░░░░░░   ░░░░░░ ░░░░░ ░░░░░░   ░░░░░ ░░░░░
     ░░░         ░░░     ░░░ ░░░        ░░░░░░   ░░░░░░  ░░░  ░░░░░░   ░░░░░  ░░░
       ░░░      ░░░          ░░░        ░░░ ░░░ ░░░ ░░░  ░░░  ░░░ ░░░  ░░░    ░░░
 ███     ░░░    █████████░░██████████ ██████ ░░██████░█████░██████ ░░█████ █████░
   ███ ░░░     ███░    ███░███░░      ██████  ░██████░░███░░██████  ░█████  ███░░
     ███      ███░░░     ░░███░░      ███░███ ███ ███░░███░░███░███  ███░░  ███░░
   ░░░ ███    ███ ░░░█████░██████░░░░░███░░█████  ███░░███░░███░░███ ███░░░ ███░░░
     ███      ███      ███ ███        ███   ███   ███  ███  ███   ██████    ███
   ███         ███     ███ ███        ███         ███  ███  ███    █████    ███
 ███            █████████  ██████████ ███         ███ █████ ███     █████  █████

Tips for getting started:
1. Ask questions, edit files, or run commands.
2. Be specific for the best results.
3. Create GEMINI.md files to customize your interactions with Gemini.
4. /help for more information.
&lt;/code&gt;&lt;/pre&gt;</description>
        </item>
        <item>
        <title>claude code 常用操作指南</title>
        <link>https://ttf248.life/p/claude-code-frequently-asked-operations-guide/</link>
        <pubDate>Sat, 08 Nov 2025 17:09:24 +0800</pubDate>
        
        <guid>https://ttf248.life/p/claude-code-frequently-asked-operations-guide/</guid>
        <description>&lt;p&gt;cc 提供的命令那么多，好用的都有哪些，抖音找个了视频学习一波，简单的记录下我认为有用的。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;生成规范化 git 日志，默认会带上版权信息，但是国内已经不用他家的模型了，对接的都是国内大模型，git 日志里面的版权也就没必要加上了（github 会展示 cc 为合作开发者）&lt;/p&gt;
&lt;p&gt;找到用户配置文件，添加配置项 &lt;code&gt;&amp;quot;includeCoAuthoredBy&amp;quot;: false&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;/init 分析分析当前文件夹生成一份概要，方便大模型更好的理解项目&lt;/p&gt;
&lt;p&gt;&lt;code&gt;/compact /clear&lt;/code&gt; 日常命令，不解释&lt;/p&gt;
&lt;p&gt;&lt;code&gt;claude --dangerously-skip-permissions&lt;/code&gt; 慎用，上篇稿子的 null 文件就是这样弄出来的，cc windows 上有很多小问题&lt;/p&gt;
&lt;p&gt;&lt;code&gt;#&lt;/code&gt; 进入记忆模式，项目级别用的比较少，我常用在用户级别，常用项目要求写到里面&lt;/p&gt;
&lt;p&gt;&lt;code&gt;/ide&lt;/code&gt; 能感知到 vscode 当前选中的文本数据，trae 里面，经常交互的时候，我是提供很多相关文件的函数片段，切换到命令行，一直没找到类似的功能，主要是很多时候项目的文件很多，制定参考的函数，能有效提供生成代码的准确率&lt;/p&gt;
&lt;p&gt;mcp ？一直没怎么尝试此类型的工具，后续有需要再来尝试，老觉得用起来有点麻烦&lt;/p&gt;
&lt;p&gt;自定义命令 没需求，用户级别的能替代之前写的 git 规范化递交&lt;/p&gt;
&lt;p&gt;&lt;code&gt;-r&lt;/code&gt; 关闭项目后重启找到之前的对话，对于复杂的项目，用处不大，国内的模型，上下文 200k，很容易就满了&lt;/p&gt;</description>
        </item>
        <item>
        <title>近期大模型的一些使用经验</title>
        <link>https://ttf248.life/p/recent-usage-experiences-of-large-models/</link>
        <pubDate>Wed, 09 Jul 2025 21:09:01 +0800</pubDate>
        
        <guid>https://ttf248.life/p/recent-usage-experiences-of-large-models/</guid>
        <description>&lt;p&gt;现在用下来并没有哪个大模型特别好，各家都有自己的优势场景。&lt;/p&gt;
&lt;h2 id=&#34;技术文档&#34;&gt;技术文档
&lt;/h2&gt;&lt;p&gt;投喂代码或者咨询IT技术类的问题：ChatGPT 和 Gemini&lt;/p&gt;
&lt;h2 id=&#34;写代码&#34;&gt;写代码
&lt;/h2&gt;&lt;p&gt;整理需求，要求修改代码：Claude&lt;/p&gt;</description>
        </item>
        
    </channel>
</rss>
